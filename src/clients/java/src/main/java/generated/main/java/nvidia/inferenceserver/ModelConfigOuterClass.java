// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: model_config.proto

package nvidia.inferenceserver;

public final class ModelConfigOuterClass {
  private ModelConfigOuterClass() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:enum:: DataType
   *&#64;&#64;
   *&#64;&#64;   Data types supported for input and output tensors.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf enum {@code nvidia.inferenceserver.DataType}
   */
  public enum DataType
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INVALID = 0
     * </pre>
     *
     * <code>TYPE_INVALID = 0;</code>
     */
    TYPE_INVALID(0),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::BOOL = 1
     * </pre>
     *
     * <code>TYPE_BOOL = 1;</code>
     */
    TYPE_BOOL(1),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT8 = 2
     * </pre>
     *
     * <code>TYPE_UINT8 = 2;</code>
     */
    TYPE_UINT8(2),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT16 = 3
     * </pre>
     *
     * <code>TYPE_UINT16 = 3;</code>
     */
    TYPE_UINT16(3),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT32 = 4
     * </pre>
     *
     * <code>TYPE_UINT32 = 4;</code>
     */
    TYPE_UINT32(4),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT64 = 5
     * </pre>
     *
     * <code>TYPE_UINT64 = 5;</code>
     */
    TYPE_UINT64(5),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT8 = 6
     * </pre>
     *
     * <code>TYPE_INT8 = 6;</code>
     */
    TYPE_INT8(6),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT16 = 7
     * </pre>
     *
     * <code>TYPE_INT16 = 7;</code>
     */
    TYPE_INT16(7),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT32 = 8
     * </pre>
     *
     * <code>TYPE_INT32 = 8;</code>
     */
    TYPE_INT32(8),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT64 = 9
     * </pre>
     *
     * <code>TYPE_INT64 = 9;</code>
     */
    TYPE_INT64(9),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP16 = 10
     * </pre>
     *
     * <code>TYPE_FP16 = 10;</code>
     */
    TYPE_FP16(10),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP32 = 11
     * </pre>
     *
     * <code>TYPE_FP32 = 11;</code>
     */
    TYPE_FP32(11),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP64 = 12
     * </pre>
     *
     * <code>TYPE_FP64 = 12;</code>
     */
    TYPE_FP64(12),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::STRING = 13
     * </pre>
     *
     * <code>TYPE_STRING = 13;</code>
     */
    TYPE_STRING(13),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INVALID = 0
     * </pre>
     *
     * <code>TYPE_INVALID = 0;</code>
     */
    public static final int TYPE_INVALID_VALUE = 0;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::BOOL = 1
     * </pre>
     *
     * <code>TYPE_BOOL = 1;</code>
     */
    public static final int TYPE_BOOL_VALUE = 1;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT8 = 2
     * </pre>
     *
     * <code>TYPE_UINT8 = 2;</code>
     */
    public static final int TYPE_UINT8_VALUE = 2;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT16 = 3
     * </pre>
     *
     * <code>TYPE_UINT16 = 3;</code>
     */
    public static final int TYPE_UINT16_VALUE = 3;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT32 = 4
     * </pre>
     *
     * <code>TYPE_UINT32 = 4;</code>
     */
    public static final int TYPE_UINT32_VALUE = 4;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT64 = 5
     * </pre>
     *
     * <code>TYPE_UINT64 = 5;</code>
     */
    public static final int TYPE_UINT64_VALUE = 5;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT8 = 6
     * </pre>
     *
     * <code>TYPE_INT8 = 6;</code>
     */
    public static final int TYPE_INT8_VALUE = 6;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT16 = 7
     * </pre>
     *
     * <code>TYPE_INT16 = 7;</code>
     */
    public static final int TYPE_INT16_VALUE = 7;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT32 = 8
     * </pre>
     *
     * <code>TYPE_INT32 = 8;</code>
     */
    public static final int TYPE_INT32_VALUE = 8;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT64 = 9
     * </pre>
     *
     * <code>TYPE_INT64 = 9;</code>
     */
    public static final int TYPE_INT64_VALUE = 9;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP16 = 10
     * </pre>
     *
     * <code>TYPE_FP16 = 10;</code>
     */
    public static final int TYPE_FP16_VALUE = 10;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP32 = 11
     * </pre>
     *
     * <code>TYPE_FP32 = 11;</code>
     */
    public static final int TYPE_FP32_VALUE = 11;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP64 = 12
     * </pre>
     *
     * <code>TYPE_FP64 = 12;</code>
     */
    public static final int TYPE_FP64_VALUE = 12;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::STRING = 13
     * </pre>
     *
     * <code>TYPE_STRING = 13;</code>
     */
    public static final int TYPE_STRING_VALUE = 13;


    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static DataType valueOf(int value) {
      return forNumber(value);
    }

    public static DataType forNumber(int value) {
      switch (value) {
        case 0: return TYPE_INVALID;
        case 1: return TYPE_BOOL;
        case 2: return TYPE_UINT8;
        case 3: return TYPE_UINT16;
        case 4: return TYPE_UINT32;
        case 5: return TYPE_UINT64;
        case 6: return TYPE_INT8;
        case 7: return TYPE_INT16;
        case 8: return TYPE_INT32;
        case 9: return TYPE_INT64;
        case 10: return TYPE_FP16;
        case 11: return TYPE_FP32;
        case 12: return TYPE_FP64;
        case 13: return TYPE_STRING;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<DataType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        DataType> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<DataType>() {
            public DataType findValueByNumber(int number) {
              return DataType.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.getDescriptor().getEnumTypes().get(0);
    }

    private static final DataType[] VALUES = values();

    public static DataType valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      if (desc.getIndex() == -1) {
        return UNRECOGNIZED;
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private DataType(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.DataType)
  }

  public interface ModelInstanceGroupOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelInstanceGroup)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
     */
    int getKindValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     */
    int getCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    java.util.List<java.lang.Integer> getGpusList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    int getGpusCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    int getGpus(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     */
    java.util.List<java.lang.String>
        getProfileList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     */
    int getProfileCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     */
    java.lang.String getProfile(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     */
    com.google.protobuf.ByteString
        getProfileBytes(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelInstanceGroup
   *&#64;&#64;
   *&#64;&#64;   A group of one or more instances of a model and resources made
   *&#64;&#64;   available for those instances.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelInstanceGroup}
   */
  public  static final class ModelInstanceGroup extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelInstanceGroup)
      ModelInstanceGroupOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelInstanceGroup.newBuilder() to construct.
    private ModelInstanceGroup(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelInstanceGroup() {
      name_ = "";
      kind_ = 0;
      count_ = 0;
      gpus_ = java.util.Collections.emptyList();
      profile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelInstanceGroup(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {

              count_ = input.readInt32();
              break;
            }
            case 24: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                gpus_ = new java.util.ArrayList<java.lang.Integer>();
                mutable_bitField0_ |= 0x00000008;
              }
              gpus_.add(input.readInt32());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008) && input.getBytesUntilLimit() > 0) {
                gpus_ = new java.util.ArrayList<java.lang.Integer>();
                mutable_bitField0_ |= 0x00000008;
              }
              while (input.getBytesUntilLimit() > 0) {
                gpus_.add(input.readInt32());
              }
              input.popLimit(limit);
              break;
            }
            case 32: {
              int rawValue = input.readEnum();

              kind_ = rawValue;
              break;
            }
            case 42: {
              java.lang.String s = input.readStringRequireUtf8();
              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
                profile_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000010;
              }
              profile_.add(s);
              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          gpus_ = java.util.Collections.unmodifiableList(gpus_);
        }
        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
          profile_ = profile_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Kind
     *&#64;&#64;
     *&#64;&#64;     Kind of this instance group.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code nvidia.inferenceserver.ModelInstanceGroup.Kind}
     */
    public enum Kind
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_AUTO = 0
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that can run on either
       *&#64;&#64;       CPU or GPU. If all GPUs listed in 'gpus' are available then
       *&#64;&#64;       instances will be created on GPU(s), otherwise instances will
       *&#64;&#64;       be created on CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_AUTO = 0;</code>
       */
      KIND_AUTO(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_GPU = 1
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_GPU = 1;</code>
       */
      KIND_GPU(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_CPU = 2
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_CPU = 2;</code>
       */
      KIND_CPU(2),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_MODEL = 3
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that should run on the
       *&#64;&#64;       CPU and/or GPU(s) as specified by the model or backend itself.
       *&#64;&#64;       The inference server will not override the model/backend
       *&#64;&#64;       settings.
       *&#64;&#64;       Currently, this option is supported only for Tensorflow models.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_MODEL = 3;</code>
       */
      KIND_MODEL(3),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_AUTO = 0
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that can run on either
       *&#64;&#64;       CPU or GPU. If all GPUs listed in 'gpus' are available then
       *&#64;&#64;       instances will be created on GPU(s), otherwise instances will
       *&#64;&#64;       be created on CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_AUTO = 0;</code>
       */
      public static final int KIND_AUTO_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_GPU = 1
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_GPU = 1;</code>
       */
      public static final int KIND_GPU_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_CPU = 2
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_CPU = 2;</code>
       */
      public static final int KIND_CPU_VALUE = 2;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_MODEL = 3
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that should run on the
       *&#64;&#64;       CPU and/or GPU(s) as specified by the model or backend itself.
       *&#64;&#64;       The inference server will not override the model/backend
       *&#64;&#64;       settings.
       *&#64;&#64;       Currently, this option is supported only for Tensorflow models.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_MODEL = 3;</code>
       */
      public static final int KIND_MODEL_VALUE = 3;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Kind valueOf(int value) {
        return forNumber(value);
      }

      public static Kind forNumber(int value) {
        switch (value) {
          case 0: return KIND_AUTO;
          case 1: return KIND_GPU;
          case 2: return KIND_CPU;
          case 3: return KIND_MODEL;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Kind> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
              public Kind findValueByNumber(int number) {
                return Kind.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDescriptor().getEnumTypes().get(0);
      }

      private static final Kind[] VALUES = values();

      public static Kind valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Kind(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ModelInstanceGroup.Kind)
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int KIND_FIELD_NUMBER = 4;
    private int kind_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
     */
    public int getKindValue() {
      return kind_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind result = nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.valueOf(kind_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.UNRECOGNIZED : result;
    }

    public static final int COUNT_FIELD_NUMBER = 2;
    private int count_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     */
    public int getCount() {
      return count_;
    }

    public static final int GPUS_FIELD_NUMBER = 3;
    private java.util.List<java.lang.Integer> gpus_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    public java.util.List<java.lang.Integer>
        getGpusList() {
      return gpus_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    public int getGpusCount() {
      return gpus_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     */
    public int getGpus(int index) {
      return gpus_.get(index);
    }
    private int gpusMemoizedSerializedSize = -1;

    public static final int PROFILE_FIELD_NUMBER = 5;
    private com.google.protobuf.LazyStringList profile_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     */
    public com.google.protobuf.ProtocolStringList
        getProfileList() {
      return profile_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     */
    public int getProfileCount() {
      return profile_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     */
    public java.lang.String getProfile(int index) {
      return profile_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     */
    public com.google.protobuf.ByteString
        getProfileBytes(int index) {
      return profile_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (count_ != 0) {
        output.writeInt32(2, count_);
      }
      if (getGpusList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(gpusMemoizedSerializedSize);
      }
      for (int i = 0; i < gpus_.size(); i++) {
        output.writeInt32NoTag(gpus_.get(i));
      }
      if (kind_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.KIND_AUTO.getNumber()) {
        output.writeEnum(4, kind_);
      }
      for (int i = 0; i < profile_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, profile_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (count_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, count_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < gpus_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt32SizeNoTag(gpus_.get(i));
        }
        size += dataSize;
        if (!getGpusList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        gpusMemoizedSerializedSize = dataSize;
      }
      if (kind_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.KIND_AUTO.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(4, kind_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < profile_.size(); i++) {
          dataSize += computeStringSizeNoTag(profile_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getProfileList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup) obj;

      boolean result = true;
      result = result && getName()
          .equals(other.getName());
      result = result && kind_ == other.kind_;
      result = result && (getCount()
          == other.getCount());
      result = result && getGpusList()
          .equals(other.getGpusList());
      result = result && getProfileList()
          .equals(other.getProfileList());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + KIND_FIELD_NUMBER;
      hash = (53 * hash) + kind_;
      hash = (37 * hash) + COUNT_FIELD_NUMBER;
      hash = (53 * hash) + getCount();
      if (getGpusCount() > 0) {
        hash = (37 * hash) + GPUS_FIELD_NUMBER;
        hash = (53 * hash) + getGpusList().hashCode();
      }
      if (getProfileCount() > 0) {
        hash = (37 * hash) + PROFILE_FIELD_NUMBER;
        hash = (53 * hash) + getProfileList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelInstanceGroup
     *&#64;&#64;
     *&#64;&#64;   A group of one or more instances of a model and resources made
     *&#64;&#64;   available for those instances.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelInstanceGroup}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelInstanceGroup)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        kind_ = 0;

        count_ = 0;

        gpus_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000008);
        profile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.name_ = name_;
        result.kind_ = kind_;
        result.count_ = count_;
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          gpus_ = java.util.Collections.unmodifiableList(gpus_);
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.gpus_ = gpus_;
        if (((bitField0_ & 0x00000010) == 0x00000010)) {
          profile_ = profile_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000010);
        }
        result.profile_ = profile_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.kind_ != 0) {
          setKindValue(other.getKindValue());
        }
        if (other.getCount() != 0) {
          setCount(other.getCount());
        }
        if (!other.gpus_.isEmpty()) {
          if (gpus_.isEmpty()) {
            gpus_ = other.gpus_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureGpusIsMutable();
            gpus_.addAll(other.gpus_);
          }
          onChanged();
        }
        if (!other.profile_.isEmpty()) {
          if (profile_.isEmpty()) {
            profile_ = other.profile_;
            bitField0_ = (bitField0_ & ~0x00000010);
          } else {
            ensureProfileIsMutable();
            profile_.addAll(other.profile_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int kind_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public int getKindValue() {
        return kind_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public Builder setKindValue(int value) {
        kind_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind result = nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.valueOf(kind_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public Builder setKind(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Kind value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        kind_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInstanceGroup.Kind kind = 4;</code>
       */
      public Builder clearKind() {
        
        kind_ = 0;
        onChanged();
        return this;
      }

      private int count_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       */
      public int getCount() {
        return count_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       */
      public Builder setCount(int value) {
        
        count_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       */
      public Builder clearCount() {
        
        count_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<java.lang.Integer> gpus_ = java.util.Collections.emptyList();
      private void ensureGpusIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          gpus_ = new java.util.ArrayList<java.lang.Integer>(gpus_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public java.util.List<java.lang.Integer>
          getGpusList() {
        return java.util.Collections.unmodifiableList(gpus_);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public int getGpusCount() {
        return gpus_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public int getGpus(int index) {
        return gpus_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public Builder setGpus(
          int index, int value) {
        ensureGpusIsMutable();
        gpus_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public Builder addGpus(int value) {
        ensureGpusIsMutable();
        gpus_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public Builder addAllGpus(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensureGpusIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, gpus_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       */
      public Builder clearGpus() {
        gpus_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList profile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureProfileIsMutable() {
        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
          profile_ = new com.google.protobuf.LazyStringArrayList(profile_);
          bitField0_ |= 0x00000010;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       */
      public com.google.protobuf.ProtocolStringList
          getProfileList() {
        return profile_.getUnmodifiableView();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       */
      public int getProfileCount() {
        return profile_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       */
      public java.lang.String getProfile(int index) {
        return profile_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       */
      public com.google.protobuf.ByteString
          getProfileBytes(int index) {
        return profile_.getByteString(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       */
      public Builder setProfile(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureProfileIsMutable();
        profile_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       */
      public Builder addProfile(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureProfileIsMutable();
        profile_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       */
      public Builder addAllProfile(
          java.lang.Iterable<java.lang.String> values) {
        ensureProfileIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, profile_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       */
      public Builder clearProfile() {
        profile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000010);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models, using inputs with dynamic shape, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       */
      public Builder addProfileBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        ensureProfileIsMutable();
        profile_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelInstanceGroup)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInstanceGroup)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelInstanceGroup>
        PARSER = new com.google.protobuf.AbstractParser<ModelInstanceGroup>() {
      @java.lang.Override
      public ModelInstanceGroup parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelInstanceGroup(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelInstanceGroup> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelInstanceGroup> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelTensorReshapeOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelTensorReshape)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    java.util.List<java.lang.Long> getShapeList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    int getShapeCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    long getShape(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelTensorReshape
   *&#64;&#64;
   *&#64;&#64;   Reshape specification for input and output tensors.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelTensorReshape}
   */
  public  static final class ModelTensorReshape extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelTensorReshape)
      ModelTensorReshapeOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelTensorReshape.newBuilder() to construct.
    private ModelTensorReshape(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelTensorReshape() {
      shape_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelTensorReshape(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                shape_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000001;
              }
              shape_.add(input.readInt64());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001) && input.getBytesUntilLimit() > 0) {
                shape_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                shape_.add(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          shape_ = java.util.Collections.unmodifiableList(shape_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder.class);
    }

    public static final int SHAPE_FIELD_NUMBER = 1;
    private java.util.List<java.lang.Long> shape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    public java.util.List<java.lang.Long>
        getShapeList() {
      return shape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    public int getShapeCount() {
      return shape_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     */
    public long getShape(int index) {
      return shape_.get(index);
    }
    private int shapeMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getShapeList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(shapeMemoizedSerializedSize);
      }
      for (int i = 0; i < shape_.size(); i++) {
        output.writeInt64NoTag(shape_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < shape_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(shape_.get(i));
        }
        size += dataSize;
        if (!getShapeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        shapeMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape) obj;

      boolean result = true;
      result = result && getShapeList()
          .equals(other.getShapeList());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getShapeCount() > 0) {
        hash = (37 * hash) + SHAPE_FIELD_NUMBER;
        hash = (53 * hash) + getShapeList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelTensorReshape
     *&#64;&#64;
     *&#64;&#64;   Reshape specification for input and output tensors.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelTensorReshape}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelTensorReshape)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        shape_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          shape_ = java.util.Collections.unmodifiableList(shape_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.shape_ = shape_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance()) return this;
        if (!other.shape_.isEmpty()) {
          if (shape_.isEmpty()) {
            shape_ = other.shape_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureShapeIsMutable();
            shape_.addAll(other.shape_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<java.lang.Long> shape_ = java.util.Collections.emptyList();
      private void ensureShapeIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          shape_ = new java.util.ArrayList<java.lang.Long>(shape_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public java.util.List<java.lang.Long>
          getShapeList() {
        return java.util.Collections.unmodifiableList(shape_);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public int getShapeCount() {
        return shape_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public long getShape(int index) {
        return shape_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public Builder setShape(
          int index, long value) {
        ensureShapeIsMutable();
        shape_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public Builder addShape(long value) {
        ensureShapeIsMutable();
        shape_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public Builder addAllShape(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureShapeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, shape_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       */
      public Builder clearShape() {
        shape_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelTensorReshape)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelTensorReshape)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelTensorReshape>
        PARSER = new com.google.protobuf.AbstractParser<ModelTensorReshape>() {
      @java.lang.Override
      public ModelTensorReshape parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelTensorReshape(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelTensorReshape> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelTensorReshape> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelInputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelInput)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
     */
    int getFormatValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format getFormat();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    java.util.List<java.lang.Long> getDimsList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    int getDimsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    long getDims(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    boolean hasReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelInput
   *&#64;&#64;
   *&#64;&#64;   An input required by the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelInput}
   */
  public  static final class ModelInput extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelInput)
      ModelInputOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelInput.newBuilder() to construct.
    private ModelInput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelInput() {
      name_ = "";
      dataType_ = 0;
      format_ = 0;
      dims_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelInput(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              dataType_ = rawValue;
              break;
            }
            case 24: {
              int rawValue = input.readEnum();

              format_ = rawValue;
              break;
            }
            case 32: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                dims_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000008;
              }
              dims_.add(input.readInt64());
              break;
            }
            case 34: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008) && input.getBytesUntilLimit() > 0) {
                dims_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000008;
              }
              while (input.getBytesUntilLimit() > 0) {
                dims_.add(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 42: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder subBuilder = null;
              if (reshape_ != null) {
                subBuilder = reshape_.toBuilder();
              }
              reshape_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reshape_);
                reshape_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          dims_ = java.util.Collections.unmodifiableList(dims_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Format
     *&#64;&#64;
     *&#64;&#64;     The format for the input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code nvidia.inferenceserver.ModelInput.Format}
     */
    public enum Format
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       The input has no specific format. This is the default.
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NONE = 0;</code>
       */
      FORMAT_NONE(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
       *&#64;&#64;
       *&#64;&#64;       HWC image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NHWC = 1;</code>
       */
      FORMAT_NHWC(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
       *&#64;&#64;
       *&#64;&#64;       CHW image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NCHW = 2;</code>
       */
      FORMAT_NCHW(2),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       The input has no specific format. This is the default.
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NONE = 0;</code>
       */
      public static final int FORMAT_NONE_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
       *&#64;&#64;
       *&#64;&#64;       HWC image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NHWC = 1;</code>
       */
      public static final int FORMAT_NHWC_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
       *&#64;&#64;
       *&#64;&#64;       CHW image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NCHW = 2;</code>
       */
      public static final int FORMAT_NCHW_VALUE = 2;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Format valueOf(int value) {
        return forNumber(value);
      }

      public static Format forNumber(int value) {
        switch (value) {
          case 0: return FORMAT_NONE;
          case 1: return FORMAT_NHWC;
          case 2: return FORMAT_NCHW;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Format>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Format> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Format>() {
              public Format findValueByNumber(int number) {
                return Format.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDescriptor().getEnumTypes().get(0);
      }

      private static final Format[] VALUES = values();

      public static Format valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Format(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ModelInput.Format)
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 2;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }

    public static final int FORMAT_FIELD_NUMBER = 3;
    private int format_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
     */
    public int getFormatValue() {
      return format_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format getFormat() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format result = nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.valueOf(format_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.UNRECOGNIZED : result;
    }

    public static final int DIMS_FIELD_NUMBER = 4;
    private java.util.List<java.lang.Long> dims_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    public java.util.List<java.lang.Long>
        getDimsList() {
      return dims_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    public int getDimsCount() {
      return dims_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     */
    public long getDims(int index) {
      return dims_.get(index);
    }
    private int dimsMemoizedSerializedSize = -1;

    public static final int RESHAPE_FIELD_NUMBER = 5;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape reshape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public boolean hasReshape() {
      return reshape_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape() {
      return reshape_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
      return getReshape();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        output.writeEnum(2, dataType_);
      }
      if (format_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.FORMAT_NONE.getNumber()) {
        output.writeEnum(3, format_);
      }
      if (getDimsList().size() > 0) {
        output.writeUInt32NoTag(34);
        output.writeUInt32NoTag(dimsMemoizedSerializedSize);
      }
      for (int i = 0; i < dims_.size(); i++) {
        output.writeInt64NoTag(dims_.get(i));
      }
      if (reshape_ != null) {
        output.writeMessage(5, getReshape());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, dataType_);
      }
      if (format_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.FORMAT_NONE.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, format_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < dims_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(dims_.get(i));
        }
        size += dataSize;
        if (!getDimsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        dimsMemoizedSerializedSize = dataSize;
      }
      if (reshape_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getReshape());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelInput)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelInput other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelInput) obj;

      boolean result = true;
      result = result && getName()
          .equals(other.getName());
      result = result && dataType_ == other.dataType_;
      result = result && format_ == other.format_;
      result = result && getDimsList()
          .equals(other.getDimsList());
      result = result && (hasReshape() == other.hasReshape());
      if (hasReshape()) {
        result = result && getReshape()
            .equals(other.getReshape());
      }
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + dataType_;
      hash = (37 * hash) + FORMAT_FIELD_NUMBER;
      hash = (53 * hash) + format_;
      if (getDimsCount() > 0) {
        hash = (37 * hash) + DIMS_FIELD_NUMBER;
        hash = (53 * hash) + getDimsList().hashCode();
      }
      if (hasReshape()) {
        hash = (37 * hash) + RESHAPE_FIELD_NUMBER;
        hash = (53 * hash) + getReshape().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelInput
     *&#64;&#64;
     *&#64;&#64;   An input required by the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelInput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelInput)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        dataType_ = 0;

        format_ = 0;

        dims_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000008);
        if (reshapeBuilder_ == null) {
          reshape_ = null;
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelInput_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInput result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInput result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelInput(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.name_ = name_;
        result.dataType_ = dataType_;
        result.format_ = format_;
        if (((bitField0_ & 0x00000008) == 0x00000008)) {
          dims_ = java.util.Collections.unmodifiableList(dims_);
          bitField0_ = (bitField0_ & ~0x00000008);
        }
        result.dims_ = dims_;
        if (reshapeBuilder_ == null) {
          result.reshape_ = reshape_;
        } else {
          result.reshape_ = reshapeBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelInput) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelInput)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.dataType_ != 0) {
          setDataTypeValue(other.getDataTypeValue());
        }
        if (other.format_ != 0) {
          setFormatValue(other.getFormatValue());
        }
        if (!other.dims_.isEmpty()) {
          if (dims_.isEmpty()) {
            dims_ = other.dims_;
            bitField0_ = (bitField0_ & ~0x00000008);
          } else {
            ensureDimsIsMutable();
            dims_.addAll(other.dims_);
          }
          onChanged();
        }
        if (other.hasReshape()) {
          mergeReshape(other.getReshape());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInput parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelInput) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int dataType_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder setDataTypeValue(int value) {
        dataType_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder setDataType(nvidia.inferenceserver.ModelConfigOuterClass.DataType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        dataType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder clearDataType() {
        
        dataType_ = 0;
        onChanged();
        return this;
      }

      private int format_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public int getFormatValue() {
        return format_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public Builder setFormatValue(int value) {
        format_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format getFormat() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format result = nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.valueOf(format_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public Builder setFormat(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Format value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        format_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelInput.Format format = 3;</code>
       */
      public Builder clearFormat() {
        
        format_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<java.lang.Long> dims_ = java.util.Collections.emptyList();
      private void ensureDimsIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          dims_ = new java.util.ArrayList<java.lang.Long>(dims_);
          bitField0_ |= 0x00000008;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public java.util.List<java.lang.Long>
          getDimsList() {
        return java.util.Collections.unmodifiableList(dims_);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public long getDims(int index) {
        return dims_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public Builder setDims(
          int index, long value) {
        ensureDimsIsMutable();
        dims_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public Builder addDims(long value) {
        ensureDimsIsMutable();
        dims_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public Builder addAllDims(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureDimsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, dims_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       */
      public Builder clearDims() {
        dims_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000008);
        onChanged();
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape reshape_ = null;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> reshapeBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public boolean hasReshape() {
        return reshapeBuilder_ != null || reshape_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape() {
        if (reshapeBuilder_ == null) {
          return reshape_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        } else {
          return reshapeBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reshape_ = value;
          onChanged();
        } else {
          reshapeBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder builderForValue) {
        if (reshapeBuilder_ == null) {
          reshape_ = builderForValue.build();
          onChanged();
        } else {
          reshapeBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder mergeReshape(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (reshape_ != null) {
            reshape_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.newBuilder(reshape_).mergeFrom(value).buildPartial();
          } else {
            reshape_ = value;
          }
          onChanged();
        } else {
          reshapeBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder clearReshape() {
        if (reshapeBuilder_ == null) {
          reshape_ = null;
          onChanged();
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder getReshapeBuilder() {
        
        onChanged();
        return getReshapeFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
        if (reshapeBuilder_ != null) {
          return reshapeBuilder_.getMessageOrBuilder();
        } else {
          return reshape_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> 
          getReshapeFieldBuilder() {
        if (reshapeBuilder_ == null) {
          reshapeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder>(
                  getReshape(),
                  getParentForChildren(),
                  isClean());
          reshape_ = null;
        }
        return reshapeBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelInput)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInput)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelInput DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelInput();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelInput>
        PARSER = new com.google.protobuf.AbstractParser<ModelInput>() {
      @java.lang.Override
      public ModelInput parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelInput(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelInput> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelInput> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelOutputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOutput)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    java.util.List<java.lang.Long> getDimsList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    int getDimsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    long getDims(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    boolean hasReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     */
    java.lang.String getLabelFilename();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     */
    com.google.protobuf.ByteString
        getLabelFilenameBytes();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelOutput
   *&#64;&#64;
   *&#64;&#64;   An output produced by the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelOutput}
   */
  public  static final class ModelOutput extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOutput)
      ModelOutputOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelOutput.newBuilder() to construct.
    private ModelOutput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelOutput() {
      name_ = "";
      dataType_ = 0;
      dims_ = java.util.Collections.emptyList();
      labelFilename_ = "";
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelOutput(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              dataType_ = rawValue;
              break;
            }
            case 24: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                dims_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000004;
              }
              dims_.add(input.readInt64());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004) && input.getBytesUntilLimit() > 0) {
                dims_ = new java.util.ArrayList<java.lang.Long>();
                mutable_bitField0_ |= 0x00000004;
              }
              while (input.getBytesUntilLimit() > 0) {
                dims_.add(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();

              labelFilename_ = s;
              break;
            }
            case 42: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder subBuilder = null;
              if (reshape_ != null) {
                subBuilder = reshape_.toBuilder();
              }
              reshape_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reshape_);
                reshape_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
          dims_ = java.util.Collections.unmodifiableList(dims_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder.class);
    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 2;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }

    public static final int DIMS_FIELD_NUMBER = 3;
    private java.util.List<java.lang.Long> dims_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    public java.util.List<java.lang.Long>
        getDimsList() {
      return dims_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    public int getDimsCount() {
      return dims_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     */
    public long getDims(int index) {
      return dims_.get(index);
    }
    private int dimsMemoizedSerializedSize = -1;

    public static final int RESHAPE_FIELD_NUMBER = 5;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape reshape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public boolean hasReshape() {
      return reshape_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape() {
      return reshape_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
      return getReshape();
    }

    public static final int LABEL_FILENAME_FIELD_NUMBER = 4;
    private volatile java.lang.Object labelFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     */
    public java.lang.String getLabelFilename() {
      java.lang.Object ref = labelFilename_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        labelFilename_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     */
    public com.google.protobuf.ByteString
        getLabelFilenameBytes() {
      java.lang.Object ref = labelFilename_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        labelFilename_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        output.writeEnum(2, dataType_);
      }
      if (getDimsList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(dimsMemoizedSerializedSize);
      }
      for (int i = 0; i < dims_.size(); i++) {
        output.writeInt64NoTag(dims_.get(i));
      }
      if (!getLabelFilenameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, labelFilename_);
      }
      if (reshape_ != null) {
        output.writeMessage(5, getReshape());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, dataType_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < dims_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(dims_.get(i));
        }
        size += dataSize;
        if (!getDimsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        dimsMemoizedSerializedSize = dataSize;
      }
      if (!getLabelFilenameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, labelFilename_);
      }
      if (reshape_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getReshape());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput) obj;

      boolean result = true;
      result = result && getName()
          .equals(other.getName());
      result = result && dataType_ == other.dataType_;
      result = result && getDimsList()
          .equals(other.getDimsList());
      result = result && (hasReshape() == other.hasReshape());
      if (hasReshape()) {
        result = result && getReshape()
            .equals(other.getReshape());
      }
      result = result && getLabelFilename()
          .equals(other.getLabelFilename());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + dataType_;
      if (getDimsCount() > 0) {
        hash = (37 * hash) + DIMS_FIELD_NUMBER;
        hash = (53 * hash) + getDimsList().hashCode();
      }
      if (hasReshape()) {
        hash = (37 * hash) + RESHAPE_FIELD_NUMBER;
        hash = (53 * hash) + getReshape().hashCode();
      }
      hash = (37 * hash) + LABEL_FILENAME_FIELD_NUMBER;
      hash = (53 * hash) + getLabelFilename().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelOutput
     *&#64;&#64;
     *&#64;&#64;   An output produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelOutput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOutput)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        dataType_ = 0;

        dims_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000004);
        if (reshapeBuilder_ == null) {
          reshape_ = null;
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }
        labelFilename_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOutput_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.name_ = name_;
        result.dataType_ = dataType_;
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          dims_ = java.util.Collections.unmodifiableList(dims_);
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.dims_ = dims_;
        if (reshapeBuilder_ == null) {
          result.reshape_ = reshape_;
        } else {
          result.reshape_ = reshapeBuilder_.build();
        }
        result.labelFilename_ = labelFilename_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.dataType_ != 0) {
          setDataTypeValue(other.getDataTypeValue());
        }
        if (!other.dims_.isEmpty()) {
          if (dims_.isEmpty()) {
            dims_ = other.dims_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureDimsIsMutable();
            dims_.addAll(other.dims_);
          }
          onChanged();
        }
        if (other.hasReshape()) {
          mergeReshape(other.getReshape());
        }
        if (!other.getLabelFilename().isEmpty()) {
          labelFilename_ = other.labelFilename_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int dataType_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder setDataTypeValue(int value) {
        dataType_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder setDataType(nvidia.inferenceserver.ModelConfigOuterClass.DataType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        dataType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 2;</code>
       */
      public Builder clearDataType() {
        
        dataType_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<java.lang.Long> dims_ = java.util.Collections.emptyList();
      private void ensureDimsIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          dims_ = new java.util.ArrayList<java.lang.Long>(dims_);
          bitField0_ |= 0x00000004;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public java.util.List<java.lang.Long>
          getDimsList() {
        return java.util.Collections.unmodifiableList(dims_);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public long getDims(int index) {
        return dims_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public Builder setDims(
          int index, long value) {
        ensureDimsIsMutable();
        dims_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public Builder addDims(long value) {
        ensureDimsIsMutable();
        dims_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public Builder addAllDims(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureDimsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, dims_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       */
      public Builder clearDims() {
        dims_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape reshape_ = null;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> reshapeBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public boolean hasReshape() {
        return reshapeBuilder_ != null || reshape_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape getReshape() {
        if (reshapeBuilder_ == null) {
          return reshape_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        } else {
          return reshapeBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reshape_ = value;
          onChanged();
        } else {
          reshapeBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder builderForValue) {
        if (reshapeBuilder_ == null) {
          reshape_ = builderForValue.build();
          onChanged();
        } else {
          reshapeBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder mergeReshape(nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (reshape_ != null) {
            reshape_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.newBuilder(reshape_).mergeFrom(value).buildPartial();
          } else {
            reshape_ = value;
          }
          onChanged();
        } else {
          reshapeBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public Builder clearReshape() {
        if (reshapeBuilder_ == null) {
          reshape_ = null;
          onChanged();
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder getReshapeBuilder() {
        
        onChanged();
        return getReshapeFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
        if (reshapeBuilder_ != null) {
          return reshapeBuilder_.getMessageOrBuilder();
        } else {
          return reshape_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelTensorReshape reshape = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> 
          getReshapeFieldBuilder() {
        if (reshapeBuilder_ == null) {
          reshapeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshape.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelTensorReshapeOrBuilder>(
                  getReshape(),
                  getParentForChildren(),
                  isClean());
          reshape_ = null;
        }
        return reshapeBuilder_;
      }

      private java.lang.Object labelFilename_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public java.lang.String getLabelFilename() {
        java.lang.Object ref = labelFilename_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          labelFilename_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public com.google.protobuf.ByteString
          getLabelFilenameBytes() {
        java.lang.Object ref = labelFilename_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          labelFilename_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public Builder setLabelFilename(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        labelFilename_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public Builder clearLabelFilename() {
        
        labelFilename_ = getDefaultInstance().getLabelFilename();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       */
      public Builder setLabelFilenameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        labelFilename_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOutput)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOutput)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelOutput>
        PARSER = new com.google.protobuf.AbstractParser<ModelOutput>() {
      @java.lang.Override
      public ModelOutput parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelOutput(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelOutput> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelOutput> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelVersionPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelVersionPolicy)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    boolean hasLatest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder getLatestOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    boolean hasAll();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getAll();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder getAllOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    boolean hasSpecific();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder getSpecificOrBuilder();

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.PolicyChoiceCase getPolicyChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelVersionPolicy
   *&#64;&#64;
   *&#64;&#64;   Policy indicating which versions of a model should be made
   *&#64;&#64;   available by the inference server.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy}
   */
  public  static final class ModelVersionPolicy extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelVersionPolicy)
      ModelVersionPolicyOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelVersionPolicy.newBuilder() to construct.
    private ModelVersionPolicy(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelVersionPolicy() {
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelVersionPolicy(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder subBuilder = null;
              if (policyChoiceCase_ == 1) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_).toBuilder();
              }
              policyChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_);
                policyChoice_ = subBuilder.buildPartial();
              }
              policyChoiceCase_ = 1;
              break;
            }
            case 18: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder subBuilder = null;
              if (policyChoiceCase_ == 2) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_).toBuilder();
              }
              policyChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_);
                policyChoice_ = subBuilder.buildPartial();
              }
              policyChoiceCase_ = 2;
              break;
            }
            case 26: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder subBuilder = null;
              if (policyChoiceCase_ == 3) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_).toBuilder();
              }
              policyChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_);
                policyChoice_ = subBuilder.buildPartial();
              }
              policyChoiceCase_ = 3;
              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder.class);
    }

    public interface LatestOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelVersionPolicy.Latest)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       */
      int getNumVersions();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Latest
     *&#64;&#64;
     *&#64;&#64;     Serve only the latest version(s) of a model. This is
     *&#64;&#64;     the default policy.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.Latest}
     */
    public  static final class Latest extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelVersionPolicy.Latest)
        LatestOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Latest.newBuilder() to construct.
      private Latest(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Latest() {
        numVersions_ = 0;
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Latest(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                numVersions_ = input.readUInt32();
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder.class);
      }

      public static final int NUM_VERSIONS_FIELD_NUMBER = 1;
      private int numVersions_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       */
      public int getNumVersions() {
        return numVersions_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (numVersions_ != 0) {
          output.writeUInt32(1, numVersions_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (numVersions_ != 0) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(1, numVersions_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) obj;

        boolean result = true;
        result = result && (getNumVersions()
            == other.getNumVersions());
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NUM_VERSIONS_FIELD_NUMBER;
        hash = (53 * hash) + getNumVersions();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Latest
       *&#64;&#64;
       *&#64;&#64;     Serve only the latest version(s) of a model. This is
       *&#64;&#64;     the default policy.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.Latest}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelVersionPolicy.Latest)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          numVersions_ = 0;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest(this);
          result.numVersions_ = numVersions_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance()) return this;
          if (other.getNumVersions() != 0) {
            setNumVersions(other.getNumVersions());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private int numVersions_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         */
        public int getNumVersions() {
          return numVersions_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         */
        public Builder setNumVersions(int value) {
          
          numVersions_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         */
        public Builder clearNumVersions() {
          
          numVersions_ = 0;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelVersionPolicy.Latest)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Latest)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Latest>
          PARSER = new com.google.protobuf.AbstractParser<Latest>() {
        @java.lang.Override
        public Latest parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Latest(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Latest> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Latest> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface AllOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelVersionPolicy.All)
        com.google.protobuf.MessageOrBuilder {
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message All
     *&#64;&#64;
     *&#64;&#64;     Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.All}
     */
    public  static final class All extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelVersionPolicy.All)
        AllOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use All.newBuilder() to construct.
      private All(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private All() {
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private All(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder.class);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) obj;

        boolean result = true;
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message All
       *&#64;&#64;
       *&#64;&#64;     Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.All}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelVersionPolicy.All)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All(this);
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance()) return this;
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelVersionPolicy.All)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.All)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<All>
          PARSER = new com.google.protobuf.AbstractParser<All>() {
        @java.lang.Override
        public All parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new All(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<All> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<All> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface SpecificOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelVersionPolicy.Specific)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      java.util.List<java.lang.Long> getVersionsList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      int getVersionsCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      long getVersions(int index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Specific
     *&#64;&#64;
     *&#64;&#64;     Serve only specific versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.Specific}
     */
    public  static final class Specific extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelVersionPolicy.Specific)
        SpecificOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Specific.newBuilder() to construct.
      private Specific(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Specific() {
        versions_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Specific(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                  versions_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000001;
                }
                versions_.add(input.readInt64());
                break;
              }
              case 10: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) == 0x00000001) && input.getBytesUntilLimit() > 0) {
                  versions_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  versions_.add(input.readInt64());
                }
                input.popLimit(limit);
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
            versions_ = java.util.Collections.unmodifiableList(versions_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder.class);
      }

      public static final int VERSIONS_FIELD_NUMBER = 1;
      private java.util.List<java.lang.Long> versions_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      public java.util.List<java.lang.Long>
          getVersionsList() {
        return versions_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      public int getVersionsCount() {
        return versions_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       */
      public long getVersions(int index) {
        return versions_.get(index);
      }
      private int versionsMemoizedSerializedSize = -1;

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (getVersionsList().size() > 0) {
          output.writeUInt32NoTag(10);
          output.writeUInt32NoTag(versionsMemoizedSerializedSize);
        }
        for (int i = 0; i < versions_.size(); i++) {
          output.writeInt64NoTag(versions_.get(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        {
          int dataSize = 0;
          for (int i = 0; i < versions_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt64SizeNoTag(versions_.get(i));
          }
          size += dataSize;
          if (!getVersionsList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          versionsMemoizedSerializedSize = dataSize;
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) obj;

        boolean result = true;
        result = result && getVersionsList()
            .equals(other.getVersionsList());
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        if (getVersionsCount() > 0) {
          hash = (37 * hash) + VERSIONS_FIELD_NUMBER;
          hash = (53 * hash) + getVersionsList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Specific
       *&#64;&#64;
       *&#64;&#64;     Serve only specific versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy.Specific}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelVersionPolicy.Specific)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          versions_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific(this);
          int from_bitField0_ = bitField0_;
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            versions_ = java.util.Collections.unmodifiableList(versions_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.versions_ = versions_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance()) return this;
          if (!other.versions_.isEmpty()) {
            if (versions_.isEmpty()) {
              versions_ = other.versions_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureVersionsIsMutable();
              versions_.addAll(other.versions_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.util.List<java.lang.Long> versions_ = java.util.Collections.emptyList();
        private void ensureVersionsIsMutable() {
          if (!((bitField0_ & 0x00000001) == 0x00000001)) {
            versions_ = new java.util.ArrayList<java.lang.Long>(versions_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public java.util.List<java.lang.Long>
            getVersionsList() {
          return java.util.Collections.unmodifiableList(versions_);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public int getVersionsCount() {
          return versions_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public long getVersions(int index) {
          return versions_.get(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public Builder setVersions(
            int index, long value) {
          ensureVersionsIsMutable();
          versions_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public Builder addVersions(long value) {
          ensureVersionsIsMutable();
          versions_.add(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public Builder addAllVersions(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureVersionsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, versions_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         */
        public Builder clearVersions() {
          versions_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelVersionPolicy.Specific)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Specific)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Specific>
          PARSER = new com.google.protobuf.AbstractParser<Specific>() {
        @java.lang.Override
        public Specific parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Specific(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Specific> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Specific> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    private int policyChoiceCase_ = 0;
    private java.lang.Object policyChoice_;
    public enum PolicyChoiceCase
        implements com.google.protobuf.Internal.EnumLite {
      LATEST(1),
      ALL(2),
      SPECIFIC(3),
      POLICYCHOICE_NOT_SET(0);
      private final int value;
      private PolicyChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static PolicyChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static PolicyChoiceCase forNumber(int value) {
        switch (value) {
          case 1: return LATEST;
          case 2: return ALL;
          case 3: return SPECIFIC;
          case 0: return POLICYCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public PolicyChoiceCase
    getPolicyChoiceCase() {
      return PolicyChoiceCase.forNumber(
          policyChoiceCase_);
    }

    public static final int LATEST_FIELD_NUMBER = 1;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    public boolean hasLatest() {
      return policyChoiceCase_ == 1;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest() {
      if (policyChoiceCase_ == 1) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder getLatestOrBuilder() {
      if (policyChoiceCase_ == 1) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
    }

    public static final int ALL_FIELD_NUMBER = 2;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    public boolean hasAll() {
      return policyChoiceCase_ == 2;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getAll() {
      if (policyChoiceCase_ == 2) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder getAllOrBuilder() {
      if (policyChoiceCase_ == 2) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
    }

    public static final int SPECIFIC_FIELD_NUMBER = 3;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    public boolean hasSpecific() {
      return policyChoiceCase_ == 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific() {
      if (policyChoiceCase_ == 3) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder getSpecificOrBuilder() {
      if (policyChoiceCase_ == 3) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (policyChoiceCase_ == 1) {
        output.writeMessage(1, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_);
      }
      if (policyChoiceCase_ == 2) {
        output.writeMessage(2, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_);
      }
      if (policyChoiceCase_ == 3) {
        output.writeMessage(3, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (policyChoiceCase_ == 1) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_);
      }
      if (policyChoiceCase_ == 2) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_);
      }
      if (policyChoiceCase_ == 3) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy) obj;

      boolean result = true;
      result = result && getPolicyChoiceCase().equals(
          other.getPolicyChoiceCase());
      if (!result) return false;
      switch (policyChoiceCase_) {
        case 1:
          result = result && getLatest()
              .equals(other.getLatest());
          break;
        case 2:
          result = result && getAll()
              .equals(other.getAll());
          break;
        case 3:
          result = result && getSpecific()
              .equals(other.getSpecific());
          break;
        case 0:
        default:
      }
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      switch (policyChoiceCase_) {
        case 1:
          hash = (37 * hash) + LATEST_FIELD_NUMBER;
          hash = (53 * hash) + getLatest().hashCode();
          break;
        case 2:
          hash = (37 * hash) + ALL_FIELD_NUMBER;
          hash = (53 * hash) + getAll().hashCode();
          break;
        case 3:
          hash = (37 * hash) + SPECIFIC_FIELD_NUMBER;
          hash = (53 * hash) + getSpecific().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelVersionPolicy
     *&#64;&#64;
     *&#64;&#64;   Policy indicating which versions of a model should be made
     *&#64;&#64;   available by the inference server.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelVersionPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelVersionPolicy)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        policyChoiceCase_ = 0;
        policyChoice_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy(this);
        if (policyChoiceCase_ == 1) {
          if (latestBuilder_ == null) {
            result.policyChoice_ = policyChoice_;
          } else {
            result.policyChoice_ = latestBuilder_.build();
          }
        }
        if (policyChoiceCase_ == 2) {
          if (allBuilder_ == null) {
            result.policyChoice_ = policyChoice_;
          } else {
            result.policyChoice_ = allBuilder_.build();
          }
        }
        if (policyChoiceCase_ == 3) {
          if (specificBuilder_ == null) {
            result.policyChoice_ = policyChoice_;
          } else {
            result.policyChoice_ = specificBuilder_.build();
          }
        }
        result.policyChoiceCase_ = policyChoiceCase_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance()) return this;
        switch (other.getPolicyChoiceCase()) {
          case LATEST: {
            mergeLatest(other.getLatest());
            break;
          }
          case ALL: {
            mergeAll(other.getAll());
            break;
          }
          case SPECIFIC: {
            mergeSpecific(other.getSpecific());
            break;
          }
          case POLICYCHOICE_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int policyChoiceCase_ = 0;
      private java.lang.Object policyChoice_;
      public PolicyChoiceCase
          getPolicyChoiceCase() {
        return PolicyChoiceCase.forNumber(
            policyChoiceCase_);
      }

      public Builder clearPolicyChoice() {
        policyChoiceCase_ = 0;
        policyChoice_ = null;
        onChanged();
        return this;
      }


      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder> latestBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public boolean hasLatest() {
        return policyChoiceCase_ == 1;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest() {
        if (latestBuilder_ == null) {
          if (policyChoiceCase_ == 1) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        } else {
          if (policyChoiceCase_ == 1) {
            return latestBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder setLatest(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
        if (latestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          policyChoice_ = value;
          onChanged();
        } else {
          latestBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 1;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder setLatest(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder builderForValue) {
        if (latestBuilder_ == null) {
          policyChoice_ = builderForValue.build();
          onChanged();
        } else {
          latestBuilder_.setMessage(builderForValue.build());
        }
        policyChoiceCase_ = 1;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder mergeLatest(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
        if (latestBuilder_ == null) {
          if (policyChoiceCase_ == 1 &&
              policyChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance()) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            policyChoice_ = value;
          }
          onChanged();
        } else {
          if (policyChoiceCase_ == 1) {
            latestBuilder_.mergeFrom(value);
          }
          latestBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 1;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder clearLatest() {
        if (latestBuilder_ == null) {
          if (policyChoiceCase_ == 1) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
            onChanged();
          }
        } else {
          if (policyChoiceCase_ == 1) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
          }
          latestBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder getLatestBuilder() {
        return getLatestFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder getLatestOrBuilder() {
        if ((policyChoiceCase_ == 1) && (latestBuilder_ != null)) {
          return latestBuilder_.getMessageOrBuilder();
        } else {
          if (policyChoiceCase_ == 1) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Latest latest = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder> 
          getLatestFieldBuilder() {
        if (latestBuilder_ == null) {
          if (!(policyChoiceCase_ == 1)) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
          }
          latestBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_,
                  getParentForChildren(),
                  isClean());
          policyChoice_ = null;
        }
        policyChoiceCase_ = 1;
        onChanged();;
        return latestBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder> allBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public boolean hasAll() {
        return policyChoiceCase_ == 2;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All getAll() {
        if (allBuilder_ == null) {
          if (policyChoiceCase_ == 2) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        } else {
          if (policyChoiceCase_ == 2) {
            return allBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder setAll(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All value) {
        if (allBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          policyChoice_ = value;
          onChanged();
        } else {
          allBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 2;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder setAll(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder builderForValue) {
        if (allBuilder_ == null) {
          policyChoice_ = builderForValue.build();
          onChanged();
        } else {
          allBuilder_.setMessage(builderForValue.build());
        }
        policyChoiceCase_ = 2;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder mergeAll(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All value) {
        if (allBuilder_ == null) {
          if (policyChoiceCase_ == 2 &&
              policyChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance()) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            policyChoice_ = value;
          }
          onChanged();
        } else {
          if (policyChoiceCase_ == 2) {
            allBuilder_.mergeFrom(value);
          }
          allBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 2;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder clearAll() {
        if (allBuilder_ == null) {
          if (policyChoiceCase_ == 2) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
            onChanged();
          }
        } else {
          if (policyChoiceCase_ == 2) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
          }
          allBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder getAllBuilder() {
        return getAllFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder getAllOrBuilder() {
        if ((policyChoiceCase_ == 2) && (allBuilder_ != null)) {
          return allBuilder_.getMessageOrBuilder();
        } else {
          if (policyChoiceCase_ == 2) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.All all = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder> 
          getAllFieldBuilder() {
        if (allBuilder_ == null) {
          if (!(policyChoiceCase_ == 2)) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
          }
          allBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_,
                  getParentForChildren(),
                  isClean());
          policyChoice_ = null;
        }
        policyChoiceCase_ = 2;
        onChanged();;
        return allBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder> specificBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public boolean hasSpecific() {
        return policyChoiceCase_ == 3;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific() {
        if (specificBuilder_ == null) {
          if (policyChoiceCase_ == 3) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        } else {
          if (policyChoiceCase_ == 3) {
            return specificBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder setSpecific(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
        if (specificBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          policyChoice_ = value;
          onChanged();
        } else {
          specificBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder setSpecific(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder builderForValue) {
        if (specificBuilder_ == null) {
          policyChoice_ = builderForValue.build();
          onChanged();
        } else {
          specificBuilder_.setMessage(builderForValue.build());
        }
        policyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder mergeSpecific(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
        if (specificBuilder_ == null) {
          if (policyChoiceCase_ == 3 &&
              policyChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance()) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            policyChoice_ = value;
          }
          onChanged();
        } else {
          if (policyChoiceCase_ == 3) {
            specificBuilder_.mergeFrom(value);
          }
          specificBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder clearSpecific() {
        if (specificBuilder_ == null) {
          if (policyChoiceCase_ == 3) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
            onChanged();
          }
        } else {
          if (policyChoiceCase_ == 3) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
          }
          specificBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder getSpecificBuilder() {
        return getSpecificFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder getSpecificOrBuilder() {
        if ((policyChoiceCase_ == 3) && (specificBuilder_ != null)) {
          return specificBuilder_.getMessageOrBuilder();
        } else {
          if (policyChoiceCase_ == 3) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy.Specific specific = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder> 
          getSpecificFieldBuilder() {
        if (specificBuilder_ == null) {
          if (!(policyChoiceCase_ == 3)) {
            policyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
          }
          specificBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_,
                  getParentForChildren(),
                  isClean());
          policyChoice_ = null;
        }
        policyChoiceCase_ = 3;
        onChanged();;
        return specificBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelVersionPolicy)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelVersionPolicy>
        PARSER = new com.google.protobuf.AbstractParser<ModelVersionPolicy>() {
      @java.lang.Override
      public ModelVersionPolicy parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelVersionPolicy(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelVersionPolicy> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelVersionPolicy> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelOptimizationPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOptimizationPolicy)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    boolean hasGraph();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder getGraphOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    int getPriorityValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    boolean hasCuda();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder getCudaOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    boolean hasExecutionAccelerators();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getExecutionAccelerators();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder getExecutionAcceleratorsOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelOptimizationPolicy
   *&#64;&#64;
   *&#64;&#64;   Optimization settings for a model. These settings control if/how a
   *&#64;&#64;   model is optimized and prioritized by the backend framework when
   *&#64;&#64;   it is loaded.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy}
   */
  public  static final class ModelOptimizationPolicy extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOptimizationPolicy)
      ModelOptimizationPolicyOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelOptimizationPolicy.newBuilder() to construct.
    private ModelOptimizationPolicy(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelOptimizationPolicy() {
      priority_ = 0;
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelOptimizationPolicy(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder subBuilder = null;
              if (graph_ != null) {
                subBuilder = graph_.toBuilder();
              }
              graph_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(graph_);
                graph_ = subBuilder.buildPartial();
              }

              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              priority_ = rawValue;
              break;
            }
            case 26: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder subBuilder = null;
              if (cuda_ != null) {
                subBuilder = cuda_.toBuilder();
              }
              cuda_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(cuda_);
                cuda_ = subBuilder.buildPartial();
              }

              break;
            }
            case 34: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder subBuilder = null;
              if (executionAccelerators_ != null) {
                subBuilder = executionAccelerators_.toBuilder();
              }
              executionAccelerators_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(executionAccelerators_);
                executionAccelerators_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: ModelPriority
     *&#64;&#64;
     *&#64;&#64;     Model priorities. A model will be given scheduling and execution
     *&#64;&#64;     preference over models at lower priorities. Current model
     *&#64;&#64;     priorities only work for TensorRT models.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority}
     */
    public enum ModelPriority
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
       *&#64;&#64;
       *&#64;&#64;       The default model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_DEFAULT = 0;</code>
       */
      PRIORITY_DEFAULT(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
       *&#64;&#64;
       *&#64;&#64;       The maximum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MAX = 1;</code>
       */
      PRIORITY_MAX(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
       *&#64;&#64;
       *&#64;&#64;       The minimum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MIN = 2;</code>
       */
      PRIORITY_MIN(2),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
       *&#64;&#64;
       *&#64;&#64;       The default model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_DEFAULT = 0;</code>
       */
      public static final int PRIORITY_DEFAULT_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
       *&#64;&#64;
       *&#64;&#64;       The maximum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MAX = 1;</code>
       */
      public static final int PRIORITY_MAX_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
       *&#64;&#64;
       *&#64;&#64;       The minimum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MIN = 2;</code>
       */
      public static final int PRIORITY_MIN_VALUE = 2;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static ModelPriority valueOf(int value) {
        return forNumber(value);
      }

      public static ModelPriority forNumber(int value) {
        switch (value) {
          case 0: return PRIORITY_DEFAULT;
          case 1: return PRIORITY_MAX;
          case 2: return PRIORITY_MIN;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<ModelPriority>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          ModelPriority> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<ModelPriority>() {
              public ModelPriority findValueByNumber(int number) {
                return ModelPriority.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDescriptor().getEnumTypes().get(0);
      }

      private static final ModelPriority[] VALUES = values();

      public static ModelPriority valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private ModelPriority(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority)
    }

    public interface GraphOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       */
      int getLevel();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Graph
     *&#64;&#64;
     *&#64;&#64;     Enable generic graph optimization of the model. If not specified
     *&#64;&#64;     the framework's default level of optimization is used. Currently
     *&#64;&#64;     only supported for TensorFlow graphdef and savedmodel models and
     *&#64;&#64;     causes XLA to be enabled/disabled for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.Graph}
     */
    public  static final class Graph extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
        GraphOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Graph.newBuilder() to construct.
      private Graph(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Graph() {
        level_ = 0;
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Graph(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                level_ = input.readInt32();
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder.class);
      }

      public static final int LEVEL_FIELD_NUMBER = 1;
      private int level_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       */
      public int getLevel() {
        return level_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (level_ != 0) {
          output.writeInt32(1, level_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (level_ != 0) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt32Size(1, level_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph) obj;

        boolean result = true;
        result = result && (getLevel()
            == other.getLevel());
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + LEVEL_FIELD_NUMBER;
        hash = (53 * hash) + getLevel();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Graph
       *&#64;&#64;
       *&#64;&#64;     Enable generic graph optimization of the model. If not specified
       *&#64;&#64;     the framework's default level of optimization is used. Currently
       *&#64;&#64;     only supported for TensorFlow graphdef and savedmodel models and
       *&#64;&#64;     causes XLA to be enabled/disabled for the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.Graph}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          level_ = 0;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph(this);
          result.level_ = level_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance()) return this;
          if (other.getLevel() != 0) {
            setLevel(other.getLevel());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private int level_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         */
        public int getLevel() {
          return level_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         */
        public Builder setLevel(int value) {
          
          level_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         */
        public Builder clearLevel() {
          
          level_ = 0;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Graph>
          PARSER = new com.google.protobuf.AbstractParser<Graph>() {
        @java.lang.Override
        public Graph parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Graph(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Graph> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Graph> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface CudaOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
       *&#64;&#64;       backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       */
      boolean getGraphs();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.Cuda}
     */
    public  static final class Cuda extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
        CudaOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Cuda.newBuilder() to construct.
      private Cuda(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Cuda() {
        graphs_ = false;
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Cuda(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                graphs_ = input.readBool();
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder.class);
      }

      public static final int GRAPHS_FIELD_NUMBER = 1;
      private boolean graphs_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
       *&#64;&#64;       backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       */
      public boolean getGraphs() {
        return graphs_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (graphs_ != false) {
          output.writeBool(1, graphs_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (graphs_ != false) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(1, graphs_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda) obj;

        boolean result = true;
        result = result && (getGraphs()
            == other.getGraphs());
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + GRAPHS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getGraphs());
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.Cuda}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          graphs_ = false;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda(this);
          result.graphs_ = graphs_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance()) return this;
          if (other.getGraphs() != false) {
            setGraphs(other.getGraphs());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private boolean graphs_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
         *&#64;&#64;       backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         */
        public boolean getGraphs() {
          return graphs_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
         *&#64;&#64;       backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         */
        public Builder setGraphs(boolean value) {
          
          graphs_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Currently only recognized by TensorRT
         *&#64;&#64;       backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         */
        public Builder clearGraphs() {
          
          graphs_ = false;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Cuda>
          PARSER = new com.google.protobuf.AbstractParser<Cuda>() {
        @java.lang.Override
        public Cuda parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Cuda(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Cuda> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Cuda> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface ExecutionAcceleratorsOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> 
          getGpuExecutionAcceleratorList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getGpuExecutionAccelerator(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      int getGpuExecutionAcceleratorCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getGpuExecutionAcceleratorOrBuilderList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getGpuExecutionAcceleratorOrBuilder(
          int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> 
          getCpuExecutionAcceleratorList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getCpuExecutionAccelerator(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      int getCpuExecutionAcceleratorCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getCpuExecutionAcceleratorOrBuilderList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getCpuExecutionAcceleratorOrBuilder(
          int index);
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message ExecutionAccelerators
     *&#64;&#64;
     *&#64;&#64;     Specify the preferred execution accelerators to be used to execute
     *&#64;&#64;     the model. Currently only recognized by ONNX Runtime backend and
     *&#64;&#64;     TensorFlow backend.
     *&#64;&#64;
     *&#64;&#64;     For ONNX Runtime backend, it will deploy the model with the execution
     *&#64;&#64;     accelerators by priority, the priority is determined based on the
     *&#64;&#64;     order that they are set, i.e. the provider at the front has highest
     *&#64;&#64;     priority. Overall, the priority will be in the following order:
     *&#64;&#64;         &lt;gpu_execution_accelerator&gt; (if instance is on GPU)
     *&#64;&#64;         CUDA Execution Provider     (if instance is on GPU)
     *&#64;&#64;         &lt;cpu_execution_accelerator&gt;
     *&#64;&#64;         Default CPU Execution Provider
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators}
     */
    public  static final class ExecutionAccelerators extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators)
        ExecutionAcceleratorsOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use ExecutionAccelerators.newBuilder() to construct.
      private ExecutionAccelerators(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private ExecutionAccelerators() {
        gpuExecutionAccelerator_ = java.util.Collections.emptyList();
        cpuExecutionAccelerator_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private ExecutionAccelerators(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                  gpuExecutionAccelerator_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator>();
                  mutable_bitField0_ |= 0x00000001;
                }
                gpuExecutionAccelerator_.add(
                    input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.parser(), extensionRegistry));
                break;
              }
              case 18: {
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                  cpuExecutionAccelerator_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator>();
                  mutable_bitField0_ |= 0x00000002;
                }
                cpuExecutionAccelerator_.add(
                    input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.parser(), extensionRegistry));
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
            gpuExecutionAccelerator_ = java.util.Collections.unmodifiableList(gpuExecutionAccelerator_);
          }
          if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
            cpuExecutionAccelerator_ = java.util.Collections.unmodifiableList(cpuExecutionAccelerator_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder.class);
      }

      public interface AcceleratorOrBuilder extends
          // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
          com.google.protobuf.MessageOrBuilder {

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        java.lang.String getName();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        com.google.protobuf.ByteString
            getNameBytes();

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        int getParametersCount();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        boolean containsParameters(
            java.lang.String key);
        /**
         * Use {@link #getParametersMap()} instead.
         */
        @java.lang.Deprecated
        java.util.Map<java.lang.String, java.lang.String>
        getParameters();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        java.util.Map<java.lang.String, java.lang.String>
        getParametersMap();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        java.lang.String getParametersOrDefault(
            java.lang.String key,
            java.lang.String defaultValue);
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        java.lang.String getParametersOrThrow(
            java.lang.String key);
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Accelerator
       *&#64;&#64;
       *&#64;&#64;     Specify the accelerator to be used to execute the model.
       *&#64;&#64;     Accelerator with the same name may accept different parameters
       *&#64;&#64;     depending on the backends.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator}
       */
      public  static final class Accelerator extends
          com.google.protobuf.GeneratedMessageV3 implements
          // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
          AcceleratorOrBuilder {
      private static final long serialVersionUID = 0L;
        // Use Accelerator.newBuilder() to construct.
        private Accelerator(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
          super(builder);
        }
        private Accelerator() {
          name_ = "";
        }

        @java.lang.Override
        public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
          return this.unknownFields;
        }
        private Accelerator(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          this();
          if (extensionRegistry == null) {
            throw new java.lang.NullPointerException();
          }
          int mutable_bitField0_ = 0;
          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
              com.google.protobuf.UnknownFieldSet.newBuilder();
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 10: {
                  java.lang.String s = input.readStringRequireUtf8();

                  name_ = s;
                  break;
                }
                case 18: {
                  if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                    parameters_ = com.google.protobuf.MapField.newMapField(
                        ParametersDefaultEntryHolder.defaultEntry);
                    mutable_bitField0_ |= 0x00000002;
                  }
                  com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
                  parameters__ = input.readMessage(
                      ParametersDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                  parameters_.getMutableMap().put(
                      parameters__.getKey(), parameters__.getValue());
                  break;
                }
                default: {
                  if (!parseUnknownFieldProto3(
                      input, unknownFields, extensionRegistry, tag)) {
                    done = true;
                  }
                  break;
                }
              }
            }
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(this);
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(
                e).setUnfinishedMessage(this);
          } finally {
            this.unknownFields = unknownFields.build();
            makeExtensionsImmutable();
          }
        }
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor;
        }

        @SuppressWarnings({"rawtypes"})
        @java.lang.Override
        protected com.google.protobuf.MapField internalGetMapField(
            int number) {
          switch (number) {
            case 2:
              return internalGetParameters();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder.class);
        }

        private int bitField0_;
        public static final int NAME_FIELD_NUMBER = 1;
        private volatile java.lang.Object name_;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (ref instanceof java.lang.String) {
            return (java.lang.String) ref;
          } else {
            com.google.protobuf.ByteString bs = 
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            name_ = s;
            return s;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof java.lang.String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }

        public static final int PARAMETERS_FIELD_NUMBER = 2;
        private static final class ParametersDefaultEntryHolder {
          static final com.google.protobuf.MapEntry<
              java.lang.String, java.lang.String> defaultEntry =
                  com.google.protobuf.MapEntry
                  .<java.lang.String, java.lang.String>newDefaultInstance(
                      nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_descriptor, 
                      com.google.protobuf.WireFormat.FieldType.STRING,
                      "",
                      com.google.protobuf.WireFormat.FieldType.STRING,
                      "");
        }
        private com.google.protobuf.MapField<
            java.lang.String, java.lang.String> parameters_;
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetParameters() {
          if (parameters_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                ParametersDefaultEntryHolder.defaultEntry);
          }
          return parameters_;
        }

        public int getParametersCount() {
          return internalGetParameters().getMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        public boolean containsParameters(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          return internalGetParameters().getMap().containsKey(key);
        }
        /**
         * Use {@link #getParametersMap()} instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getParameters() {
          return getParametersMap();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        public java.util.Map<java.lang.String, java.lang.String> getParametersMap() {
          return internalGetParameters().getMap();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        public java.lang.String getParametersOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetParameters().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        public java.lang.String getParametersOrThrow(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetParameters().getMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }

        private byte memoizedIsInitialized = -1;
        @java.lang.Override
        public final boolean isInitialized() {
          byte isInitialized = memoizedIsInitialized;
          if (isInitialized == 1) return true;
          if (isInitialized == 0) return false;

          memoizedIsInitialized = 1;
          return true;
        }

        @java.lang.Override
        public void writeTo(com.google.protobuf.CodedOutputStream output)
                            throws java.io.IOException {
          if (!getNameBytes().isEmpty()) {
            com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
          }
          com.google.protobuf.GeneratedMessageV3
            .serializeStringMapTo(
              output,
              internalGetParameters(),
              ParametersDefaultEntryHolder.defaultEntry,
              2);
          unknownFields.writeTo(output);
        }

        @java.lang.Override
        public int getSerializedSize() {
          int size = memoizedSize;
          if (size != -1) return size;

          size = 0;
          if (!getNameBytes().isEmpty()) {
            size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
          }
          for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
               : internalGetParameters().getMap().entrySet()) {
            com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
            parameters__ = ParametersDefaultEntryHolder.defaultEntry.newBuilderForType()
                .setKey(entry.getKey())
                .setValue(entry.getValue())
                .build();
            size += com.google.protobuf.CodedOutputStream
                .computeMessageSize(2, parameters__);
          }
          size += unknownFields.getSerializedSize();
          memoizedSize = size;
          return size;
        }

        @java.lang.Override
        public boolean equals(final java.lang.Object obj) {
          if (obj == this) {
           return true;
          }
          if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)) {
            return super.equals(obj);
          }
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator) obj;

          boolean result = true;
          result = result && getName()
              .equals(other.getName());
          result = result && internalGetParameters().equals(
              other.internalGetParameters());
          result = result && unknownFields.equals(other.unknownFields);
          return result;
        }

        @java.lang.Override
        public int hashCode() {
          if (memoizedHashCode != 0) {
            return memoizedHashCode;
          }
          int hash = 41;
          hash = (19 * hash) + getDescriptor().hashCode();
          hash = (37 * hash) + NAME_FIELD_NUMBER;
          hash = (53 * hash) + getName().hashCode();
          if (!internalGetParameters().getMap().isEmpty()) {
            hash = (37 * hash) + PARAMETERS_FIELD_NUMBER;
            hash = (53 * hash) + internalGetParameters().hashCode();
          }
          hash = (29 * hash) + unknownFields.hashCode();
          memoizedHashCode = hash;
          return hash;
        }

        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            java.nio.ByteBuffer data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            java.nio.ByteBuffer data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.ByteString data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(byte[] data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseDelimitedFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.CodedInputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }

        @java.lang.Override
        public Builder newBuilderForType() { return newBuilder(); }
        public static Builder newBuilder() {
          return DEFAULT_INSTANCE.toBuilder();
        }
        public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator prototype) {
          return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
        }
        @java.lang.Override
        public Builder toBuilder() {
          return this == DEFAULT_INSTANCE
              ? new Builder() : new Builder().mergeFrom(this);
        }

        @java.lang.Override
        protected Builder newBuilderForType(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          Builder builder = new Builder(parent);
          return builder;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;  .. cpp:var:: message Accelerator
         *&#64;&#64;
         *&#64;&#64;     Specify the accelerator to be used to execute the model.
         *&#64;&#64;     Accelerator with the same name may accept different parameters
         *&#64;&#64;     depending on the backends.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator}
         */
        public static final class Builder extends
            com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
            // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder {
          public static final com.google.protobuf.Descriptors.Descriptor
              getDescriptor() {
            return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor;
          }

          @SuppressWarnings({"rawtypes"})
          protected com.google.protobuf.MapField internalGetMapField(
              int number) {
            switch (number) {
              case 2:
                return internalGetParameters();
              default:
                throw new RuntimeException(
                    "Invalid map field number: " + number);
            }
          }
          @SuppressWarnings({"rawtypes"})
          protected com.google.protobuf.MapField internalGetMutableMapField(
              int number) {
            switch (number) {
              case 2:
                return internalGetMutableParameters();
              default:
                throw new RuntimeException(
                    "Invalid map field number: " + number);
            }
          }
          @java.lang.Override
          protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
              internalGetFieldAccessorTable() {
            return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_fieldAccessorTable
                .ensureFieldAccessorsInitialized(
                    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder.class);
          }

          // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.newBuilder()
          private Builder() {
            maybeForceBuilderInitialization();
          }

          private Builder(
              com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
            super(parent);
            maybeForceBuilderInitialization();
          }
          private void maybeForceBuilderInitialization() {
            if (com.google.protobuf.GeneratedMessageV3
                    .alwaysUseFieldBuilders) {
            }
          }
          @java.lang.Override
          public Builder clear() {
            super.clear();
            name_ = "";

            internalGetMutableParameters().clear();
            return this;
          }

          @java.lang.Override
          public com.google.protobuf.Descriptors.Descriptor
              getDescriptorForType() {
            return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor;
          }

          @java.lang.Override
          public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getDefaultInstanceForType() {
            return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance();
          }

          @java.lang.Override
          public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator build() {
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator result = buildPartial();
            if (!result.isInitialized()) {
              throw newUninitializedMessageException(result);
            }
            return result;
          }

          @java.lang.Override
          public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator buildPartial() {
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator(this);
            int from_bitField0_ = bitField0_;
            int to_bitField0_ = 0;
            result.name_ = name_;
            result.parameters_ = internalGetParameters();
            result.parameters_.makeImmutable();
            result.bitField0_ = to_bitField0_;
            onBuilt();
            return result;
          }

          @java.lang.Override
          public Builder clone() {
            return (Builder) super.clone();
          }
          @java.lang.Override
          public Builder setField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return (Builder) super.setField(field, value);
          }
          @java.lang.Override
          public Builder clearField(
              com.google.protobuf.Descriptors.FieldDescriptor field) {
            return (Builder) super.clearField(field);
          }
          @java.lang.Override
          public Builder clearOneof(
              com.google.protobuf.Descriptors.OneofDescriptor oneof) {
            return (Builder) super.clearOneof(oneof);
          }
          @java.lang.Override
          public Builder setRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              int index, java.lang.Object value) {
            return (Builder) super.setRepeatedField(field, index, value);
          }
          @java.lang.Override
          public Builder addRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return (Builder) super.addRepeatedField(field, value);
          }
          @java.lang.Override
          public Builder mergeFrom(com.google.protobuf.Message other) {
            if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator) {
              return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)other);
            } else {
              super.mergeFrom(other);
              return this;
            }
          }

          public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator other) {
            if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance()) return this;
            if (!other.getName().isEmpty()) {
              name_ = other.name_;
              onChanged();
            }
            internalGetMutableParameters().mergeFrom(
                other.internalGetParameters());
            this.mergeUnknownFields(other.unknownFields);
            onChanged();
            return this;
          }

          @java.lang.Override
          public final boolean isInitialized() {
            return true;
          }

          @java.lang.Override
          public Builder mergeFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parsedMessage = null;
            try {
              parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator) e.getUnfinishedMessage();
              throw e.unwrapIOException();
            } finally {
              if (parsedMessage != null) {
                mergeFrom(parsedMessage);
              }
            }
            return this;
          }
          private int bitField0_;

          private java.lang.Object name_ = "";
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           */
          public java.lang.String getName() {
            java.lang.Object ref = name_;
            if (!(ref instanceof java.lang.String)) {
              com.google.protobuf.ByteString bs =
                  (com.google.protobuf.ByteString) ref;
              java.lang.String s = bs.toStringUtf8();
              name_ = s;
              return s;
            } else {
              return (java.lang.String) ref;
            }
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           */
          public com.google.protobuf.ByteString
              getNameBytes() {
            java.lang.Object ref = name_;
            if (ref instanceof String) {
              com.google.protobuf.ByteString b = 
                  com.google.protobuf.ByteString.copyFromUtf8(
                      (java.lang.String) ref);
              name_ = b;
              return b;
            } else {
              return (com.google.protobuf.ByteString) ref;
            }
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           */
          public Builder setName(
              java.lang.String value) {
            if (value == null) {
    throw new NullPointerException();
  }
  
            name_ = value;
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           */
          public Builder clearName() {
            
            name_ = getDefaultInstance().getName();
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           */
          public Builder setNameBytes(
              com.google.protobuf.ByteString value) {
            if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
            
            name_ = value;
            onChanged();
            return this;
          }

          private com.google.protobuf.MapField<
              java.lang.String, java.lang.String> parameters_;
          private com.google.protobuf.MapField<java.lang.String, java.lang.String>
          internalGetParameters() {
            if (parameters_ == null) {
              return com.google.protobuf.MapField.emptyMapField(
                  ParametersDefaultEntryHolder.defaultEntry);
            }
            return parameters_;
          }
          private com.google.protobuf.MapField<java.lang.String, java.lang.String>
          internalGetMutableParameters() {
            onChanged();;
            if (parameters_ == null) {
              parameters_ = com.google.protobuf.MapField.newMapField(
                  ParametersDefaultEntryHolder.defaultEntry);
            }
            if (!parameters_.isMutable()) {
              parameters_ = parameters_.copy();
            }
            return parameters_;
          }

          public int getParametersCount() {
            return internalGetParameters().getMap().size();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          public boolean containsParameters(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            return internalGetParameters().getMap().containsKey(key);
          }
          /**
           * Use {@link #getParametersMap()} instead.
           */
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, java.lang.String> getParameters() {
            return getParametersMap();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          public java.util.Map<java.lang.String, java.lang.String> getParametersMap() {
            return internalGetParameters().getMap();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          public java.lang.String getParametersOrDefault(
              java.lang.String key,
              java.lang.String defaultValue) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            java.util.Map<java.lang.String, java.lang.String> map =
                internalGetParameters().getMap();
            return map.containsKey(key) ? map.get(key) : defaultValue;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          public java.lang.String getParametersOrThrow(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            java.util.Map<java.lang.String, java.lang.String> map =
                internalGetParameters().getMap();
            if (!map.containsKey(key)) {
              throw new java.lang.IllegalArgumentException();
            }
            return map.get(key);
          }

          public Builder clearParameters() {
            internalGetMutableParameters().getMutableMap()
                .clear();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          public Builder removeParameters(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            internalGetMutableParameters().getMutableMap()
                .remove(key);
            return this;
          }
          /**
           * Use alternate mutation accessors instead.
           */
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, java.lang.String>
          getMutableParameters() {
            return internalGetMutableParameters().getMutableMap();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          public Builder putParameters(
              java.lang.String key,
              java.lang.String value) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            if (value == null) { throw new java.lang.NullPointerException(); }
            internalGetMutableParameters().getMutableMap()
                .put(key, value);
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          public Builder putAllParameters(
              java.util.Map<java.lang.String, java.lang.String> values) {
            internalGetMutableParameters().getMutableMap()
                .putAll(values);
            return this;
          }
          @java.lang.Override
          public final Builder setUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.setUnknownFieldsProto3(unknownFields);
          }

          @java.lang.Override
          public final Builder mergeUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.mergeUnknownFields(unknownFields);
          }


          // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
        }

        // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
        private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator DEFAULT_INSTANCE;
        static {
          DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator();
        }

        public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getDefaultInstance() {
          return DEFAULT_INSTANCE;
        }

        private static final com.google.protobuf.Parser<Accelerator>
            PARSER = new com.google.protobuf.AbstractParser<Accelerator>() {
          @java.lang.Override
          public Accelerator parsePartialFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return new Accelerator(input, extensionRegistry);
          }
        };

        public static com.google.protobuf.Parser<Accelerator> parser() {
          return PARSER;
        }

        @java.lang.Override
        public com.google.protobuf.Parser<Accelerator> getParserForType() {
          return PARSER;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getDefaultInstanceForType() {
          return DEFAULT_INSTANCE;
        }

      }

      public static final int GPU_EXECUTION_ACCELERATOR_FIELD_NUMBER = 1;
      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> gpuExecutionAccelerator_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getGpuExecutionAcceleratorList() {
        return gpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getGpuExecutionAcceleratorOrBuilderList() {
        return gpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      public int getGpuExecutionAcceleratorCount() {
        return gpuExecutionAccelerator_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getGpuExecutionAccelerator(int index) {
        return gpuExecutionAccelerator_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getGpuExecutionAcceleratorOrBuilder(
          int index) {
        return gpuExecutionAccelerator_.get(index);
      }

      public static final int CPU_EXECUTION_ACCELERATOR_FIELD_NUMBER = 2;
      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> cpuExecutionAccelerator_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getCpuExecutionAcceleratorList() {
        return cpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getCpuExecutionAcceleratorOrBuilderList() {
        return cpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      public int getCpuExecutionAcceleratorCount() {
        return cpuExecutionAccelerator_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getCpuExecutionAccelerator(int index) {
        return cpuExecutionAccelerator_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getCpuExecutionAcceleratorOrBuilder(
          int index) {
        return cpuExecutionAccelerator_.get(index);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        for (int i = 0; i < gpuExecutionAccelerator_.size(); i++) {
          output.writeMessage(1, gpuExecutionAccelerator_.get(i));
        }
        for (int i = 0; i < cpuExecutionAccelerator_.size(); i++) {
          output.writeMessage(2, cpuExecutionAccelerator_.get(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        for (int i = 0; i < gpuExecutionAccelerator_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(1, gpuExecutionAccelerator_.get(i));
        }
        for (int i = 0; i < cpuExecutionAccelerator_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, cpuExecutionAccelerator_.get(i));
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators) obj;

        boolean result = true;
        result = result && getGpuExecutionAcceleratorList()
            .equals(other.getGpuExecutionAcceleratorList());
        result = result && getCpuExecutionAcceleratorList()
            .equals(other.getCpuExecutionAcceleratorList());
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        if (getGpuExecutionAcceleratorCount() > 0) {
          hash = (37 * hash) + GPU_EXECUTION_ACCELERATOR_FIELD_NUMBER;
          hash = (53 * hash) + getGpuExecutionAcceleratorList().hashCode();
        }
        if (getCpuExecutionAcceleratorCount() > 0) {
          hash = (37 * hash) + CPU_EXECUTION_ACCELERATOR_FIELD_NUMBER;
          hash = (53 * hash) + getCpuExecutionAcceleratorList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message ExecutionAccelerators
       *&#64;&#64;
       *&#64;&#64;     Specify the preferred execution accelerators to be used to execute
       *&#64;&#64;     the model. Currently only recognized by ONNX Runtime backend and
       *&#64;&#64;     TensorFlow backend.
       *&#64;&#64;
       *&#64;&#64;     For ONNX Runtime backend, it will deploy the model with the execution
       *&#64;&#64;     accelerators by priority, the priority is determined based on the
       *&#64;&#64;     order that they are set, i.e. the provider at the front has highest
       *&#64;&#64;     priority. Overall, the priority will be in the following order:
       *&#64;&#64;         &lt;gpu_execution_accelerator&gt; (if instance is on GPU)
       *&#64;&#64;         CUDA Execution Provider     (if instance is on GPU)
       *&#64;&#64;         &lt;cpu_execution_accelerator&gt;
       *&#64;&#64;         Default CPU Execution Provider
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
            getGpuExecutionAcceleratorFieldBuilder();
            getCpuExecutionAcceleratorFieldBuilder();
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          if (gpuExecutionAcceleratorBuilder_ == null) {
            gpuExecutionAccelerator_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            gpuExecutionAcceleratorBuilder_.clear();
          }
          if (cpuExecutionAcceleratorBuilder_ == null) {
            cpuExecutionAccelerator_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            cpuExecutionAcceleratorBuilder_.clear();
          }
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators(this);
          int from_bitField0_ = bitField0_;
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (((bitField0_ & 0x00000001) == 0x00000001)) {
              gpuExecutionAccelerator_ = java.util.Collections.unmodifiableList(gpuExecutionAccelerator_);
              bitField0_ = (bitField0_ & ~0x00000001);
            }
            result.gpuExecutionAccelerator_ = gpuExecutionAccelerator_;
          } else {
            result.gpuExecutionAccelerator_ = gpuExecutionAcceleratorBuilder_.build();
          }
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (((bitField0_ & 0x00000002) == 0x00000002)) {
              cpuExecutionAccelerator_ = java.util.Collections.unmodifiableList(cpuExecutionAccelerator_);
              bitField0_ = (bitField0_ & ~0x00000002);
            }
            result.cpuExecutionAccelerator_ = cpuExecutionAccelerator_;
          } else {
            result.cpuExecutionAccelerator_ = cpuExecutionAcceleratorBuilder_.build();
          }
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance()) return this;
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (!other.gpuExecutionAccelerator_.isEmpty()) {
              if (gpuExecutionAccelerator_.isEmpty()) {
                gpuExecutionAccelerator_ = other.gpuExecutionAccelerator_;
                bitField0_ = (bitField0_ & ~0x00000001);
              } else {
                ensureGpuExecutionAcceleratorIsMutable();
                gpuExecutionAccelerator_.addAll(other.gpuExecutionAccelerator_);
              }
              onChanged();
            }
          } else {
            if (!other.gpuExecutionAccelerator_.isEmpty()) {
              if (gpuExecutionAcceleratorBuilder_.isEmpty()) {
                gpuExecutionAcceleratorBuilder_.dispose();
                gpuExecutionAcceleratorBuilder_ = null;
                gpuExecutionAccelerator_ = other.gpuExecutionAccelerator_;
                bitField0_ = (bitField0_ & ~0x00000001);
                gpuExecutionAcceleratorBuilder_ = 
                  com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                     getGpuExecutionAcceleratorFieldBuilder() : null;
              } else {
                gpuExecutionAcceleratorBuilder_.addAllMessages(other.gpuExecutionAccelerator_);
              }
            }
          }
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (!other.cpuExecutionAccelerator_.isEmpty()) {
              if (cpuExecutionAccelerator_.isEmpty()) {
                cpuExecutionAccelerator_ = other.cpuExecutionAccelerator_;
                bitField0_ = (bitField0_ & ~0x00000002);
              } else {
                ensureCpuExecutionAcceleratorIsMutable();
                cpuExecutionAccelerator_.addAll(other.cpuExecutionAccelerator_);
              }
              onChanged();
            }
          } else {
            if (!other.cpuExecutionAccelerator_.isEmpty()) {
              if (cpuExecutionAcceleratorBuilder_.isEmpty()) {
                cpuExecutionAcceleratorBuilder_.dispose();
                cpuExecutionAcceleratorBuilder_ = null;
                cpuExecutionAccelerator_ = other.cpuExecutionAccelerator_;
                bitField0_ = (bitField0_ & ~0x00000002);
                cpuExecutionAcceleratorBuilder_ = 
                  com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                     getCpuExecutionAcceleratorFieldBuilder() : null;
              } else {
                cpuExecutionAcceleratorBuilder_.addAllMessages(other.cpuExecutionAccelerator_);
              }
            }
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> gpuExecutionAccelerator_ =
          java.util.Collections.emptyList();
        private void ensureGpuExecutionAcceleratorIsMutable() {
          if (!((bitField0_ & 0x00000001) == 0x00000001)) {
            gpuExecutionAccelerator_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator>(gpuExecutionAccelerator_);
            bitField0_ |= 0x00000001;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> gpuExecutionAcceleratorBuilder_;

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getGpuExecutionAcceleratorList() {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            return java.util.Collections.unmodifiableList(gpuExecutionAccelerator_);
          } else {
            return gpuExecutionAcceleratorBuilder_.getMessageList();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public int getGpuExecutionAcceleratorCount() {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            return gpuExecutionAccelerator_.size();
          } else {
            return gpuExecutionAcceleratorBuilder_.getCount();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getGpuExecutionAccelerator(int index) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            return gpuExecutionAccelerator_.get(index);
          } else {
            return gpuExecutionAcceleratorBuilder_.getMessage(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder setGpuExecutionAccelerator(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.set(index, value);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder setGpuExecutionAccelerator(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.set(index, builderForValue.build());
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.add(value);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.add(index, value);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.add(builderForValue.build());
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.add(index, builderForValue.build());
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addAllGpuExecutionAccelerator(
            java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> values) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, gpuExecutionAccelerator_);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder clearGpuExecutionAccelerator() {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            gpuExecutionAccelerator_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.clear();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder removeGpuExecutionAccelerator(int index) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.remove(index);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder getGpuExecutionAcceleratorBuilder(
            int index) {
          return getGpuExecutionAcceleratorFieldBuilder().getBuilder(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getGpuExecutionAcceleratorOrBuilder(
            int index) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            return gpuExecutionAccelerator_.get(index);  } else {
            return gpuExecutionAcceleratorBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
             getGpuExecutionAcceleratorOrBuilderList() {
          if (gpuExecutionAcceleratorBuilder_ != null) {
            return gpuExecutionAcceleratorBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(gpuExecutionAccelerator_);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder addGpuExecutionAcceleratorBuilder() {
          return getGpuExecutionAcceleratorFieldBuilder().addBuilder(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder addGpuExecutionAcceleratorBuilder(
            int index) {
          return getGpuExecutionAcceleratorFieldBuilder().addBuilder(
              index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder> 
             getGpuExecutionAcceleratorBuilderList() {
          return getGpuExecutionAcceleratorFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
            getGpuExecutionAcceleratorFieldBuilder() {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            gpuExecutionAcceleratorBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder>(
                    gpuExecutionAccelerator_,
                    ((bitField0_ & 0x00000001) == 0x00000001),
                    getParentForChildren(),
                    isClean());
            gpuExecutionAccelerator_ = null;
          }
          return gpuExecutionAcceleratorBuilder_;
        }

        private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> cpuExecutionAccelerator_ =
          java.util.Collections.emptyList();
        private void ensureCpuExecutionAcceleratorIsMutable() {
          if (!((bitField0_ & 0x00000002) == 0x00000002)) {
            cpuExecutionAccelerator_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator>(cpuExecutionAccelerator_);
            bitField0_ |= 0x00000002;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> cpuExecutionAcceleratorBuilder_;

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getCpuExecutionAcceleratorList() {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            return java.util.Collections.unmodifiableList(cpuExecutionAccelerator_);
          } else {
            return cpuExecutionAcceleratorBuilder_.getMessageList();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public int getCpuExecutionAcceleratorCount() {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            return cpuExecutionAccelerator_.size();
          } else {
            return cpuExecutionAcceleratorBuilder_.getCount();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getCpuExecutionAccelerator(int index) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            return cpuExecutionAccelerator_.get(index);
          } else {
            return cpuExecutionAcceleratorBuilder_.getMessage(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder setCpuExecutionAccelerator(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.set(index, value);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder setCpuExecutionAccelerator(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.set(index, builderForValue.build());
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.add(value);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.add(index, value);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.add(builderForValue.build());
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.add(index, builderForValue.build());
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addAllCpuExecutionAccelerator(
            java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> values) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, cpuExecutionAccelerator_);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder clearCpuExecutionAccelerator() {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            cpuExecutionAccelerator_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000002);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.clear();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder removeCpuExecutionAccelerator(int index) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.remove(index);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder getCpuExecutionAcceleratorBuilder(
            int index) {
          return getCpuExecutionAcceleratorFieldBuilder().getBuilder(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getCpuExecutionAcceleratorOrBuilder(
            int index) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            return cpuExecutionAccelerator_.get(index);  } else {
            return cpuExecutionAcceleratorBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
             getCpuExecutionAcceleratorOrBuilderList() {
          if (cpuExecutionAcceleratorBuilder_ != null) {
            return cpuExecutionAcceleratorBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(cpuExecutionAccelerator_);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder addCpuExecutionAcceleratorBuilder() {
          return getCpuExecutionAcceleratorFieldBuilder().addBuilder(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder addCpuExecutionAcceleratorBuilder(
            int index) {
          return getCpuExecutionAcceleratorFieldBuilder().addBuilder(
              index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder> 
             getCpuExecutionAcceleratorBuilderList() {
          return getCpuExecutionAcceleratorFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
            getCpuExecutionAcceleratorFieldBuilder() {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            cpuExecutionAcceleratorBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder>(
                    cpuExecutionAccelerator_,
                    ((bitField0_ & 0x00000002) == 0x00000002),
                    getParentForChildren(),
                    isClean());
            cpuExecutionAccelerator_ = null;
          }
          return cpuExecutionAcceleratorBuilder_;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<ExecutionAccelerators>
          PARSER = new com.google.protobuf.AbstractParser<ExecutionAccelerators>() {
        @java.lang.Override
        public ExecutionAccelerators parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new ExecutionAccelerators(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<ExecutionAccelerators> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<ExecutionAccelerators> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int GRAPH_FIELD_NUMBER = 1;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph graph_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    public boolean hasGraph() {
      return graph_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph() {
      return graph_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder getGraphOrBuilder() {
      return getGraph();
    }

    public static final int PRIORITY_FIELD_NUMBER = 2;
    private int priority_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    public int getPriorityValue() {
      return priority_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority() {
      @SuppressWarnings("deprecation")
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority result = nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.valueOf(priority_);
      return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.UNRECOGNIZED : result;
    }

    public static final int CUDA_FIELD_NUMBER = 3;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda cuda_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    public boolean hasCuda() {
      return cuda_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda() {
      return cuda_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder getCudaOrBuilder() {
      return getCuda();
    }

    public static final int EXECUTION_ACCELERATORS_FIELD_NUMBER = 4;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators executionAccelerators_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    public boolean hasExecutionAccelerators() {
      return executionAccelerators_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getExecutionAccelerators() {
      return executionAccelerators_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance() : executionAccelerators_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder getExecutionAcceleratorsOrBuilder() {
      return getExecutionAccelerators();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (graph_ != null) {
        output.writeMessage(1, getGraph());
      }
      if (priority_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.PRIORITY_DEFAULT.getNumber()) {
        output.writeEnum(2, priority_);
      }
      if (cuda_ != null) {
        output.writeMessage(3, getCuda());
      }
      if (executionAccelerators_ != null) {
        output.writeMessage(4, getExecutionAccelerators());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (graph_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getGraph());
      }
      if (priority_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.PRIORITY_DEFAULT.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, priority_);
      }
      if (cuda_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getCuda());
      }
      if (executionAccelerators_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, getExecutionAccelerators());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy) obj;

      boolean result = true;
      result = result && (hasGraph() == other.hasGraph());
      if (hasGraph()) {
        result = result && getGraph()
            .equals(other.getGraph());
      }
      result = result && priority_ == other.priority_;
      result = result && (hasCuda() == other.hasCuda());
      if (hasCuda()) {
        result = result && getCuda()
            .equals(other.getCuda());
      }
      result = result && (hasExecutionAccelerators() == other.hasExecutionAccelerators());
      if (hasExecutionAccelerators()) {
        result = result && getExecutionAccelerators()
            .equals(other.getExecutionAccelerators());
      }
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasGraph()) {
        hash = (37 * hash) + GRAPH_FIELD_NUMBER;
        hash = (53 * hash) + getGraph().hashCode();
      }
      hash = (37 * hash) + PRIORITY_FIELD_NUMBER;
      hash = (53 * hash) + priority_;
      if (hasCuda()) {
        hash = (37 * hash) + CUDA_FIELD_NUMBER;
        hash = (53 * hash) + getCuda().hashCode();
      }
      if (hasExecutionAccelerators()) {
        hash = (37 * hash) + EXECUTION_ACCELERATORS_FIELD_NUMBER;
        hash = (53 * hash) + getExecutionAccelerators().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelOptimizationPolicy
     *&#64;&#64;
     *&#64;&#64;   Optimization settings for a model. These settings control if/how a
     *&#64;&#64;   model is optimized and prioritized by the backend framework when
     *&#64;&#64;   it is loaded.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelOptimizationPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelOptimizationPolicy)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (graphBuilder_ == null) {
          graph_ = null;
        } else {
          graph_ = null;
          graphBuilder_ = null;
        }
        priority_ = 0;

        if (cudaBuilder_ == null) {
          cuda_ = null;
        } else {
          cuda_ = null;
          cudaBuilder_ = null;
        }
        if (executionAcceleratorsBuilder_ == null) {
          executionAccelerators_ = null;
        } else {
          executionAccelerators_ = null;
          executionAcceleratorsBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy(this);
        if (graphBuilder_ == null) {
          result.graph_ = graph_;
        } else {
          result.graph_ = graphBuilder_.build();
        }
        result.priority_ = priority_;
        if (cudaBuilder_ == null) {
          result.cuda_ = cuda_;
        } else {
          result.cuda_ = cudaBuilder_.build();
        }
        if (executionAcceleratorsBuilder_ == null) {
          result.executionAccelerators_ = executionAccelerators_;
        } else {
          result.executionAccelerators_ = executionAcceleratorsBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance()) return this;
        if (other.hasGraph()) {
          mergeGraph(other.getGraph());
        }
        if (other.priority_ != 0) {
          setPriorityValue(other.getPriorityValue());
        }
        if (other.hasCuda()) {
          mergeCuda(other.getCuda());
        }
        if (other.hasExecutionAccelerators()) {
          mergeExecutionAccelerators(other.getExecutionAccelerators());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph graph_ = null;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder> graphBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public boolean hasGraph() {
        return graphBuilder_ != null || graph_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph() {
        if (graphBuilder_ == null) {
          return graph_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
        } else {
          return graphBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder setGraph(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
        if (graphBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          graph_ = value;
          onChanged();
        } else {
          graphBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder setGraph(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder builderForValue) {
        if (graphBuilder_ == null) {
          graph_ = builderForValue.build();
          onChanged();
        } else {
          graphBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder mergeGraph(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
        if (graphBuilder_ == null) {
          if (graph_ != null) {
            graph_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.newBuilder(graph_).mergeFrom(value).buildPartial();
          } else {
            graph_ = value;
          }
          onChanged();
        } else {
          graphBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder clearGraph() {
        if (graphBuilder_ == null) {
          graph_ = null;
          onChanged();
        } else {
          graph_ = null;
          graphBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder getGraphBuilder() {
        
        onChanged();
        return getGraphFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder getGraphOrBuilder() {
        if (graphBuilder_ != null) {
          return graphBuilder_.getMessageOrBuilder();
        } else {
          return graph_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder> 
          getGraphFieldBuilder() {
        if (graphBuilder_ == null) {
          graphBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder>(
                  getGraph(),
                  getParentForChildren(),
                  isClean());
          graph_ = null;
        }
        return graphBuilder_;
      }

      private int priority_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public int getPriorityValue() {
        return priority_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public Builder setPriorityValue(int value) {
        priority_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority result = nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.valueOf(priority_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public Builder setPriority(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        priority_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       */
      public Builder clearPriority() {
        
        priority_ = 0;
        onChanged();
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda cuda_ = null;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder> cudaBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public boolean hasCuda() {
        return cudaBuilder_ != null || cuda_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda() {
        if (cudaBuilder_ == null) {
          return cuda_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
        } else {
          return cudaBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder setCuda(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
        if (cudaBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          cuda_ = value;
          onChanged();
        } else {
          cudaBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder setCuda(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder builderForValue) {
        if (cudaBuilder_ == null) {
          cuda_ = builderForValue.build();
          onChanged();
        } else {
          cudaBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder mergeCuda(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
        if (cudaBuilder_ == null) {
          if (cuda_ != null) {
            cuda_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.newBuilder(cuda_).mergeFrom(value).buildPartial();
          } else {
            cuda_ = value;
          }
          onChanged();
        } else {
          cudaBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder clearCuda() {
        if (cudaBuilder_ == null) {
          cuda_ = null;
          onChanged();
        } else {
          cuda_ = null;
          cudaBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder getCudaBuilder() {
        
        onChanged();
        return getCudaFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder getCudaOrBuilder() {
        if (cudaBuilder_ != null) {
          return cudaBuilder_.getMessageOrBuilder();
        } else {
          return cuda_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder> 
          getCudaFieldBuilder() {
        if (cudaBuilder_ == null) {
          cudaBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder>(
                  getCuda(),
                  getParentForChildren(),
                  isClean());
          cuda_ = null;
        }
        return cudaBuilder_;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators executionAccelerators_ = null;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder> executionAcceleratorsBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public boolean hasExecutionAccelerators() {
        return executionAcceleratorsBuilder_ != null || executionAccelerators_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getExecutionAccelerators() {
        if (executionAcceleratorsBuilder_ == null) {
          return executionAccelerators_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance() : executionAccelerators_;
        } else {
          return executionAcceleratorsBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder setExecutionAccelerators(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators value) {
        if (executionAcceleratorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          executionAccelerators_ = value;
          onChanged();
        } else {
          executionAcceleratorsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder setExecutionAccelerators(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder builderForValue) {
        if (executionAcceleratorsBuilder_ == null) {
          executionAccelerators_ = builderForValue.build();
          onChanged();
        } else {
          executionAcceleratorsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder mergeExecutionAccelerators(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators value) {
        if (executionAcceleratorsBuilder_ == null) {
          if (executionAccelerators_ != null) {
            executionAccelerators_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.newBuilder(executionAccelerators_).mergeFrom(value).buildPartial();
          } else {
            executionAccelerators_ = value;
          }
          onChanged();
        } else {
          executionAcceleratorsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder clearExecutionAccelerators() {
        if (executionAcceleratorsBuilder_ == null) {
          executionAccelerators_ = null;
          onChanged();
        } else {
          executionAccelerators_ = null;
          executionAcceleratorsBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder getExecutionAcceleratorsBuilder() {
        
        onChanged();
        return getExecutionAcceleratorsFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder getExecutionAcceleratorsOrBuilder() {
        if (executionAcceleratorsBuilder_ != null) {
          return executionAcceleratorsBuilder_.getMessageOrBuilder();
        } else {
          return executionAccelerators_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance() : executionAccelerators_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder> 
          getExecutionAcceleratorsFieldBuilder() {
        if (executionAcceleratorsBuilder_ == null) {
          executionAcceleratorsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder>(
                  getExecutionAccelerators(),
                  getParentForChildren(),
                  isClean());
          executionAccelerators_ = null;
        }
        return executionAcceleratorsBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelOptimizationPolicy)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelOptimizationPolicy>
        PARSER = new com.google.protobuf.AbstractParser<ModelOptimizationPolicy>() {
      @java.lang.Override
      public ModelOptimizationPolicy parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelOptimizationPolicy(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelOptimizationPolicy> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelOptimizationPolicy> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelDynamicBatchingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelDynamicBatching)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    java.util.List<java.lang.Integer> getPreferredBatchSizeList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    int getPreferredBatchSizeCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    int getPreferredBatchSize(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     */
    long getMaxQueueDelayMicroseconds();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelDynamicBatching
   *&#64;&#64;
   *&#64;&#64;   Dynamic batching configuration. These settings control how dynamic
   *&#64;&#64;   batching operates for the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelDynamicBatching}
   */
  public  static final class ModelDynamicBatching extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelDynamicBatching)
      ModelDynamicBatchingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelDynamicBatching.newBuilder() to construct.
    private ModelDynamicBatching(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelDynamicBatching() {
      preferredBatchSize_ = java.util.Collections.emptyList();
      maxQueueDelayMicroseconds_ = 0L;
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelDynamicBatching(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                preferredBatchSize_ = new java.util.ArrayList<java.lang.Integer>();
                mutable_bitField0_ |= 0x00000001;
              }
              preferredBatchSize_.add(input.readInt32());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001) && input.getBytesUntilLimit() > 0) {
                preferredBatchSize_ = new java.util.ArrayList<java.lang.Integer>();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                preferredBatchSize_.add(input.readInt32());
              }
              input.popLimit(limit);
              break;
            }
            case 16: {

              maxQueueDelayMicroseconds_ = input.readUInt64();
              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          preferredBatchSize_ = java.util.Collections.unmodifiableList(preferredBatchSize_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder.class);
    }

    private int bitField0_;
    public static final int PREFERRED_BATCH_SIZE_FIELD_NUMBER = 1;
    private java.util.List<java.lang.Integer> preferredBatchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    public java.util.List<java.lang.Integer>
        getPreferredBatchSizeList() {
      return preferredBatchSize_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    public int getPreferredBatchSizeCount() {
      return preferredBatchSize_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     */
    public int getPreferredBatchSize(int index) {
      return preferredBatchSize_.get(index);
    }
    private int preferredBatchSizeMemoizedSerializedSize = -1;

    public static final int MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER = 2;
    private long maxQueueDelayMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     */
    public long getMaxQueueDelayMicroseconds() {
      return maxQueueDelayMicroseconds_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getPreferredBatchSizeList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(preferredBatchSizeMemoizedSerializedSize);
      }
      for (int i = 0; i < preferredBatchSize_.size(); i++) {
        output.writeInt32NoTag(preferredBatchSize_.get(i));
      }
      if (maxQueueDelayMicroseconds_ != 0L) {
        output.writeUInt64(2, maxQueueDelayMicroseconds_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < preferredBatchSize_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt32SizeNoTag(preferredBatchSize_.get(i));
        }
        size += dataSize;
        if (!getPreferredBatchSizeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        preferredBatchSizeMemoizedSerializedSize = dataSize;
      }
      if (maxQueueDelayMicroseconds_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, maxQueueDelayMicroseconds_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) obj;

      boolean result = true;
      result = result && getPreferredBatchSizeList()
          .equals(other.getPreferredBatchSizeList());
      result = result && (getMaxQueueDelayMicroseconds()
          == other.getMaxQueueDelayMicroseconds());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getPreferredBatchSizeCount() > 0) {
        hash = (37 * hash) + PREFERRED_BATCH_SIZE_FIELD_NUMBER;
        hash = (53 * hash) + getPreferredBatchSizeList().hashCode();
      }
      hash = (37 * hash) + MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMaxQueueDelayMicroseconds());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelDynamicBatching
     *&#64;&#64;
     *&#64;&#64;   Dynamic batching configuration. These settings control how dynamic
     *&#64;&#64;   batching operates for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelDynamicBatching}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelDynamicBatching)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        preferredBatchSize_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        maxQueueDelayMicroseconds_ = 0L;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          preferredBatchSize_ = java.util.Collections.unmodifiableList(preferredBatchSize_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.preferredBatchSize_ = preferredBatchSize_;
        result.maxQueueDelayMicroseconds_ = maxQueueDelayMicroseconds_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance()) return this;
        if (!other.preferredBatchSize_.isEmpty()) {
          if (preferredBatchSize_.isEmpty()) {
            preferredBatchSize_ = other.preferredBatchSize_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensurePreferredBatchSizeIsMutable();
            preferredBatchSize_.addAll(other.preferredBatchSize_);
          }
          onChanged();
        }
        if (other.getMaxQueueDelayMicroseconds() != 0L) {
          setMaxQueueDelayMicroseconds(other.getMaxQueueDelayMicroseconds());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<java.lang.Integer> preferredBatchSize_ = java.util.Collections.emptyList();
      private void ensurePreferredBatchSizeIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          preferredBatchSize_ = new java.util.ArrayList<java.lang.Integer>(preferredBatchSize_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public java.util.List<java.lang.Integer>
          getPreferredBatchSizeList() {
        return java.util.Collections.unmodifiableList(preferredBatchSize_);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public int getPreferredBatchSizeCount() {
        return preferredBatchSize_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public int getPreferredBatchSize(int index) {
        return preferredBatchSize_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public Builder setPreferredBatchSize(
          int index, int value) {
        ensurePreferredBatchSizeIsMutable();
        preferredBatchSize_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public Builder addPreferredBatchSize(int value) {
        ensurePreferredBatchSizeIsMutable();
        preferredBatchSize_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public Builder addAllPreferredBatchSize(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensurePreferredBatchSizeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, preferredBatchSize_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       */
      public Builder clearPreferredBatchSize() {
        preferredBatchSize_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private long maxQueueDelayMicroseconds_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       */
      public long getMaxQueueDelayMicroseconds() {
        return maxQueueDelayMicroseconds_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       */
      public Builder setMaxQueueDelayMicroseconds(long value) {
        
        maxQueueDelayMicroseconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       */
      public Builder clearMaxQueueDelayMicroseconds() {
        
        maxQueueDelayMicroseconds_ = 0L;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelDynamicBatching)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelDynamicBatching)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelDynamicBatching>
        PARSER = new com.google.protobuf.AbstractParser<ModelDynamicBatching>() {
      @java.lang.Override
      public ModelDynamicBatching parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelDynamicBatching(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelDynamicBatching> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelDynamicBatching> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelSequenceBatchingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelSequenceBatching)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    boolean hasDirect();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDirect();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder getDirectOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    boolean hasOldest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getOldest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder getOldestOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     */
    long getMaxSequenceIdleMicroseconds();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> 
        getControlInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    int getControlInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
        getControlInputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
        int index);

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyChoiceCase getStrategyChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelSequenceBatching
   *&#64;&#64;
   *&#64;&#64;   Sequence batching configuration. These settings control how sequence
   *&#64;&#64;   batching operates for the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching}
   */
  public  static final class ModelSequenceBatching extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelSequenceBatching)
      ModelSequenceBatchingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelSequenceBatching.newBuilder() to construct.
    private ModelSequenceBatching(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelSequenceBatching() {
      maxSequenceIdleMicroseconds_ = 0L;
      controlInput_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelSequenceBatching(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              maxSequenceIdleMicroseconds_ = input.readUInt64();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                controlInput_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput>();
                mutable_bitField0_ |= 0x00000008;
              }
              controlInput_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.parser(), extensionRegistry));
              break;
            }
            case 26: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder subBuilder = null;
              if (strategyChoiceCase_ == 3) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_).toBuilder();
              }
              strategyChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_);
                strategyChoice_ = subBuilder.buildPartial();
              }
              strategyChoiceCase_ = 3;
              break;
            }
            case 34: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder subBuilder = null;
              if (strategyChoiceCase_ == 4) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_).toBuilder();
              }
              strategyChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_);
                strategyChoice_ = subBuilder.buildPartial();
              }
              strategyChoiceCase_ = 4;
              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
          controlInput_ = java.util.Collections.unmodifiableList(controlInput_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder.class);
    }

    public interface ControlOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelSequenceBatching.Control)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
       */
      int getKindValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      java.util.List<java.lang.Integer> getInt32FalseTrueList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      int getInt32FalseTrueCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      int getInt32FalseTrue(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      java.util.List<java.lang.Float> getFp32FalseTrueList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      int getFp32FalseTrueCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      float getFp32FalseTrue(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 4;</code>
       */
      int getDataTypeValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 4;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Control
     *&#64;&#64;
     *&#64;&#64;     A control is a signal that the sequence batcher uses to
     *&#64;&#64;     communicate with a backend.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.Control}
     */
    public  static final class Control extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelSequenceBatching.Control)
        ControlOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Control.newBuilder() to construct.
      private Control(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Control() {
        kind_ = 0;
        int32FalseTrue_ = java.util.Collections.emptyList();
        fp32FalseTrue_ = java.util.Collections.emptyList();
        dataType_ = 0;
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Control(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                int rawValue = input.readEnum();

                kind_ = rawValue;
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                  int32FalseTrue_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000002;
                }
                int32FalseTrue_.add(input.readInt32());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002) && input.getBytesUntilLimit() > 0) {
                  int32FalseTrue_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000002;
                }
                while (input.getBytesUntilLimit() > 0) {
                  int32FalseTrue_.add(input.readInt32());
                }
                input.popLimit(limit);
                break;
              }
              case 29: {
                if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                  fp32FalseTrue_ = new java.util.ArrayList<java.lang.Float>();
                  mutable_bitField0_ |= 0x00000004;
                }
                fp32FalseTrue_.add(input.readFloat());
                break;
              }
              case 26: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000004) == 0x00000004) && input.getBytesUntilLimit() > 0) {
                  fp32FalseTrue_ = new java.util.ArrayList<java.lang.Float>();
                  mutable_bitField0_ |= 0x00000004;
                }
                while (input.getBytesUntilLimit() > 0) {
                  fp32FalseTrue_.add(input.readFloat());
                }
                input.popLimit(limit);
                break;
              }
              case 32: {
                int rawValue = input.readEnum();

                dataType_ = rawValue;
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
            int32FalseTrue_ = java.util.Collections.unmodifiableList(int32FalseTrue_);
          }
          if (((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
            fp32FalseTrue_ = java.util.Collections.unmodifiableList(fp32FalseTrue_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder.class);
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:enum:: Kind
       *&#64;&#64;
       *&#64;&#64;       The kind of the control.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf enum {@code nvidia.inferenceserver.ModelSequenceBatching.Control.Kind}
       */
      public enum Kind
          implements com.google.protobuf.ProtocolMessageEnum {
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
         *&#64;&#64;
         *&#64;&#64;         A new sequence is/is-not starting. If true a sequence is
         *&#64;&#64;         starting, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_START = 0;</code>
         */
        CONTROL_SEQUENCE_START(0),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ready for inference. If true the
         *&#64;&#64;         input tensor data is valid and should be used. If false
         *&#64;&#64;         the input tensor data is invalid and inferencing should
         *&#64;&#64;         be "skipped".  Must specify either int32_false_true or
         *&#64;&#64;         fp32_false_true for this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_READY = 1;</code>
         */
        CONTROL_SEQUENCE_READY(1),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_END = 2
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ending. If true a sequence is
         *&#64;&#64;         ending, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_END = 2;</code>
         */
        CONTROL_SEQUENCE_END(2),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_CORRID = 3
         *&#64;&#64;
         *&#64;&#64;         The correlation ID of the sequence. The correlation ID
         *&#64;&#64;         is an uint64_t value that is communicated in whole or
         *&#64;&#64;         in part by the tensor. The tensor's datatype must be
         *&#64;&#64;         specified by data_type and must be TYPE_UINT64, TYPE_INT64,
         *&#64;&#64;         TYPE_UINT32 or TYPE_INT32. If a 32-bit datatype is specified
         *&#64;&#64;         the correlation ID will be truncated to the low-order 32
         *&#64;&#64;         bits. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_CORRID = 3;</code>
         */
        CONTROL_SEQUENCE_CORRID(3),
        UNRECOGNIZED(-1),
        ;

        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
         *&#64;&#64;
         *&#64;&#64;         A new sequence is/is-not starting. If true a sequence is
         *&#64;&#64;         starting, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_START = 0;</code>
         */
        public static final int CONTROL_SEQUENCE_START_VALUE = 0;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ready for inference. If true the
         *&#64;&#64;         input tensor data is valid and should be used. If false
         *&#64;&#64;         the input tensor data is invalid and inferencing should
         *&#64;&#64;         be "skipped".  Must specify either int32_false_true or
         *&#64;&#64;         fp32_false_true for this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_READY = 1;</code>
         */
        public static final int CONTROL_SEQUENCE_READY_VALUE = 1;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_END = 2
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ending. If true a sequence is
         *&#64;&#64;         ending, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_END = 2;</code>
         */
        public static final int CONTROL_SEQUENCE_END_VALUE = 2;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_CORRID = 3
         *&#64;&#64;
         *&#64;&#64;         The correlation ID of the sequence. The correlation ID
         *&#64;&#64;         is an uint64_t value that is communicated in whole or
         *&#64;&#64;         in part by the tensor. The tensor's datatype must be
         *&#64;&#64;         specified by data_type and must be TYPE_UINT64, TYPE_INT64,
         *&#64;&#64;         TYPE_UINT32 or TYPE_INT32. If a 32-bit datatype is specified
         *&#64;&#64;         the correlation ID will be truncated to the low-order 32
         *&#64;&#64;         bits. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_CORRID = 3;</code>
         */
        public static final int CONTROL_SEQUENCE_CORRID_VALUE = 3;


        public final int getNumber() {
          if (this == UNRECOGNIZED) {
            throw new java.lang.IllegalArgumentException(
                "Can't get the number of an unknown enum value.");
          }
          return value;
        }

        /**
         * @deprecated Use {@link #forNumber(int)} instead.
         */
        @java.lang.Deprecated
        public static Kind valueOf(int value) {
          return forNumber(value);
        }

        public static Kind forNumber(int value) {
          switch (value) {
            case 0: return CONTROL_SEQUENCE_START;
            case 1: return CONTROL_SEQUENCE_READY;
            case 2: return CONTROL_SEQUENCE_END;
            case 3: return CONTROL_SEQUENCE_CORRID;
            default: return null;
          }
        }

        public static com.google.protobuf.Internal.EnumLiteMap<Kind>
            internalGetValueMap() {
          return internalValueMap;
        }
        private static final com.google.protobuf.Internal.EnumLiteMap<
            Kind> internalValueMap =
              new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
                public Kind findValueByNumber(int number) {
                  return Kind.forNumber(number);
                }
              };

        public final com.google.protobuf.Descriptors.EnumValueDescriptor
            getValueDescriptor() {
          return getDescriptor().getValues().get(ordinal());
        }
        public final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptorForType() {
          return getDescriptor();
        }
        public static final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDescriptor().getEnumTypes().get(0);
        }

        private static final Kind[] VALUES = values();

        public static Kind valueOf(
            com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
          if (desc.getType() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "EnumValueDescriptor is not for this type.");
          }
          if (desc.getIndex() == -1) {
            return UNRECOGNIZED;
          }
          return VALUES[desc.getIndex()];
        }

        private final int value;

        private Kind(int value) {
          this.value = value;
        }

        // @@protoc_insertion_point(enum_scope:nvidia.inferenceserver.ModelSequenceBatching.Control.Kind)
      }

      private int bitField0_;
      public static final int KIND_FIELD_NUMBER = 1;
      private int kind_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
       */
      public int getKindValue() {
        return kind_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind result = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.valueOf(kind_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.UNRECOGNIZED : result;
      }

      public static final int INT32_FALSE_TRUE_FIELD_NUMBER = 2;
      private java.util.List<java.lang.Integer> int32FalseTrue_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      public java.util.List<java.lang.Integer>
          getInt32FalseTrueList() {
        return int32FalseTrue_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      public int getInt32FalseTrueCount() {
        return int32FalseTrue_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       */
      public int getInt32FalseTrue(int index) {
        return int32FalseTrue_.get(index);
      }
      private int int32FalseTrueMemoizedSerializedSize = -1;

      public static final int FP32_FALSE_TRUE_FIELD_NUMBER = 3;
      private java.util.List<java.lang.Float> fp32FalseTrue_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      public java.util.List<java.lang.Float>
          getFp32FalseTrueList() {
        return fp32FalseTrue_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      public int getFp32FalseTrueCount() {
        return fp32FalseTrue_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       */
      public float getFp32FalseTrue(int index) {
        return fp32FalseTrue_.get(index);
      }
      private int fp32FalseTrueMemoizedSerializedSize = -1;

      public static final int DATA_TYPE_FIELD_NUMBER = 4;
      private int dataType_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 4;</code>
       */
      public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 4;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (kind_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.CONTROL_SEQUENCE_START.getNumber()) {
          output.writeEnum(1, kind_);
        }
        if (getInt32FalseTrueList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(int32FalseTrueMemoizedSerializedSize);
        }
        for (int i = 0; i < int32FalseTrue_.size(); i++) {
          output.writeInt32NoTag(int32FalseTrue_.get(i));
        }
        if (getFp32FalseTrueList().size() > 0) {
          output.writeUInt32NoTag(26);
          output.writeUInt32NoTag(fp32FalseTrueMemoizedSerializedSize);
        }
        for (int i = 0; i < fp32FalseTrue_.size(); i++) {
          output.writeFloatNoTag(fp32FalseTrue_.get(i));
        }
        if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
          output.writeEnum(4, dataType_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (kind_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.CONTROL_SEQUENCE_START.getNumber()) {
          size += com.google.protobuf.CodedOutputStream
            .computeEnumSize(1, kind_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < int32FalseTrue_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(int32FalseTrue_.get(i));
          }
          size += dataSize;
          if (!getInt32FalseTrueList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          int32FalseTrueMemoizedSerializedSize = dataSize;
        }
        {
          int dataSize = 0;
          dataSize = 4 * getFp32FalseTrueList().size();
          size += dataSize;
          if (!getFp32FalseTrueList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          fp32FalseTrueMemoizedSerializedSize = dataSize;
        }
        if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
          size += com.google.protobuf.CodedOutputStream
            .computeEnumSize(4, dataType_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control) obj;

        boolean result = true;
        result = result && kind_ == other.kind_;
        result = result && getInt32FalseTrueList()
            .equals(other.getInt32FalseTrueList());
        result = result && getFp32FalseTrueList()
            .equals(other.getFp32FalseTrueList());
        result = result && dataType_ == other.dataType_;
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + KIND_FIELD_NUMBER;
        hash = (53 * hash) + kind_;
        if (getInt32FalseTrueCount() > 0) {
          hash = (37 * hash) + INT32_FALSE_TRUE_FIELD_NUMBER;
          hash = (53 * hash) + getInt32FalseTrueList().hashCode();
        }
        if (getFp32FalseTrueCount() > 0) {
          hash = (37 * hash) + FP32_FALSE_TRUE_FIELD_NUMBER;
          hash = (53 * hash) + getFp32FalseTrueList().hashCode();
        }
        hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + dataType_;
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Control
       *&#64;&#64;
       *&#64;&#64;     A control is a signal that the sequence batcher uses to
       *&#64;&#64;     communicate with a backend.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.Control}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelSequenceBatching.Control)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          kind_ = 0;

          int32FalseTrue_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          fp32FalseTrue_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          dataType_ = 0;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          result.kind_ = kind_;
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            int32FalseTrue_ = java.util.Collections.unmodifiableList(int32FalseTrue_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.int32FalseTrue_ = int32FalseTrue_;
          if (((bitField0_ & 0x00000004) == 0x00000004)) {
            fp32FalseTrue_ = java.util.Collections.unmodifiableList(fp32FalseTrue_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.fp32FalseTrue_ = fp32FalseTrue_;
          result.dataType_ = dataType_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance()) return this;
          if (other.kind_ != 0) {
            setKindValue(other.getKindValue());
          }
          if (!other.int32FalseTrue_.isEmpty()) {
            if (int32FalseTrue_.isEmpty()) {
              int32FalseTrue_ = other.int32FalseTrue_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureInt32FalseTrueIsMutable();
              int32FalseTrue_.addAll(other.int32FalseTrue_);
            }
            onChanged();
          }
          if (!other.fp32FalseTrue_.isEmpty()) {
            if (fp32FalseTrue_.isEmpty()) {
              fp32FalseTrue_ = other.fp32FalseTrue_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureFp32FalseTrueIsMutable();
              fp32FalseTrue_.addAll(other.fp32FalseTrue_);
            }
            onChanged();
          }
          if (other.dataType_ != 0) {
            setDataTypeValue(other.getDataTypeValue());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private int kind_ = 0;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public int getKindValue() {
          return kind_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public Builder setKindValue(int value) {
          kind_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind() {
          @SuppressWarnings("deprecation")
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind result = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.valueOf(kind_);
          return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.UNRECOGNIZED : result;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public Builder setKind(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind value) {
          if (value == null) {
            throw new NullPointerException();
          }
          
          kind_ = value.getNumber();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind kind = 1;</code>
         */
        public Builder clearKind() {
          
          kind_ = 0;
          onChanged();
          return this;
        }

        private java.util.List<java.lang.Integer> int32FalseTrue_ = java.util.Collections.emptyList();
        private void ensureInt32FalseTrueIsMutable() {
          if (!((bitField0_ & 0x00000002) == 0x00000002)) {
            int32FalseTrue_ = new java.util.ArrayList<java.lang.Integer>(int32FalseTrue_);
            bitField0_ |= 0x00000002;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public java.util.List<java.lang.Integer>
            getInt32FalseTrueList() {
          return java.util.Collections.unmodifiableList(int32FalseTrue_);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public int getInt32FalseTrueCount() {
          return int32FalseTrue_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public int getInt32FalseTrue(int index) {
          return int32FalseTrue_.get(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public Builder setInt32FalseTrue(
            int index, int value) {
          ensureInt32FalseTrueIsMutable();
          int32FalseTrue_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public Builder addInt32FalseTrue(int value) {
          ensureInt32FalseTrueIsMutable();
          int32FalseTrue_.add(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public Builder addAllInt32FalseTrue(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensureInt32FalseTrueIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, int32FalseTrue_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         */
        public Builder clearInt32FalseTrue() {
          int32FalseTrue_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
          return this;
        }

        private java.util.List<java.lang.Float> fp32FalseTrue_ = java.util.Collections.emptyList();
        private void ensureFp32FalseTrueIsMutable() {
          if (!((bitField0_ & 0x00000004) == 0x00000004)) {
            fp32FalseTrue_ = new java.util.ArrayList<java.lang.Float>(fp32FalseTrue_);
            bitField0_ |= 0x00000004;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public java.util.List<java.lang.Float>
            getFp32FalseTrueList() {
          return java.util.Collections.unmodifiableList(fp32FalseTrue_);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public int getFp32FalseTrueCount() {
          return fp32FalseTrue_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public float getFp32FalseTrue(int index) {
          return fp32FalseTrue_.get(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public Builder setFp32FalseTrue(
            int index, float value) {
          ensureFp32FalseTrueIsMutable();
          fp32FalseTrue_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public Builder addFp32FalseTrue(float value) {
          ensureFp32FalseTrueIsMutable();
          fp32FalseTrue_.add(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public Builder addAllFp32FalseTrue(
            java.lang.Iterable<? extends java.lang.Float> values) {
          ensureFp32FalseTrueIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, fp32FalseTrue_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         */
        public Builder clearFp32FalseTrue() {
          fp32FalseTrue_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
          return this;
        }

        private int dataType_ = 0;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 4;</code>
         */
        public int getDataTypeValue() {
          return dataType_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 4;</code>
         */
        public Builder setDataTypeValue(int value) {
          dataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 4;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
          @SuppressWarnings("deprecation")
          nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
          return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 4;</code>
         */
        public Builder setDataType(nvidia.inferenceserver.ModelConfigOuterClass.DataType value) {
          if (value == null) {
            throw new NullPointerException();
          }
          
          dataType_ = value.getNumber();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 4;</code>
         */
        public Builder clearDataType() {
          
          dataType_ = 0;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelSequenceBatching.Control)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.Control)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Control>
          PARSER = new com.google.protobuf.AbstractParser<Control>() {
        @java.lang.Override
        public Control parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Control(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Control> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Control> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface ControlInputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> 
          getControlList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      int getControlCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
          getControlOrBuilderList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
          int index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message ControlInput
     *&#64;&#64;
     *&#64;&#64;     The sequence control values to communicate by a model input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.ControlInput}
     */
    public  static final class ControlInput extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
        ControlInputOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use ControlInput.newBuilder() to construct.
      private ControlInput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private ControlInput() {
        name_ = "";
        control_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private ControlInput(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                name_ = s;
                break;
              }
              case 18: {
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                  control_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control>();
                  mutable_bitField0_ |= 0x00000002;
                }
                control_.add(
                    input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.parser(), extensionRegistry));
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
            control_ = java.util.Collections.unmodifiableList(control_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder.class);
      }

      private int bitField0_;
      public static final int NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object name_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int CONTROL_FIELD_NUMBER = 2;
      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> control_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> getControlList() {
        return control_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
          getControlOrBuilderList() {
        return control_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public int getControlCount() {
        return control_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index) {
        return control_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
          int index) {
        return control_.get(index);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!getNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
        }
        for (int i = 0; i < control_.size(); i++) {
          output.writeMessage(2, control_.get(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
        }
        for (int i = 0; i < control_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, control_.get(i));
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput) obj;

        boolean result = true;
        result = result && getName()
            .equals(other.getName());
        result = result && getControlList()
            .equals(other.getControlList());
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
        if (getControlCount() > 0) {
          hash = (37 * hash) + CONTROL_FIELD_NUMBER;
          hash = (53 * hash) + getControlList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message ControlInput
       *&#64;&#64;
       *&#64;&#64;     The sequence control values to communicate by a model input.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.ControlInput}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
            getControlFieldBuilder();
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          name_ = "";

          if (controlBuilder_ == null) {
            control_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            controlBuilder_.clear();
          }
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          result.name_ = name_;
          if (controlBuilder_ == null) {
            if (((bitField0_ & 0x00000002) == 0x00000002)) {
              control_ = java.util.Collections.unmodifiableList(control_);
              bitField0_ = (bitField0_ & ~0x00000002);
            }
            result.control_ = control_;
          } else {
            result.control_ = controlBuilder_.build();
          }
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance()) return this;
          if (!other.getName().isEmpty()) {
            name_ = other.name_;
            onChanged();
          }
          if (controlBuilder_ == null) {
            if (!other.control_.isEmpty()) {
              if (control_.isEmpty()) {
                control_ = other.control_;
                bitField0_ = (bitField0_ & ~0x00000002);
              } else {
                ensureControlIsMutable();
                control_.addAll(other.control_);
              }
              onChanged();
            }
          } else {
            if (!other.control_.isEmpty()) {
              if (controlBuilder_.isEmpty()) {
                controlBuilder_.dispose();
                controlBuilder_ = null;
                control_ = other.control_;
                bitField0_ = (bitField0_ & ~0x00000002);
                controlBuilder_ = 
                  com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                     getControlFieldBuilder() : null;
              } else {
                controlBuilder_.addAllMessages(other.control_);
              }
            }
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.lang.Object name_ = "";
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder clearName() {
          
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          name_ = value;
          onChanged();
          return this;
        }

        private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> control_ =
          java.util.Collections.emptyList();
        private void ensureControlIsMutable() {
          if (!((bitField0_ & 0x00000002) == 0x00000002)) {
            control_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control>(control_);
            bitField0_ |= 0x00000002;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> controlBuilder_;

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> getControlList() {
          if (controlBuilder_ == null) {
            return java.util.Collections.unmodifiableList(control_);
          } else {
            return controlBuilder_.getMessageList();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public int getControlCount() {
          if (controlBuilder_ == null) {
            return control_.size();
          } else {
            return controlBuilder_.getCount();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index) {
          if (controlBuilder_ == null) {
            return control_.get(index);
          } else {
            return controlBuilder_.getMessage(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder setControl(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          if (controlBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureControlIsMutable();
            control_.set(index, value);
            onChanged();
          } else {
            controlBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder setControl(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.set(index, builderForValue.build());
            onChanged();
          } else {
            controlBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          if (controlBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureControlIsMutable();
            control_.add(value);
            onChanged();
          } else {
            controlBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          if (controlBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureControlIsMutable();
            control_.add(index, value);
            onChanged();
          } else {
            controlBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.add(builderForValue.build());
            onChanged();
          } else {
            controlBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.add(index, builderForValue.build());
            onChanged();
          } else {
            controlBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addAllControl(
            java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control> values) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, control_);
            onChanged();
          } else {
            controlBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder clearControl() {
          if (controlBuilder_ == null) {
            control_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000002);
            onChanged();
          } else {
            controlBuilder_.clear();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder removeControl(int index) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.remove(index);
            onChanged();
          } else {
            controlBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder getControlBuilder(
            int index) {
          return getControlFieldBuilder().getBuilder(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
            int index) {
          if (controlBuilder_ == null) {
            return control_.get(index);  } else {
            return controlBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
             getControlOrBuilderList() {
          if (controlBuilder_ != null) {
            return controlBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(control_);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder addControlBuilder() {
          return getControlFieldBuilder().addBuilder(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder addControlBuilder(
            int index) {
          return getControlFieldBuilder().addBuilder(
              index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.Control control = 2;</code>
         */
        public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder> 
             getControlBuilderList() {
          return getControlFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilderV3<
            nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
            getControlFieldBuilder() {
          if (controlBuilder_ == null) {
            controlBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder>(
                    control_,
                    ((bitField0_ & 0x00000002) == 0x00000002),
                    getParentForChildren(),
                    isClean());
            control_ = null;
          }
          return controlBuilder_;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<ControlInput>
          PARSER = new com.google.protobuf.AbstractParser<ControlInput>() {
        @java.lang.Override
        public ControlInput parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new ControlInput(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<ControlInput> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<ControlInput> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface StrategyDirectOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect)
        com.google.protobuf.MessageOrBuilder {
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message StrategyDirect
     *&#64;&#64;
     *&#64;&#64;     The sequence batcher uses a specific, unique batch
     *&#64;&#64;     slot for each sequence. All inference requests in a
     *&#64;&#64;     sequence are directed to the same batch slot in the same
     *&#64;&#64;     model instance over the lifetime of the sequence. This
     *&#64;&#64;     is the default strategy.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect}
     */
    public  static final class StrategyDirect extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect)
        StrategyDirectOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use StrategyDirect.newBuilder() to construct.
      private StrategyDirect(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private StrategyDirect() {
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private StrategyDirect(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder.class);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) obj;

        boolean result = true;
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message StrategyDirect
       *&#64;&#64;
       *&#64;&#64;     The sequence batcher uses a specific, unique batch
       *&#64;&#64;     slot for each sequence. All inference requests in a
       *&#64;&#64;     sequence are directed to the same batch slot in the same
       *&#64;&#64;     model instance over the lifetime of the sequence. This
       *&#64;&#64;     is the default strategy.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect(this);
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance()) return this;
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<StrategyDirect>
          PARSER = new com.google.protobuf.AbstractParser<StrategyDirect>() {
        @java.lang.Override
        public StrategyDirect parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new StrategyDirect(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<StrategyDirect> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<StrategyDirect> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface StrategyOldestOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
       *&#64;&#64;
       *&#64;&#64;       Maximum number of candidate sequences that the batcher
       *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
       *&#64;&#64;       and become candidates when existing candidate sequences
       *&#64;&#64;       complete.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_candidate_sequences = 1;</code>
       */
      int getMaxCandidateSequences();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately.  If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       */
      java.util.List<java.lang.Integer> getPreferredBatchSizeList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately.  If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       */
      int getPreferredBatchSizeCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately.  If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       */
      int getPreferredBatchSize(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 3;</code>
       */
      long getMaxQueueDelayMicroseconds();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message StrategyOldest
     *&#64;&#64;
     *&#64;&#64;     The sequence batcher maintains up to 'max_candidate_sequences'
     *&#64;&#64;     candidate sequences. 'max_candidate_sequences' can be greater
     *&#64;&#64;     than the model's 'max_batch_size'. For inferencing the batcher
     *&#64;&#64;     chooses from the candidate sequences up to 'max_batch_size'
     *&#64;&#64;     inference requests. Requests are chosen in an oldest-first
     *&#64;&#64;     manner across all candidate sequences. A given sequence is
     *&#64;&#64;     not guaranteed to be assigned to the same batch slot for
     *&#64;&#64;     all inference requests of that sequence.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest}
     */
    public  static final class StrategyOldest extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest)
        StrategyOldestOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use StrategyOldest.newBuilder() to construct.
      private StrategyOldest(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private StrategyOldest() {
        maxCandidateSequences_ = 0;
        preferredBatchSize_ = java.util.Collections.emptyList();
        maxQueueDelayMicroseconds_ = 0L;
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private StrategyOldest(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                maxCandidateSequences_ = input.readInt32();
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                  preferredBatchSize_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000002;
                }
                preferredBatchSize_.add(input.readInt32());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002) && input.getBytesUntilLimit() > 0) {
                  preferredBatchSize_ = new java.util.ArrayList<java.lang.Integer>();
                  mutable_bitField0_ |= 0x00000002;
                }
                while (input.getBytesUntilLimit() > 0) {
                  preferredBatchSize_.add(input.readInt32());
                }
                input.popLimit(limit);
                break;
              }
              case 24: {

                maxQueueDelayMicroseconds_ = input.readUInt64();
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
            preferredBatchSize_ = java.util.Collections.unmodifiableList(preferredBatchSize_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder.class);
      }

      private int bitField0_;
      public static final int MAX_CANDIDATE_SEQUENCES_FIELD_NUMBER = 1;
      private int maxCandidateSequences_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
       *&#64;&#64;
       *&#64;&#64;       Maximum number of candidate sequences that the batcher
       *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
       *&#64;&#64;       and become candidates when existing candidate sequences
       *&#64;&#64;       complete.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_candidate_sequences = 1;</code>
       */
      public int getMaxCandidateSequences() {
        return maxCandidateSequences_;
      }

      public static final int PREFERRED_BATCH_SIZE_FIELD_NUMBER = 2;
      private java.util.List<java.lang.Integer> preferredBatchSize_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately.  If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       */
      public java.util.List<java.lang.Integer>
          getPreferredBatchSizeList() {
        return preferredBatchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately.  If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       */
      public int getPreferredBatchSizeCount() {
        return preferredBatchSize_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately.  If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       */
      public int getPreferredBatchSize(int index) {
        return preferredBatchSize_.get(index);
      }
      private int preferredBatchSizeMemoizedSerializedSize = -1;

      public static final int MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER = 3;
      private long maxQueueDelayMicroseconds_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 3;</code>
       */
      public long getMaxQueueDelayMicroseconds() {
        return maxQueueDelayMicroseconds_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (maxCandidateSequences_ != 0) {
          output.writeInt32(1, maxCandidateSequences_);
        }
        if (getPreferredBatchSizeList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(preferredBatchSizeMemoizedSerializedSize);
        }
        for (int i = 0; i < preferredBatchSize_.size(); i++) {
          output.writeInt32NoTag(preferredBatchSize_.get(i));
        }
        if (maxQueueDelayMicroseconds_ != 0L) {
          output.writeUInt64(3, maxQueueDelayMicroseconds_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (maxCandidateSequences_ != 0) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt32Size(1, maxCandidateSequences_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < preferredBatchSize_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(preferredBatchSize_.get(i));
          }
          size += dataSize;
          if (!getPreferredBatchSizeList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          preferredBatchSizeMemoizedSerializedSize = dataSize;
        }
        if (maxQueueDelayMicroseconds_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(3, maxQueueDelayMicroseconds_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) obj;

        boolean result = true;
        result = result && (getMaxCandidateSequences()
            == other.getMaxCandidateSequences());
        result = result && getPreferredBatchSizeList()
            .equals(other.getPreferredBatchSizeList());
        result = result && (getMaxQueueDelayMicroseconds()
            == other.getMaxQueueDelayMicroseconds());
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + MAX_CANDIDATE_SEQUENCES_FIELD_NUMBER;
        hash = (53 * hash) + getMaxCandidateSequences();
        if (getPreferredBatchSizeCount() > 0) {
          hash = (37 * hash) + PREFERRED_BATCH_SIZE_FIELD_NUMBER;
          hash = (53 * hash) + getPreferredBatchSizeList().hashCode();
        }
        hash = (37 * hash) + MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getMaxQueueDelayMicroseconds());
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message StrategyOldest
       *&#64;&#64;
       *&#64;&#64;     The sequence batcher maintains up to 'max_candidate_sequences'
       *&#64;&#64;     candidate sequences. 'max_candidate_sequences' can be greater
       *&#64;&#64;     than the model's 'max_batch_size'. For inferencing the batcher
       *&#64;&#64;     chooses from the candidate sequences up to 'max_batch_size'
       *&#64;&#64;     inference requests. Requests are chosen in an oldest-first
       *&#64;&#64;     manner across all candidate sequences. A given sequence is
       *&#64;&#64;     not guaranteed to be assigned to the same batch slot for
       *&#64;&#64;     all inference requests of that sequence.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          maxCandidateSequences_ = 0;

          preferredBatchSize_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          maxQueueDelayMicroseconds_ = 0L;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          result.maxCandidateSequences_ = maxCandidateSequences_;
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            preferredBatchSize_ = java.util.Collections.unmodifiableList(preferredBatchSize_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.preferredBatchSize_ = preferredBatchSize_;
          result.maxQueueDelayMicroseconds_ = maxQueueDelayMicroseconds_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance()) return this;
          if (other.getMaxCandidateSequences() != 0) {
            setMaxCandidateSequences(other.getMaxCandidateSequences());
          }
          if (!other.preferredBatchSize_.isEmpty()) {
            if (preferredBatchSize_.isEmpty()) {
              preferredBatchSize_ = other.preferredBatchSize_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensurePreferredBatchSizeIsMutable();
              preferredBatchSize_.addAll(other.preferredBatchSize_);
            }
            onChanged();
          }
          if (other.getMaxQueueDelayMicroseconds() != 0L) {
            setMaxQueueDelayMicroseconds(other.getMaxQueueDelayMicroseconds());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private int maxCandidateSequences_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
         *&#64;&#64;
         *&#64;&#64;       Maximum number of candidate sequences that the batcher
         *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
         *&#64;&#64;       and become candidates when existing candidate sequences
         *&#64;&#64;       complete.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 max_candidate_sequences = 1;</code>
         */
        public int getMaxCandidateSequences() {
          return maxCandidateSequences_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
         *&#64;&#64;
         *&#64;&#64;       Maximum number of candidate sequences that the batcher
         *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
         *&#64;&#64;       and become candidates when existing candidate sequences
         *&#64;&#64;       complete.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 max_candidate_sequences = 1;</code>
         */
        public Builder setMaxCandidateSequences(int value) {
          
          maxCandidateSequences_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
         *&#64;&#64;
         *&#64;&#64;       Maximum number of candidate sequences that the batcher
         *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
         *&#64;&#64;       and become candidates when existing candidate sequences
         *&#64;&#64;       complete.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 max_candidate_sequences = 1;</code>
         */
        public Builder clearMaxCandidateSequences() {
          
          maxCandidateSequences_ = 0;
          onChanged();
          return this;
        }

        private java.util.List<java.lang.Integer> preferredBatchSize_ = java.util.Collections.emptyList();
        private void ensurePreferredBatchSizeIsMutable() {
          if (!((bitField0_ & 0x00000002) == 0x00000002)) {
            preferredBatchSize_ = new java.util.ArrayList<java.lang.Integer>(preferredBatchSize_);
            bitField0_ |= 0x00000002;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately.  If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         */
        public java.util.List<java.lang.Integer>
            getPreferredBatchSizeList() {
          return java.util.Collections.unmodifiableList(preferredBatchSize_);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately.  If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         */
        public int getPreferredBatchSizeCount() {
          return preferredBatchSize_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately.  If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         */
        public int getPreferredBatchSize(int index) {
          return preferredBatchSize_.get(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately.  If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         */
        public Builder setPreferredBatchSize(
            int index, int value) {
          ensurePreferredBatchSizeIsMutable();
          preferredBatchSize_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately.  If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         */
        public Builder addPreferredBatchSize(int value) {
          ensurePreferredBatchSizeIsMutable();
          preferredBatchSize_.add(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately.  If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         */
        public Builder addAllPreferredBatchSize(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensurePreferredBatchSizeIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, preferredBatchSize_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately.  If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         */
        public Builder clearPreferredBatchSize() {
          preferredBatchSize_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
          return this;
        }

        private long maxQueueDelayMicroseconds_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 3;</code>
         */
        public long getMaxQueueDelayMicroseconds() {
          return maxQueueDelayMicroseconds_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 3;</code>
         */
        public Builder setMaxQueueDelayMicroseconds(long value) {
          
          maxQueueDelayMicroseconds_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 3;</code>
         */
        public Builder clearMaxQueueDelayMicroseconds() {
          
          maxQueueDelayMicroseconds_ = 0L;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<StrategyOldest>
          PARSER = new com.google.protobuf.AbstractParser<StrategyOldest>() {
        @java.lang.Override
        public StrategyOldest parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new StrategyOldest(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<StrategyOldest> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<StrategyOldest> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    private int bitField0_;
    private int strategyChoiceCase_ = 0;
    private java.lang.Object strategyChoice_;
    public enum StrategyChoiceCase
        implements com.google.protobuf.Internal.EnumLite {
      DIRECT(3),
      OLDEST(4),
      STRATEGYCHOICE_NOT_SET(0);
      private final int value;
      private StrategyChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static StrategyChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static StrategyChoiceCase forNumber(int value) {
        switch (value) {
          case 3: return DIRECT;
          case 4: return OLDEST;
          case 0: return STRATEGYCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public StrategyChoiceCase
    getStrategyChoiceCase() {
      return StrategyChoiceCase.forNumber(
          strategyChoiceCase_);
    }

    public static final int DIRECT_FIELD_NUMBER = 3;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    public boolean hasDirect() {
      return strategyChoiceCase_ == 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDirect() {
      if (strategyChoiceCase_ == 3) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder getDirectOrBuilder() {
      if (strategyChoiceCase_ == 3) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
    }

    public static final int OLDEST_FIELD_NUMBER = 4;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    public boolean hasOldest() {
      return strategyChoiceCase_ == 4;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getOldest() {
      if (strategyChoiceCase_ == 4) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder getOldestOrBuilder() {
      if (strategyChoiceCase_ == 4) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
    }

    public static final int MAX_SEQUENCE_IDLE_MICROSECONDS_FIELD_NUMBER = 1;
    private long maxSequenceIdleMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     */
    public long getMaxSequenceIdleMicroseconds() {
      return maxSequenceIdleMicroseconds_;
    }

    public static final int CONTROL_INPUT_FIELD_NUMBER = 2;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> controlInput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> getControlInputList() {
      return controlInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
        getControlInputOrBuilderList() {
      return controlInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public int getControlInputCount() {
      return controlInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index) {
      return controlInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
        int index) {
      return controlInput_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (maxSequenceIdleMicroseconds_ != 0L) {
        output.writeUInt64(1, maxSequenceIdleMicroseconds_);
      }
      for (int i = 0; i < controlInput_.size(); i++) {
        output.writeMessage(2, controlInput_.get(i));
      }
      if (strategyChoiceCase_ == 3) {
        output.writeMessage(3, (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_);
      }
      if (strategyChoiceCase_ == 4) {
        output.writeMessage(4, (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (maxSequenceIdleMicroseconds_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, maxSequenceIdleMicroseconds_);
      }
      for (int i = 0; i < controlInput_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, controlInput_.get(i));
      }
      if (strategyChoiceCase_ == 3) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_);
      }
      if (strategyChoiceCase_ == 4) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) obj;

      boolean result = true;
      result = result && (getMaxSequenceIdleMicroseconds()
          == other.getMaxSequenceIdleMicroseconds());
      result = result && getControlInputList()
          .equals(other.getControlInputList());
      result = result && getStrategyChoiceCase().equals(
          other.getStrategyChoiceCase());
      if (!result) return false;
      switch (strategyChoiceCase_) {
        case 3:
          result = result && getDirect()
              .equals(other.getDirect());
          break;
        case 4:
          result = result && getOldest()
              .equals(other.getOldest());
          break;
        case 0:
        default:
      }
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + MAX_SEQUENCE_IDLE_MICROSECONDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMaxSequenceIdleMicroseconds());
      if (getControlInputCount() > 0) {
        hash = (37 * hash) + CONTROL_INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getControlInputList().hashCode();
      }
      switch (strategyChoiceCase_) {
        case 3:
          hash = (37 * hash) + DIRECT_FIELD_NUMBER;
          hash = (53 * hash) + getDirect().hashCode();
          break;
        case 4:
          hash = (37 * hash) + OLDEST_FIELD_NUMBER;
          hash = (53 * hash) + getOldest().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelSequenceBatching
     *&#64;&#64;
     *&#64;&#64;   Sequence batching configuration. These settings control how sequence
     *&#64;&#64;   batching operates for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelSequenceBatching}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelSequenceBatching)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getControlInputFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        maxSequenceIdleMicroseconds_ = 0L;

        if (controlInputBuilder_ == null) {
          controlInput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          controlInputBuilder_.clear();
        }
        strategyChoiceCase_ = 0;
        strategyChoice_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (strategyChoiceCase_ == 3) {
          if (directBuilder_ == null) {
            result.strategyChoice_ = strategyChoice_;
          } else {
            result.strategyChoice_ = directBuilder_.build();
          }
        }
        if (strategyChoiceCase_ == 4) {
          if (oldestBuilder_ == null) {
            result.strategyChoice_ = strategyChoice_;
          } else {
            result.strategyChoice_ = oldestBuilder_.build();
          }
        }
        result.maxSequenceIdleMicroseconds_ = maxSequenceIdleMicroseconds_;
        if (controlInputBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008)) {
            controlInput_ = java.util.Collections.unmodifiableList(controlInput_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.controlInput_ = controlInput_;
        } else {
          result.controlInput_ = controlInputBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        result.strategyChoiceCase_ = strategyChoiceCase_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance()) return this;
        if (other.getMaxSequenceIdleMicroseconds() != 0L) {
          setMaxSequenceIdleMicroseconds(other.getMaxSequenceIdleMicroseconds());
        }
        if (controlInputBuilder_ == null) {
          if (!other.controlInput_.isEmpty()) {
            if (controlInput_.isEmpty()) {
              controlInput_ = other.controlInput_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureControlInputIsMutable();
              controlInput_.addAll(other.controlInput_);
            }
            onChanged();
          }
        } else {
          if (!other.controlInput_.isEmpty()) {
            if (controlInputBuilder_.isEmpty()) {
              controlInputBuilder_.dispose();
              controlInputBuilder_ = null;
              controlInput_ = other.controlInput_;
              bitField0_ = (bitField0_ & ~0x00000008);
              controlInputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getControlInputFieldBuilder() : null;
            } else {
              controlInputBuilder_.addAllMessages(other.controlInput_);
            }
          }
        }
        switch (other.getStrategyChoiceCase()) {
          case DIRECT: {
            mergeDirect(other.getDirect());
            break;
          }
          case OLDEST: {
            mergeOldest(other.getOldest());
            break;
          }
          case STRATEGYCHOICE_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int strategyChoiceCase_ = 0;
      private java.lang.Object strategyChoice_;
      public StrategyChoiceCase
          getStrategyChoiceCase() {
        return StrategyChoiceCase.forNumber(
            strategyChoiceCase_);
      }

      public Builder clearStrategyChoice() {
        strategyChoiceCase_ = 0;
        strategyChoice_ = null;
        onChanged();
        return this;
      }

      private int bitField0_;

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder> directBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public boolean hasDirect() {
        return strategyChoiceCase_ == 3;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDirect() {
        if (directBuilder_ == null) {
          if (strategyChoiceCase_ == 3) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
        } else {
          if (strategyChoiceCase_ == 3) {
            return directBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder setDirect(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect value) {
        if (directBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          strategyChoice_ = value;
          onChanged();
        } else {
          directBuilder_.setMessage(value);
        }
        strategyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder setDirect(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder builderForValue) {
        if (directBuilder_ == null) {
          strategyChoice_ = builderForValue.build();
          onChanged();
        } else {
          directBuilder_.setMessage(builderForValue.build());
        }
        strategyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder mergeDirect(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect value) {
        if (directBuilder_ == null) {
          if (strategyChoiceCase_ == 3 &&
              strategyChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance()) {
            strategyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            strategyChoice_ = value;
          }
          onChanged();
        } else {
          if (strategyChoiceCase_ == 3) {
            directBuilder_.mergeFrom(value);
          }
          directBuilder_.setMessage(value);
        }
        strategyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder clearDirect() {
        if (directBuilder_ == null) {
          if (strategyChoiceCase_ == 3) {
            strategyChoiceCase_ = 0;
            strategyChoice_ = null;
            onChanged();
          }
        } else {
          if (strategyChoiceCase_ == 3) {
            strategyChoiceCase_ = 0;
            strategyChoice_ = null;
          }
          directBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder getDirectBuilder() {
        return getDirectFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder getDirectOrBuilder() {
        if ((strategyChoiceCase_ == 3) && (directBuilder_ != null)) {
          return directBuilder_.getMessageOrBuilder();
        } else {
          if (strategyChoiceCase_ == 3) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder> 
          getDirectFieldBuilder() {
        if (directBuilder_ == null) {
          if (!(strategyChoiceCase_ == 3)) {
            strategyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
          }
          directBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_,
                  getParentForChildren(),
                  isClean());
          strategyChoice_ = null;
        }
        strategyChoiceCase_ = 3;
        onChanged();;
        return directBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder> oldestBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public boolean hasOldest() {
        return strategyChoiceCase_ == 4;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getOldest() {
        if (oldestBuilder_ == null) {
          if (strategyChoiceCase_ == 4) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
        } else {
          if (strategyChoiceCase_ == 4) {
            return oldestBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder setOldest(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest value) {
        if (oldestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          strategyChoice_ = value;
          onChanged();
        } else {
          oldestBuilder_.setMessage(value);
        }
        strategyChoiceCase_ = 4;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder setOldest(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder builderForValue) {
        if (oldestBuilder_ == null) {
          strategyChoice_ = builderForValue.build();
          onChanged();
        } else {
          oldestBuilder_.setMessage(builderForValue.build());
        }
        strategyChoiceCase_ = 4;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder mergeOldest(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest value) {
        if (oldestBuilder_ == null) {
          if (strategyChoiceCase_ == 4 &&
              strategyChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance()) {
            strategyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            strategyChoice_ = value;
          }
          onChanged();
        } else {
          if (strategyChoiceCase_ == 4) {
            oldestBuilder_.mergeFrom(value);
          }
          oldestBuilder_.setMessage(value);
        }
        strategyChoiceCase_ = 4;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder clearOldest() {
        if (oldestBuilder_ == null) {
          if (strategyChoiceCase_ == 4) {
            strategyChoiceCase_ = 0;
            strategyChoice_ = null;
            onChanged();
          }
        } else {
          if (strategyChoiceCase_ == 4) {
            strategyChoiceCase_ = 0;
            strategyChoice_ = null;
          }
          oldestBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder getOldestBuilder() {
        return getOldestFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder getOldestOrBuilder() {
        if ((strategyChoiceCase_ == 4) && (oldestBuilder_ != null)) {
          return oldestBuilder_.getMessageOrBuilder();
        } else {
          if (strategyChoiceCase_ == 4) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder> 
          getOldestFieldBuilder() {
        if (oldestBuilder_ == null) {
          if (!(strategyChoiceCase_ == 4)) {
            strategyChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
          }
          oldestBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_,
                  getParentForChildren(),
                  isClean());
          strategyChoice_ = null;
        }
        strategyChoiceCase_ = 4;
        onChanged();;
        return oldestBuilder_;
      }

      private long maxSequenceIdleMicroseconds_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       */
      public long getMaxSequenceIdleMicroseconds() {
        return maxSequenceIdleMicroseconds_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       */
      public Builder setMaxSequenceIdleMicroseconds(long value) {
        
        maxSequenceIdleMicroseconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       */
      public Builder clearMaxSequenceIdleMicroseconds() {
        
        maxSequenceIdleMicroseconds_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> controlInput_ =
        java.util.Collections.emptyList();
      private void ensureControlInputIsMutable() {
        if (!((bitField0_ & 0x00000008) == 0x00000008)) {
          controlInput_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput>(controlInput_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> controlInputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> getControlInputList() {
        if (controlInputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(controlInput_);
        } else {
          return controlInputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public int getControlInputCount() {
        if (controlInputBuilder_ == null) {
          return controlInput_.size();
        } else {
          return controlInputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index) {
        if (controlInputBuilder_ == null) {
          return controlInput_.get(index);
        } else {
          return controlInputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder setControlInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        if (controlInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureControlInputIsMutable();
          controlInput_.set(index, value);
          onChanged();
        } else {
          controlInputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder setControlInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.set(index, builderForValue.build());
          onChanged();
        } else {
          controlInputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        if (controlInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureControlInputIsMutable();
          controlInput_.add(value);
          onChanged();
        } else {
          controlInputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        if (controlInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureControlInputIsMutable();
          controlInput_.add(index, value);
          onChanged();
        } else {
          controlInputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.add(builderForValue.build());
          onChanged();
        } else {
          controlInputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.add(index, builderForValue.build());
          onChanged();
        } else {
          controlInputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addAllControlInput(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> values) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, controlInput_);
          onChanged();
        } else {
          controlInputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder clearControlInput() {
        if (controlInputBuilder_ == null) {
          controlInput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          controlInputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder removeControlInput(int index) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.remove(index);
          onChanged();
        } else {
          controlInputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder getControlInputBuilder(
          int index) {
        return getControlInputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
          int index) {
        if (controlInputBuilder_ == null) {
          return controlInput_.get(index);  } else {
          return controlInputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
           getControlInputOrBuilderList() {
        if (controlInputBuilder_ != null) {
          return controlInputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(controlInput_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder addControlInputBuilder() {
        return getControlInputFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder addControlInputBuilder(
          int index) {
        return getControlInputFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder> 
           getControlInputBuilderList() {
        return getControlInputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
          getControlInputFieldBuilder() {
        if (controlInputBuilder_ == null) {
          controlInputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder>(
                  controlInput_,
                  ((bitField0_ & 0x00000008) == 0x00000008),
                  getParentForChildren(),
                  isClean());
          controlInput_ = null;
        }
        return controlInputBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelSequenceBatching)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelSequenceBatching>
        PARSER = new com.google.protobuf.AbstractParser<ModelSequenceBatching>() {
      @java.lang.Override
      public ModelSequenceBatching parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelSequenceBatching(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelSequenceBatching> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelSequenceBatching> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelEnsemblingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelEnsembling)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> 
        getStepList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    int getStepCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
        getStepOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
        int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelEnsembling
   *&#64;&#64;
   *&#64;&#64;   Model ensembling configuration. These settings specify the models that
   *&#64;&#64;   compose the ensemble and how data flows between the models.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelEnsembling}
   */
  public  static final class ModelEnsembling extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelEnsembling)
      ModelEnsemblingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelEnsembling.newBuilder() to construct.
    private ModelEnsembling(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelEnsembling() {
      step_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelEnsembling(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
                step_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step>();
                mutable_bitField0_ |= 0x00000001;
              }
              step_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) == 0x00000001)) {
          step_ = java.util.Collections.unmodifiableList(step_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder.class);
    }

    public interface StepOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelEnsembling.Step)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      java.lang.String getModelName();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      com.google.protobuf.ByteString
          getModelNameBytes();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       */
      long getModelVersion();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      int getInputMapCount();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      boolean containsInputMap(
          java.lang.String key);
      /**
       * Use {@link #getInputMapMap()} instead.
       */
      @java.lang.Deprecated
      java.util.Map<java.lang.String, java.lang.String>
      getInputMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      java.util.Map<java.lang.String, java.lang.String>
      getInputMapMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      java.lang.String getInputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue);
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      java.lang.String getInputMapOrThrow(
          java.lang.String key);

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      int getOutputMapCount();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      boolean containsOutputMap(
          java.lang.String key);
      /**
       * Use {@link #getOutputMapMap()} instead.
       */
      @java.lang.Deprecated
      java.util.Map<java.lang.String, java.lang.String>
      getOutputMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      java.util.Map<java.lang.String, java.lang.String>
      getOutputMapMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      java.lang.String getOutputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue);
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      java.lang.String getOutputMapOrThrow(
          java.lang.String key);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Step
     *&#64;&#64;
     *&#64;&#64;     Each step specifies a model included in the ensemble,
     *&#64;&#64;     maps ensemble tensor names to the model input tensors,
     *&#64;&#64;     and maps model output tensors to ensemble tensor names
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelEnsembling.Step}
     */
    public  static final class Step extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelEnsembling.Step)
        StepOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Step.newBuilder() to construct.
      private Step(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Step() {
        modelName_ = "";
        modelVersion_ = 0L;
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Step(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                modelName_ = s;
                break;
              }
              case 16: {

                modelVersion_ = input.readInt64();
                break;
              }
              case 26: {
                if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                  inputMap_ = com.google.protobuf.MapField.newMapField(
                      InputMapDefaultEntryHolder.defaultEntry);
                  mutable_bitField0_ |= 0x00000004;
                }
                com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
                inputMap__ = input.readMessage(
                    InputMapDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                inputMap_.getMutableMap().put(
                    inputMap__.getKey(), inputMap__.getValue());
                break;
              }
              case 34: {
                if (!((mutable_bitField0_ & 0x00000008) == 0x00000008)) {
                  outputMap_ = com.google.protobuf.MapField.newMapField(
                      OutputMapDefaultEntryHolder.defaultEntry);
                  mutable_bitField0_ |= 0x00000008;
                }
                com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
                outputMap__ = input.readMessage(
                    OutputMapDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                outputMap_.getMutableMap().put(
                    outputMap__.getKey(), outputMap__.getValue());
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      @java.lang.Override
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetInputMap();
          case 4:
            return internalGetOutputMap();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder.class);
      }

      private int bitField0_;
      public static final int MODEL_NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object modelName_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      public java.lang.String getModelName() {
        java.lang.Object ref = modelName_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          modelName_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getModelNameBytes() {
        java.lang.Object ref = modelName_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          modelName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int MODEL_VERSION_FIELD_NUMBER = 2;
      private long modelVersion_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       */
      public long getModelVersion() {
        return modelVersion_;
      }

      public static final int INPUT_MAP_FIELD_NUMBER = 3;
      private static final class InputMapDefaultEntryHolder {
        static final com.google.protobuf.MapEntry<
            java.lang.String, java.lang.String> defaultEntry =
                com.google.protobuf.MapEntry
                .<java.lang.String, java.lang.String>newDefaultInstance(
                    nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_descriptor, 
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "");
      }
      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> inputMap_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetInputMap() {
        if (inputMap_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              InputMapDefaultEntryHolder.defaultEntry);
        }
        return inputMap_;
      }

      public int getInputMapCount() {
        return internalGetInputMap().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      public boolean containsInputMap(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetInputMap().getMap().containsKey(key);
      }
      /**
       * Use {@link #getInputMapMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getInputMap() {
        return getInputMapMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getInputMapMap() {
        return internalGetInputMap().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      public java.lang.String getInputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetInputMap().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      public java.lang.String getInputMapOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetInputMap().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public static final int OUTPUT_MAP_FIELD_NUMBER = 4;
      private static final class OutputMapDefaultEntryHolder {
        static final com.google.protobuf.MapEntry<
            java.lang.String, java.lang.String> defaultEntry =
                com.google.protobuf.MapEntry
                .<java.lang.String, java.lang.String>newDefaultInstance(
                    nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_descriptor, 
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "");
      }
      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> outputMap_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetOutputMap() {
        if (outputMap_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              OutputMapDefaultEntryHolder.defaultEntry);
        }
        return outputMap_;
      }

      public int getOutputMapCount() {
        return internalGetOutputMap().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      public boolean containsOutputMap(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetOutputMap().getMap().containsKey(key);
      }
      /**
       * Use {@link #getOutputMapMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getOutputMap() {
        return getOutputMapMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getOutputMapMap() {
        return internalGetOutputMap().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      public java.lang.String getOutputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetOutputMap().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      public java.lang.String getOutputMapOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetOutputMap().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!getModelNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, modelName_);
        }
        if (modelVersion_ != 0L) {
          output.writeInt64(2, modelVersion_);
        }
        com.google.protobuf.GeneratedMessageV3
          .serializeStringMapTo(
            output,
            internalGetInputMap(),
            InputMapDefaultEntryHolder.defaultEntry,
            3);
        com.google.protobuf.GeneratedMessageV3
          .serializeStringMapTo(
            output,
            internalGetOutputMap(),
            OutputMapDefaultEntryHolder.defaultEntry,
            4);
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getModelNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, modelName_);
        }
        if (modelVersion_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt64Size(2, modelVersion_);
        }
        for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
             : internalGetInputMap().getMap().entrySet()) {
          com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
          inputMap__ = InputMapDefaultEntryHolder.defaultEntry.newBuilderForType()
              .setKey(entry.getKey())
              .setValue(entry.getValue())
              .build();
          size += com.google.protobuf.CodedOutputStream
              .computeMessageSize(3, inputMap__);
        }
        for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
             : internalGetOutputMap().getMap().entrySet()) {
          com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
          outputMap__ = OutputMapDefaultEntryHolder.defaultEntry.newBuilderForType()
              .setKey(entry.getKey())
              .setValue(entry.getValue())
              .build();
          size += com.google.protobuf.CodedOutputStream
              .computeMessageSize(4, outputMap__);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step) obj;

        boolean result = true;
        result = result && getModelName()
            .equals(other.getModelName());
        result = result && (getModelVersion()
            == other.getModelVersion());
        result = result && internalGetInputMap().equals(
            other.internalGetInputMap());
        result = result && internalGetOutputMap().equals(
            other.internalGetOutputMap());
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + MODEL_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getModelName().hashCode();
        hash = (37 * hash) + MODEL_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getModelVersion());
        if (!internalGetInputMap().getMap().isEmpty()) {
          hash = (37 * hash) + INPUT_MAP_FIELD_NUMBER;
          hash = (53 * hash) + internalGetInputMap().hashCode();
        }
        if (!internalGetOutputMap().getMap().isEmpty()) {
          hash = (37 * hash) + OUTPUT_MAP_FIELD_NUMBER;
          hash = (53 * hash) + internalGetOutputMap().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Step
       *&#64;&#64;
       *&#64;&#64;     Each step specifies a model included in the ensemble,
       *&#64;&#64;     maps ensemble tensor names to the model input tensors,
       *&#64;&#64;     and maps model output tensors to ensemble tensor names
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelEnsembling.Step}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelEnsembling.Step)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor;
        }

        @SuppressWarnings({"rawtypes"})
        protected com.google.protobuf.MapField internalGetMapField(
            int number) {
          switch (number) {
            case 3:
              return internalGetInputMap();
            case 4:
              return internalGetOutputMap();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @SuppressWarnings({"rawtypes"})
        protected com.google.protobuf.MapField internalGetMutableMapField(
            int number) {
          switch (number) {
            case 3:
              return internalGetMutableInputMap();
            case 4:
              return internalGetMutableOutputMap();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          modelName_ = "";

          modelVersion_ = 0L;

          internalGetMutableInputMap().clear();
          internalGetMutableOutputMap().clear();
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          result.modelName_ = modelName_;
          result.modelVersion_ = modelVersion_;
          result.inputMap_ = internalGetInputMap();
          result.inputMap_.makeImmutable();
          result.outputMap_ = internalGetOutputMap();
          result.outputMap_.makeImmutable();
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance()) return this;
          if (!other.getModelName().isEmpty()) {
            modelName_ = other.modelName_;
            onChanged();
          }
          if (other.getModelVersion() != 0L) {
            setModelVersion(other.getModelVersion());
          }
          internalGetMutableInputMap().mergeFrom(
              other.internalGetInputMap());
          internalGetMutableOutputMap().mergeFrom(
              other.internalGetOutputMap());
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.lang.Object modelName_ = "";
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public java.lang.String getModelName() {
          java.lang.Object ref = modelName_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            modelName_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public com.google.protobuf.ByteString
            getModelNameBytes() {
          java.lang.Object ref = modelName_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            modelName_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public Builder setModelName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          modelName_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public Builder clearModelName() {
          
          modelName_ = getDefaultInstance().getModelName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         */
        public Builder setModelNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          modelName_ = value;
          onChanged();
          return this;
        }

        private long modelVersion_ ;
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         */
        public long getModelVersion() {
          return modelVersion_;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         */
        public Builder setModelVersion(long value) {
          
          modelVersion_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         */
        public Builder clearModelVersion() {
          
          modelVersion_ = 0L;
          onChanged();
          return this;
        }

        private com.google.protobuf.MapField<
            java.lang.String, java.lang.String> inputMap_;
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetInputMap() {
          if (inputMap_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                InputMapDefaultEntryHolder.defaultEntry);
          }
          return inputMap_;
        }
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetMutableInputMap() {
          onChanged();;
          if (inputMap_ == null) {
            inputMap_ = com.google.protobuf.MapField.newMapField(
                InputMapDefaultEntryHolder.defaultEntry);
          }
          if (!inputMap_.isMutable()) {
            inputMap_ = inputMap_.copy();
          }
          return inputMap_;
        }

        public int getInputMapCount() {
          return internalGetInputMap().getMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public boolean containsInputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          return internalGetInputMap().getMap().containsKey(key);
        }
        /**
         * Use {@link #getInputMapMap()} instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getInputMap() {
          return getInputMapMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public java.util.Map<java.lang.String, java.lang.String> getInputMapMap() {
          return internalGetInputMap().getMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public java.lang.String getInputMapOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetInputMap().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public java.lang.String getInputMapOrThrow(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetInputMap().getMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }

        public Builder clearInputMap() {
          internalGetMutableInputMap().getMutableMap()
              .clear();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public Builder removeInputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableInputMap().getMutableMap()
              .remove(key);
          return this;
        }
        /**
         * Use alternate mutation accessors instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String>
        getMutableInputMap() {
          return internalGetMutableInputMap().getMutableMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        public Builder putInputMap(
            java.lang.String key,
            java.lang.String value) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          if (value == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableInputMap().getMutableMap()
              .put(key, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public Builder putAllInputMap(
            java.util.Map<java.lang.String, java.lang.String> values) {
          internalGetMutableInputMap().getMutableMap()
              .putAll(values);
          return this;
        }

        private com.google.protobuf.MapField<
            java.lang.String, java.lang.String> outputMap_;
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetOutputMap() {
          if (outputMap_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                OutputMapDefaultEntryHolder.defaultEntry);
          }
          return outputMap_;
        }
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetMutableOutputMap() {
          onChanged();;
          if (outputMap_ == null) {
            outputMap_ = com.google.protobuf.MapField.newMapField(
                OutputMapDefaultEntryHolder.defaultEntry);
          }
          if (!outputMap_.isMutable()) {
            outputMap_ = outputMap_.copy();
          }
          return outputMap_;
        }

        public int getOutputMapCount() {
          return internalGetOutputMap().getMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public boolean containsOutputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          return internalGetOutputMap().getMap().containsKey(key);
        }
        /**
         * Use {@link #getOutputMapMap()} instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getOutputMap() {
          return getOutputMapMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public java.util.Map<java.lang.String, java.lang.String> getOutputMapMap() {
          return internalGetOutputMap().getMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public java.lang.String getOutputMapOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetOutputMap().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public java.lang.String getOutputMapOrThrow(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetOutputMap().getMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }

        public Builder clearOutputMap() {
          internalGetMutableOutputMap().getMutableMap()
              .clear();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public Builder removeOutputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableOutputMap().getMutableMap()
              .remove(key);
          return this;
        }
        /**
         * Use alternate mutation accessors instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String>
        getMutableOutputMap() {
          return internalGetMutableOutputMap().getMutableMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        public Builder putOutputMap(
            java.lang.String key,
            java.lang.String value) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          if (value == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableOutputMap().getMutableMap()
              .put(key, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public Builder putAllOutputMap(
            java.util.Map<java.lang.String, java.lang.String> values) {
          internalGetMutableOutputMap().getMutableMap()
              .putAll(values);
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelEnsembling.Step)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling.Step)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Step>
          PARSER = new com.google.protobuf.AbstractParser<Step>() {
        @java.lang.Override
        public Step parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Step(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Step> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Step> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int STEP_FIELD_NUMBER = 1;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> step_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> getStepList() {
      return step_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
        getStepOrBuilderList() {
      return step_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public int getStepCount() {
      return step_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index) {
      return step_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
        int index) {
      return step_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < step_.size(); i++) {
        output.writeMessage(1, step_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < step_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, step_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) obj;

      boolean result = true;
      result = result && getStepList()
          .equals(other.getStepList());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getStepCount() > 0) {
        hash = (37 * hash) + STEP_FIELD_NUMBER;
        hash = (53 * hash) + getStepList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelEnsembling
     *&#64;&#64;
     *&#64;&#64;   Model ensembling configuration. These settings specify the models that
     *&#64;&#64;   compose the ensemble and how data flows between the models.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelEnsembling}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelEnsembling)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getStepFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (stepBuilder_ == null) {
          step_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          stepBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling(this);
        int from_bitField0_ = bitField0_;
        if (stepBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            step_ = java.util.Collections.unmodifiableList(step_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.step_ = step_;
        } else {
          result.step_ = stepBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance()) return this;
        if (stepBuilder_ == null) {
          if (!other.step_.isEmpty()) {
            if (step_.isEmpty()) {
              step_ = other.step_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureStepIsMutable();
              step_.addAll(other.step_);
            }
            onChanged();
          }
        } else {
          if (!other.step_.isEmpty()) {
            if (stepBuilder_.isEmpty()) {
              stepBuilder_.dispose();
              stepBuilder_ = null;
              step_ = other.step_;
              bitField0_ = (bitField0_ & ~0x00000001);
              stepBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getStepFieldBuilder() : null;
            } else {
              stepBuilder_.addAllMessages(other.step_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> step_ =
        java.util.Collections.emptyList();
      private void ensureStepIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          step_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step>(step_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> stepBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> getStepList() {
        if (stepBuilder_ == null) {
          return java.util.Collections.unmodifiableList(step_);
        } else {
          return stepBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public int getStepCount() {
        if (stepBuilder_ == null) {
          return step_.size();
        } else {
          return stepBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index) {
        if (stepBuilder_ == null) {
          return step_.get(index);
        } else {
          return stepBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder setStep(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step value) {
        if (stepBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStepIsMutable();
          step_.set(index, value);
          onChanged();
        } else {
          stepBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder setStep(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.set(index, builderForValue.build());
          onChanged();
        } else {
          stepBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step value) {
        if (stepBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStepIsMutable();
          step_.add(value);
          onChanged();
        } else {
          stepBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step value) {
        if (stepBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStepIsMutable();
          step_.add(index, value);
          onChanged();
        } else {
          stepBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.add(builderForValue.build());
          onChanged();
        } else {
          stepBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.add(index, builderForValue.build());
          onChanged();
        } else {
          stepBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addAllStep(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step> values) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, step_);
          onChanged();
        } else {
          stepBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder clearStep() {
        if (stepBuilder_ == null) {
          step_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          stepBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public Builder removeStep(int index) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.remove(index);
          onChanged();
        } else {
          stepBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder getStepBuilder(
          int index) {
        return getStepFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
          int index) {
        if (stepBuilder_ == null) {
          return step_.get(index);  } else {
          return stepBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
           getStepOrBuilderList() {
        if (stepBuilder_ != null) {
          return stepBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(step_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder addStepBuilder() {
        return getStepFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder addStepBuilder(
          int index) {
        return getStepFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelEnsembling.Step step = 1;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder> 
           getStepBuilderList() {
        return getStepFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
          getStepFieldBuilder() {
        if (stepBuilder_ == null) {
          stepBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Step.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder>(
                  step_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          step_ = null;
        }
        return stepBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelEnsembling)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelEnsembling>
        PARSER = new com.google.protobuf.AbstractParser<ModelEnsembling>() {
      @java.lang.Override
      public ModelEnsembling parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelEnsembling(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelEnsembling> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelEnsembling> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelParameterOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelParameter)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     */
    java.lang.String getStringValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     */
    com.google.protobuf.ByteString
        getStringValueBytes();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelParameter
   *&#64;&#64;
   *&#64;&#64;   A model parameter.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelParameter}
   */
  public  static final class ModelParameter extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelParameter)
      ModelParameterOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelParameter.newBuilder() to construct.
    private ModelParameter(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelParameter() {
      stringValue_ = "";
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelParameter(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              stringValue_ = s;
              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.Builder.class);
    }

    public static final int STRING_VALUE_FIELD_NUMBER = 1;
    private volatile java.lang.Object stringValue_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     */
    public java.lang.String getStringValue() {
      java.lang.Object ref = stringValue_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        stringValue_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     */
    public com.google.protobuf.ByteString
        getStringValueBytes() {
      java.lang.Object ref = stringValue_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        stringValue_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getStringValueBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, stringValue_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getStringValueBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, stringValue_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter) obj;

      boolean result = true;
      result = result && getStringValue()
          .equals(other.getStringValue());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STRING_VALUE_FIELD_NUMBER;
      hash = (53 * hash) + getStringValue().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelParameter
     *&#64;&#64;
     *&#64;&#64;   A model parameter.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelParameter}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelParameter)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameterOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        stringValue_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelParameter_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter(this);
        result.stringValue_ = stringValue_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.getDefaultInstance()) return this;
        if (!other.getStringValue().isEmpty()) {
          stringValue_ = other.stringValue_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object stringValue_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public java.lang.String getStringValue() {
        java.lang.Object ref = stringValue_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          stringValue_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public com.google.protobuf.ByteString
          getStringValueBytes() {
        java.lang.Object ref = stringValue_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          stringValue_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public Builder setStringValue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        stringValue_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public Builder clearStringValue() {
        
        stringValue_ = getDefaultInstance().getStringValue();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       */
      public Builder setStringValueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        stringValue_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelParameter)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelParameter)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelParameter>
        PARSER = new com.google.protobuf.AbstractParser<ModelParameter>() {
      @java.lang.Override
      public ModelParameter parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelParameter(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelParameter> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelParameter> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelWarmupOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelWarmup)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1. If
     *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
     *&#64;&#64;     match the batch size requested.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 2;</code>
     */
    int getBatchSize();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    int getInputsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    boolean containsInputs(
        java.lang.String key);
    /**
     * Use {@link #getInputsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input>
    getInputs();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input>
    getInputsMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input getInputsOrDefault(
        java.lang.String key,
        nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input getInputsOrThrow(
        java.lang.String key);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelWarmup
   *&#64;&#64;
   *&#64;&#64;   Settings used to construct the request sample for model warmup.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelWarmup}
   */
  public  static final class ModelWarmup extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelWarmup)
      ModelWarmupOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelWarmup.newBuilder() to construct.
    private ModelWarmup(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelWarmup() {
      name_ = "";
      batchSize_ = 0;
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelWarmup(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {

              batchSize_ = input.readUInt32();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000004) == 0x00000004)) {
                inputs_ = com.google.protobuf.MapField.newMapField(
                    InputsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000004;
              }
              com.google.protobuf.MapEntry<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input>
              inputs__ = input.readMessage(
                  InputsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              inputs_.getMutableMap().put(
                  inputs__.getKey(), inputs__.getValue());
              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 3:
          return internalGetInputs();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder.class);
    }

    public interface InputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelWarmup.Input)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 1;</code>
       */
      int getDataTypeValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 1;</code>
       */
      nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      java.util.List<java.lang.Long> getDimsList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      int getDimsCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      long getDims(int index);

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       */
      boolean getZeroData();

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       */
      boolean getRandomData();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       */
      java.lang.String getInputDataFile();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       */
      com.google.protobuf.ByteString
          getInputDataFileBytes();

      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input.InputDataTypeCase getInputDataTypeCase();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Input
     *&#64;&#64;
     *&#64;&#64;     Meta data associated with an input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelWarmup.Input}
     */
    public  static final class Input extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelWarmup.Input)
        InputOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Input.newBuilder() to construct.
      private Input(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Input() {
        dataType_ = 0;
        dims_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Input(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                int rawValue = input.readEnum();

                dataType_ = rawValue;
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
                  dims_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000002;
                }
                dims_.add(input.readInt64());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000002) == 0x00000002) && input.getBytesUntilLimit() > 0) {
                  dims_ = new java.util.ArrayList<java.lang.Long>();
                  mutable_bitField0_ |= 0x00000002;
                }
                while (input.getBytesUntilLimit() > 0) {
                  dims_.add(input.readInt64());
                }
                input.popLimit(limit);
                break;
              }
              case 24: {
                inputDataTypeCase_ = 3;
                inputDataType_ = input.readBool();
                break;
              }
              case 32: {
                inputDataTypeCase_ = 4;
                inputDataType_ = input.readBool();
                break;
              }
              case 42: {
                java.lang.String s = input.readStringRequireUtf8();
                inputDataTypeCase_ = 5;
                inputDataType_ = s;
                break;
              }
              default: {
                if (!parseUnknownFieldProto3(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000002) == 0x00000002)) {
            dims_ = java.util.Collections.unmodifiableList(dims_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_Input_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_Input_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input.Builder.class);
      }

      private int bitField0_;
      private int inputDataTypeCase_ = 0;
      private java.lang.Object inputDataType_;
      public enum InputDataTypeCase
          implements com.google.protobuf.Internal.EnumLite {
        ZERO_DATA(3),
        RANDOM_DATA(4),
        INPUT_DATA_FILE(5),
        INPUTDATATYPE_NOT_SET(0);
        private final int value;
        private InputDataTypeCase(int value) {
          this.value = value;
        }
        /**
         * @deprecated Use {@link #forNumber(int)} instead.
         */
        @java.lang.Deprecated
        public static InputDataTypeCase valueOf(int value) {
          return forNumber(value);
        }

        public static InputDataTypeCase forNumber(int value) {
          switch (value) {
            case 3: return ZERO_DATA;
            case 4: return RANDOM_DATA;
            case 5: return INPUT_DATA_FILE;
            case 0: return INPUTDATATYPE_NOT_SET;
            default: return null;
          }
        }
        public int getNumber() {
          return this.value;
        }
      };

      public InputDataTypeCase
      getInputDataTypeCase() {
        return InputDataTypeCase.forNumber(
            inputDataTypeCase_);
      }

      public static final int DATA_TYPE_FIELD_NUMBER = 1;
      private int dataType_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 1;</code>
       */
      public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.DataType data_type = 1;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }

      public static final int DIMS_FIELD_NUMBER = 2;
      private java.util.List<java.lang.Long> dims_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      public java.util.List<java.lang.Long>
          getDimsList() {
        return dims_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       */
      public long getDims(int index) {
        return dims_.get(index);
      }
      private int dimsMemoizedSerializedSize = -1;

      public static final int ZERO_DATA_FIELD_NUMBER = 3;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       */
      public boolean getZeroData() {
        if (inputDataTypeCase_ == 3) {
          return (java.lang.Boolean) inputDataType_;
        }
        return false;
      }

      public static final int RANDOM_DATA_FIELD_NUMBER = 4;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       */
      public boolean getRandomData() {
        if (inputDataTypeCase_ == 4) {
          return (java.lang.Boolean) inputDataType_;
        }
        return false;
      }

      public static final int INPUT_DATA_FILE_FIELD_NUMBER = 5;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       */
      public java.lang.String getInputDataFile() {
        java.lang.Object ref = "";
        if (inputDataTypeCase_ == 5) {
          ref = inputDataType_;
        }
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (inputDataTypeCase_ == 5) {
            inputDataType_ = s;
          }
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       */
      public com.google.protobuf.ByteString
          getInputDataFileBytes() {
        java.lang.Object ref = "";
        if (inputDataTypeCase_ == 5) {
          ref = inputDataType_;
        }
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          if (inputDataTypeCase_ == 5) {
            inputDataType_ = b;
          }
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
          output.writeEnum(1, dataType_);
        }
        if (getDimsList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(dimsMemoizedSerializedSize);
        }
        for (int i = 0; i < dims_.size(); i++) {
          output.writeInt64NoTag(dims_.get(i));
        }
        if (inputDataTypeCase_ == 3) {
          output.writeBool(
              3, (boolean)((java.lang.Boolean) inputDataType_));
        }
        if (inputDataTypeCase_ == 4) {
          output.writeBool(
              4, (boolean)((java.lang.Boolean) inputDataType_));
        }
        if (inputDataTypeCase_ == 5) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 5, inputDataType_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (dataType_ != nvidia.inferenceserver.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
          size += com.google.protobuf.CodedOutputStream
            .computeEnumSize(1, dataType_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < dims_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt64SizeNoTag(dims_.get(i));
          }
          size += dataSize;
          if (!getDimsList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          dimsMemoizedSerializedSize = dataSize;
        }
        if (inputDataTypeCase_ == 3) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(
                3, (boolean)((java.lang.Boolean) inputDataType_));
        }
        if (inputDataTypeCase_ == 4) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(
                4, (boolean)((java.lang.Boolean) inputDataType_));
        }
        if (inputDataTypeCase_ == 5) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, inputDataType_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input)) {
          return super.equals(obj);
        }
        nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input) obj;

        boolean result = true;
        result = result && dataType_ == other.dataType_;
        result = result && getDimsList()
            .equals(other.getDimsList());
        result = result && getInputDataTypeCase().equals(
            other.getInputDataTypeCase());
        if (!result) return false;
        switch (inputDataTypeCase_) {
          case 3:
            result = result && (getZeroData()
                == other.getZeroData());
            break;
          case 4:
            result = result && (getRandomData()
                == other.getRandomData());
            break;
          case 5:
            result = result && getInputDataFile()
                .equals(other.getInputDataFile());
            break;
          case 0:
          default:
        }
        result = result && unknownFields.equals(other.unknownFields);
        return result;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + dataType_;
        if (getDimsCount() > 0) {
          hash = (37 * hash) + DIMS_FIELD_NUMBER;
          hash = (53 * hash) + getDimsList().hashCode();
        }
        switch (inputDataTypeCase_) {
          case 3:
            hash = (37 * hash) + ZERO_DATA_FIELD_NUMBER;
            hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
                getZeroData());
            break;
          case 4:
            hash = (37 * hash) + RANDOM_DATA_FIELD_NUMBER;
            hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
                getRandomData());
            break;
          case 5:
            hash = (37 * hash) + INPUT_DATA_FILE_FIELD_NUMBER;
            hash = (53 * hash) + getInputDataFile().hashCode();
            break;
          case 0:
          default:
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Input
       *&#64;&#64;
       *&#64;&#64;     Meta data associated with an input.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code nvidia.inferenceserver.ModelWarmup.Input}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelWarmup.Input)
          nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.InputOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_Input_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_Input_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input.Builder.class);
        }

        // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          dataType_ = 0;

          dims_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          inputDataTypeCase_ = 0;
          inputDataType_ = null;
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_Input_descriptor;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input getDefaultInstanceForType() {
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input.getDefaultInstance();
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input build() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input buildPartial() {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          result.dataType_ = dataType_;
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            dims_ = java.util.Collections.unmodifiableList(dims_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.dims_ = dims_;
          if (inputDataTypeCase_ == 3) {
            result.inputDataType_ = inputDataType_;
          }
          if (inputDataTypeCase_ == 4) {
            result.inputDataType_ = inputDataType_;
          }
          if (inputDataTypeCase_ == 5) {
            result.inputDataType_ = inputDataType_;
          }
          result.bitField0_ = to_bitField0_;
          result.inputDataTypeCase_ = inputDataTypeCase_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return (Builder) super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return (Builder) super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return (Builder) super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return (Builder) super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return (Builder) super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input) {
            return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input other) {
          if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input.getDefaultInstance()) return this;
          if (other.dataType_ != 0) {
            setDataTypeValue(other.getDataTypeValue());
          }
          if (!other.dims_.isEmpty()) {
            if (dims_.isEmpty()) {
              dims_ = other.dims_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureDimsIsMutable();
              dims_.addAll(other.dims_);
            }
            onChanged();
          }
          switch (other.getInputDataTypeCase()) {
            case ZERO_DATA: {
              setZeroData(other.getZeroData());
              break;
            }
            case RANDOM_DATA: {
              setRandomData(other.getRandomData());
              break;
            }
            case INPUT_DATA_FILE: {
              inputDataTypeCase_ = 5;
              inputDataType_ = other.inputDataType_;
              onChanged();
              break;
            }
            case INPUTDATATYPE_NOT_SET: {
              break;
            }
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int inputDataTypeCase_ = 0;
        private java.lang.Object inputDataType_;
        public InputDataTypeCase
            getInputDataTypeCase() {
          return InputDataTypeCase.forNumber(
              inputDataTypeCase_);
        }

        public Builder clearInputDataType() {
          inputDataTypeCase_ = 0;
          inputDataType_ = null;
          onChanged();
          return this;
        }

        private int bitField0_;

        private int dataType_ = 0;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 1;</code>
         */
        public int getDataTypeValue() {
          return dataType_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 1;</code>
         */
        public Builder setDataTypeValue(int value) {
          dataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 1;</code>
         */
        public nvidia.inferenceserver.ModelConfigOuterClass.DataType getDataType() {
          @SuppressWarnings("deprecation")
          nvidia.inferenceserver.ModelConfigOuterClass.DataType result = nvidia.inferenceserver.ModelConfigOuterClass.DataType.valueOf(dataType_);
          return result == null ? nvidia.inferenceserver.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 1;</code>
         */
        public Builder setDataType(nvidia.inferenceserver.ModelConfigOuterClass.DataType value) {
          if (value == null) {
            throw new NullPointerException();
          }
          
          dataType_ = value.getNumber();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.nvidia.inferenceserver.DataType data_type = 1;</code>
         */
        public Builder clearDataType() {
          
          dataType_ = 0;
          onChanged();
          return this;
        }

        private java.util.List<java.lang.Long> dims_ = java.util.Collections.emptyList();
        private void ensureDimsIsMutable() {
          if (!((bitField0_ & 0x00000002) == 0x00000002)) {
            dims_ = new java.util.ArrayList<java.lang.Long>(dims_);
            bitField0_ |= 0x00000002;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public java.util.List<java.lang.Long>
            getDimsList() {
          return java.util.Collections.unmodifiableList(dims_);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public int getDimsCount() {
          return dims_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public long getDims(int index) {
          return dims_.get(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public Builder setDims(
            int index, long value) {
          ensureDimsIsMutable();
          dims_.set(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public Builder addDims(long value) {
          ensureDimsIsMutable();
          dims_.add(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public Builder addAllDims(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureDimsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, dims_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         */
        public Builder clearDims() {
          dims_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         */
        public boolean getZeroData() {
          if (inputDataTypeCase_ == 3) {
            return (java.lang.Boolean) inputDataType_;
          }
          return false;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         */
        public Builder setZeroData(boolean value) {
          inputDataTypeCase_ = 3;
          inputDataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         */
        public Builder clearZeroData() {
          if (inputDataTypeCase_ == 3) {
            inputDataTypeCase_ = 0;
            inputDataType_ = null;
            onChanged();
          }
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         */
        public boolean getRandomData() {
          if (inputDataTypeCase_ == 4) {
            return (java.lang.Boolean) inputDataType_;
          }
          return false;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         */
        public Builder setRandomData(boolean value) {
          inputDataTypeCase_ = 4;
          inputDataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         */
        public Builder clearRandomData() {
          if (inputDataTypeCase_ == 4) {
            inputDataTypeCase_ = 0;
            inputDataType_ = null;
            onChanged();
          }
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         */
        public java.lang.String getInputDataFile() {
          java.lang.Object ref = "";
          if (inputDataTypeCase_ == 5) {
            ref = inputDataType_;
          }
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            if (inputDataTypeCase_ == 5) {
              inputDataType_ = s;
            }
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         */
        public com.google.protobuf.ByteString
            getInputDataFileBytes() {
          java.lang.Object ref = "";
          if (inputDataTypeCase_ == 5) {
            ref = inputDataType_;
          }
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            if (inputDataTypeCase_ == 5) {
              inputDataType_ = b;
            }
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         */
        public Builder setInputDataFile(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  inputDataTypeCase_ = 5;
          inputDataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         */
        public Builder clearInputDataFile() {
          if (inputDataTypeCase_ == 5) {
            inputDataTypeCase_ = 0;
            inputDataType_ = null;
            onChanged();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         */
        public Builder setInputDataFileBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          inputDataTypeCase_ = 5;
          inputDataType_ = value;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFieldsProto3(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelWarmup.Input)
      }

      // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelWarmup.Input)
      private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input();
      }

      public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Input>
          PARSER = new com.google.protobuf.AbstractParser<Input>() {
        @java.lang.Override
        public Input parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Input(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Input> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Input> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    private int bitField0_;
    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BATCH_SIZE_FIELD_NUMBER = 2;
    private int batchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1. If
     *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
     *&#64;&#64;     match the batch size requested.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 2;</code>
     */
    public int getBatchSize() {
      return batchSize_;
    }

    public static final int INPUTS_FIELD_NUMBER = 3;
    private static final class InputsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input>newDefaultInstance(
                  nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_InputsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> inputs_;
    private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input>
    internalGetInputs() {
      if (inputs_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            InputsDefaultEntryHolder.defaultEntry);
      }
      return inputs_;
    }

    public int getInputsCount() {
      return internalGetInputs().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    public boolean containsInputs(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetInputs().getMap().containsKey(key);
    }
    /**
     * Use {@link #getInputsMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> getInputs() {
      return getInputsMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> getInputsMap() {
      return internalGetInputs().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input getInputsOrDefault(
        java.lang.String key,
        nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> map =
          internalGetInputs().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input getInputsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> map =
          internalGetInputs().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (batchSize_ != 0) {
        output.writeUInt32(2, batchSize_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetInputs(),
          InputsDefaultEntryHolder.defaultEntry,
          3);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (batchSize_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, batchSize_);
      }
      for (java.util.Map.Entry<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> entry
           : internalGetInputs().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input>
        inputs__ = InputsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(3, inputs__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup) obj;

      boolean result = true;
      result = result && getName()
          .equals(other.getName());
      result = result && (getBatchSize()
          == other.getBatchSize());
      result = result && internalGetInputs().equals(
          other.internalGetInputs());
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + BATCH_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + getBatchSize();
      if (!internalGetInputs().getMap().isEmpty()) {
        hash = (37 * hash) + INPUTS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetInputs().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelWarmup
     *&#64;&#64;
     *&#64;&#64;   Settings used to construct the request sample for model warmup.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelWarmup}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelWarmup)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetInputs();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetMutableInputs();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        batchSize_ = 0;

        internalGetMutableInputs().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelWarmup_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.name_ = name_;
        result.batchSize_ = batchSize_;
        result.inputs_ = internalGetInputs();
        result.inputs_.makeImmutable();
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.getBatchSize() != 0) {
          setBatchSize(other.getBatchSize());
        }
        internalGetMutableInputs().mergeFrom(
            other.internalGetInputs());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int batchSize_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1. If
       *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
       *&#64;&#64;     match the batch size requested.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 2;</code>
       */
      public int getBatchSize() {
        return batchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1. If
       *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
       *&#64;&#64;     match the batch size requested.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 2;</code>
       */
      public Builder setBatchSize(int value) {
        
        batchSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1. If
       *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
       *&#64;&#64;     match the batch size requested.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 2;</code>
       */
      public Builder clearBatchSize() {
        
        batchSize_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> inputs_;
      private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input>
      internalGetInputs() {
        if (inputs_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              InputsDefaultEntryHolder.defaultEntry);
        }
        return inputs_;
      }
      private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input>
      internalGetMutableInputs() {
        onChanged();;
        if (inputs_ == null) {
          inputs_ = com.google.protobuf.MapField.newMapField(
              InputsDefaultEntryHolder.defaultEntry);
        }
        if (!inputs_.isMutable()) {
          inputs_ = inputs_.copy();
        }
        return inputs_;
      }

      public int getInputsCount() {
        return internalGetInputs().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      public boolean containsInputs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetInputs().getMap().containsKey(key);
      }
      /**
       * Use {@link #getInputsMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> getInputs() {
        return getInputsMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> getInputsMap() {
        return internalGetInputs().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input getInputsOrDefault(
          java.lang.String key,
          nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> map =
            internalGetInputs().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input getInputsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> map =
            internalGetInputs().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearInputs() {
        internalGetMutableInputs().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      public Builder removeInputs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableInputs().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input>
      getMutableInputs() {
        return internalGetMutableInputs().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      public Builder putInputs(
          java.lang.String key,
          nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableInputs().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      public Builder putAllInputs(
          java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Input> values) {
        internalGetMutableInputs().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelWarmup)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelWarmup)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelWarmup>
        PARSER = new com.google.protobuf.AbstractParser<ModelWarmup>() {
      @java.lang.Override
      public ModelWarmup parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelWarmup(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelWarmup> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelWarmup> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelConfigOrBuilder extends
      // @@protoc_insertion_point(interface_extends:nvidia.inferenceserver.ModelConfig)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
     *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     */
    java.lang.String getPlatform();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
     *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     */
    com.google.protobuf.ByteString
        getPlatformBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    boolean hasVersionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder getVersionPolicyOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     */
    int getMaxBatchSize();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> 
        getInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    int getInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> 
        getInputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> 
        getOutputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getOutput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    int getOutputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> 
        getOutputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    boolean hasOptimization();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder getOptimizationOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    boolean hasDynamicBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder getDynamicBatchingOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    boolean hasSequenceBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder getSequenceBatchingOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    boolean hasEnsembleScheduling();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder getEnsembleSchedulingOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> 
        getInstanceGroupList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    int getInstanceGroupCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
        getInstanceGroupOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.netdef' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     */
    java.lang.String getDefaultModelFilename();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.netdef' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     */
    com.google.protobuf.ByteString
        getDefaultModelFilenameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    int getCcModelFilenamesCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    boolean containsCcModelFilenames(
        java.lang.String key);
    /**
     * Use {@link #getCcModelFilenamesMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getCcModelFilenames();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getCcModelFilenamesMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    java.lang.String getCcModelFilenamesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    java.lang.String getCcModelFilenamesOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    int getMetricTagsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    boolean containsMetricTags(
        java.lang.String key);
    /**
     * Use {@link #getMetricTagsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getMetricTags();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getMetricTagsMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    java.lang.String getMetricTagsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    java.lang.String getMetricTagsOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */
    int getParametersCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */
    boolean containsParameters(
        java.lang.String key);
    /**
     * Use {@link #getParametersMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
    getParameters();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */
    java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
    getParametersMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
        java.lang.String key,
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup> 
        getModelWarmupList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup getModelWarmup(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    int getModelWarmupCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder> 
        getModelWarmupOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder getModelWarmupOrBuilder(
        int index);

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.SchedulingChoiceCase getSchedulingChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelConfig
   *&#64;&#64;
   *&#64;&#64;   A model configuration.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code nvidia.inferenceserver.ModelConfig}
   */
  public  static final class ModelConfig extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:nvidia.inferenceserver.ModelConfig)
      ModelConfigOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelConfig.newBuilder() to construct.
    private ModelConfig(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelConfig() {
      name_ = "";
      platform_ = "";
      maxBatchSize_ = 0;
      input_ = java.util.Collections.emptyList();
      output_ = java.util.Collections.emptyList();
      instanceGroup_ = java.util.Collections.emptyList();
      defaultModelFilename_ = "";
      modelWarmup_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelConfig(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();

              platform_ = s;
              break;
            }
            case 26: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder subBuilder = null;
              if (versionPolicy_ != null) {
                subBuilder = versionPolicy_.toBuilder();
              }
              versionPolicy_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(versionPolicy_);
                versionPolicy_ = subBuilder.buildPartial();
              }

              break;
            }
            case 32: {

              maxBatchSize_ = input.readInt32();
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
                input_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput>();
                mutable_bitField0_ |= 0x00000010;
              }
              input_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.parser(), extensionRegistry));
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
                output_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput>();
                mutable_bitField0_ |= 0x00000020;
              }
              output_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.parser(), extensionRegistry));
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
                instanceGroup_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup>();
                mutable_bitField0_ |= 0x00000400;
              }
              instanceGroup_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.parser(), extensionRegistry));
              break;
            }
            case 66: {
              java.lang.String s = input.readStringRequireUtf8();

              defaultModelFilename_ = s;
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00001000) == 0x00001000)) {
                ccModelFilenames_ = com.google.protobuf.MapField.newMapField(
                    CcModelFilenamesDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00001000;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              ccModelFilenames__ = input.readMessage(
                  CcModelFilenamesDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              ccModelFilenames_.getMutableMap().put(
                  ccModelFilenames__.getKey(), ccModelFilenames__.getValue());
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00002000) == 0x00002000)) {
                metricTags_ = com.google.protobuf.MapField.newMapField(
                    MetricTagsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00002000;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              metricTags__ = input.readMessage(
                  MetricTagsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              metricTags_.getMutableMap().put(
                  metricTags__.getKey(), metricTags__.getValue());
              break;
            }
            case 90: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder subBuilder = null;
              if (schedulingChoiceCase_ == 11) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_).toBuilder();
              }
              schedulingChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_);
                schedulingChoice_ = subBuilder.buildPartial();
              }
              schedulingChoiceCase_ = 11;
              break;
            }
            case 98: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder subBuilder = null;
              if (optimization_ != null) {
                subBuilder = optimization_.toBuilder();
              }
              optimization_ = input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(optimization_);
                optimization_ = subBuilder.buildPartial();
              }

              break;
            }
            case 106: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder subBuilder = null;
              if (schedulingChoiceCase_ == 13) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_).toBuilder();
              }
              schedulingChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_);
                schedulingChoice_ = subBuilder.buildPartial();
              }
              schedulingChoiceCase_ = 13;
              break;
            }
            case 114: {
              if (!((mutable_bitField0_ & 0x00004000) == 0x00004000)) {
                parameters_ = com.google.protobuf.MapField.newMapField(
                    ParametersDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00004000;
              }
              com.google.protobuf.MapEntry<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
              parameters__ = input.readMessage(
                  ParametersDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              parameters_.getMutableMap().put(
                  parameters__.getKey(), parameters__.getValue());
              break;
            }
            case 122: {
              nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder subBuilder = null;
              if (schedulingChoiceCase_ == 15) {
                subBuilder = ((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_).toBuilder();
              }
              schedulingChoice_ =
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_);
                schedulingChoice_ = subBuilder.buildPartial();
              }
              schedulingChoiceCase_ = 15;
              break;
            }
            case 130: {
              if (!((mutable_bitField0_ & 0x00008000) == 0x00008000)) {
                modelWarmup_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup>();
                mutable_bitField0_ |= 0x00008000;
              }
              modelWarmup_.add(
                  input.readMessage(nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownFieldProto3(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000010) == 0x00000010)) {
          input_ = java.util.Collections.unmodifiableList(input_);
        }
        if (((mutable_bitField0_ & 0x00000020) == 0x00000020)) {
          output_ = java.util.Collections.unmodifiableList(output_);
        }
        if (((mutable_bitField0_ & 0x00000400) == 0x00000400)) {
          instanceGroup_ = java.util.Collections.unmodifiableList(instanceGroup_);
        }
        if (((mutable_bitField0_ & 0x00008000) == 0x00008000)) {
          modelWarmup_ = java.util.Collections.unmodifiableList(modelWarmup_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 9:
          return internalGetCcModelFilenames();
        case 10:
          return internalGetMetricTags();
        case 14:
          return internalGetParameters();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder.class);
    }

    private int bitField0_;
    private int schedulingChoiceCase_ = 0;
    private java.lang.Object schedulingChoice_;
    public enum SchedulingChoiceCase
        implements com.google.protobuf.Internal.EnumLite {
      DYNAMIC_BATCHING(11),
      SEQUENCE_BATCHING(13),
      ENSEMBLE_SCHEDULING(15),
      SCHEDULINGCHOICE_NOT_SET(0);
      private final int value;
      private SchedulingChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static SchedulingChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static SchedulingChoiceCase forNumber(int value) {
        switch (value) {
          case 11: return DYNAMIC_BATCHING;
          case 13: return SEQUENCE_BATCHING;
          case 15: return ENSEMBLE_SCHEDULING;
          case 0: return SCHEDULINGCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public SchedulingChoiceCase
    getSchedulingChoiceCase() {
      return SchedulingChoiceCase.forNumber(
          schedulingChoiceCase_);
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     */
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int PLATFORM_FIELD_NUMBER = 2;
    private volatile java.lang.Object platform_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
     *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     */
    public java.lang.String getPlatform() {
      java.lang.Object ref = platform_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        platform_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
     *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     */
    public com.google.protobuf.ByteString
        getPlatformBytes() {
      java.lang.Object ref = platform_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        platform_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VERSION_POLICY_FIELD_NUMBER = 3;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy versionPolicy_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    public boolean hasVersionPolicy() {
      return versionPolicy_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy() {
      return versionPolicy_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder getVersionPolicyOrBuilder() {
      return getVersionPolicy();
    }

    public static final int MAX_BATCH_SIZE_FIELD_NUMBER = 4;
    private int maxBatchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     */
    public int getMaxBatchSize() {
      return maxBatchSize_;
    }

    public static final int INPUT_FIELD_NUMBER = 5;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> input_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> getInputList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> 
        getInputOrBuilderList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public int getInputCount() {
      return input_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getInput(int index) {
      return input_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
        int index) {
      return input_.get(index);
    }

    public static final int OUTPUT_FIELD_NUMBER = 6;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> output_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> getOutputList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> 
        getOutputOrBuilderList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public int getOutputCount() {
      return output_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getOutput(int index) {
      return output_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
        int index) {
      return output_.get(index);
    }

    public static final int OPTIMIZATION_FIELD_NUMBER = 12;
    private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy optimization_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    public boolean hasOptimization() {
      return optimization_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization() {
      return optimization_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder getOptimizationOrBuilder() {
      return getOptimization();
    }

    public static final int DYNAMIC_BATCHING_FIELD_NUMBER = 11;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    public boolean hasDynamicBatching() {
      return schedulingChoiceCase_ == 11;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching() {
      if (schedulingChoiceCase_ == 11) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder getDynamicBatchingOrBuilder() {
      if (schedulingChoiceCase_ == 11) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
    }

    public static final int SEQUENCE_BATCHING_FIELD_NUMBER = 13;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    public boolean hasSequenceBatching() {
      return schedulingChoiceCase_ == 13;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching() {
      if (schedulingChoiceCase_ == 13) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder getSequenceBatchingOrBuilder() {
      if (schedulingChoiceCase_ == 13) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
    }

    public static final int ENSEMBLE_SCHEDULING_FIELD_NUMBER = 15;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    public boolean hasEnsembleScheduling() {
      return schedulingChoiceCase_ == 15;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling() {
      if (schedulingChoiceCase_ == 15) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder getEnsembleSchedulingOrBuilder() {
      if (schedulingChoiceCase_ == 15) {
         return (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
      }
      return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
    }

    public static final int INSTANCE_GROUP_FIELD_NUMBER = 7;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> instanceGroup_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> getInstanceGroupList() {
      return instanceGroup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
        getInstanceGroupOrBuilderList() {
      return instanceGroup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public int getInstanceGroupCount() {
      return instanceGroup_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index) {
      return instanceGroup_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
        int index) {
      return instanceGroup_.get(index);
    }

    public static final int DEFAULT_MODEL_FILENAME_FIELD_NUMBER = 8;
    private volatile java.lang.Object defaultModelFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.netdef' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     */
    public java.lang.String getDefaultModelFilename() {
      java.lang.Object ref = defaultModelFilename_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        defaultModelFilename_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.netdef' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     */
    public com.google.protobuf.ByteString
        getDefaultModelFilenameBytes() {
      java.lang.Object ref = defaultModelFilename_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        defaultModelFilename_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int CC_MODEL_FILENAMES_FIELD_NUMBER = 9;
    private static final class CcModelFilenamesDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> ccModelFilenames_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetCcModelFilenames() {
      if (ccModelFilenames_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            CcModelFilenamesDefaultEntryHolder.defaultEntry);
      }
      return ccModelFilenames_;
    }

    public int getCcModelFilenamesCount() {
      return internalGetCcModelFilenames().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    public boolean containsCcModelFilenames(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetCcModelFilenames().getMap().containsKey(key);
    }
    /**
     * Use {@link #getCcModelFilenamesMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenames() {
      return getCcModelFilenamesMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenamesMap() {
      return internalGetCcModelFilenames().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    public java.lang.String getCcModelFilenamesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetCcModelFilenames().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    public java.lang.String getCcModelFilenamesOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetCcModelFilenames().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int METRIC_TAGS_FIELD_NUMBER = 10;
    private static final class MetricTagsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> metricTags_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetMetricTags() {
      if (metricTags_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            MetricTagsDefaultEntryHolder.defaultEntry);
      }
      return metricTags_;
    }

    public int getMetricTagsCount() {
      return internalGetMetricTags().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    public boolean containsMetricTags(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetMetricTags().getMap().containsKey(key);
    }
    /**
     * Use {@link #getMetricTagsMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getMetricTags() {
      return getMetricTagsMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    public java.util.Map<java.lang.String, java.lang.String> getMetricTagsMap() {
      return internalGetMetricTags().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    public java.lang.String getMetricTagsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetricTags().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    public java.lang.String getMetricTagsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetricTags().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int PARAMETERS_FIELD_NUMBER = 14;
    private static final class ParametersDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>newDefaultInstance(
                  nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> parameters_;
    private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
    internalGetParameters() {
      if (parameters_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ParametersDefaultEntryHolder.defaultEntry);
      }
      return parameters_;
    }

    public int getParametersCount() {
      return internalGetParameters().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    public boolean containsParameters(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetParameters().getMap().containsKey(key);
    }
    /**
     * Use {@link #getParametersMap()} instead.
     */
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> getParameters() {
      return getParametersMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> getParametersMap() {
      return internalGetParameters().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
        java.lang.String key,
        nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> map =
          internalGetParameters().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
     */

    public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> map =
          internalGetParameters().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int MODEL_WARMUP_FIELD_NUMBER = 16;
    private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup> modelWarmup_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup> getModelWarmupList() {
      return modelWarmup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder> 
        getModelWarmupOrBuilderList() {
      return modelWarmup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    public int getModelWarmupCount() {
      return modelWarmup_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup getModelWarmup(int index) {
      return modelWarmup_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
     */
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder getModelWarmupOrBuilder(
        int index) {
      return modelWarmup_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (!getPlatformBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, platform_);
      }
      if (versionPolicy_ != null) {
        output.writeMessage(3, getVersionPolicy());
      }
      if (maxBatchSize_ != 0) {
        output.writeInt32(4, maxBatchSize_);
      }
      for (int i = 0; i < input_.size(); i++) {
        output.writeMessage(5, input_.get(i));
      }
      for (int i = 0; i < output_.size(); i++) {
        output.writeMessage(6, output_.get(i));
      }
      for (int i = 0; i < instanceGroup_.size(); i++) {
        output.writeMessage(7, instanceGroup_.get(i));
      }
      if (!getDefaultModelFilenameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, defaultModelFilename_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetCcModelFilenames(),
          CcModelFilenamesDefaultEntryHolder.defaultEntry,
          9);
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetMetricTags(),
          MetricTagsDefaultEntryHolder.defaultEntry,
          10);
      if (schedulingChoiceCase_ == 11) {
        output.writeMessage(11, (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_);
      }
      if (optimization_ != null) {
        output.writeMessage(12, getOptimization());
      }
      if (schedulingChoiceCase_ == 13) {
        output.writeMessage(13, (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetParameters(),
          ParametersDefaultEntryHolder.defaultEntry,
          14);
      if (schedulingChoiceCase_ == 15) {
        output.writeMessage(15, (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_);
      }
      for (int i = 0; i < modelWarmup_.size(); i++) {
        output.writeMessage(16, modelWarmup_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (!getPlatformBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, platform_);
      }
      if (versionPolicy_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getVersionPolicy());
      }
      if (maxBatchSize_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, maxBatchSize_);
      }
      for (int i = 0; i < input_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, input_.get(i));
      }
      for (int i = 0; i < output_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, output_.get(i));
      }
      for (int i = 0; i < instanceGroup_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, instanceGroup_.get(i));
      }
      if (!getDefaultModelFilenameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, defaultModelFilename_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetCcModelFilenames().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        ccModelFilenames__ = CcModelFilenamesDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(9, ccModelFilenames__);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetMetricTags().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        metricTags__ = MetricTagsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(10, metricTags__);
      }
      if (schedulingChoiceCase_ == 11) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_);
      }
      if (optimization_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(12, getOptimization());
      }
      if (schedulingChoiceCase_ == 13) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(13, (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_);
      }
      for (java.util.Map.Entry<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> entry
           : internalGetParameters().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
        parameters__ = ParametersDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(14, parameters__);
      }
      if (schedulingChoiceCase_ == 15) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(15, (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_);
      }
      for (int i = 0; i < modelWarmup_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(16, modelWarmup_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig)) {
        return super.equals(obj);
      }
      nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig other = (nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig) obj;

      boolean result = true;
      result = result && getName()
          .equals(other.getName());
      result = result && getPlatform()
          .equals(other.getPlatform());
      result = result && (hasVersionPolicy() == other.hasVersionPolicy());
      if (hasVersionPolicy()) {
        result = result && getVersionPolicy()
            .equals(other.getVersionPolicy());
      }
      result = result && (getMaxBatchSize()
          == other.getMaxBatchSize());
      result = result && getInputList()
          .equals(other.getInputList());
      result = result && getOutputList()
          .equals(other.getOutputList());
      result = result && (hasOptimization() == other.hasOptimization());
      if (hasOptimization()) {
        result = result && getOptimization()
            .equals(other.getOptimization());
      }
      result = result && getInstanceGroupList()
          .equals(other.getInstanceGroupList());
      result = result && getDefaultModelFilename()
          .equals(other.getDefaultModelFilename());
      result = result && internalGetCcModelFilenames().equals(
          other.internalGetCcModelFilenames());
      result = result && internalGetMetricTags().equals(
          other.internalGetMetricTags());
      result = result && internalGetParameters().equals(
          other.internalGetParameters());
      result = result && getModelWarmupList()
          .equals(other.getModelWarmupList());
      result = result && getSchedulingChoiceCase().equals(
          other.getSchedulingChoiceCase());
      if (!result) return false;
      switch (schedulingChoiceCase_) {
        case 11:
          result = result && getDynamicBatching()
              .equals(other.getDynamicBatching());
          break;
        case 13:
          result = result && getSequenceBatching()
              .equals(other.getSequenceBatching());
          break;
        case 15:
          result = result && getEnsembleScheduling()
              .equals(other.getEnsembleScheduling());
          break;
        case 0:
        default:
      }
      result = result && unknownFields.equals(other.unknownFields);
      return result;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + PLATFORM_FIELD_NUMBER;
      hash = (53 * hash) + getPlatform().hashCode();
      if (hasVersionPolicy()) {
        hash = (37 * hash) + VERSION_POLICY_FIELD_NUMBER;
        hash = (53 * hash) + getVersionPolicy().hashCode();
      }
      hash = (37 * hash) + MAX_BATCH_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + getMaxBatchSize();
      if (getInputCount() > 0) {
        hash = (37 * hash) + INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getInputList().hashCode();
      }
      if (getOutputCount() > 0) {
        hash = (37 * hash) + OUTPUT_FIELD_NUMBER;
        hash = (53 * hash) + getOutputList().hashCode();
      }
      if (hasOptimization()) {
        hash = (37 * hash) + OPTIMIZATION_FIELD_NUMBER;
        hash = (53 * hash) + getOptimization().hashCode();
      }
      if (getInstanceGroupCount() > 0) {
        hash = (37 * hash) + INSTANCE_GROUP_FIELD_NUMBER;
        hash = (53 * hash) + getInstanceGroupList().hashCode();
      }
      hash = (37 * hash) + DEFAULT_MODEL_FILENAME_FIELD_NUMBER;
      hash = (53 * hash) + getDefaultModelFilename().hashCode();
      if (!internalGetCcModelFilenames().getMap().isEmpty()) {
        hash = (37 * hash) + CC_MODEL_FILENAMES_FIELD_NUMBER;
        hash = (53 * hash) + internalGetCcModelFilenames().hashCode();
      }
      if (!internalGetMetricTags().getMap().isEmpty()) {
        hash = (37 * hash) + METRIC_TAGS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetMetricTags().hashCode();
      }
      if (!internalGetParameters().getMap().isEmpty()) {
        hash = (37 * hash) + PARAMETERS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetParameters().hashCode();
      }
      if (getModelWarmupCount() > 0) {
        hash = (37 * hash) + MODEL_WARMUP_FIELD_NUMBER;
        hash = (53 * hash) + getModelWarmupList().hashCode();
      }
      switch (schedulingChoiceCase_) {
        case 11:
          hash = (37 * hash) + DYNAMIC_BATCHING_FIELD_NUMBER;
          hash = (53 * hash) + getDynamicBatching().hashCode();
          break;
        case 13:
          hash = (37 * hash) + SEQUENCE_BATCHING_FIELD_NUMBER;
          hash = (53 * hash) + getSequenceBatching().hashCode();
          break;
        case 15:
          hash = (37 * hash) + ENSEMBLE_SCHEDULING_FIELD_NUMBER;
          hash = (53 * hash) + getEnsembleScheduling().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelConfig
     *&#64;&#64;
     *&#64;&#64;   A model configuration.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code nvidia.inferenceserver.ModelConfig}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:nvidia.inferenceserver.ModelConfig)
        nvidia.inferenceserver.ModelConfigOuterClass.ModelConfigOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 9:
            return internalGetCcModelFilenames();
          case 10:
            return internalGetMetricTags();
          case 14:
            return internalGetParameters();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 9:
            return internalGetMutableCcModelFilenames();
          case 10:
            return internalGetMutableMetricTags();
          case 14:
            return internalGetMutableParameters();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.class, nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.Builder.class);
      }

      // Construct using nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getInputFieldBuilder();
          getOutputFieldBuilder();
          getInstanceGroupFieldBuilder();
          getModelWarmupFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        platform_ = "";

        if (versionPolicyBuilder_ == null) {
          versionPolicy_ = null;
        } else {
          versionPolicy_ = null;
          versionPolicyBuilder_ = null;
        }
        maxBatchSize_ = 0;

        if (inputBuilder_ == null) {
          input_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          inputBuilder_.clear();
        }
        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          outputBuilder_.clear();
        }
        if (optimizationBuilder_ == null) {
          optimization_ = null;
        } else {
          optimization_ = null;
          optimizationBuilder_ = null;
        }
        if (instanceGroupBuilder_ == null) {
          instanceGroup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000400);
        } else {
          instanceGroupBuilder_.clear();
        }
        defaultModelFilename_ = "";

        internalGetMutableCcModelFilenames().clear();
        internalGetMutableMetricTags().clear();
        internalGetMutableParameters().clear();
        if (modelWarmupBuilder_ == null) {
          modelWarmup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00008000);
        } else {
          modelWarmupBuilder_.clear();
        }
        schedulingChoiceCase_ = 0;
        schedulingChoice_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.internal_static_nvidia_inferenceserver_ModelConfig_descriptor;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig getDefaultInstanceForType() {
        return nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.getDefaultInstance();
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig build() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig buildPartial() {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig result = new nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        result.name_ = name_;
        result.platform_ = platform_;
        if (versionPolicyBuilder_ == null) {
          result.versionPolicy_ = versionPolicy_;
        } else {
          result.versionPolicy_ = versionPolicyBuilder_.build();
        }
        result.maxBatchSize_ = maxBatchSize_;
        if (inputBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010)) {
            input_ = java.util.Collections.unmodifiableList(input_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.input_ = input_;
        } else {
          result.input_ = inputBuilder_.build();
        }
        if (outputBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            output_ = java.util.Collections.unmodifiableList(output_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.output_ = output_;
        } else {
          result.output_ = outputBuilder_.build();
        }
        if (optimizationBuilder_ == null) {
          result.optimization_ = optimization_;
        } else {
          result.optimization_ = optimizationBuilder_.build();
        }
        if (schedulingChoiceCase_ == 11) {
          if (dynamicBatchingBuilder_ == null) {
            result.schedulingChoice_ = schedulingChoice_;
          } else {
            result.schedulingChoice_ = dynamicBatchingBuilder_.build();
          }
        }
        if (schedulingChoiceCase_ == 13) {
          if (sequenceBatchingBuilder_ == null) {
            result.schedulingChoice_ = schedulingChoice_;
          } else {
            result.schedulingChoice_ = sequenceBatchingBuilder_.build();
          }
        }
        if (schedulingChoiceCase_ == 15) {
          if (ensembleSchedulingBuilder_ == null) {
            result.schedulingChoice_ = schedulingChoice_;
          } else {
            result.schedulingChoice_ = ensembleSchedulingBuilder_.build();
          }
        }
        if (instanceGroupBuilder_ == null) {
          if (((bitField0_ & 0x00000400) == 0x00000400)) {
            instanceGroup_ = java.util.Collections.unmodifiableList(instanceGroup_);
            bitField0_ = (bitField0_ & ~0x00000400);
          }
          result.instanceGroup_ = instanceGroup_;
        } else {
          result.instanceGroup_ = instanceGroupBuilder_.build();
        }
        result.defaultModelFilename_ = defaultModelFilename_;
        result.ccModelFilenames_ = internalGetCcModelFilenames();
        result.ccModelFilenames_.makeImmutable();
        result.metricTags_ = internalGetMetricTags();
        result.metricTags_.makeImmutable();
        result.parameters_ = internalGetParameters();
        result.parameters_.makeImmutable();
        if (modelWarmupBuilder_ == null) {
          if (((bitField0_ & 0x00008000) == 0x00008000)) {
            modelWarmup_ = java.util.Collections.unmodifiableList(modelWarmup_);
            bitField0_ = (bitField0_ & ~0x00008000);
          }
          result.modelWarmup_ = modelWarmup_;
        } else {
          result.modelWarmup_ = modelWarmupBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        result.schedulingChoiceCase_ = schedulingChoiceCase_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return (Builder) super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return (Builder) super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return (Builder) super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return (Builder) super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return (Builder) super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig) {
          return mergeFrom((nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig other) {
        if (other == nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (!other.getPlatform().isEmpty()) {
          platform_ = other.platform_;
          onChanged();
        }
        if (other.hasVersionPolicy()) {
          mergeVersionPolicy(other.getVersionPolicy());
        }
        if (other.getMaxBatchSize() != 0) {
          setMaxBatchSize(other.getMaxBatchSize());
        }
        if (inputBuilder_ == null) {
          if (!other.input_.isEmpty()) {
            if (input_.isEmpty()) {
              input_ = other.input_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureInputIsMutable();
              input_.addAll(other.input_);
            }
            onChanged();
          }
        } else {
          if (!other.input_.isEmpty()) {
            if (inputBuilder_.isEmpty()) {
              inputBuilder_.dispose();
              inputBuilder_ = null;
              input_ = other.input_;
              bitField0_ = (bitField0_ & ~0x00000010);
              inputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getInputFieldBuilder() : null;
            } else {
              inputBuilder_.addAllMessages(other.input_);
            }
          }
        }
        if (outputBuilder_ == null) {
          if (!other.output_.isEmpty()) {
            if (output_.isEmpty()) {
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureOutputIsMutable();
              output_.addAll(other.output_);
            }
            onChanged();
          }
        } else {
          if (!other.output_.isEmpty()) {
            if (outputBuilder_.isEmpty()) {
              outputBuilder_.dispose();
              outputBuilder_ = null;
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000020);
              outputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getOutputFieldBuilder() : null;
            } else {
              outputBuilder_.addAllMessages(other.output_);
            }
          }
        }
        if (other.hasOptimization()) {
          mergeOptimization(other.getOptimization());
        }
        if (instanceGroupBuilder_ == null) {
          if (!other.instanceGroup_.isEmpty()) {
            if (instanceGroup_.isEmpty()) {
              instanceGroup_ = other.instanceGroup_;
              bitField0_ = (bitField0_ & ~0x00000400);
            } else {
              ensureInstanceGroupIsMutable();
              instanceGroup_.addAll(other.instanceGroup_);
            }
            onChanged();
          }
        } else {
          if (!other.instanceGroup_.isEmpty()) {
            if (instanceGroupBuilder_.isEmpty()) {
              instanceGroupBuilder_.dispose();
              instanceGroupBuilder_ = null;
              instanceGroup_ = other.instanceGroup_;
              bitField0_ = (bitField0_ & ~0x00000400);
              instanceGroupBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getInstanceGroupFieldBuilder() : null;
            } else {
              instanceGroupBuilder_.addAllMessages(other.instanceGroup_);
            }
          }
        }
        if (!other.getDefaultModelFilename().isEmpty()) {
          defaultModelFilename_ = other.defaultModelFilename_;
          onChanged();
        }
        internalGetMutableCcModelFilenames().mergeFrom(
            other.internalGetCcModelFilenames());
        internalGetMutableMetricTags().mergeFrom(
            other.internalGetMetricTags());
        internalGetMutableParameters().mergeFrom(
            other.internalGetParameters());
        if (modelWarmupBuilder_ == null) {
          if (!other.modelWarmup_.isEmpty()) {
            if (modelWarmup_.isEmpty()) {
              modelWarmup_ = other.modelWarmup_;
              bitField0_ = (bitField0_ & ~0x00008000);
            } else {
              ensureModelWarmupIsMutable();
              modelWarmup_.addAll(other.modelWarmup_);
            }
            onChanged();
          }
        } else {
          if (!other.modelWarmup_.isEmpty()) {
            if (modelWarmupBuilder_.isEmpty()) {
              modelWarmupBuilder_.dispose();
              modelWarmupBuilder_ = null;
              modelWarmup_ = other.modelWarmup_;
              bitField0_ = (bitField0_ & ~0x00008000);
              modelWarmupBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getModelWarmupFieldBuilder() : null;
            } else {
              modelWarmupBuilder_.addAllMessages(other.modelWarmup_);
            }
          }
        }
        switch (other.getSchedulingChoiceCase()) {
          case DYNAMIC_BATCHING: {
            mergeDynamicBatching(other.getDynamicBatching());
            break;
          }
          case SEQUENCE_BATCHING: {
            mergeSequenceBatching(other.getSequenceBatching());
            break;
          }
          case ENSEMBLE_SCHEDULING: {
            mergeEnsembleScheduling(other.getEnsembleScheduling());
            break;
          }
          case SCHEDULINGCHOICE_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int schedulingChoiceCase_ = 0;
      private java.lang.Object schedulingChoice_;
      public SchedulingChoiceCase
          getSchedulingChoiceCase() {
        return SchedulingChoiceCase.forNumber(
            schedulingChoiceCase_);
      }

      public Builder clearSchedulingChoice() {
        schedulingChoiceCase_ = 0;
        schedulingChoice_ = null;
        onChanged();
        return this;
      }

      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object platform_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public java.lang.String getPlatform() {
        java.lang.Object ref = platform_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          platform_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public com.google.protobuf.ByteString
          getPlatformBytes() {
        java.lang.Object ref = platform_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          platform_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public Builder setPlatform(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        platform_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public Builder clearPlatform() {
        
        platform_ = getDefaultInstance().getPlatform();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "caffe2_netdef",
       *&#64;&#64;     "onnxruntime_onnx", "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       */
      public Builder setPlatformBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        platform_ = value;
        onChanged();
        return this;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy versionPolicy_ = null;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder> versionPolicyBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public boolean hasVersionPolicy() {
        return versionPolicyBuilder_ != null || versionPolicy_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy() {
        if (versionPolicyBuilder_ == null) {
          return versionPolicy_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
        } else {
          return versionPolicyBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder setVersionPolicy(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy value) {
        if (versionPolicyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          versionPolicy_ = value;
          onChanged();
        } else {
          versionPolicyBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder setVersionPolicy(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder builderForValue) {
        if (versionPolicyBuilder_ == null) {
          versionPolicy_ = builderForValue.build();
          onChanged();
        } else {
          versionPolicyBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder mergeVersionPolicy(nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy value) {
        if (versionPolicyBuilder_ == null) {
          if (versionPolicy_ != null) {
            versionPolicy_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.newBuilder(versionPolicy_).mergeFrom(value).buildPartial();
          } else {
            versionPolicy_ = value;
          }
          onChanged();
        } else {
          versionPolicyBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder clearVersionPolicy() {
        if (versionPolicyBuilder_ == null) {
          versionPolicy_ = null;
          onChanged();
        } else {
          versionPolicy_ = null;
          versionPolicyBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder getVersionPolicyBuilder() {
        
        onChanged();
        return getVersionPolicyFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder getVersionPolicyOrBuilder() {
        if (versionPolicyBuilder_ != null) {
          return versionPolicyBuilder_.getMessageOrBuilder();
        } else {
          return versionPolicy_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelVersionPolicy version_policy = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder> 
          getVersionPolicyFieldBuilder() {
        if (versionPolicyBuilder_ == null) {
          versionPolicyBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelVersionPolicyOrBuilder>(
                  getVersionPolicy(),
                  getParentForChildren(),
                  isClean());
          versionPolicy_ = null;
        }
        return versionPolicyBuilder_;
      }

      private int maxBatchSize_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       */
      public int getMaxBatchSize() {
        return maxBatchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       */
      public Builder setMaxBatchSize(int value) {
        
        maxBatchSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       */
      public Builder clearMaxBatchSize() {
        
        maxBatchSize_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> input_ =
        java.util.Collections.emptyList();
      private void ensureInputIsMutable() {
        if (!((bitField0_ & 0x00000010) == 0x00000010)) {
          input_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput>(input_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> inputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> getInputList() {
        if (inputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(input_);
        } else {
          return inputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public int getInputCount() {
        if (inputBuilder_ == null) {
          return input_.size();
        } else {
          return inputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput getInput(int index) {
        if (inputBuilder_ == null) {
          return input_.get(index);
        } else {
          return inputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder setInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.set(index, value);
          onChanged();
        } else {
          inputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder setInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.set(index, builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addInput(nvidia.inferenceserver.ModelConfigOuterClass.ModelInput value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.add(value);
          onChanged();
        } else {
          inputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.add(index, value);
          onChanged();
        } else {
          inputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addInput(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.add(builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addInput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.add(index, builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder addAllInput(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInput> values) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, input_);
          onChanged();
        } else {
          inputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder clearInput() {
        if (inputBuilder_ == null) {
          input_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          inputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public Builder removeInput(int index) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.remove(index);
          onChanged();
        } else {
          inputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder getInputBuilder(
          int index) {
        return getInputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
          int index) {
        if (inputBuilder_ == null) {
          return input_.get(index);  } else {
          return inputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> 
           getInputOrBuilderList() {
        if (inputBuilder_ != null) {
          return inputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(input_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder addInputBuilder() {
        return getInputFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder addInputBuilder(
          int index) {
        return getInputFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInput input = 5;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder> 
           getInputBuilderList() {
        return getInputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder> 
          getInputFieldBuilder() {
        if (inputBuilder_ == null) {
          inputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelInput, nvidia.inferenceserver.ModelConfigOuterClass.ModelInput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInputOrBuilder>(
                  input_,
                  ((bitField0_ & 0x00000010) == 0x00000010),
                  getParentForChildren(),
                  isClean());
          input_ = null;
        }
        return inputBuilder_;
      }

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> output_ =
        java.util.Collections.emptyList();
      private void ensureOutputIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          output_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput>(output_);
          bitField0_ |= 0x00000020;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> outputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> getOutputList() {
        if (outputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(output_);
        } else {
          return outputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public int getOutputCount() {
        if (outputBuilder_ == null) {
          return output_.size();
        } else {
          return outputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput getOutput(int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);
        } else {
          return outputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder setOutput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.set(index, value);
          onChanged();
        } else {
          outputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder setOutput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.set(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addOutput(nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(value);
          onChanged();
        } else {
          outputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(index, value);
          onChanged();
        } else {
          outputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder addAllOutput(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput> values) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, output_);
          onChanged();
        } else {
          outputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder clearOutput() {
        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          outputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public Builder removeOutput(int index) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.remove(index);
          onChanged();
        } else {
          outputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder getOutputBuilder(
          int index) {
        return getOutputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
          int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);  } else {
          return outputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> 
           getOutputOrBuilderList() {
        if (outputBuilder_ != null) {
          return outputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(output_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder addOutputBuilder() {
        return getOutputFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder addOutputBuilder(
          int index) {
        return getOutputFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelOutput output = 6;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder> 
           getOutputBuilderList() {
        return getOutputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder> 
          getOutputFieldBuilder() {
        if (outputBuilder_ == null) {
          outputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutput.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOutputOrBuilder>(
                  output_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          output_ = null;
        }
        return outputBuilder_;
      }

      private nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy optimization_ = null;
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder> optimizationBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public boolean hasOptimization() {
        return optimizationBuilder_ != null || optimization_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization() {
        if (optimizationBuilder_ == null) {
          return optimization_ == null ? nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
        } else {
          return optimizationBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder setOptimization(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy value) {
        if (optimizationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          optimization_ = value;
          onChanged();
        } else {
          optimizationBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder setOptimization(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder builderForValue) {
        if (optimizationBuilder_ == null) {
          optimization_ = builderForValue.build();
          onChanged();
        } else {
          optimizationBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder mergeOptimization(nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy value) {
        if (optimizationBuilder_ == null) {
          if (optimization_ != null) {
            optimization_ =
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.newBuilder(optimization_).mergeFrom(value).buildPartial();
          } else {
            optimization_ = value;
          }
          onChanged();
        } else {
          optimizationBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder clearOptimization() {
        if (optimizationBuilder_ == null) {
          optimization_ = null;
          onChanged();
        } else {
          optimization_ = null;
          optimizationBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder getOptimizationBuilder() {
        
        onChanged();
        return getOptimizationFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder getOptimizationOrBuilder() {
        if (optimizationBuilder_ != null) {
          return optimizationBuilder_.getMessageOrBuilder();
        } else {
          return optimization_ == null ?
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelOptimizationPolicy optimization = 12;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder> 
          getOptimizationFieldBuilder() {
        if (optimizationBuilder_ == null) {
          optimizationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicy.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder>(
                  getOptimization(),
                  getParentForChildren(),
                  isClean());
          optimization_ = null;
        }
        return optimizationBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder> dynamicBatchingBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public boolean hasDynamicBatching() {
        return schedulingChoiceCase_ == 11;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching() {
        if (dynamicBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 11) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
        } else {
          if (schedulingChoiceCase_ == 11) {
            return dynamicBatchingBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder setDynamicBatching(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching value) {
        if (dynamicBatchingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          schedulingChoice_ = value;
          onChanged();
        } else {
          dynamicBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 11;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder setDynamicBatching(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder builderForValue) {
        if (dynamicBatchingBuilder_ == null) {
          schedulingChoice_ = builderForValue.build();
          onChanged();
        } else {
          dynamicBatchingBuilder_.setMessage(builderForValue.build());
        }
        schedulingChoiceCase_ = 11;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder mergeDynamicBatching(nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching value) {
        if (dynamicBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 11 &&
              schedulingChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance()) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            schedulingChoice_ = value;
          }
          onChanged();
        } else {
          if (schedulingChoiceCase_ == 11) {
            dynamicBatchingBuilder_.mergeFrom(value);
          }
          dynamicBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 11;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder clearDynamicBatching() {
        if (dynamicBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 11) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
            onChanged();
          }
        } else {
          if (schedulingChoiceCase_ == 11) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
          }
          dynamicBatchingBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder getDynamicBatchingBuilder() {
        return getDynamicBatchingFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder getDynamicBatchingOrBuilder() {
        if ((schedulingChoiceCase_ == 11) && (dynamicBatchingBuilder_ != null)) {
          return dynamicBatchingBuilder_.getMessageOrBuilder();
        } else {
          if (schedulingChoiceCase_ == 11) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder> 
          getDynamicBatchingFieldBuilder() {
        if (dynamicBatchingBuilder_ == null) {
          if (!(schedulingChoiceCase_ == 11)) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
          }
          dynamicBatchingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_,
                  getParentForChildren(),
                  isClean());
          schedulingChoice_ = null;
        }
        schedulingChoiceCase_ = 11;
        onChanged();;
        return dynamicBatchingBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder> sequenceBatchingBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public boolean hasSequenceBatching() {
        return schedulingChoiceCase_ == 13;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching() {
        if (sequenceBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 13) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
        } else {
          if (schedulingChoiceCase_ == 13) {
            return sequenceBatchingBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder setSequenceBatching(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching value) {
        if (sequenceBatchingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          schedulingChoice_ = value;
          onChanged();
        } else {
          sequenceBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 13;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder setSequenceBatching(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder builderForValue) {
        if (sequenceBatchingBuilder_ == null) {
          schedulingChoice_ = builderForValue.build();
          onChanged();
        } else {
          sequenceBatchingBuilder_.setMessage(builderForValue.build());
        }
        schedulingChoiceCase_ = 13;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder mergeSequenceBatching(nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching value) {
        if (sequenceBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 13 &&
              schedulingChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance()) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            schedulingChoice_ = value;
          }
          onChanged();
        } else {
          if (schedulingChoiceCase_ == 13) {
            sequenceBatchingBuilder_.mergeFrom(value);
          }
          sequenceBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 13;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder clearSequenceBatching() {
        if (sequenceBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 13) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
            onChanged();
          }
        } else {
          if (schedulingChoiceCase_ == 13) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
          }
          sequenceBatchingBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder getSequenceBatchingBuilder() {
        return getSequenceBatchingFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder getSequenceBatchingOrBuilder() {
        if ((schedulingChoiceCase_ == 13) && (sequenceBatchingBuilder_ != null)) {
          return sequenceBatchingBuilder_.getMessageOrBuilder();
        } else {
          if (schedulingChoiceCase_ == 13) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelSequenceBatching sequence_batching = 13;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder> 
          getSequenceBatchingFieldBuilder() {
        if (sequenceBatchingBuilder_ == null) {
          if (!(schedulingChoiceCase_ == 13)) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
          }
          sequenceBatchingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_,
                  getParentForChildren(),
                  isClean());
          schedulingChoice_ = null;
        }
        schedulingChoiceCase_ = 13;
        onChanged();;
        return sequenceBatchingBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder> ensembleSchedulingBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public boolean hasEnsembleScheduling() {
        return schedulingChoiceCase_ == 15;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling() {
        if (ensembleSchedulingBuilder_ == null) {
          if (schedulingChoiceCase_ == 15) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
        } else {
          if (schedulingChoiceCase_ == 15) {
            return ensembleSchedulingBuilder_.getMessage();
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder setEnsembleScheduling(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling value) {
        if (ensembleSchedulingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          schedulingChoice_ = value;
          onChanged();
        } else {
          ensembleSchedulingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 15;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder setEnsembleScheduling(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder builderForValue) {
        if (ensembleSchedulingBuilder_ == null) {
          schedulingChoice_ = builderForValue.build();
          onChanged();
        } else {
          ensembleSchedulingBuilder_.setMessage(builderForValue.build());
        }
        schedulingChoiceCase_ = 15;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder mergeEnsembleScheduling(nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling value) {
        if (ensembleSchedulingBuilder_ == null) {
          if (schedulingChoiceCase_ == 15 &&
              schedulingChoice_ != nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance()) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.newBuilder((nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            schedulingChoice_ = value;
          }
          onChanged();
        } else {
          if (schedulingChoiceCase_ == 15) {
            ensembleSchedulingBuilder_.mergeFrom(value);
          }
          ensembleSchedulingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 15;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder clearEnsembleScheduling() {
        if (ensembleSchedulingBuilder_ == null) {
          if (schedulingChoiceCase_ == 15) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
            onChanged();
          }
        } else {
          if (schedulingChoiceCase_ == 15) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
          }
          ensembleSchedulingBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder getEnsembleSchedulingBuilder() {
        return getEnsembleSchedulingFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder getEnsembleSchedulingOrBuilder() {
        if ((schedulingChoiceCase_ == 15) && (ensembleSchedulingBuilder_ != null)) {
          return ensembleSchedulingBuilder_.getMessageOrBuilder();
        } else {
          if (schedulingChoiceCase_ == 15) {
            return (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
          }
          return nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.nvidia.inferenceserver.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder> 
          getEnsembleSchedulingFieldBuilder() {
        if (ensembleSchedulingBuilder_ == null) {
          if (!(schedulingChoiceCase_ == 15)) {
            schedulingChoice_ = nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
          }
          ensembleSchedulingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsemblingOrBuilder>(
                  (nvidia.inferenceserver.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_,
                  getParentForChildren(),
                  isClean());
          schedulingChoice_ = null;
        }
        schedulingChoiceCase_ = 15;
        onChanged();;
        return ensembleSchedulingBuilder_;
      }

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> instanceGroup_ =
        java.util.Collections.emptyList();
      private void ensureInstanceGroupIsMutable() {
        if (!((bitField0_ & 0x00000400) == 0x00000400)) {
          instanceGroup_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup>(instanceGroup_);
          bitField0_ |= 0x00000400;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> instanceGroupBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> getInstanceGroupList() {
        if (instanceGroupBuilder_ == null) {
          return java.util.Collections.unmodifiableList(instanceGroup_);
        } else {
          return instanceGroupBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public int getInstanceGroupCount() {
        if (instanceGroupBuilder_ == null) {
          return instanceGroup_.size();
        } else {
          return instanceGroupBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index) {
        if (instanceGroupBuilder_ == null) {
          return instanceGroup_.get(index);
        } else {
          return instanceGroupBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder setInstanceGroup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup value) {
        if (instanceGroupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInstanceGroupIsMutable();
          instanceGroup_.set(index, value);
          onChanged();
        } else {
          instanceGroupBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder setInstanceGroup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.set(index, builderForValue.build());
          onChanged();
        } else {
          instanceGroupBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup value) {
        if (instanceGroupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(value);
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup value) {
        if (instanceGroupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(index, value);
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(builderForValue.build());
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(index, builderForValue.build());
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addAllInstanceGroup(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup> values) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, instanceGroup_);
          onChanged();
        } else {
          instanceGroupBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder clearInstanceGroup() {
        if (instanceGroupBuilder_ == null) {
          instanceGroup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000400);
          onChanged();
        } else {
          instanceGroupBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder removeInstanceGroup(int index) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.remove(index);
          onChanged();
        } else {
          instanceGroupBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder getInstanceGroupBuilder(
          int index) {
        return getInstanceGroupFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
          int index) {
        if (instanceGroupBuilder_ == null) {
          return instanceGroup_.get(index);  } else {
          return instanceGroupBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
           getInstanceGroupOrBuilderList() {
        if (instanceGroupBuilder_ != null) {
          return instanceGroupBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(instanceGroup_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder addInstanceGroupBuilder() {
        return getInstanceGroupFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder addInstanceGroupBuilder(
          int index) {
        return getInstanceGroupFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelInstanceGroup instance_group = 7;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder> 
           getInstanceGroupBuilderList() {
        return getInstanceGroupFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
          getInstanceGroupFieldBuilder() {
        if (instanceGroupBuilder_ == null) {
          instanceGroupBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroup.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelInstanceGroupOrBuilder>(
                  instanceGroup_,
                  ((bitField0_ & 0x00000400) == 0x00000400),
                  getParentForChildren(),
                  isClean());
          instanceGroup_ = null;
        }
        return instanceGroupBuilder_;
      }

      private java.lang.Object defaultModelFilename_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public java.lang.String getDefaultModelFilename() {
        java.lang.Object ref = defaultModelFilename_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          defaultModelFilename_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public com.google.protobuf.ByteString
          getDefaultModelFilenameBytes() {
        java.lang.Object ref = defaultModelFilename_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          defaultModelFilename_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public Builder setDefaultModelFilename(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        defaultModelFilename_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public Builder clearDefaultModelFilename() {
        
        defaultModelFilename_ = getDefaultInstance().getDefaultModelFilename();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.netdef' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       */
      public Builder setDefaultModelFilenameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        defaultModelFilename_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> ccModelFilenames_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetCcModelFilenames() {
        if (ccModelFilenames_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              CcModelFilenamesDefaultEntryHolder.defaultEntry);
        }
        return ccModelFilenames_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableCcModelFilenames() {
        onChanged();;
        if (ccModelFilenames_ == null) {
          ccModelFilenames_ = com.google.protobuf.MapField.newMapField(
              CcModelFilenamesDefaultEntryHolder.defaultEntry);
        }
        if (!ccModelFilenames_.isMutable()) {
          ccModelFilenames_ = ccModelFilenames_.copy();
        }
        return ccModelFilenames_;
      }

      public int getCcModelFilenamesCount() {
        return internalGetCcModelFilenames().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public boolean containsCcModelFilenames(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetCcModelFilenames().getMap().containsKey(key);
      }
      /**
       * Use {@link #getCcModelFilenamesMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenames() {
        return getCcModelFilenamesMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenamesMap() {
        return internalGetCcModelFilenames().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public java.lang.String getCcModelFilenamesOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetCcModelFilenames().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public java.lang.String getCcModelFilenamesOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetCcModelFilenames().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearCcModelFilenames() {
        internalGetMutableCcModelFilenames().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public Builder removeCcModelFilenames(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableCcModelFilenames().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableCcModelFilenames() {
        return internalGetMutableCcModelFilenames().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      public Builder putCcModelFilenames(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableCcModelFilenames().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public Builder putAllCcModelFilenames(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableCcModelFilenames().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> metricTags_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMetricTags() {
        if (metricTags_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              MetricTagsDefaultEntryHolder.defaultEntry);
        }
        return metricTags_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableMetricTags() {
        onChanged();;
        if (metricTags_ == null) {
          metricTags_ = com.google.protobuf.MapField.newMapField(
              MetricTagsDefaultEntryHolder.defaultEntry);
        }
        if (!metricTags_.isMutable()) {
          metricTags_ = metricTags_.copy();
        }
        return metricTags_;
      }

      public int getMetricTagsCount() {
        return internalGetMetricTags().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public boolean containsMetricTags(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetMetricTags().getMap().containsKey(key);
      }
      /**
       * Use {@link #getMetricTagsMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getMetricTags() {
        return getMetricTagsMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public java.util.Map<java.lang.String, java.lang.String> getMetricTagsMap() {
        return internalGetMetricTags().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public java.lang.String getMetricTagsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetricTags().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public java.lang.String getMetricTagsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetricTags().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearMetricTags() {
        internalGetMutableMetricTags().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public Builder removeMetricTags(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetricTags().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableMetricTags() {
        return internalGetMutableMetricTags().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      public Builder putMetricTags(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetricTags().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public Builder putAllMetricTags(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableMetricTags().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> parameters_;
      private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
      internalGetParameters() {
        if (parameters_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ParametersDefaultEntryHolder.defaultEntry);
        }
        return parameters_;
      }
      private com.google.protobuf.MapField<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
      internalGetMutableParameters() {
        onChanged();;
        if (parameters_ == null) {
          parameters_ = com.google.protobuf.MapField.newMapField(
              ParametersDefaultEntryHolder.defaultEntry);
        }
        if (!parameters_.isMutable()) {
          parameters_ = parameters_.copy();
        }
        return parameters_;
      }

      public int getParametersCount() {
        return internalGetParameters().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public boolean containsParameters(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetParameters().getMap().containsKey(key);
      }
      /**
       * Use {@link #getParametersMap()} instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> getParameters() {
        return getParametersMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> getParametersMap() {
        return internalGetParameters().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
          java.lang.String key,
          nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> map =
            internalGetParameters().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> map =
            internalGetParameters().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearParameters() {
        internalGetMutableParameters().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public Builder removeParameters(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableParameters().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter>
      getMutableParameters() {
        return internalGetMutableParameters().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */
      public Builder putParameters(
          java.lang.String key,
          nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableParameters().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .nvidia.inferenceserver.ModelParameter&gt; parameters = 14;</code>
       */

      public Builder putAllParameters(
          java.util.Map<java.lang.String, nvidia.inferenceserver.ModelConfigOuterClass.ModelParameter> values) {
        internalGetMutableParameters().getMutableMap()
            .putAll(values);
        return this;
      }

      private java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup> modelWarmup_ =
        java.util.Collections.emptyList();
      private void ensureModelWarmupIsMutable() {
        if (!((bitField0_ & 0x00008000) == 0x00008000)) {
          modelWarmup_ = new java.util.ArrayList<nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup>(modelWarmup_);
          bitField0_ |= 0x00008000;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder> modelWarmupBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup> getModelWarmupList() {
        if (modelWarmupBuilder_ == null) {
          return java.util.Collections.unmodifiableList(modelWarmup_);
        } else {
          return modelWarmupBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public int getModelWarmupCount() {
        if (modelWarmupBuilder_ == null) {
          return modelWarmup_.size();
        } else {
          return modelWarmupBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup getModelWarmup(int index) {
        if (modelWarmupBuilder_ == null) {
          return modelWarmup_.get(index);
        } else {
          return modelWarmupBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public Builder setModelWarmup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup value) {
        if (modelWarmupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureModelWarmupIsMutable();
          modelWarmup_.set(index, value);
          onChanged();
        } else {
          modelWarmupBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public Builder setModelWarmup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder builderForValue) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          modelWarmup_.set(index, builderForValue.build());
          onChanged();
        } else {
          modelWarmupBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup value) {
        if (modelWarmupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureModelWarmupIsMutable();
          modelWarmup_.add(value);
          onChanged();
        } else {
          modelWarmupBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup value) {
        if (modelWarmupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureModelWarmupIsMutable();
          modelWarmup_.add(index, value);
          onChanged();
        } else {
          modelWarmupBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(
          nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder builderForValue) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          modelWarmup_.add(builderForValue.build());
          onChanged();
        } else {
          modelWarmupBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(
          int index, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder builderForValue) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          modelWarmup_.add(index, builderForValue.build());
          onChanged();
        } else {
          modelWarmupBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addAllModelWarmup(
          java.lang.Iterable<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup> values) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, modelWarmup_);
          onChanged();
        } else {
          modelWarmupBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public Builder clearModelWarmup() {
        if (modelWarmupBuilder_ == null) {
          modelWarmup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00008000);
          onChanged();
        } else {
          modelWarmupBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public Builder removeModelWarmup(int index) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          modelWarmup_.remove(index);
          onChanged();
        } else {
          modelWarmupBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder getModelWarmupBuilder(
          int index) {
        return getModelWarmupFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder getModelWarmupOrBuilder(
          int index) {
        if (modelWarmupBuilder_ == null) {
          return modelWarmup_.get(index);  } else {
          return modelWarmupBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public java.util.List<? extends nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder> 
           getModelWarmupOrBuilderList() {
        if (modelWarmupBuilder_ != null) {
          return modelWarmupBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(modelWarmup_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder addModelWarmupBuilder() {
        return getModelWarmupFieldBuilder().addBuilder(
            nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder addModelWarmupBuilder(
          int index) {
        return getModelWarmupFieldBuilder().addBuilder(
            index, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .nvidia.inferenceserver.ModelWarmup model_warmup = 16;</code>
       */
      public java.util.List<nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder> 
           getModelWarmupBuilderList() {
        return getModelWarmupFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder> 
          getModelWarmupFieldBuilder() {
        if (modelWarmupBuilder_ == null) {
          modelWarmupBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmup.Builder, nvidia.inferenceserver.ModelConfigOuterClass.ModelWarmupOrBuilder>(
                  modelWarmup_,
                  ((bitField0_ & 0x00008000) == 0x00008000),
                  getParentForChildren(),
                  isClean());
          modelWarmup_ = null;
        }
        return modelWarmupBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFieldsProto3(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:nvidia.inferenceserver.ModelConfig)
    }

    // @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig)
    private static final nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig();
    }

    public static nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelConfig>
        PARSER = new com.google.protobuf.AbstractParser<ModelConfig>() {
      @java.lang.Override
      public ModelConfig parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelConfig(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelConfig> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelConfig> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public nvidia.inferenceserver.ModelConfigOuterClass.ModelConfig getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelInstanceGroup_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelTensorReshape_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelInput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelInput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOutput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOutput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelDynamicBatching_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelEnsembling_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelParameter_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelParameter_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelWarmup_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelWarmup_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelWarmup_Input_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelWarmup_Input_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelWarmup_InputsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelWarmup_InputsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelConfig_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelConfig_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\022model_config.proto\022\026nvidia.inferencese" +
      "rver\"\322\001\n\022ModelInstanceGroup\022\014\n\004name\030\001 \001(" +
      "\t\022=\n\004kind\030\004 \001(\0162/.nvidia.inferenceserver" +
      ".ModelInstanceGroup.Kind\022\r\n\005count\030\002 \001(\005\022" +
      "\014\n\004gpus\030\003 \003(\005\022\017\n\007profile\030\005 \003(\t\"A\n\004Kind\022\r" +
      "\n\tKIND_AUTO\020\000\022\014\n\010KIND_GPU\020\001\022\014\n\010KIND_CPU\020" +
      "\002\022\016\n\nKIND_MODEL\020\003\"#\n\022ModelTensorReshape\022" +
      "\r\n\005shape\030\001 \003(\003\"\222\002\n\nModelInput\022\014\n\004name\030\001 " +
      "\001(\t\0223\n\tdata_type\030\002 \001(\0162 .nvidia.inferenc" +
      "eserver.DataType\0229\n\006format\030\003 \001(\0162).nvidi" +
      "a.inferenceserver.ModelInput.Format\022\014\n\004d" +
      "ims\030\004 \003(\003\022;\n\007reshape\030\005 \001(\0132*.nvidia.infe" +
      "renceserver.ModelTensorReshape\";\n\006Format" +
      "\022\017\n\013FORMAT_NONE\020\000\022\017\n\013FORMAT_NHWC\020\001\022\017\n\013FO" +
      "RMAT_NCHW\020\002\"\263\001\n\013ModelOutput\022\014\n\004name\030\001 \001(" +
      "\t\0223\n\tdata_type\030\002 \001(\0162 .nvidia.inferences" +
      "erver.DataType\022\014\n\004dims\030\003 \003(\003\022;\n\007reshape\030" +
      "\005 \001(\0132*.nvidia.inferenceserver.ModelTens" +
      "orReshape\022\026\n\016label_filename\030\004 \001(\t\"\267\002\n\022Mo" +
      "delVersionPolicy\022C\n\006latest\030\001 \001(\01321.nvidi" +
      "a.inferenceserver.ModelVersionPolicy.Lat" +
      "estH\000\022=\n\003all\030\002 \001(\0132..nvidia.inferenceser" +
      "ver.ModelVersionPolicy.AllH\000\022G\n\010specific" +
      "\030\003 \001(\01323.nvidia.inferenceserver.ModelVer" +
      "sionPolicy.SpecificH\000\032\036\n\006Latest\022\024\n\014num_v" +
      "ersions\030\001 \001(\r\032\005\n\003All\032\034\n\010Specific\022\020\n\010vers" +
      "ions\030\001 \003(\003B\017\n\rpolicy_choice\"\244\007\n\027ModelOpt" +
      "imizationPolicy\022D\n\005graph\030\001 \001(\01325.nvidia." +
      "inferenceserver.ModelOptimizationPolicy." +
      "Graph\022O\n\010priority\030\002 \001(\0162=.nvidia.inferen" +
      "ceserver.ModelOptimizationPolicy.ModelPr" +
      "iority\022B\n\004cuda\030\003 \001(\01324.nvidia.inferences" +
      "erver.ModelOptimizationPolicy.Cuda\022e\n\026ex" +
      "ecution_accelerators\030\004 \001(\0132E.nvidia.infe" +
      "renceserver.ModelOptimizationPolicy.Exec" +
      "utionAccelerators\032\026\n\005Graph\022\r\n\005level\030\001 \001(" +
      "\005\032\026\n\004Cuda\022\016\n\006graphs\030\001 \001(\010\032\313\003\n\025ExecutionA" +
      "ccelerators\022t\n\031gpu_execution_accelerator" +
      "\030\001 \003(\0132Q.nvidia.inferenceserver.ModelOpt" +
      "imizationPolicy.ExecutionAccelerators.Ac" +
      "celerator\022t\n\031cpu_execution_accelerator\030\002" +
      " \003(\0132Q.nvidia.inferenceserver.ModelOptim" +
      "izationPolicy.ExecutionAccelerators.Acce" +
      "lerator\032\305\001\n\013Accelerator\022\014\n\004name\030\001 \001(\t\022u\n" +
      "\nparameters\030\002 \003(\0132a.nvidia.inferenceserv" +
      "er.ModelOptimizationPolicy.ExecutionAcce" +
      "lerators.Accelerator.ParametersEntry\0321\n\017" +
      "ParametersEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 " +
      "\001(\t:\0028\001\"I\n\rModelPriority\022\024\n\020PRIORITY_DEF" +
      "AULT\020\000\022\020\n\014PRIORITY_MAX\020\001\022\020\n\014PRIORITY_MIN" +
      "\020\002\"Z\n\024ModelDynamicBatching\022\034\n\024preferred_" +
      "batch_size\030\001 \003(\005\022$\n\034max_queue_delay_micr" +
      "oseconds\030\002 \001(\004\"\351\006\n\025ModelSequenceBatching" +
      "\022N\n\006direct\030\003 \001(\0132<.nvidia.inferenceserve" +
      "r.ModelSequenceBatching.StrategyDirectH\000" +
      "\022N\n\006oldest\030\004 \001(\0132<.nvidia.inferenceserve" +
      "r.ModelSequenceBatching.StrategyOldestH\000" +
      "\022&\n\036max_sequence_idle_microseconds\030\001 \001(\004" +
      "\022Q\n\rcontrol_input\030\002 \003(\0132:.nvidia.inferen" +
      "ceserver.ModelSequenceBatching.ControlIn" +
      "put\032\262\002\n\007Control\022H\n\004kind\030\001 \001(\0162:.nvidia.i" +
      "nferenceserver.ModelSequenceBatching.Con" +
      "trol.Kind\022\030\n\020int32_false_true\030\002 \003(\005\022\027\n\017f" +
      "p32_false_true\030\003 \003(\002\0223\n\tdata_type\030\004 \001(\0162" +
      " .nvidia.inferenceserver.DataType\"u\n\004Kin" +
      "d\022\032\n\026CONTROL_SEQUENCE_START\020\000\022\032\n\026CONTROL" +
      "_SEQUENCE_READY\020\001\022\030\n\024CONTROL_SEQUENCE_EN" +
      "D\020\002\022\033\n\027CONTROL_SEQUENCE_CORRID\020\003\032d\n\014Cont" +
      "rolInput\022\014\n\004name\030\001 \001(\t\022F\n\007control\030\002 \003(\0132" +
      "5.nvidia.inferenceserver.ModelSequenceBa" +
      "tching.Control\032\020\n\016StrategyDirect\032u\n\016Stra" +
      "tegyOldest\022\037\n\027max_candidate_sequences\030\001 " +
      "\001(\005\022\034\n\024preferred_batch_size\030\002 \003(\005\022$\n\034max" +
      "_queue_delay_microseconds\030\003 \001(\004B\021\n\017strat" +
      "egy_choice\"\204\003\n\017ModelEnsembling\022:\n\004step\030\001" +
      " \003(\0132,.nvidia.inferenceserver.ModelEnsem" +
      "bling.Step\032\264\002\n\004Step\022\022\n\nmodel_name\030\001 \001(\t\022" +
      "\025\n\rmodel_version\030\002 \001(\003\022M\n\tinput_map\030\003 \003(" +
      "\0132:.nvidia.inferenceserver.ModelEnsembli" +
      "ng.Step.InputMapEntry\022O\n\noutput_map\030\004 \003(" +
      "\0132;.nvidia.inferenceserver.ModelEnsembli" +
      "ng.Step.OutputMapEntry\032/\n\rInputMapEntry\022" +
      "\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001\0320\n\016Outpu" +
      "tMapEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028" +
      "\001\"&\n\016ModelParameter\022\024\n\014string_value\030\001 \001(" +
      "\t\"\361\002\n\013ModelWarmup\022\014\n\004name\030\001 \001(\t\022\022\n\nbatch" +
      "_size\030\002 \001(\r\022?\n\006inputs\030\003 \003(\0132/.nvidia.inf" +
      "erenceserver.ModelWarmup.InputsEntry\032\244\001\n" +
      "\005Input\0223\n\tdata_type\030\001 \001(\0162 .nvidia.infer" +
      "enceserver.DataType\022\014\n\004dims\030\002 \003(\003\022\023\n\tzer" +
      "o_data\030\003 \001(\010H\000\022\025\n\013random_data\030\004 \001(\010H\000\022\031\n" +
      "\017input_data_file\030\005 \001(\tH\000B\021\n\017input_data_t" +
      "ype\032X\n\013InputsEntry\022\013\n\003key\030\001 \001(\t\0228\n\005value" +
      "\030\002 \001(\0132).nvidia.inferenceserver.ModelWar" +
      "mup.Input:\0028\001\"\373\010\n\013ModelConfig\022\014\n\004name\030\001 " +
      "\001(\t\022\020\n\010platform\030\002 \001(\t\022B\n\016version_policy\030" +
      "\003 \001(\0132*.nvidia.inferenceserver.ModelVers" +
      "ionPolicy\022\026\n\016max_batch_size\030\004 \001(\005\0221\n\005inp" +
      "ut\030\005 \003(\0132\".nvidia.inferenceserver.ModelI" +
      "nput\0223\n\006output\030\006 \003(\0132#.nvidia.inferences" +
      "erver.ModelOutput\022E\n\014optimization\030\014 \001(\0132" +
      "/.nvidia.inferenceserver.ModelOptimizati" +
      "onPolicy\022H\n\020dynamic_batching\030\013 \001(\0132,.nvi" +
      "dia.inferenceserver.ModelDynamicBatching" +
      "H\000\022J\n\021sequence_batching\030\r \001(\0132-.nvidia.i" +
      "nferenceserver.ModelSequenceBatchingH\000\022F" +
      "\n\023ensemble_scheduling\030\017 \001(\0132\'.nvidia.inf" +
      "erenceserver.ModelEnsemblingH\000\022B\n\016instan" +
      "ce_group\030\007 \003(\0132*.nvidia.inferenceserver." +
      "ModelInstanceGroup\022\036\n\026default_model_file" +
      "name\030\010 \001(\t\022U\n\022cc_model_filenames\030\t \003(\01329" +
      ".nvidia.inferenceserver.ModelConfig.CcMo" +
      "delFilenamesEntry\022H\n\013metric_tags\030\n \003(\01323" +
      ".nvidia.inferenceserver.ModelConfig.Metr" +
      "icTagsEntry\022G\n\nparameters\030\016 \003(\01323.nvidia" +
      ".inferenceserver.ModelConfig.ParametersE" +
      "ntry\0229\n\014model_warmup\030\020 \003(\0132#.nvidia.infe" +
      "renceserver.ModelWarmup\0327\n\025CcModelFilena" +
      "mesEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001" +
      "\0321\n\017MetricTagsEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005valu" +
      "e\030\002 \001(\t:\0028\001\032Y\n\017ParametersEntry\022\013\n\003key\030\001 " +
      "\001(\t\0225\n\005value\030\002 \001(\0132&.nvidia.inferenceser" +
      "ver.ModelParameter:\0028\001B\023\n\021scheduling_cho" +
      "ice*\353\001\n\010DataType\022\020\n\014TYPE_INVALID\020\000\022\r\n\tTY" +
      "PE_BOOL\020\001\022\016\n\nTYPE_UINT8\020\002\022\017\n\013TYPE_UINT16" +
      "\020\003\022\017\n\013TYPE_UINT32\020\004\022\017\n\013TYPE_UINT64\020\005\022\r\n\t" +
      "TYPE_INT8\020\006\022\016\n\nTYPE_INT16\020\007\022\016\n\nTYPE_INT3" +
      "2\020\010\022\016\n\nTYPE_INT64\020\t\022\r\n\tTYPE_FP16\020\n\022\r\n\tTY" +
      "PE_FP32\020\013\022\r\n\tTYPE_FP64\020\014\022\017\n\013TYPE_STRING\020" +
      "\rb\006proto3"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
        new com.google.protobuf.Descriptors.FileDescriptor.    InternalDescriptorAssigner() {
          public com.google.protobuf.ExtensionRegistry assignDescriptors(
              com.google.protobuf.Descriptors.FileDescriptor root) {
            descriptor = root;
            return null;
          }
        };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        }, assigner);
    internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelInstanceGroup_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelInstanceGroup_descriptor,
        new java.lang.String[] { "Name", "Kind", "Count", "Gpus", "Profile", });
    internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelTensorReshape_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelTensorReshape_descriptor,
        new java.lang.String[] { "Shape", });
    internal_static_nvidia_inferenceserver_ModelInput_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_nvidia_inferenceserver_ModelInput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelInput_descriptor,
        new java.lang.String[] { "Name", "DataType", "Format", "Dims", "Reshape", });
    internal_static_nvidia_inferenceserver_ModelOutput_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_nvidia_inferenceserver_ModelOutput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOutput_descriptor,
        new java.lang.String[] { "Name", "DataType", "Dims", "Reshape", "LabelFilename", });
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor,
        new java.lang.String[] { "Latest", "All", "Specific", "PolicyChoice", });
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor =
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionPolicy_Latest_descriptor,
        new java.lang.String[] { "NumVersions", });
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor =
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionPolicy_All_descriptor,
        new java.lang.String[] { });
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor =
      internal_static_nvidia_inferenceserver_ModelVersionPolicy_descriptor.getNestedTypes().get(2);
    internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelVersionPolicy_Specific_descriptor,
        new java.lang.String[] { "Versions", });
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor,
        new java.lang.String[] { "Graph", "Priority", "Cuda", "ExecutionAccelerators", });
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor =
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Graph_descriptor,
        new java.lang.String[] { "Level", });
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor =
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_Cuda_descriptor,
        new java.lang.String[] { "Graphs", });
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_descriptor =
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_descriptor.getNestedTypes().get(2);
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_descriptor,
        new java.lang.String[] { "GpuExecutionAccelerator", "CpuExecutionAccelerator", });
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor =
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor,
        new java.lang.String[] { "Name", "Parameters", });
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_nvidia_inferenceserver_ModelDynamicBatching_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelDynamicBatching_descriptor,
        new java.lang.String[] { "PreferredBatchSize", "MaxQueueDelayMicroseconds", });
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor,
        new java.lang.String[] { "Direct", "Oldest", "MaxSequenceIdleMicroseconds", "ControlInput", "StrategyChoice", });
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor =
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelSequenceBatching_Control_descriptor,
        new java.lang.String[] { "Kind", "Int32FalseTrue", "Fp32FalseTrue", "DataType", });
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor =
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelSequenceBatching_ControlInput_descriptor,
        new java.lang.String[] { "Name", "Control", });
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_descriptor =
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor.getNestedTypes().get(2);
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyDirect_descriptor,
        new java.lang.String[] { });
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_descriptor =
      internal_static_nvidia_inferenceserver_ModelSequenceBatching_descriptor.getNestedTypes().get(3);
    internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelSequenceBatching_StrategyOldest_descriptor,
        new java.lang.String[] { "MaxCandidateSequences", "PreferredBatchSize", "MaxQueueDelayMicroseconds", });
    internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_nvidia_inferenceserver_ModelEnsembling_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor,
        new java.lang.String[] { "Step", });
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor =
      internal_static_nvidia_inferenceserver_ModelEnsembling_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor,
        new java.lang.String[] { "ModelName", "ModelVersion", "InputMap", "OutputMap", });
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelEnsembling_Step_InputMapEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelEnsembling_Step_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelEnsembling_Step_OutputMapEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelParameter_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_nvidia_inferenceserver_ModelParameter_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelParameter_descriptor,
        new java.lang.String[] { "StringValue", });
    internal_static_nvidia_inferenceserver_ModelWarmup_descriptor =
      getDescriptor().getMessageTypes().get(10);
    internal_static_nvidia_inferenceserver_ModelWarmup_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelWarmup_descriptor,
        new java.lang.String[] { "Name", "BatchSize", "Inputs", });
    internal_static_nvidia_inferenceserver_ModelWarmup_Input_descriptor =
      internal_static_nvidia_inferenceserver_ModelWarmup_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelWarmup_Input_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelWarmup_Input_descriptor,
        new java.lang.String[] { "DataType", "Dims", "ZeroData", "RandomData", "InputDataFile", "InputDataType", });
    internal_static_nvidia_inferenceserver_ModelWarmup_InputsEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelWarmup_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelWarmup_InputsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelWarmup_InputsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelConfig_descriptor =
      getDescriptor().getMessageTypes().get(11);
    internal_static_nvidia_inferenceserver_ModelConfig_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelConfig_descriptor,
        new java.lang.String[] { "Name", "Platform", "VersionPolicy", "MaxBatchSize", "Input", "Output", "Optimization", "DynamicBatching", "SequenceBatching", "EnsembleScheduling", "InstanceGroup", "DefaultModelFilename", "CcModelFilenames", "MetricTags", "Parameters", "ModelWarmup", "SchedulingChoice", });
    internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelConfig_descriptor.getNestedTypes().get(0);
    internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelConfig_CcModelFilenamesEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelConfig_descriptor.getNestedTypes().get(1);
    internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelConfig_MetricTagsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_descriptor =
      internal_static_nvidia_inferenceserver_ModelConfig_descriptor.getNestedTypes().get(2);
    internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_nvidia_inferenceserver_ModelConfig_ParametersEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
  }

  // @@protoc_insertion_point(outer_class_scope)
}
