// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: model_config.proto

package inference;

public final class ModelConfigOuterClass {
  private ModelConfigOuterClass() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistryLite registry) {
  }

  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
    registerAllExtensions(
        (com.google.protobuf.ExtensionRegistryLite) registry);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:enum:: DataType
   *&#64;&#64;
   *&#64;&#64;   Data types supported for input and output tensors.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf enum {@code inference.DataType}
   */
  public enum DataType
      implements com.google.protobuf.ProtocolMessageEnum {
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INVALID = 0
     * </pre>
     *
     * <code>TYPE_INVALID = 0;</code>
     */
    TYPE_INVALID(0),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::BOOL = 1
     * </pre>
     *
     * <code>TYPE_BOOL = 1;</code>
     */
    TYPE_BOOL(1),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT8 = 2
     * </pre>
     *
     * <code>TYPE_UINT8 = 2;</code>
     */
    TYPE_UINT8(2),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT16 = 3
     * </pre>
     *
     * <code>TYPE_UINT16 = 3;</code>
     */
    TYPE_UINT16(3),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT32 = 4
     * </pre>
     *
     * <code>TYPE_UINT32 = 4;</code>
     */
    TYPE_UINT32(4),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT64 = 5
     * </pre>
     *
     * <code>TYPE_UINT64 = 5;</code>
     */
    TYPE_UINT64(5),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT8 = 6
     * </pre>
     *
     * <code>TYPE_INT8 = 6;</code>
     */
    TYPE_INT8(6),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT16 = 7
     * </pre>
     *
     * <code>TYPE_INT16 = 7;</code>
     */
    TYPE_INT16(7),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT32 = 8
     * </pre>
     *
     * <code>TYPE_INT32 = 8;</code>
     */
    TYPE_INT32(8),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT64 = 9
     * </pre>
     *
     * <code>TYPE_INT64 = 9;</code>
     */
    TYPE_INT64(9),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP16 = 10
     * </pre>
     *
     * <code>TYPE_FP16 = 10;</code>
     */
    TYPE_FP16(10),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP32 = 11
     * </pre>
     *
     * <code>TYPE_FP32 = 11;</code>
     */
    TYPE_FP32(11),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP64 = 12
     * </pre>
     *
     * <code>TYPE_FP64 = 12;</code>
     */
    TYPE_FP64(12),
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::STRING = 13
     * </pre>
     *
     * <code>TYPE_STRING = 13;</code>
     */
    TYPE_STRING(13),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INVALID = 0
     * </pre>
     *
     * <code>TYPE_INVALID = 0;</code>
     */
    public static final int TYPE_INVALID_VALUE = 0;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::BOOL = 1
     * </pre>
     *
     * <code>TYPE_BOOL = 1;</code>
     */
    public static final int TYPE_BOOL_VALUE = 1;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT8 = 2
     * </pre>
     *
     * <code>TYPE_UINT8 = 2;</code>
     */
    public static final int TYPE_UINT8_VALUE = 2;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT16 = 3
     * </pre>
     *
     * <code>TYPE_UINT16 = 3;</code>
     */
    public static final int TYPE_UINT16_VALUE = 3;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT32 = 4
     * </pre>
     *
     * <code>TYPE_UINT32 = 4;</code>
     */
    public static final int TYPE_UINT32_VALUE = 4;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::UINT64 = 5
     * </pre>
     *
     * <code>TYPE_UINT64 = 5;</code>
     */
    public static final int TYPE_UINT64_VALUE = 5;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT8 = 6
     * </pre>
     *
     * <code>TYPE_INT8 = 6;</code>
     */
    public static final int TYPE_INT8_VALUE = 6;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT16 = 7
     * </pre>
     *
     * <code>TYPE_INT16 = 7;</code>
     */
    public static final int TYPE_INT16_VALUE = 7;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT32 = 8
     * </pre>
     *
     * <code>TYPE_INT32 = 8;</code>
     */
    public static final int TYPE_INT32_VALUE = 8;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::INT64 = 9
     * </pre>
     *
     * <code>TYPE_INT64 = 9;</code>
     */
    public static final int TYPE_INT64_VALUE = 9;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP16 = 10
     * </pre>
     *
     * <code>TYPE_FP16 = 10;</code>
     */
    public static final int TYPE_FP16_VALUE = 10;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP32 = 11
     * </pre>
     *
     * <code>TYPE_FP32 = 11;</code>
     */
    public static final int TYPE_FP32_VALUE = 11;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::FP64 = 12
     * </pre>
     *
     * <code>TYPE_FP64 = 12;</code>
     */
    public static final int TYPE_FP64_VALUE = 12;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:enumerator:: DataType::STRING = 13
     * </pre>
     *
     * <code>TYPE_STRING = 13;</code>
     */
    public static final int TYPE_STRING_VALUE = 13;


    public final int getNumber() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalArgumentException(
            "Can't get the number of an unknown enum value.");
      }
      return value;
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static DataType valueOf(int value) {
      return forNumber(value);
    }

    /**
     * @param value The numeric wire value of the corresponding enum entry.
     * @return The enum associated with the given numeric wire value.
     */
    public static DataType forNumber(int value) {
      switch (value) {
        case 0: return TYPE_INVALID;
        case 1: return TYPE_BOOL;
        case 2: return TYPE_UINT8;
        case 3: return TYPE_UINT16;
        case 4: return TYPE_UINT32;
        case 5: return TYPE_UINT64;
        case 6: return TYPE_INT8;
        case 7: return TYPE_INT16;
        case 8: return TYPE_INT32;
        case 9: return TYPE_INT64;
        case 10: return TYPE_FP16;
        case 11: return TYPE_FP32;
        case 12: return TYPE_FP64;
        case 13: return TYPE_STRING;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<DataType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        DataType> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<DataType>() {
            public DataType findValueByNumber(int number) {
              return DataType.forNumber(number);
            }
          };

    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      if (this == UNRECOGNIZED) {
        throw new java.lang.IllegalStateException(
            "Can't get the descriptor of an unrecognized enum value.");
      }
      return getDescriptor().getValues().get(ordinal());
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.getDescriptor().getEnumTypes().get(0);
    }

    private static final DataType[] VALUES = values();

    public static DataType valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      if (desc.getIndex() == -1) {
        return UNRECOGNIZED;
      }
      return VALUES[desc.getIndex()];
    }

    private final int value;

    private DataType(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:inference.DataType)
  }

  public interface ModelRateLimiterOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelRateLimiter)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> 
        getResourcesList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    inference.ModelConfigOuterClass.ModelRateLimiter.Resource getResources(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    int getResourcesCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    java.util.List<? extends inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder> 
        getResourcesOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder getResourcesOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority
     *&#64;&#64;
     *&#64;&#64;     The weighting value to be used for prioritizing across instances.
     *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
     *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
     *&#64;&#64;     default priority is 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority = 2;</code>
     * @return The priority.
     */
    int getPriority();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;  .. cpp:var:: message ModelRateLimiter
   *&#64;&#64;
   *&#64;&#64;     The specifications required by the rate limiter to properly
   *&#64;&#64;     schedule the inference requests across the different models
   *&#64;&#64;     and their instances.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelRateLimiter}
   */
  public static final class ModelRateLimiter extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelRateLimiter)
      ModelRateLimiterOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelRateLimiter.newBuilder() to construct.
    private ModelRateLimiter(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelRateLimiter() {
      resources_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelRateLimiter();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelRateLimiter(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                resources_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelRateLimiter.Resource>();
                mutable_bitField0_ |= 0x00000001;
              }
              resources_.add(
                  input.readMessage(inference.ModelConfigOuterClass.ModelRateLimiter.Resource.parser(), extensionRegistry));
              break;
            }
            case 16: {

              priority_ = input.readUInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          resources_ = java.util.Collections.unmodifiableList(resources_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelRateLimiter.class, inference.ModelConfigOuterClass.ModelRateLimiter.Builder.class);
    }

    public interface ResourceOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelRateLimiter.Resource)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool global
       *&#64;&#64;
       *&#64;&#64;     Whether or not the resource is global. If true then the resource
       *&#64;&#64;     is assumed to be shared among the devices otherwise specified
       *&#64;&#64;     count of the resource is assumed for each device associated
       *&#64;&#64;     with the instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool global = 2;</code>
       * @return The global.
       */
      boolean getGlobal();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 count
       *&#64;&#64;
       *&#64;&#64;     The number of resources required for the execution of the model
       *&#64;&#64;     instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 count = 3;</code>
       * @return The count.
       */
      int getCount();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Resource
     *&#64;&#64;
     *&#64;&#64;     The resource property.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelRateLimiter.Resource}
     */
    public static final class Resource extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelRateLimiter.Resource)
        ResourceOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Resource.newBuilder() to construct.
      private Resource(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Resource() {
        name_ = "";
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Resource();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Resource(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                name_ = s;
                break;
              }
              case 16: {

                global_ = input.readBool();
                break;
              }
              case 24: {

                count_ = input.readUInt32();
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_Resource_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_Resource_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelRateLimiter.Resource.class, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder.class);
      }

      public static final int NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object name_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name associated with the resource.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int GLOBAL_FIELD_NUMBER = 2;
      private boolean global_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool global
       *&#64;&#64;
       *&#64;&#64;     Whether or not the resource is global. If true then the resource
       *&#64;&#64;     is assumed to be shared among the devices otherwise specified
       *&#64;&#64;     count of the resource is assumed for each device associated
       *&#64;&#64;     with the instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool global = 2;</code>
       * @return The global.
       */
      @java.lang.Override
      public boolean getGlobal() {
        return global_;
      }

      public static final int COUNT_FIELD_NUMBER = 3;
      private int count_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 count
       *&#64;&#64;
       *&#64;&#64;     The number of resources required for the execution of the model
       *&#64;&#64;     instance.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 count = 3;</code>
       * @return The count.
       */
      @java.lang.Override
      public int getCount() {
        return count_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!getNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
        }
        if (global_ != false) {
          output.writeBool(2, global_);
        }
        if (count_ != 0) {
          output.writeUInt32(3, count_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
        }
        if (global_ != false) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(2, global_);
        }
        if (count_ != 0) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(3, count_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelRateLimiter.Resource)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelRateLimiter.Resource other = (inference.ModelConfigOuterClass.ModelRateLimiter.Resource) obj;

        if (!getName()
            .equals(other.getName())) return false;
        if (getGlobal()
            != other.getGlobal()) return false;
        if (getCount()
            != other.getCount()) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
        hash = (37 * hash) + GLOBAL_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getGlobal());
        hash = (37 * hash) + COUNT_FIELD_NUMBER;
        hash = (53 * hash) + getCount();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelRateLimiter.Resource prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Resource
       *&#64;&#64;
       *&#64;&#64;     The resource property.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelRateLimiter.Resource}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelRateLimiter.Resource)
          inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_Resource_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_Resource_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelRateLimiter.Resource.class, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelRateLimiter.Resource.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          name_ = "";

          global_ = false;

          count_ = 0;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_Resource_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelRateLimiter.Resource getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelRateLimiter.Resource.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelRateLimiter.Resource build() {
          inference.ModelConfigOuterClass.ModelRateLimiter.Resource result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelRateLimiter.Resource buildPartial() {
          inference.ModelConfigOuterClass.ModelRateLimiter.Resource result = new inference.ModelConfigOuterClass.ModelRateLimiter.Resource(this);
          result.name_ = name_;
          result.global_ = global_;
          result.count_ = count_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelRateLimiter.Resource) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelRateLimiter.Resource)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelRateLimiter.Resource other) {
          if (other == inference.ModelConfigOuterClass.ModelRateLimiter.Resource.getDefaultInstance()) return this;
          if (!other.getName().isEmpty()) {
            name_ = other.name_;
            onChanged();
          }
          if (other.getGlobal() != false) {
            setGlobal(other.getGlobal());
          }
          if (other.getCount() != 0) {
            setCount(other.getCount());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelRateLimiter.Resource parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelRateLimiter.Resource) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private java.lang.Object name_ = "";
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The name.
         */
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The name to set.
         * @return This builder for chaining.
         */
        public Builder setName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearName() {
          
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;     The name associated with the resource.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The bytes for name to set.
         * @return This builder for chaining.
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          name_ = value;
          onChanged();
          return this;
        }

        private boolean global_ ;
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: bool global
         *&#64;&#64;
         *&#64;&#64;     Whether or not the resource is global. If true then the resource
         *&#64;&#64;     is assumed to be shared among the devices otherwise specified
         *&#64;&#64;     count of the resource is assumed for each device associated
         *&#64;&#64;     with the instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool global = 2;</code>
         * @return The global.
         */
        @java.lang.Override
        public boolean getGlobal() {
          return global_;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: bool global
         *&#64;&#64;
         *&#64;&#64;     Whether or not the resource is global. If true then the resource
         *&#64;&#64;     is assumed to be shared among the devices otherwise specified
         *&#64;&#64;     count of the resource is assumed for each device associated
         *&#64;&#64;     with the instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool global = 2;</code>
         * @param value The global to set.
         * @return This builder for chaining.
         */
        public Builder setGlobal(boolean value) {
          
          global_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: bool global
         *&#64;&#64;
         *&#64;&#64;     Whether or not the resource is global. If true then the resource
         *&#64;&#64;     is assumed to be shared among the devices otherwise specified
         *&#64;&#64;     count of the resource is assumed for each device associated
         *&#64;&#64;     with the instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool global = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearGlobal() {
          
          global_ = false;
          onChanged();
          return this;
        }

        private int count_ ;
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: uint32 count
         *&#64;&#64;
         *&#64;&#64;     The number of resources required for the execution of the model
         *&#64;&#64;     instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 count = 3;</code>
         * @return The count.
         */
        @java.lang.Override
        public int getCount() {
          return count_;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: uint32 count
         *&#64;&#64;
         *&#64;&#64;     The number of resources required for the execution of the model
         *&#64;&#64;     instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 count = 3;</code>
         * @param value The count to set.
         * @return This builder for chaining.
         */
        public Builder setCount(int value) {
          
          count_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: uint32 count
         *&#64;&#64;
         *&#64;&#64;     The number of resources required for the execution of the model
         *&#64;&#64;     instance.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 count = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearCount() {
          
          count_ = 0;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelRateLimiter.Resource)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelRateLimiter.Resource)
      private static final inference.ModelConfigOuterClass.ModelRateLimiter.Resource DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelRateLimiter.Resource();
      }

      public static inference.ModelConfigOuterClass.ModelRateLimiter.Resource getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Resource>
          PARSER = new com.google.protobuf.AbstractParser<Resource>() {
        @java.lang.Override
        public Resource parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Resource(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Resource> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Resource> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelRateLimiter.Resource getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int RESOURCES_FIELD_NUMBER = 1;
    private java.util.List<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> resources_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> getResourcesList() {
      return resources_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder> 
        getResourcesOrBuilderList() {
      return resources_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    @java.lang.Override
    public int getResourcesCount() {
      return resources_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelRateLimiter.Resource getResources(int index) {
      return resources_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
     *&#64;&#64;
     *&#64;&#64;     The resources required to execute the request on a model instance.
     *&#64;&#64;     Resources are just names with a corresponding count. The execution
     *&#64;&#64;     of the instance will be blocked until the specificied resources are
     *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder getResourcesOrBuilder(
        int index) {
      return resources_.get(index);
    }

    public static final int PRIORITY_FIELD_NUMBER = 2;
    private int priority_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority
     *&#64;&#64;
     *&#64;&#64;     The weighting value to be used for prioritizing across instances.
     *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
     *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
     *&#64;&#64;     default priority is 1.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority = 2;</code>
     * @return The priority.
     */
    @java.lang.Override
    public int getPriority() {
      return priority_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < resources_.size(); i++) {
        output.writeMessage(1, resources_.get(i));
      }
      if (priority_ != 0) {
        output.writeUInt32(2, priority_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < resources_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, resources_.get(i));
      }
      if (priority_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, priority_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelRateLimiter)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelRateLimiter other = (inference.ModelConfigOuterClass.ModelRateLimiter) obj;

      if (!getResourcesList()
          .equals(other.getResourcesList())) return false;
      if (getPriority()
          != other.getPriority()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getResourcesCount() > 0) {
        hash = (37 * hash) + RESOURCES_FIELD_NUMBER;
        hash = (53 * hash) + getResourcesList().hashCode();
      }
      hash = (37 * hash) + PRIORITY_FIELD_NUMBER;
      hash = (53 * hash) + getPriority();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelRateLimiter parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelRateLimiter prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message ModelRateLimiter
     *&#64;&#64;
     *&#64;&#64;     The specifications required by the rate limiter to properly
     *&#64;&#64;     schedule the inference requests across the different models
     *&#64;&#64;     and their instances.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelRateLimiter}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelRateLimiter)
        inference.ModelConfigOuterClass.ModelRateLimiterOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelRateLimiter.class, inference.ModelConfigOuterClass.ModelRateLimiter.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelRateLimiter.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getResourcesFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (resourcesBuilder_ == null) {
          resources_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          resourcesBuilder_.clear();
        }
        priority_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelRateLimiter_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelRateLimiter getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelRateLimiter.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelRateLimiter build() {
        inference.ModelConfigOuterClass.ModelRateLimiter result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelRateLimiter buildPartial() {
        inference.ModelConfigOuterClass.ModelRateLimiter result = new inference.ModelConfigOuterClass.ModelRateLimiter(this);
        int from_bitField0_ = bitField0_;
        if (resourcesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            resources_ = java.util.Collections.unmodifiableList(resources_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.resources_ = resources_;
        } else {
          result.resources_ = resourcesBuilder_.build();
        }
        result.priority_ = priority_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelRateLimiter) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelRateLimiter)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelRateLimiter other) {
        if (other == inference.ModelConfigOuterClass.ModelRateLimiter.getDefaultInstance()) return this;
        if (resourcesBuilder_ == null) {
          if (!other.resources_.isEmpty()) {
            if (resources_.isEmpty()) {
              resources_ = other.resources_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureResourcesIsMutable();
              resources_.addAll(other.resources_);
            }
            onChanged();
          }
        } else {
          if (!other.resources_.isEmpty()) {
            if (resourcesBuilder_.isEmpty()) {
              resourcesBuilder_.dispose();
              resourcesBuilder_ = null;
              resources_ = other.resources_;
              bitField0_ = (bitField0_ & ~0x00000001);
              resourcesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getResourcesFieldBuilder() : null;
            } else {
              resourcesBuilder_.addAllMessages(other.resources_);
            }
          }
        }
        if (other.getPriority() != 0) {
          setPriority(other.getPriority());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelRateLimiter parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelRateLimiter) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> resources_ =
        java.util.Collections.emptyList();
      private void ensureResourcesIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          resources_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelRateLimiter.Resource>(resources_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelRateLimiter.Resource, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder, inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder> resourcesBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelRateLimiter.Resource> getResourcesList() {
        if (resourcesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(resources_);
        } else {
          return resourcesBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public int getResourcesCount() {
        if (resourcesBuilder_ == null) {
          return resources_.size();
        } else {
          return resourcesBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelRateLimiter.Resource getResources(int index) {
        if (resourcesBuilder_ == null) {
          return resources_.get(index);
        } else {
          return resourcesBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder setResources(
          int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource value) {
        if (resourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureResourcesIsMutable();
          resources_.set(index, value);
          onChanged();
        } else {
          resourcesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder setResources(
          int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder builderForValue) {
        if (resourcesBuilder_ == null) {
          ensureResourcesIsMutable();
          resources_.set(index, builderForValue.build());
          onChanged();
        } else {
          resourcesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addResources(inference.ModelConfigOuterClass.ModelRateLimiter.Resource value) {
        if (resourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureResourcesIsMutable();
          resources_.add(value);
          onChanged();
        } else {
          resourcesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addResources(
          int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource value) {
        if (resourcesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureResourcesIsMutable();
          resources_.add(index, value);
          onChanged();
        } else {
          resourcesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addResources(
          inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder builderForValue) {
        if (resourcesBuilder_ == null) {
          ensureResourcesIsMutable();
          resources_.add(builderForValue.build());
          onChanged();
        } else {
          resourcesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addResources(
          int index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder builderForValue) {
        if (resourcesBuilder_ == null) {
          ensureResourcesIsMutable();
          resources_.add(index, builderForValue.build());
          onChanged();
        } else {
          resourcesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder addAllResources(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelRateLimiter.Resource> values) {
        if (resourcesBuilder_ == null) {
          ensureResourcesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, resources_);
          onChanged();
        } else {
          resourcesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder clearResources() {
        if (resourcesBuilder_ == null) {
          resources_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          resourcesBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public Builder removeResources(int index) {
        if (resourcesBuilder_ == null) {
          ensureResourcesIsMutable();
          resources_.remove(index);
          onChanged();
        } else {
          resourcesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder getResourcesBuilder(
          int index) {
        return getResourcesFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder getResourcesOrBuilder(
          int index) {
        if (resourcesBuilder_ == null) {
          return resources_.get(index);  } else {
          return resourcesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder> 
           getResourcesOrBuilderList() {
        if (resourcesBuilder_ != null) {
          return resourcesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(resources_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder addResourcesBuilder() {
        return getResourcesFieldBuilder().addBuilder(
            inference.ModelConfigOuterClass.ModelRateLimiter.Resource.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder addResourcesBuilder(
          int index) {
        return getResourcesFieldBuilder().addBuilder(
            index, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Resource resources (repeated)
       *&#64;&#64;
       *&#64;&#64;     The resources required to execute the request on a model instance.
       *&#64;&#64;     Resources are just names with a corresponding count. The execution
       *&#64;&#64;     of the instance will be blocked until the specificied resources are
       *&#64;&#64;     available. By default an instance uses no rate-limiter resources.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelRateLimiter.Resource resources = 1;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder> 
           getResourcesBuilderList() {
        return getResourcesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelRateLimiter.Resource, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder, inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder> 
          getResourcesFieldBuilder() {
        if (resourcesBuilder_ == null) {
          resourcesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelRateLimiter.Resource, inference.ModelConfigOuterClass.ModelRateLimiter.Resource.Builder, inference.ModelConfigOuterClass.ModelRateLimiter.ResourceOrBuilder>(
                  resources_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          resources_ = null;
        }
        return resourcesBuilder_;
      }

      private int priority_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority
       *&#64;&#64;
       *&#64;&#64;     The weighting value to be used for prioritizing across instances.
       *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
       *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
       *&#64;&#64;     default priority is 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority = 2;</code>
       * @return The priority.
       */
      @java.lang.Override
      public int getPriority() {
        return priority_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority
       *&#64;&#64;
       *&#64;&#64;     The weighting value to be used for prioritizing across instances.
       *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
       *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
       *&#64;&#64;     default priority is 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority = 2;</code>
       * @param value The priority to set.
       * @return This builder for chaining.
       */
      public Builder setPriority(int value) {
        
        priority_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority
       *&#64;&#64;
       *&#64;&#64;     The weighting value to be used for prioritizing across instances.
       *&#64;&#64;     An instance with priority 2 will be given 1/2 the number of
       *&#64;&#64;     scheduling chances as an instance_group with priority 1. The
       *&#64;&#64;     default priority is 1.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearPriority() {
        
        priority_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelRateLimiter)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelRateLimiter)
    private static final inference.ModelConfigOuterClass.ModelRateLimiter DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelRateLimiter();
    }

    public static inference.ModelConfigOuterClass.ModelRateLimiter getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelRateLimiter>
        PARSER = new com.google.protobuf.AbstractParser<ModelRateLimiter>() {
      @java.lang.Override
      public ModelRateLimiter parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelRateLimiter(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelRateLimiter> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelRateLimiter> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelRateLimiter getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelInstanceGroupOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelInstanceGroup)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @return The enum numeric value on the wire for kind.
     */
    int getKindValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @return The kind.
     */
    inference.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     * @return The count.
     */
    int getCount();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     * @return Whether the rateLimiter field is set.
     */
    boolean hasRateLimiter();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     * @return The rateLimiter.
     */
    inference.ModelConfigOuterClass.ModelRateLimiter getRateLimiter();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     */
    inference.ModelConfigOuterClass.ModelRateLimiterOrBuilder getRateLimiterOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @return A list containing the gpus.
     */
    java.util.List<java.lang.Integer> getGpusList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @return The count of gpus.
     */
    int getGpusCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @param index The index of the element to return.
     * @return The gpus at the given index.
     */
    int getGpus(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @return A list containing the profile.
     */
    java.util.List<java.lang.String>
        getProfileList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @return The count of profile.
     */
    int getProfileCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param index The index of the element to return.
     * @return The profile at the given index.
     */
    java.lang.String getProfile(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param index The index of the value to return.
     * @return The bytes of the profile at the given index.
     */
    com.google.protobuf.ByteString
        getProfileBytes(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelInstanceGroup
   *&#64;&#64;
   *&#64;&#64;   A group of one or more instances of a model and resources made
   *&#64;&#64;   available for those instances.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelInstanceGroup}
   */
  public static final class ModelInstanceGroup extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelInstanceGroup)
      ModelInstanceGroupOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelInstanceGroup.newBuilder() to construct.
    private ModelInstanceGroup(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelInstanceGroup() {
      name_ = "";
      kind_ = 0;
      gpus_ = emptyIntList();
      profile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelInstanceGroup();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelInstanceGroup(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {

              count_ = input.readInt32();
              break;
            }
            case 24: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                gpus_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              gpus_.addInt(input.readInt32());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                gpus_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                gpus_.addInt(input.readInt32());
              }
              input.popLimit(limit);
              break;
            }
            case 32: {
              int rawValue = input.readEnum();

              kind_ = rawValue;
              break;
            }
            case 42: {
              java.lang.String s = input.readStringRequireUtf8();
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                profile_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000002;
              }
              profile_.add(s);
              break;
            }
            case 50: {
              inference.ModelConfigOuterClass.ModelRateLimiter.Builder subBuilder = null;
              if (rateLimiter_ != null) {
                subBuilder = rateLimiter_.toBuilder();
              }
              rateLimiter_ = input.readMessage(inference.ModelConfigOuterClass.ModelRateLimiter.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(rateLimiter_);
                rateLimiter_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          gpus_.makeImmutable(); // C
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          profile_ = profile_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelInstanceGroup_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelInstanceGroup_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelInstanceGroup.class, inference.ModelConfigOuterClass.ModelInstanceGroup.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Kind
     *&#64;&#64;
     *&#64;&#64;     Kind of this instance group.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.ModelInstanceGroup.Kind}
     */
    public enum Kind
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_AUTO = 0
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that can run on either
       *&#64;&#64;       CPU or GPU. If all GPUs listed in 'gpus' are available then
       *&#64;&#64;       instances will be created on GPU(s), otherwise instances will
       *&#64;&#64;       be created on CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_AUTO = 0;</code>
       */
      KIND_AUTO(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_GPU = 1
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_GPU = 1;</code>
       */
      KIND_GPU(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_CPU = 2
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_CPU = 2;</code>
       */
      KIND_CPU(2),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_MODEL = 3
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that should run on the
       *&#64;&#64;       CPU and/or GPU(s) as specified by the model or backend itself.
       *&#64;&#64;       The inference server will not override the model/backend
       *&#64;&#64;       settings.
       *&#64;&#64;       Currently, this option is supported only for Tensorflow models.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_MODEL = 3;</code>
       */
      KIND_MODEL(3),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_AUTO = 0
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that can run on either
       *&#64;&#64;       CPU or GPU. If all GPUs listed in 'gpus' are available then
       *&#64;&#64;       instances will be created on GPU(s), otherwise instances will
       *&#64;&#64;       be created on CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_AUTO = 0;</code>
       */
      public static final int KIND_AUTO_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_GPU = 1
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_GPU = 1;</code>
       */
      public static final int KIND_GPU_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_CPU = 2
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that must run on the
       *&#64;&#64;       CPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_CPU = 2;</code>
       */
      public static final int KIND_CPU_VALUE = 2;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::KIND_MODEL = 3
       *&#64;&#64;
       *&#64;&#64;       This instance group represents instances that should run on the
       *&#64;&#64;       CPU and/or GPU(s) as specified by the model or backend itself.
       *&#64;&#64;       The inference server will not override the model/backend
       *&#64;&#64;       settings.
       *&#64;&#64;       Currently, this option is supported only for Tensorflow models.
       *&#64;&#64;
       * </pre>
       *
       * <code>KIND_MODEL = 3;</code>
       */
      public static final int KIND_MODEL_VALUE = 3;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Kind valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static Kind forNumber(int value) {
        switch (value) {
          case 0: return KIND_AUTO;
          case 1: return KIND_GPU;
          case 2: return KIND_CPU;
          case 3: return KIND_MODEL;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Kind> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
              public Kind findValueByNumber(int number) {
                return Kind.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalStateException(
              "Can't get the descriptor of an unrecognized enum value.");
        }
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.ModelInstanceGroup.getDescriptor().getEnumTypes().get(0);
      }

      private static final Kind[] VALUES = values();

      public static Kind valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Kind(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.ModelInstanceGroup.Kind)
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     Optional name of this group of instances. If not specified the
     *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
     *&#64;&#64;     individual instances will be further formed by a unique instance
     *&#64;&#64;     number and GPU index:
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int KIND_FIELD_NUMBER = 4;
    private int kind_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @return The enum numeric value on the wire for kind.
     */
    @java.lang.Override public int getKindValue() {
      return kind_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
     *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
     *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
     *&#64;&#64;     and 'gpu' cannot be specified.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
     * @return The kind.
     */
    @java.lang.Override public inference.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind() {
      @SuppressWarnings("deprecation")
      inference.ModelConfigOuterClass.ModelInstanceGroup.Kind result = inference.ModelConfigOuterClass.ModelInstanceGroup.Kind.valueOf(kind_);
      return result == null ? inference.ModelConfigOuterClass.ModelInstanceGroup.Kind.UNRECOGNIZED : result;
    }

    public static final int COUNT_FIELD_NUMBER = 2;
    private int count_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 count
     *&#64;&#64;
     *&#64;&#64;     For a group assigned to GPU, the number of instances created for
     *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
     *&#64;&#64;     of instances created. Default is 1.
     * </pre>
     *
     * <code>int32 count = 2;</code>
     * @return The count.
     */
    @java.lang.Override
    public int getCount() {
      return count_;
    }

    public static final int RATE_LIMITER_FIELD_NUMBER = 6;
    private inference.ModelConfigOuterClass.ModelRateLimiter rateLimiter_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     * @return Whether the rateLimiter field is set.
     */
    @java.lang.Override
    public boolean hasRateLimiter() {
      return rateLimiter_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     * @return The rateLimiter.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelRateLimiter getRateLimiter() {
      return rateLimiter_ == null ? inference.ModelConfigOuterClass.ModelRateLimiter.getDefaultInstance() : rateLimiter_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
     *&#64;&#64;
     *&#64;&#64;     The rate limiter specific settings to be associated with this
     *&#64;&#64;     instance group. Optional, if not specified no rate limiting
     *&#64;&#64;     will be applied to this instance group.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelRateLimiterOrBuilder getRateLimiterOrBuilder() {
      return getRateLimiter();
    }

    public static final int GPUS_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.IntList gpus_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @return A list containing the gpus.
     */
    @java.lang.Override
    public java.util.List<java.lang.Integer>
        getGpusList() {
      return gpus_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @return The count of gpus.
     */
    public int getGpusCount() {
      return gpus_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
     *&#64;&#64;
     *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
     *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
     *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
     *&#64;&#64;     available GPUs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 gpus = 3;</code>
     * @param index The index of the element to return.
     * @return The gpus at the given index.
     */
    public int getGpus(int index) {
      return gpus_.getInt(index);
    }
    private int gpusMemoizedSerializedSize = -1;

    public static final int PROFILE_FIELD_NUMBER = 5;
    private com.google.protobuf.LazyStringList profile_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @return A list containing the profile.
     */
    public com.google.protobuf.ProtocolStringList
        getProfileList() {
      return profile_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @return The count of profile.
     */
    public int getProfileCount() {
      return profile_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param index The index of the element to return.
     * @return The profile at the given index.
     */
    public java.lang.String getProfile(int index) {
      return profile_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string profile (repeated)
     *&#64;&#64;
     *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
     *&#64;&#64;     parameter specifies a set of optimization profiles available to this
     *&#64;&#64;     instance group. The inference server will choose the optimal profile
     *&#64;&#64;     based on the shapes of the input tensors. This field should lie
     *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
     *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
     *&#64;&#64;     be generated. If not specified, the server will select the first
     *&#64;&#64;     optimization profile by default.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string profile = 5;</code>
     * @param index The index of the value to return.
     * @return The bytes of the profile at the given index.
     */
    public com.google.protobuf.ByteString
        getProfileBytes(int index) {
      return profile_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (count_ != 0) {
        output.writeInt32(2, count_);
      }
      if (getGpusList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(gpusMemoizedSerializedSize);
      }
      for (int i = 0; i < gpus_.size(); i++) {
        output.writeInt32NoTag(gpus_.getInt(i));
      }
      if (kind_ != inference.ModelConfigOuterClass.ModelInstanceGroup.Kind.KIND_AUTO.getNumber()) {
        output.writeEnum(4, kind_);
      }
      for (int i = 0; i < profile_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 5, profile_.getRaw(i));
      }
      if (rateLimiter_ != null) {
        output.writeMessage(6, getRateLimiter());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (count_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, count_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < gpus_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt32SizeNoTag(gpus_.getInt(i));
        }
        size += dataSize;
        if (!getGpusList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        gpusMemoizedSerializedSize = dataSize;
      }
      if (kind_ != inference.ModelConfigOuterClass.ModelInstanceGroup.Kind.KIND_AUTO.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(4, kind_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < profile_.size(); i++) {
          dataSize += computeStringSizeNoTag(profile_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getProfileList().size();
      }
      if (rateLimiter_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, getRateLimiter());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelInstanceGroup)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelInstanceGroup other = (inference.ModelConfigOuterClass.ModelInstanceGroup) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (kind_ != other.kind_) return false;
      if (getCount()
          != other.getCount()) return false;
      if (hasRateLimiter() != other.hasRateLimiter()) return false;
      if (hasRateLimiter()) {
        if (!getRateLimiter()
            .equals(other.getRateLimiter())) return false;
      }
      if (!getGpusList()
          .equals(other.getGpusList())) return false;
      if (!getProfileList()
          .equals(other.getProfileList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + KIND_FIELD_NUMBER;
      hash = (53 * hash) + kind_;
      hash = (37 * hash) + COUNT_FIELD_NUMBER;
      hash = (53 * hash) + getCount();
      if (hasRateLimiter()) {
        hash = (37 * hash) + RATE_LIMITER_FIELD_NUMBER;
        hash = (53 * hash) + getRateLimiter().hashCode();
      }
      if (getGpusCount() > 0) {
        hash = (37 * hash) + GPUS_FIELD_NUMBER;
        hash = (53 * hash) + getGpusList().hashCode();
      }
      if (getProfileCount() > 0) {
        hash = (37 * hash) + PROFILE_FIELD_NUMBER;
        hash = (53 * hash) + getProfileList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelInstanceGroup parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelInstanceGroup prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelInstanceGroup
     *&#64;&#64;
     *&#64;&#64;   A group of one or more instances of a model and resources made
     *&#64;&#64;   available for those instances.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelInstanceGroup}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelInstanceGroup)
        inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelInstanceGroup_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelInstanceGroup_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelInstanceGroup.class, inference.ModelConfigOuterClass.ModelInstanceGroup.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelInstanceGroup.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        kind_ = 0;

        count_ = 0;

        if (rateLimiterBuilder_ == null) {
          rateLimiter_ = null;
        } else {
          rateLimiter_ = null;
          rateLimiterBuilder_ = null;
        }
        gpus_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        profile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelInstanceGroup_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInstanceGroup build() {
        inference.ModelConfigOuterClass.ModelInstanceGroup result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInstanceGroup buildPartial() {
        inference.ModelConfigOuterClass.ModelInstanceGroup result = new inference.ModelConfigOuterClass.ModelInstanceGroup(this);
        int from_bitField0_ = bitField0_;
        result.name_ = name_;
        result.kind_ = kind_;
        result.count_ = count_;
        if (rateLimiterBuilder_ == null) {
          result.rateLimiter_ = rateLimiter_;
        } else {
          result.rateLimiter_ = rateLimiterBuilder_.build();
        }
        if (((bitField0_ & 0x00000001) != 0)) {
          gpus_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.gpus_ = gpus_;
        if (((bitField0_ & 0x00000002) != 0)) {
          profile_ = profile_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.profile_ = profile_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelInstanceGroup) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelInstanceGroup)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelInstanceGroup other) {
        if (other == inference.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.kind_ != 0) {
          setKindValue(other.getKindValue());
        }
        if (other.getCount() != 0) {
          setCount(other.getCount());
        }
        if (other.hasRateLimiter()) {
          mergeRateLimiter(other.getRateLimiter());
        }
        if (!other.gpus_.isEmpty()) {
          if (gpus_.isEmpty()) {
            gpus_ = other.gpus_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureGpusIsMutable();
            gpus_.addAll(other.gpus_);
          }
          onChanged();
        }
        if (!other.profile_.isEmpty()) {
          if (profile_.isEmpty()) {
            profile_ = other.profile_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureProfileIsMutable();
            profile_.addAll(other.profile_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelInstanceGroup parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelInstanceGroup) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     Optional name of this group of instances. If not specified the
       *&#64;&#64;     name will be formed as &lt;model name&gt;_&lt;group number&gt;. The name of
       *&#64;&#64;     individual instances will be further formed by a unique instance
       *&#64;&#64;     number and GPU index:
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int kind_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @return The enum numeric value on the wire for kind.
       */
      @java.lang.Override public int getKindValue() {
        return kind_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @param value The enum numeric value on the wire for kind to set.
       * @return This builder for chaining.
       */
      public Builder setKindValue(int value) {
        
        kind_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @return The kind.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInstanceGroup.Kind getKind() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.ModelInstanceGroup.Kind result = inference.ModelConfigOuterClass.ModelInstanceGroup.Kind.valueOf(kind_);
        return result == null ? inference.ModelConfigOuterClass.ModelInstanceGroup.Kind.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @param value The kind to set.
       * @return This builder for chaining.
       */
      public Builder setKind(inference.ModelConfigOuterClass.ModelInstanceGroup.Kind value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        kind_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this instance group. Default is KIND_AUTO. If
       *&#64;&#64;     KIND_AUTO or KIND_GPU then both 'count' and 'gpu' are valid and
       *&#64;&#64;     may be specified. If KIND_CPU or KIND_MODEL only 'count' is valid
       *&#64;&#64;     and 'gpu' cannot be specified.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInstanceGroup.Kind kind = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearKind() {
        
        kind_ = 0;
        onChanged();
        return this;
      }

      private int count_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       * @return The count.
       */
      @java.lang.Override
      public int getCount() {
        return count_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       * @param value The count to set.
       * @return This builder for chaining.
       */
      public Builder setCount(int value) {
        
        count_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 count
       *&#64;&#64;
       *&#64;&#64;     For a group assigned to GPU, the number of instances created for
       *&#64;&#64;     each GPU listed in 'gpus'. For a group assigned to CPU the number
       *&#64;&#64;     of instances created. Default is 1.
       * </pre>
       *
       * <code>int32 count = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearCount() {
        
        count_ = 0;
        onChanged();
        return this;
      }

      private inference.ModelConfigOuterClass.ModelRateLimiter rateLimiter_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelRateLimiter, inference.ModelConfigOuterClass.ModelRateLimiter.Builder, inference.ModelConfigOuterClass.ModelRateLimiterOrBuilder> rateLimiterBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       * @return Whether the rateLimiter field is set.
       */
      public boolean hasRateLimiter() {
        return rateLimiterBuilder_ != null || rateLimiter_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       * @return The rateLimiter.
       */
      public inference.ModelConfigOuterClass.ModelRateLimiter getRateLimiter() {
        if (rateLimiterBuilder_ == null) {
          return rateLimiter_ == null ? inference.ModelConfigOuterClass.ModelRateLimiter.getDefaultInstance() : rateLimiter_;
        } else {
          return rateLimiterBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public Builder setRateLimiter(inference.ModelConfigOuterClass.ModelRateLimiter value) {
        if (rateLimiterBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          rateLimiter_ = value;
          onChanged();
        } else {
          rateLimiterBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public Builder setRateLimiter(
          inference.ModelConfigOuterClass.ModelRateLimiter.Builder builderForValue) {
        if (rateLimiterBuilder_ == null) {
          rateLimiter_ = builderForValue.build();
          onChanged();
        } else {
          rateLimiterBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public Builder mergeRateLimiter(inference.ModelConfigOuterClass.ModelRateLimiter value) {
        if (rateLimiterBuilder_ == null) {
          if (rateLimiter_ != null) {
            rateLimiter_ =
              inference.ModelConfigOuterClass.ModelRateLimiter.newBuilder(rateLimiter_).mergeFrom(value).buildPartial();
          } else {
            rateLimiter_ = value;
          }
          onChanged();
        } else {
          rateLimiterBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public Builder clearRateLimiter() {
        if (rateLimiterBuilder_ == null) {
          rateLimiter_ = null;
          onChanged();
        } else {
          rateLimiter_ = null;
          rateLimiterBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelRateLimiter.Builder getRateLimiterBuilder() {
        
        onChanged();
        return getRateLimiterFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelRateLimiterOrBuilder getRateLimiterOrBuilder() {
        if (rateLimiterBuilder_ != null) {
          return rateLimiterBuilder_.getMessageOrBuilder();
        } else {
          return rateLimiter_ == null ?
              inference.ModelConfigOuterClass.ModelRateLimiter.getDefaultInstance() : rateLimiter_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelRateLimiter rate_limiter
       *&#64;&#64;
       *&#64;&#64;     The rate limiter specific settings to be associated with this
       *&#64;&#64;     instance group. Optional, if not specified no rate limiting
       *&#64;&#64;     will be applied to this instance group.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelRateLimiter rate_limiter = 6;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelRateLimiter, inference.ModelConfigOuterClass.ModelRateLimiter.Builder, inference.ModelConfigOuterClass.ModelRateLimiterOrBuilder> 
          getRateLimiterFieldBuilder() {
        if (rateLimiterBuilder_ == null) {
          rateLimiterBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelRateLimiter, inference.ModelConfigOuterClass.ModelRateLimiter.Builder, inference.ModelConfigOuterClass.ModelRateLimiterOrBuilder>(
                  getRateLimiter(),
                  getParentForChildren(),
                  isClean());
          rateLimiter_ = null;
        }
        return rateLimiterBuilder_;
      }

      private com.google.protobuf.Internal.IntList gpus_ = emptyIntList();
      private void ensureGpusIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          gpus_ = mutableCopy(gpus_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @return A list containing the gpus.
       */
      public java.util.List<java.lang.Integer>
          getGpusList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(gpus_) : gpus_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @return The count of gpus.
       */
      public int getGpusCount() {
        return gpus_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @param index The index of the element to return.
       * @return The gpus at the given index.
       */
      public int getGpus(int index) {
        return gpus_.getInt(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @param index The index to set the value at.
       * @param value The gpus to set.
       * @return This builder for chaining.
       */
      public Builder setGpus(
          int index, int value) {
        ensureGpusIsMutable();
        gpus_.setInt(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @param value The gpus to add.
       * @return This builder for chaining.
       */
      public Builder addGpus(int value) {
        ensureGpusIsMutable();
        gpus_.addInt(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @param values The gpus to add.
       * @return This builder for chaining.
       */
      public Builder addAllGpus(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensureGpusIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, gpus_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 gpus (repeated)
       *&#64;&#64;
       *&#64;&#64;     GPU(s) where instances should be available. For each GPU listed,
       *&#64;&#64;     'count' instances of the model will be available. Setting 'gpus'
       *&#64;&#64;     to empty (or not specifying at all) is eqivalent to listing all
       *&#64;&#64;     available GPUs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 gpus = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearGpus() {
        gpus_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList profile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureProfileIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          profile_ = new com.google.protobuf.LazyStringArrayList(profile_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @return A list containing the profile.
       */
      public com.google.protobuf.ProtocolStringList
          getProfileList() {
        return profile_.getUnmodifiableView();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @return The count of profile.
       */
      public int getProfileCount() {
        return profile_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param index The index of the element to return.
       * @return The profile at the given index.
       */
      public java.lang.String getProfile(int index) {
        return profile_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param index The index of the value to return.
       * @return The bytes of the profile at the given index.
       */
      public com.google.protobuf.ByteString
          getProfileBytes(int index) {
        return profile_.getByteString(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param index The index to set the value at.
       * @param value The profile to set.
       * @return This builder for chaining.
       */
      public Builder setProfile(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureProfileIsMutable();
        profile_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param value The profile to add.
       * @return This builder for chaining.
       */
      public Builder addProfile(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureProfileIsMutable();
        profile_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param values The profile to add.
       * @return This builder for chaining.
       */
      public Builder addAllProfile(
          java.lang.Iterable<java.lang.String> values) {
        ensureProfileIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, profile_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearProfile() {
        profile_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string profile (repeated)
       *&#64;&#64;
       *&#64;&#64;     For TensorRT models containing multiple optimization profile, this
       *&#64;&#64;     parameter specifies a set of optimization profiles available to this
       *&#64;&#64;     instance group. The inference server will choose the optimal profile
       *&#64;&#64;     based on the shapes of the input tensors. This field should lie
       *&#64;&#64;     between 0 and &lt;TotalNumberOfOptimizationProfilesInPlanModel&gt; - 1
       *&#64;&#64;     and be specified only for TensorRT backend, otherwise an error will
       *&#64;&#64;     be generated. If not specified, the server will select the first
       *&#64;&#64;     optimization profile by default.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string profile = 5;</code>
       * @param value The bytes of the profile to add.
       * @return This builder for chaining.
       */
      public Builder addProfileBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        ensureProfileIsMutable();
        profile_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelInstanceGroup)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelInstanceGroup)
    private static final inference.ModelConfigOuterClass.ModelInstanceGroup DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelInstanceGroup();
    }

    public static inference.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelInstanceGroup>
        PARSER = new com.google.protobuf.AbstractParser<ModelInstanceGroup>() {
      @java.lang.Override
      public ModelInstanceGroup parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelInstanceGroup(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelInstanceGroup> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelInstanceGroup> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInstanceGroup getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelTensorReshapeOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelTensorReshape)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @return A list containing the shape.
     */
    java.util.List<java.lang.Long> getShapeList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @return The count of shape.
     */
    int getShapeCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @param index The index of the element to return.
     * @return The shape at the given index.
     */
    long getShape(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelTensorReshape
   *&#64;&#64;
   *&#64;&#64;   Reshape specification for input and output tensors.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelTensorReshape}
   */
  public static final class ModelTensorReshape extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelTensorReshape)
      ModelTensorReshapeOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelTensorReshape.newBuilder() to construct.
    private ModelTensorReshape(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelTensorReshape() {
      shape_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelTensorReshape();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelTensorReshape(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                shape_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              shape_.addLong(input.readInt64());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                shape_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                shape_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          shape_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelTensorReshape_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelTensorReshape_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelTensorReshape.class, inference.ModelConfigOuterClass.ModelTensorReshape.Builder.class);
    }

    public static final int SHAPE_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.LongList shape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @return A list containing the shape.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getShapeList() {
      return shape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @return The count of shape.
     */
    public int getShapeCount() {
      return shape_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
     *&#64;&#64;
     *&#64;&#64;     The shape to use for reshaping.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 shape = 1;</code>
     * @param index The index of the element to return.
     * @return The shape at the given index.
     */
    public long getShape(int index) {
      return shape_.getLong(index);
    }
    private int shapeMemoizedSerializedSize = -1;

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getShapeList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(shapeMemoizedSerializedSize);
      }
      for (int i = 0; i < shape_.size(); i++) {
        output.writeInt64NoTag(shape_.getLong(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < shape_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(shape_.getLong(i));
        }
        size += dataSize;
        if (!getShapeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        shapeMemoizedSerializedSize = dataSize;
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelTensorReshape)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelTensorReshape other = (inference.ModelConfigOuterClass.ModelTensorReshape) obj;

      if (!getShapeList()
          .equals(other.getShapeList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getShapeCount() > 0) {
        hash = (37 * hash) + SHAPE_FIELD_NUMBER;
        hash = (53 * hash) + getShapeList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelTensorReshape parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelTensorReshape prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelTensorReshape
     *&#64;&#64;
     *&#64;&#64;   Reshape specification for input and output tensors.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelTensorReshape}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelTensorReshape)
        inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelTensorReshape_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelTensorReshape_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelTensorReshape.class, inference.ModelConfigOuterClass.ModelTensorReshape.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelTensorReshape.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        shape_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelTensorReshape_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelTensorReshape getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelTensorReshape build() {
        inference.ModelConfigOuterClass.ModelTensorReshape result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelTensorReshape buildPartial() {
        inference.ModelConfigOuterClass.ModelTensorReshape result = new inference.ModelConfigOuterClass.ModelTensorReshape(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          shape_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.shape_ = shape_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelTensorReshape) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelTensorReshape)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelTensorReshape other) {
        if (other == inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance()) return this;
        if (!other.shape_.isEmpty()) {
          if (shape_.isEmpty()) {
            shape_ = other.shape_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureShapeIsMutable();
            shape_.addAll(other.shape_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelTensorReshape parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelTensorReshape) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.LongList shape_ = emptyLongList();
      private void ensureShapeIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          shape_ = mutableCopy(shape_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @return A list containing the shape.
       */
      public java.util.List<java.lang.Long>
          getShapeList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(shape_) : shape_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @return The count of shape.
       */
      public int getShapeCount() {
        return shape_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @param index The index of the element to return.
       * @return The shape at the given index.
       */
      public long getShape(int index) {
        return shape_.getLong(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @param index The index to set the value at.
       * @param value The shape to set.
       * @return This builder for chaining.
       */
      public Builder setShape(
          int index, long value) {
        ensureShapeIsMutable();
        shape_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @param value The shape to add.
       * @return This builder for chaining.
       */
      public Builder addShape(long value) {
        ensureShapeIsMutable();
        shape_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @param values The shape to add.
       * @return This builder for chaining.
       */
      public Builder addAllShape(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureShapeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, shape_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 shape (repeated)
       *&#64;&#64;
       *&#64;&#64;     The shape to use for reshaping.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 shape = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearShape() {
        shape_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelTensorReshape)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelTensorReshape)
    private static final inference.ModelConfigOuterClass.ModelTensorReshape DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelTensorReshape();
    }

    public static inference.ModelConfigOuterClass.ModelTensorReshape getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelTensorReshape>
        PARSER = new com.google.protobuf.AbstractParser<ModelTensorReshape>() {
      @java.lang.Override
      public ModelTensorReshape parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelTensorReshape(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelTensorReshape> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelTensorReshape> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTensorReshape getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelInputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelInput)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The dataType.
     */
    inference.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @return The enum numeric value on the wire for format.
     */
    int getFormatValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @return The format.
     */
    inference.ModelConfigOuterClass.ModelInput.Format getFormat();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @return A list containing the dims.
     */
    java.util.List<java.lang.Long> getDimsList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @return The count of dims.
     */
    int getDimsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @param index The index of the element to return.
     * @return The dims at the given index.
     */
    long getDims(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return Whether the reshape field is set.
     */
    boolean hasReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return The reshape.
     */
    inference.ModelConfigOuterClass.ModelTensorReshape getReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @return The isShapeTensor.
     */
    boolean getIsShapeTensor();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
     *&#64;&#64;     created batch. Default is false indicating that two requests will
     *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
     *&#64;&#64;     True indicates that two requests can be batched even if this tensor
     *&#64;&#64;     has a different shape in each request. A true value is currently
     *&#64;&#64;     supported only for custom models.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_ragged_batch = 7;</code>
     * @return The allowRaggedBatch.
     */
    boolean getAllowRaggedBatch();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelInput
   *&#64;&#64;
   *&#64;&#64;   An input required by the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelInput}
   */
  public static final class ModelInput extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelInput)
      ModelInputOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelInput.newBuilder() to construct.
    private ModelInput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelInput() {
      name_ = "";
      dataType_ = 0;
      format_ = 0;
      dims_ = emptyLongList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelInput();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelInput(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              dataType_ = rawValue;
              break;
            }
            case 24: {
              int rawValue = input.readEnum();

              format_ = rawValue;
              break;
            }
            case 32: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                dims_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              dims_.addLong(input.readInt64());
              break;
            }
            case 34: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                dims_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                dims_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 42: {
              inference.ModelConfigOuterClass.ModelTensorReshape.Builder subBuilder = null;
              if (reshape_ != null) {
                subBuilder = reshape_.toBuilder();
              }
              reshape_ = input.readMessage(inference.ModelConfigOuterClass.ModelTensorReshape.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reshape_);
                reshape_ = subBuilder.buildPartial();
              }

              break;
            }
            case 48: {

              isShapeTensor_ = input.readBool();
              break;
            }
            case 56: {

              allowRaggedBatch_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          dims_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelInput_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelInput_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelInput.class, inference.ModelConfigOuterClass.ModelInput.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Format
     *&#64;&#64;
     *&#64;&#64;     The format for the input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.ModelInput.Format}
     */
    public enum Format
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       The input has no specific format. This is the default.
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NONE = 0;</code>
       */
      FORMAT_NONE(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
       *&#64;&#64;
       *&#64;&#64;       HWC image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NHWC = 1;</code>
       */
      FORMAT_NHWC(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
       *&#64;&#64;
       *&#64;&#64;       CHW image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NCHW = 2;</code>
       */
      FORMAT_NCHW(2),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NONE = 0
       *&#64;&#64;
       *&#64;&#64;       The input has no specific format. This is the default.
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NONE = 0;</code>
       */
      public static final int FORMAT_NONE_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NHWC = 1
       *&#64;&#64;
       *&#64;&#64;       HWC image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NHWC = 1;</code>
       */
      public static final int FORMAT_NHWC_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Format::FORMAT_NCHW = 2
       *&#64;&#64;
       *&#64;&#64;       CHW image format. Tensors with this format require 3 dimensions
       *&#64;&#64;       if the model does not support batching (max_batch_size = 0) or 4
       *&#64;&#64;       dimensions if the model does support batching (max_batch_size
       *&#64;&#64;       &gt;= 1). In either case the 'dims' below should only specify the
       *&#64;&#64;       3 non-batch dimensions (i.e. HWC or CHW).
       *&#64;&#64;
       * </pre>
       *
       * <code>FORMAT_NCHW = 2;</code>
       */
      public static final int FORMAT_NCHW_VALUE = 2;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Format valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static Format forNumber(int value) {
        switch (value) {
          case 0: return FORMAT_NONE;
          case 1: return FORMAT_NHWC;
          case 2: return FORMAT_NCHW;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Format>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Format> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Format>() {
              public Format findValueByNumber(int number) {
                return Format.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalStateException(
              "Can't get the descriptor of an unrecognized enum value.");
        }
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.ModelInput.getDescriptor().getEnumTypes().get(0);
      }

      private static final Format[] VALUES = values();

      public static Format valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Format(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.ModelInput.Format)
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 2;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    @java.lang.Override public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The dataType.
     */
    @java.lang.Override public inference.ModelConfigOuterClass.DataType getDataType() {
      @SuppressWarnings("deprecation")
      inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
      return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }

    public static final int FORMAT_FIELD_NUMBER = 3;
    private int format_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @return The enum numeric value on the wire for format.
     */
    @java.lang.Override public int getFormatValue() {
      return format_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Format format
     *&#64;&#64;
     *&#64;&#64;     The format of the input. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelInput.Format format = 3;</code>
     * @return The format.
     */
    @java.lang.Override public inference.ModelConfigOuterClass.ModelInput.Format getFormat() {
      @SuppressWarnings("deprecation")
      inference.ModelConfigOuterClass.ModelInput.Format result = inference.ModelConfigOuterClass.ModelInput.Format.valueOf(format_);
      return result == null ? inference.ModelConfigOuterClass.ModelInput.Format.UNRECOGNIZED : result;
    }

    public static final int DIMS_FIELD_NUMBER = 4;
    private com.google.protobuf.Internal.LongList dims_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @return A list containing the dims.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getDimsList() {
      return dims_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @return The count of dims.
     */
    public int getDimsCount() {
      return dims_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
     *&#64;&#64;     when invoking the inference API for this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 4;</code>
     * @param index The index of the element to return.
     * @return The dims at the given index.
     */
    public long getDims(int index) {
      return dims_.getLong(index);
    }
    private int dimsMemoizedSerializedSize = -1;

    public static final int RESHAPE_FIELD_NUMBER = 5;
    private inference.ModelConfigOuterClass.ModelTensorReshape reshape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return Whether the reshape field is set.
     */
    @java.lang.Override
    public boolean hasReshape() {
      return reshape_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return The reshape.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTensorReshape getReshape() {
      return reshape_ == null ? inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape expected for this input by the backend. The input will
     *&#64;&#64;     be reshaped to this before being presented to the backend. The
     *&#64;&#64;     reshape must have the same number of elements as the input shape
     *&#64;&#64;     specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
      return getReshape();
    }

    public static final int IS_SHAPE_TENSOR_FIELD_NUMBER = 6;
    private boolean isShapeTensor_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @return The isShapeTensor.
     */
    @java.lang.Override
    public boolean getIsShapeTensor() {
      return isShapeTensor_;
    }

    public static final int ALLOW_RAGGED_BATCH_FIELD_NUMBER = 7;
    private boolean allowRaggedBatch_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
     *&#64;&#64;
     *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
     *&#64;&#64;     created batch. Default is false indicating that two requests will
     *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
     *&#64;&#64;     True indicates that two requests can be batched even if this tensor
     *&#64;&#64;     has a different shape in each request. A true value is currently
     *&#64;&#64;     supported only for custom models.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_ragged_batch = 7;</code>
     * @return The allowRaggedBatch.
     */
    @java.lang.Override
    public boolean getAllowRaggedBatch() {
      return allowRaggedBatch_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        output.writeEnum(2, dataType_);
      }
      if (format_ != inference.ModelConfigOuterClass.ModelInput.Format.FORMAT_NONE.getNumber()) {
        output.writeEnum(3, format_);
      }
      if (getDimsList().size() > 0) {
        output.writeUInt32NoTag(34);
        output.writeUInt32NoTag(dimsMemoizedSerializedSize);
      }
      for (int i = 0; i < dims_.size(); i++) {
        output.writeInt64NoTag(dims_.getLong(i));
      }
      if (reshape_ != null) {
        output.writeMessage(5, getReshape());
      }
      if (isShapeTensor_ != false) {
        output.writeBool(6, isShapeTensor_);
      }
      if (allowRaggedBatch_ != false) {
        output.writeBool(7, allowRaggedBatch_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, dataType_);
      }
      if (format_ != inference.ModelConfigOuterClass.ModelInput.Format.FORMAT_NONE.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, format_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < dims_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(dims_.getLong(i));
        }
        size += dataSize;
        if (!getDimsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        dimsMemoizedSerializedSize = dataSize;
      }
      if (reshape_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getReshape());
      }
      if (isShapeTensor_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(6, isShapeTensor_);
      }
      if (allowRaggedBatch_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(7, allowRaggedBatch_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelInput)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelInput other = (inference.ModelConfigOuterClass.ModelInput) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (dataType_ != other.dataType_) return false;
      if (format_ != other.format_) return false;
      if (!getDimsList()
          .equals(other.getDimsList())) return false;
      if (hasReshape() != other.hasReshape()) return false;
      if (hasReshape()) {
        if (!getReshape()
            .equals(other.getReshape())) return false;
      }
      if (getIsShapeTensor()
          != other.getIsShapeTensor()) return false;
      if (getAllowRaggedBatch()
          != other.getAllowRaggedBatch()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + dataType_;
      hash = (37 * hash) + FORMAT_FIELD_NUMBER;
      hash = (53 * hash) + format_;
      if (getDimsCount() > 0) {
        hash = (37 * hash) + DIMS_FIELD_NUMBER;
        hash = (53 * hash) + getDimsList().hashCode();
      }
      if (hasReshape()) {
        hash = (37 * hash) + RESHAPE_FIELD_NUMBER;
        hash = (53 * hash) + getReshape().hashCode();
      }
      hash = (37 * hash) + IS_SHAPE_TENSOR_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsShapeTensor());
      hash = (37 * hash) + ALLOW_RAGGED_BATCH_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getAllowRaggedBatch());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelInput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelInput prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelInput
     *&#64;&#64;
     *&#64;&#64;   An input required by the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelInput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelInput)
        inference.ModelConfigOuterClass.ModelInputOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelInput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelInput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelInput.class, inference.ModelConfigOuterClass.ModelInput.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelInput.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        dataType_ = 0;

        format_ = 0;

        dims_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        if (reshapeBuilder_ == null) {
          reshape_ = null;
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }
        isShapeTensor_ = false;

        allowRaggedBatch_ = false;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelInput_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInput getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelInput.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInput build() {
        inference.ModelConfigOuterClass.ModelInput result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInput buildPartial() {
        inference.ModelConfigOuterClass.ModelInput result = new inference.ModelConfigOuterClass.ModelInput(this);
        int from_bitField0_ = bitField0_;
        result.name_ = name_;
        result.dataType_ = dataType_;
        result.format_ = format_;
        if (((bitField0_ & 0x00000001) != 0)) {
          dims_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.dims_ = dims_;
        if (reshapeBuilder_ == null) {
          result.reshape_ = reshape_;
        } else {
          result.reshape_ = reshapeBuilder_.build();
        }
        result.isShapeTensor_ = isShapeTensor_;
        result.allowRaggedBatch_ = allowRaggedBatch_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelInput) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelInput)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelInput other) {
        if (other == inference.ModelConfigOuterClass.ModelInput.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.dataType_ != 0) {
          setDataTypeValue(other.getDataTypeValue());
        }
        if (other.format_ != 0) {
          setFormatValue(other.getFormatValue());
        }
        if (!other.dims_.isEmpty()) {
          if (dims_.isEmpty()) {
            dims_ = other.dims_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureDimsIsMutable();
            dims_.addAll(other.dims_);
          }
          onChanged();
        }
        if (other.hasReshape()) {
          mergeReshape(other.getReshape());
        }
        if (other.getIsShapeTensor() != false) {
          setIsShapeTensor(other.getIsShapeTensor());
        }
        if (other.getAllowRaggedBatch() != false) {
          setAllowRaggedBatch(other.getAllowRaggedBatch());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelInput parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelInput) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int dataType_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @param value The enum numeric value on the wire for dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataTypeValue(int value) {
        
        dataType_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return The dataType.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @param value The dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        dataType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearDataType() {
        
        dataType_ = 0;
        onChanged();
        return this;
      }

      private int format_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @return The enum numeric value on the wire for format.
       */
      @java.lang.Override public int getFormatValue() {
        return format_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @param value The enum numeric value on the wire for format to set.
       * @return This builder for chaining.
       */
      public Builder setFormatValue(int value) {
        
        format_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @return The format.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelInput.Format getFormat() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.ModelInput.Format result = inference.ModelConfigOuterClass.ModelInput.Format.valueOf(format_);
        return result == null ? inference.ModelConfigOuterClass.ModelInput.Format.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @param value The format to set.
       * @return This builder for chaining.
       */
      public Builder setFormat(inference.ModelConfigOuterClass.ModelInput.Format value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        format_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Format format
       *&#64;&#64;
       *&#64;&#64;     The format of the input. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelInput.Format format = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearFormat() {
        
        format_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList dims_ = emptyLongList();
      private void ensureDimsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          dims_ = mutableCopy(dims_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @return A list containing the dims.
       */
      public java.util.List<java.lang.Long>
          getDimsList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(dims_) : dims_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @return The count of dims.
       */
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @param index The index of the element to return.
       * @return The dims at the given index.
       */
      public long getDims(int index) {
        return dims_.getLong(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @param index The index to set the value at.
       * @param value The dims to set.
       * @return This builder for chaining.
       */
      public Builder setDims(
          int index, long value) {
        ensureDimsIsMutable();
        dims_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @param value The dims to add.
       * @return This builder for chaining.
       */
      public Builder addDims(long value) {
        ensureDimsIsMutable();
        dims_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @param values The dims to add.
       * @return This builder for chaining.
       */
      public Builder addAllDims(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureDimsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, dims_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the input tensor that must be provided
       *&#64;&#64;     when invoking the inference API for this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearDims() {
        dims_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private inference.ModelConfigOuterClass.ModelTensorReshape reshape_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelTensorReshape, inference.ModelConfigOuterClass.ModelTensorReshape.Builder, inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> reshapeBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       * @return Whether the reshape field is set.
       */
      public boolean hasReshape() {
        return reshapeBuilder_ != null || reshape_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       * @return The reshape.
       */
      public inference.ModelConfigOuterClass.ModelTensorReshape getReshape() {
        if (reshapeBuilder_ == null) {
          return reshape_ == null ? inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        } else {
          return reshapeBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reshape_ = value;
          onChanged();
        } else {
          reshapeBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(
          inference.ModelConfigOuterClass.ModelTensorReshape.Builder builderForValue) {
        if (reshapeBuilder_ == null) {
          reshape_ = builderForValue.build();
          onChanged();
        } else {
          reshapeBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder mergeReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (reshape_ != null) {
            reshape_ =
              inference.ModelConfigOuterClass.ModelTensorReshape.newBuilder(reshape_).mergeFrom(value).buildPartial();
          } else {
            reshape_ = value;
          }
          onChanged();
        } else {
          reshapeBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder clearReshape() {
        if (reshapeBuilder_ == null) {
          reshape_ = null;
          onChanged();
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelTensorReshape.Builder getReshapeBuilder() {
        
        onChanged();
        return getReshapeFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
        if (reshapeBuilder_ != null) {
          return reshapeBuilder_.getMessageOrBuilder();
        } else {
          return reshape_ == null ?
              inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape expected for this input by the backend. The input will
       *&#64;&#64;     be reshaped to this before being presented to the backend. The
       *&#64;&#64;     reshape must have the same number of elements as the input shape
       *&#64;&#64;     specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelTensorReshape, inference.ModelConfigOuterClass.ModelTensorReshape.Builder, inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> 
          getReshapeFieldBuilder() {
        if (reshapeBuilder_ == null) {
          reshapeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelTensorReshape, inference.ModelConfigOuterClass.ModelTensorReshape.Builder, inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder>(
                  getReshape(),
                  getParentForChildren(),
                  isClean());
          reshape_ = null;
        }
        return reshapeBuilder_;
      }

      private boolean isShapeTensor_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @return The isShapeTensor.
       */
      @java.lang.Override
      public boolean getIsShapeTensor() {
        return isShapeTensor_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @param value The isShapeTensor to set.
       * @return This builder for chaining.
       */
      public Builder setIsShapeTensor(boolean value) {
        
        isShapeTensor_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsShapeTensor() {
        
        isShapeTensor_ = false;
        onChanged();
        return this;
      }

      private boolean allowRaggedBatch_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
       *&#64;&#64;     created batch. Default is false indicating that two requests will
       *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
       *&#64;&#64;     True indicates that two requests can be batched even if this tensor
       *&#64;&#64;     has a different shape in each request. A true value is currently
       *&#64;&#64;     supported only for custom models.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_ragged_batch = 7;</code>
       * @return The allowRaggedBatch.
       */
      @java.lang.Override
      public boolean getAllowRaggedBatch() {
        return allowRaggedBatch_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
       *&#64;&#64;     created batch. Default is false indicating that two requests will
       *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
       *&#64;&#64;     True indicates that two requests can be batched even if this tensor
       *&#64;&#64;     has a different shape in each request. A true value is currently
       *&#64;&#64;     supported only for custom models.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_ragged_batch = 7;</code>
       * @param value The allowRaggedBatch to set.
       * @return This builder for chaining.
       */
      public Builder setAllowRaggedBatch(boolean value) {
        
        allowRaggedBatch_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool allow_ragged_batch
       *&#64;&#64;
       *&#64;&#64;     Whether or not the input is allowed to be "ragged" in a dynamically
       *&#64;&#64;     created batch. Default is false indicating that two requests will
       *&#64;&#64;     only be batched if this tensor has the same shape in both requests.
       *&#64;&#64;     True indicates that two requests can be batched even if this tensor
       *&#64;&#64;     has a different shape in each request. A true value is currently
       *&#64;&#64;     supported only for custom models.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_ragged_batch = 7;</code>
       * @return This builder for chaining.
       */
      public Builder clearAllowRaggedBatch() {
        
        allowRaggedBatch_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelInput)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelInput)
    private static final inference.ModelConfigOuterClass.ModelInput DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelInput();
    }

    public static inference.ModelConfigOuterClass.ModelInput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelInput>
        PARSER = new com.google.protobuf.AbstractParser<ModelInput>() {
      @java.lang.Override
      public ModelInput parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelInput(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelInput> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelInput> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInput getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelOutputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelOutput)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The dataType.
     */
    inference.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @return A list containing the dims.
     */
    java.util.List<java.lang.Long> getDimsList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @return The count of dims.
     */
    int getDimsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @param index The index of the element to return.
     * @return The dims at the given index.
     */
    long getDims(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return Whether the reshape field is set.
     */
    boolean hasReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return The reshape.
     */
    inference.ModelConfigOuterClass.ModelTensorReshape getReshape();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @return The labelFilename.
     */
    java.lang.String getLabelFilename();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @return The bytes for labelFilename.
     */
    com.google.protobuf.ByteString
        getLabelFilenameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @return The isShapeTensor.
     */
    boolean getIsShapeTensor();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelOutput
   *&#64;&#64;
   *&#64;&#64;   An output produced by the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelOutput}
   */
  public static final class ModelOutput extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelOutput)
      ModelOutputOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelOutput.newBuilder() to construct.
    private ModelOutput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelOutput() {
      name_ = "";
      dataType_ = 0;
      dims_ = emptyLongList();
      labelFilename_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelOutput();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelOutput(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              dataType_ = rawValue;
              break;
            }
            case 24: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                dims_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              dims_.addLong(input.readInt64());
              break;
            }
            case 26: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                dims_ = newLongList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                dims_.addLong(input.readInt64());
              }
              input.popLimit(limit);
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();

              labelFilename_ = s;
              break;
            }
            case 42: {
              inference.ModelConfigOuterClass.ModelTensorReshape.Builder subBuilder = null;
              if (reshape_ != null) {
                subBuilder = reshape_.toBuilder();
              }
              reshape_ = input.readMessage(inference.ModelConfigOuterClass.ModelTensorReshape.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(reshape_);
                reshape_ = subBuilder.buildPartial();
              }

              break;
            }
            case 48: {

              isShapeTensor_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          dims_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelOutput_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelOutput_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelOutput.class, inference.ModelConfigOuterClass.ModelOutput.Builder.class);
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 2;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    @java.lang.Override public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;     The data-type of the output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 2;</code>
     * @return The dataType.
     */
    @java.lang.Override public inference.ModelConfigOuterClass.DataType getDataType() {
      @SuppressWarnings("deprecation")
      inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
      return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }

    public static final int DIMS_FIELD_NUMBER = 3;
    private com.google.protobuf.Internal.LongList dims_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @return A list containing the dims.
     */
    @java.lang.Override
    public java.util.List<java.lang.Long>
        getDimsList() {
      return dims_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @return The count of dims.
     */
    public int getDimsCount() {
      return dims_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
     *&#64;&#64;
     *&#64;&#64;     The dimensions/shape of the output tensor.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int64 dims = 3;</code>
     * @param index The index of the element to return.
     * @return The dims at the given index.
     */
    public long getDims(int index) {
      return dims_.getLong(index);
    }
    private int dimsMemoizedSerializedSize = -1;

    public static final int RESHAPE_FIELD_NUMBER = 5;
    private inference.ModelConfigOuterClass.ModelTensorReshape reshape_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return Whether the reshape field is set.
     */
    @java.lang.Override
    public boolean hasReshape() {
      return reshape_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     * @return The reshape.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTensorReshape getReshape() {
      return reshape_ == null ? inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
     *&#64;&#64;
     *&#64;&#64;     The shape produced for this output by the backend. The output will
     *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
     *&#64;&#64;     returned in the inference response. The reshape must have the same
     *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTensorReshape reshape = 5;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
      return getReshape();
    }

    public static final int LABEL_FILENAME_FIELD_NUMBER = 4;
    private volatile java.lang.Object labelFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @return The labelFilename.
     */
    @java.lang.Override
    public java.lang.String getLabelFilename() {
      java.lang.Object ref = labelFilename_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        labelFilename_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string label_filename
     *&#64;&#64;
     *&#64;&#64;     The label file associated with this output. Should be specified only
     *&#64;&#64;     for outputs that represent classifications. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>string label_filename = 4;</code>
     * @return The bytes for labelFilename.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getLabelFilenameBytes() {
      java.lang.Object ref = labelFilename_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        labelFilename_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int IS_SHAPE_TENSOR_FIELD_NUMBER = 6;
    private boolean isShapeTensor_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
     *&#64;&#64;
     *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
     *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
     *&#64;&#64;     generated if this specification does not comply with underlying
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool is_shape_tensor = 6;</code>
     * @return The isShapeTensor.
     */
    @java.lang.Override
    public boolean getIsShapeTensor() {
      return isShapeTensor_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        output.writeEnum(2, dataType_);
      }
      if (getDimsList().size() > 0) {
        output.writeUInt32NoTag(26);
        output.writeUInt32NoTag(dimsMemoizedSerializedSize);
      }
      for (int i = 0; i < dims_.size(); i++) {
        output.writeInt64NoTag(dims_.getLong(i));
      }
      if (!getLabelFilenameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, labelFilename_);
      }
      if (reshape_ != null) {
        output.writeMessage(5, getReshape());
      }
      if (isShapeTensor_ != false) {
        output.writeBool(6, isShapeTensor_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, dataType_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < dims_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt64SizeNoTag(dims_.getLong(i));
        }
        size += dataSize;
        if (!getDimsList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        dimsMemoizedSerializedSize = dataSize;
      }
      if (!getLabelFilenameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(4, labelFilename_);
      }
      if (reshape_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getReshape());
      }
      if (isShapeTensor_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(6, isShapeTensor_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelOutput)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelOutput other = (inference.ModelConfigOuterClass.ModelOutput) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (dataType_ != other.dataType_) return false;
      if (!getDimsList()
          .equals(other.getDimsList())) return false;
      if (hasReshape() != other.hasReshape()) return false;
      if (hasReshape()) {
        if (!getReshape()
            .equals(other.getReshape())) return false;
      }
      if (!getLabelFilename()
          .equals(other.getLabelFilename())) return false;
      if (getIsShapeTensor()
          != other.getIsShapeTensor()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + dataType_;
      if (getDimsCount() > 0) {
        hash = (37 * hash) + DIMS_FIELD_NUMBER;
        hash = (53 * hash) + getDimsList().hashCode();
      }
      if (hasReshape()) {
        hash = (37 * hash) + RESHAPE_FIELD_NUMBER;
        hash = (53 * hash) + getReshape().hashCode();
      }
      hash = (37 * hash) + LABEL_FILENAME_FIELD_NUMBER;
      hash = (53 * hash) + getLabelFilename().hashCode();
      hash = (37 * hash) + IS_SHAPE_TENSOR_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getIsShapeTensor());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelOutput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOutput prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelOutput
     *&#64;&#64;
     *&#64;&#64;   An output produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOutput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelOutput)
        inference.ModelConfigOuterClass.ModelOutputOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOutput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOutput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelOutput.class, inference.ModelConfigOuterClass.ModelOutput.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelOutput.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        dataType_ = 0;

        dims_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        if (reshapeBuilder_ == null) {
          reshape_ = null;
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }
        labelFilename_ = "";

        isShapeTensor_ = false;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOutput_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOutput getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelOutput.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOutput build() {
        inference.ModelConfigOuterClass.ModelOutput result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOutput buildPartial() {
        inference.ModelConfigOuterClass.ModelOutput result = new inference.ModelConfigOuterClass.ModelOutput(this);
        int from_bitField0_ = bitField0_;
        result.name_ = name_;
        result.dataType_ = dataType_;
        if (((bitField0_ & 0x00000001) != 0)) {
          dims_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.dims_ = dims_;
        if (reshapeBuilder_ == null) {
          result.reshape_ = reshape_;
        } else {
          result.reshape_ = reshapeBuilder_.build();
        }
        result.labelFilename_ = labelFilename_;
        result.isShapeTensor_ = isShapeTensor_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelOutput) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelOutput)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOutput other) {
        if (other == inference.ModelConfigOuterClass.ModelOutput.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.dataType_ != 0) {
          setDataTypeValue(other.getDataTypeValue());
        }
        if (!other.dims_.isEmpty()) {
          if (dims_.isEmpty()) {
            dims_ = other.dims_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureDimsIsMutable();
            dims_.addAll(other.dims_);
          }
          onChanged();
        }
        if (other.hasReshape()) {
          mergeReshape(other.getReshape());
        }
        if (!other.getLabelFilename().isEmpty()) {
          labelFilename_ = other.labelFilename_;
          onChanged();
        }
        if (other.getIsShapeTensor() != false) {
          setIsShapeTensor(other.getIsShapeTensor());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelOutput parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelOutput) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int dataType_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @param value The enum numeric value on the wire for dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataTypeValue(int value) {
        
        dataType_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return The dataType.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @param value The dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        dataType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;     The data-type of the output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearDataType() {
        
        dataType_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.Internal.LongList dims_ = emptyLongList();
      private void ensureDimsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          dims_ = mutableCopy(dims_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @return A list containing the dims.
       */
      public java.util.List<java.lang.Long>
          getDimsList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(dims_) : dims_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @return The count of dims.
       */
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @param index The index of the element to return.
       * @return The dims at the given index.
       */
      public long getDims(int index) {
        return dims_.getLong(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @param index The index to set the value at.
       * @param value The dims to set.
       * @return This builder for chaining.
       */
      public Builder setDims(
          int index, long value) {
        ensureDimsIsMutable();
        dims_.setLong(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @param value The dims to add.
       * @return This builder for chaining.
       */
      public Builder addDims(long value) {
        ensureDimsIsMutable();
        dims_.addLong(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @param values The dims to add.
       * @return This builder for chaining.
       */
      public Builder addAllDims(
          java.lang.Iterable<? extends java.lang.Long> values) {
        ensureDimsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, dims_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;     The dimensions/shape of the output tensor.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDims() {
        dims_ = emptyLongList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private inference.ModelConfigOuterClass.ModelTensorReshape reshape_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelTensorReshape, inference.ModelConfigOuterClass.ModelTensorReshape.Builder, inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> reshapeBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       * @return Whether the reshape field is set.
       */
      public boolean hasReshape() {
        return reshapeBuilder_ != null || reshape_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       * @return The reshape.
       */
      public inference.ModelConfigOuterClass.ModelTensorReshape getReshape() {
        if (reshapeBuilder_ == null) {
          return reshape_ == null ? inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        } else {
          return reshapeBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          reshape_ = value;
          onChanged();
        } else {
          reshapeBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder setReshape(
          inference.ModelConfigOuterClass.ModelTensorReshape.Builder builderForValue) {
        if (reshapeBuilder_ == null) {
          reshape_ = builderForValue.build();
          onChanged();
        } else {
          reshapeBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder mergeReshape(inference.ModelConfigOuterClass.ModelTensorReshape value) {
        if (reshapeBuilder_ == null) {
          if (reshape_ != null) {
            reshape_ =
              inference.ModelConfigOuterClass.ModelTensorReshape.newBuilder(reshape_).mergeFrom(value).buildPartial();
          } else {
            reshape_ = value;
          }
          onChanged();
        } else {
          reshapeBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public Builder clearReshape() {
        if (reshapeBuilder_ == null) {
          reshape_ = null;
          onChanged();
        } else {
          reshape_ = null;
          reshapeBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelTensorReshape.Builder getReshapeBuilder() {
        
        onChanged();
        return getReshapeFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder getReshapeOrBuilder() {
        if (reshapeBuilder_ != null) {
          return reshapeBuilder_.getMessageOrBuilder();
        } else {
          return reshape_ == null ?
              inference.ModelConfigOuterClass.ModelTensorReshape.getDefaultInstance() : reshape_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTensorReshape reshape
       *&#64;&#64;
       *&#64;&#64;     The shape produced for this output by the backend. The output will
       *&#64;&#64;     be reshaped from this to the shape specifed in 'dims' before being
       *&#64;&#64;     returned in the inference response. The reshape must have the same
       *&#64;&#64;     number of elements as the output shape specified by 'dims'. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTensorReshape reshape = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelTensorReshape, inference.ModelConfigOuterClass.ModelTensorReshape.Builder, inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder> 
          getReshapeFieldBuilder() {
        if (reshapeBuilder_ == null) {
          reshapeBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelTensorReshape, inference.ModelConfigOuterClass.ModelTensorReshape.Builder, inference.ModelConfigOuterClass.ModelTensorReshapeOrBuilder>(
                  getReshape(),
                  getParentForChildren(),
                  isClean());
          reshape_ = null;
        }
        return reshapeBuilder_;
      }

      private java.lang.Object labelFilename_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @return The labelFilename.
       */
      public java.lang.String getLabelFilename() {
        java.lang.Object ref = labelFilename_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          labelFilename_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @return The bytes for labelFilename.
       */
      public com.google.protobuf.ByteString
          getLabelFilenameBytes() {
        java.lang.Object ref = labelFilename_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          labelFilename_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @param value The labelFilename to set.
       * @return This builder for chaining.
       */
      public Builder setLabelFilename(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        labelFilename_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearLabelFilename() {
        
        labelFilename_ = getDefaultInstance().getLabelFilename();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string label_filename
       *&#64;&#64;
       *&#64;&#64;     The label file associated with this output. Should be specified only
       *&#64;&#64;     for outputs that represent classifications. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>string label_filename = 4;</code>
       * @param value The bytes for labelFilename to set.
       * @return This builder for chaining.
       */
      public Builder setLabelFilenameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        labelFilename_ = value;
        onChanged();
        return this;
      }

      private boolean isShapeTensor_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @return The isShapeTensor.
       */
      @java.lang.Override
      public boolean getIsShapeTensor() {
        return isShapeTensor_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @param value The isShapeTensor to set.
       * @return This builder for chaining.
       */
      public Builder setIsShapeTensor(boolean value) {
        
        isShapeTensor_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool is_shape_tensor
       *&#64;&#64;
       *&#64;&#64;     Whether or not the output is a shape tensor to the model. This field
       *&#64;&#64;     is currently supported only for the TensorRT model. An error will be
       *&#64;&#64;     generated if this specification does not comply with underlying
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool is_shape_tensor = 6;</code>
       * @return This builder for chaining.
       */
      public Builder clearIsShapeTensor() {
        
        isShapeTensor_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelOutput)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelOutput)
    private static final inference.ModelConfigOuterClass.ModelOutput DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOutput();
    }

    public static inference.ModelConfigOuterClass.ModelOutput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelOutput>
        PARSER = new com.google.protobuf.AbstractParser<ModelOutput>() {
      @java.lang.Override
      public ModelOutput parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelOutput(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelOutput> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelOutput> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOutput getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BatchInputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.BatchInput)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @return The enum numeric value on the wire for kind.
     */
    int getKindValue();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @return The kind.
     */
    inference.ModelConfigOuterClass.BatchInput.Kind getKind();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @return A list containing the targetName.
     */
    java.util.List<java.lang.String>
        getTargetNameList();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @return The count of targetName.
     */
    int getTargetNameCount();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    java.lang.String getTargetName(int index);
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the targetName at the given index.
     */
    com.google.protobuf.ByteString
        getTargetNameBytes(int index);

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    int getDataTypeValue();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @return The dataType.
     */
    inference.ModelConfigOuterClass.DataType getDataType();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @return A list containing the sourceInput.
     */
    java.util.List<java.lang.String>
        getSourceInputList();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @return The count of sourceInput.
     */
    int getSourceInputCount();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    java.lang.String getSourceInput(int index);
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param index The index of the value to return.
     * @return The bytes of the sourceInput at the given index.
     */
    com.google.protobuf.ByteString
        getSourceInputBytes(int index);
  }
  /**
   * <pre>
   *&#64;&#64;  .. cpp:var:: message BatchInput
   *&#64;&#64;
   *&#64;&#64;     A batch input is an additional input that must be added by
   *&#64;&#64;     the backend based on all the requests in a batch.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.BatchInput}
   */
  public static final class BatchInput extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.BatchInput)
      BatchInputOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BatchInput.newBuilder() to construct.
    private BatchInput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BatchInput() {
      kind_ = 0;
      targetName_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      dataType_ = 0;
      sourceInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BatchInput();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BatchInput(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              int rawValue = input.readEnum();

              kind_ = rawValue;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                targetName_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000001;
              }
              targetName_.add(s);
              break;
            }
            case 24: {
              int rawValue = input.readEnum();

              dataType_ = rawValue;
              break;
            }
            case 34: {
              java.lang.String s = input.readStringRequireUtf8();
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                sourceInput_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000002;
              }
              sourceInput_.add(s);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          targetName_ = targetName_.getUnmodifiableView();
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          sourceInput_ = sourceInput_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_BatchInput_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_BatchInput_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.BatchInput.class, inference.ModelConfigOuterClass.BatchInput.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;    .. cpp:enum:: Kind
     *&#64;&#64;
     *&#64;&#64;       The kind of the batch input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.BatchInput.Kind}
     */
    public enum Kind
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_ELEMENT_COUNT = 0
       *&#64;&#64;
       *&#64;&#64;         The element count of the 'source_input' will be added as
       *&#64;&#64;         input with shape [1].
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ELEMENT_COUNT = 0;</code>
       */
      BATCH_ELEMENT_COUNT(0),
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_ACCUMULATED_ELEMENT_COUNT = 1
       *&#64;&#64;
       *&#64;&#64;         The accumulated element count of the 'source_input' will be
       *&#64;&#64;         added as input with shape [1]. For example, if there is a
       *&#64;&#64;         batch of two request, each with 2 elements, an input of value
       *&#64;&#64;         2 will be added to the first request, and an input of value
       *&#64;&#64;         4 will be added to the second request.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ACCUMULATED_ELEMENT_COUNT = 1;</code>
       */
      BATCH_ACCUMULATED_ELEMENT_COUNT(1),
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator::
       *&#64;&#64;         Kind::BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO = 2
       *&#64;&#64;
       *&#64;&#64;         The accumulated element count of the 'source_input' will be
       *&#64;&#64;         added as input with shape [1], except for the first request
       *&#64;&#64;         in the batch. For the first request in the batch, the input
       *&#64;&#64;         will have shape [2] where the first element is value 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO = 2;</code>
       */
      BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO(2),
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_MAX_ELEMENT_COUNT_AS_SHAPE = 3
       *&#64;&#64;
       *&#64;&#64;         Among the requests in the batch, the max element count of the
       *&#64;&#64;         'source_input' will be added as input with shape
       *&#64;&#64;         [max_element_count] for the first request in the batch.
       *&#64;&#64;         For other requests, such input will be with shape [0].
       *&#64;&#64;         The data of the tensor will be uninitialized.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_MAX_ELEMENT_COUNT_AS_SHAPE = 3;</code>
       */
      BATCH_MAX_ELEMENT_COUNT_AS_SHAPE(3),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_ELEMENT_COUNT = 0
       *&#64;&#64;
       *&#64;&#64;         The element count of the 'source_input' will be added as
       *&#64;&#64;         input with shape [1].
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ELEMENT_COUNT = 0;</code>
       */
      public static final int BATCH_ELEMENT_COUNT_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_ACCUMULATED_ELEMENT_COUNT = 1
       *&#64;&#64;
       *&#64;&#64;         The accumulated element count of the 'source_input' will be
       *&#64;&#64;         added as input with shape [1]. For example, if there is a
       *&#64;&#64;         batch of two request, each with 2 elements, an input of value
       *&#64;&#64;         2 will be added to the first request, and an input of value
       *&#64;&#64;         4 will be added to the second request.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ACCUMULATED_ELEMENT_COUNT = 1;</code>
       */
      public static final int BATCH_ACCUMULATED_ELEMENT_COUNT_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator::
       *&#64;&#64;         Kind::BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO = 2
       *&#64;&#64;
       *&#64;&#64;         The accumulated element count of the 'source_input' will be
       *&#64;&#64;         added as input with shape [1], except for the first request
       *&#64;&#64;         in the batch. For the first request in the batch, the input
       *&#64;&#64;         will have shape [2] where the first element is value 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO = 2;</code>
       */
      public static final int BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO_VALUE = 2;
      /**
       * <pre>
       *&#64;&#64;      .. cpp:enumerator:: Kind::BATCH_MAX_ELEMENT_COUNT_AS_SHAPE = 3
       *&#64;&#64;
       *&#64;&#64;         Among the requests in the batch, the max element count of the
       *&#64;&#64;         'source_input' will be added as input with shape
       *&#64;&#64;         [max_element_count] for the first request in the batch.
       *&#64;&#64;         For other requests, such input will be with shape [0].
       *&#64;&#64;         The data of the tensor will be uninitialized.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_MAX_ELEMENT_COUNT_AS_SHAPE = 3;</code>
       */
      public static final int BATCH_MAX_ELEMENT_COUNT_AS_SHAPE_VALUE = 3;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Kind valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static Kind forNumber(int value) {
        switch (value) {
          case 0: return BATCH_ELEMENT_COUNT;
          case 1: return BATCH_ACCUMULATED_ELEMENT_COUNT;
          case 2: return BATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO;
          case 3: return BATCH_MAX_ELEMENT_COUNT_AS_SHAPE;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Kind> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
              public Kind findValueByNumber(int number) {
                return Kind.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalStateException(
              "Can't get the descriptor of an unrecognized enum value.");
        }
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.BatchInput.getDescriptor().getEnumTypes().get(0);
      }

      private static final Kind[] VALUES = values();

      public static Kind valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Kind(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.BatchInput.Kind)
    }

    public static final int KIND_FIELD_NUMBER = 1;
    private int kind_;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @return The enum numeric value on the wire for kind.
     */
    @java.lang.Override public int getKindValue() {
      return kind_;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;       The kind of this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchInput.Kind kind = 1;</code>
     * @return The kind.
     */
    @java.lang.Override public inference.ModelConfigOuterClass.BatchInput.Kind getKind() {
      @SuppressWarnings("deprecation")
      inference.ModelConfigOuterClass.BatchInput.Kind result = inference.ModelConfigOuterClass.BatchInput.Kind.valueOf(kind_);
      return result == null ? inference.ModelConfigOuterClass.BatchInput.Kind.UNRECOGNIZED : result;
    }

    public static final int TARGET_NAME_FIELD_NUMBER = 2;
    private com.google.protobuf.LazyStringList targetName_;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @return A list containing the targetName.
     */
    public com.google.protobuf.ProtocolStringList
        getTargetNameList() {
      return targetName_;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @return The count of targetName.
     */
    public int getTargetNameCount() {
      return targetName_.size();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    public java.lang.String getTargetName(int index) {
      return targetName_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;       The name of the model inputs that the backend will create
     *&#64;&#64;       for this batch input.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 2;</code>
     * @param index The index of the value to return.
     * @return The bytes of the targetName at the given index.
     */
    public com.google.protobuf.ByteString
        getTargetNameBytes(int index) {
      return targetName_.getByteString(index);
    }

    public static final int DATA_TYPE_FIELD_NUMBER = 3;
    private int dataType_;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @return The enum numeric value on the wire for dataType.
     */
    @java.lang.Override public int getDataTypeValue() {
      return dataType_;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: DataType data_type
     *&#64;&#64;
     *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
     *&#64;&#64;       TYPE_FP32.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.DataType data_type = 3;</code>
     * @return The dataType.
     */
    @java.lang.Override public inference.ModelConfigOuterClass.DataType getDataType() {
      @SuppressWarnings("deprecation")
      inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
      return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
    }

    public static final int SOURCE_INPUT_FIELD_NUMBER = 4;
    private com.google.protobuf.LazyStringList sourceInput_;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @return A list containing the sourceInput.
     */
    public com.google.protobuf.ProtocolStringList
        getSourceInputList() {
      return sourceInput_;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @return The count of sourceInput.
     */
    public int getSourceInputCount() {
      return sourceInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    public java.lang.String getSourceInput(int index) {
      return sourceInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;       The backend derives the value for each batch input from one or
     *&#64;&#64;       more other inputs. 'source_input' gives the names of those
     *&#64;&#64;       inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 4;</code>
     * @param index The index of the value to return.
     * @return The bytes of the sourceInput at the given index.
     */
    public com.google.protobuf.ByteString
        getSourceInputBytes(int index) {
      return sourceInput_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (kind_ != inference.ModelConfigOuterClass.BatchInput.Kind.BATCH_ELEMENT_COUNT.getNumber()) {
        output.writeEnum(1, kind_);
      }
      for (int i = 0; i < targetName_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, targetName_.getRaw(i));
      }
      if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        output.writeEnum(3, dataType_);
      }
      for (int i = 0; i < sourceInput_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 4, sourceInput_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (kind_ != inference.ModelConfigOuterClass.BatchInput.Kind.BATCH_ELEMENT_COUNT.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, kind_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < targetName_.size(); i++) {
          dataSize += computeStringSizeNoTag(targetName_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getTargetNameList().size();
      }
      if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, dataType_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < sourceInput_.size(); i++) {
          dataSize += computeStringSizeNoTag(sourceInput_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getSourceInputList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.BatchInput)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.BatchInput other = (inference.ModelConfigOuterClass.BatchInput) obj;

      if (kind_ != other.kind_) return false;
      if (!getTargetNameList()
          .equals(other.getTargetNameList())) return false;
      if (dataType_ != other.dataType_) return false;
      if (!getSourceInputList()
          .equals(other.getSourceInputList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + KIND_FIELD_NUMBER;
      hash = (53 * hash) + kind_;
      if (getTargetNameCount() > 0) {
        hash = (37 * hash) + TARGET_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTargetNameList().hashCode();
      }
      hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + dataType_;
      if (getSourceInputCount() > 0) {
        hash = (37 * hash) + SOURCE_INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getSourceInputList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.BatchInput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.BatchInput prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message BatchInput
     *&#64;&#64;
     *&#64;&#64;     A batch input is an additional input that must be added by
     *&#64;&#64;     the backend based on all the requests in a batch.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.BatchInput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.BatchInput)
        inference.ModelConfigOuterClass.BatchInputOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_BatchInput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_BatchInput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.BatchInput.class, inference.ModelConfigOuterClass.BatchInput.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.BatchInput.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        kind_ = 0;

        targetName_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        dataType_ = 0;

        sourceInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_BatchInput_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchInput getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.BatchInput.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchInput build() {
        inference.ModelConfigOuterClass.BatchInput result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchInput buildPartial() {
        inference.ModelConfigOuterClass.BatchInput result = new inference.ModelConfigOuterClass.BatchInput(this);
        int from_bitField0_ = bitField0_;
        result.kind_ = kind_;
        if (((bitField0_ & 0x00000001) != 0)) {
          targetName_ = targetName_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.targetName_ = targetName_;
        result.dataType_ = dataType_;
        if (((bitField0_ & 0x00000002) != 0)) {
          sourceInput_ = sourceInput_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.sourceInput_ = sourceInput_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.BatchInput) {
          return mergeFrom((inference.ModelConfigOuterClass.BatchInput)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.BatchInput other) {
        if (other == inference.ModelConfigOuterClass.BatchInput.getDefaultInstance()) return this;
        if (other.kind_ != 0) {
          setKindValue(other.getKindValue());
        }
        if (!other.targetName_.isEmpty()) {
          if (targetName_.isEmpty()) {
            targetName_ = other.targetName_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureTargetNameIsMutable();
            targetName_.addAll(other.targetName_);
          }
          onChanged();
        }
        if (other.dataType_ != 0) {
          setDataTypeValue(other.getDataTypeValue());
        }
        if (!other.sourceInput_.isEmpty()) {
          if (sourceInput_.isEmpty()) {
            sourceInput_ = other.sourceInput_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureSourceInputIsMutable();
            sourceInput_.addAll(other.sourceInput_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.BatchInput parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.BatchInput) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private int kind_ = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @return The enum numeric value on the wire for kind.
       */
      @java.lang.Override public int getKindValue() {
        return kind_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @param value The enum numeric value on the wire for kind to set.
       * @return This builder for chaining.
       */
      public Builder setKindValue(int value) {
        
        kind_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @return The kind.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchInput.Kind getKind() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.BatchInput.Kind result = inference.ModelConfigOuterClass.BatchInput.Kind.valueOf(kind_);
        return result == null ? inference.ModelConfigOuterClass.BatchInput.Kind.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @param value The kind to set.
       * @return This builder for chaining.
       */
      public Builder setKind(inference.ModelConfigOuterClass.BatchInput.Kind value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        kind_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchInput.Kind kind = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearKind() {
        
        kind_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList targetName_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureTargetNameIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          targetName_ = new com.google.protobuf.LazyStringArrayList(targetName_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @return A list containing the targetName.
       */
      public com.google.protobuf.ProtocolStringList
          getTargetNameList() {
        return targetName_.getUnmodifiableView();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @return The count of targetName.
       */
      public int getTargetNameCount() {
        return targetName_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param index The index of the element to return.
       * @return The targetName at the given index.
       */
      public java.lang.String getTargetName(int index) {
        return targetName_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param index The index of the value to return.
       * @return The bytes of the targetName at the given index.
       */
      public com.google.protobuf.ByteString
          getTargetNameBytes(int index) {
        return targetName_.getByteString(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param index The index to set the value at.
       * @param value The targetName to set.
       * @return This builder for chaining.
       */
      public Builder setTargetName(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureTargetNameIsMutable();
        targetName_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param value The targetName to add.
       * @return This builder for chaining.
       */
      public Builder addTargetName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureTargetNameIsMutable();
        targetName_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param values The targetName to add.
       * @return This builder for chaining.
       */
      public Builder addAllTargetName(
          java.lang.Iterable<java.lang.String> values) {
        ensureTargetNameIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, targetName_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearTargetName() {
        targetName_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;       The name of the model inputs that the backend will create
       *&#64;&#64;       for this batch input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 2;</code>
       * @param value The bytes of the targetName to add.
       * @return This builder for chaining.
       */
      public Builder addTargetNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        ensureTargetNameIsMutable();
        targetName_.add(value);
        onChanged();
        return this;
      }

      private int dataType_ = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @param value The enum numeric value on the wire for dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataTypeValue(int value) {
        
        dataType_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @return The dataType.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @param value The dataType to set.
       * @return This builder for chaining.
       */
      public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        dataType_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The input's datatype. The data type can be TYPE_INT32 or
       *&#64;&#64;       TYPE_FP32.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearDataType() {
        
        dataType_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList sourceInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureSourceInputIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          sourceInput_ = new com.google.protobuf.LazyStringArrayList(sourceInput_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @return A list containing the sourceInput.
       */
      public com.google.protobuf.ProtocolStringList
          getSourceInputList() {
        return sourceInput_.getUnmodifiableView();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @return The count of sourceInput.
       */
      public int getSourceInputCount() {
        return sourceInput_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param index The index of the element to return.
       * @return The sourceInput at the given index.
       */
      public java.lang.String getSourceInput(int index) {
        return sourceInput_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param index The index of the value to return.
       * @return The bytes of the sourceInput at the given index.
       */
      public com.google.protobuf.ByteString
          getSourceInputBytes(int index) {
        return sourceInput_.getByteString(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param index The index to set the value at.
       * @param value The sourceInput to set.
       * @return This builder for chaining.
       */
      public Builder setSourceInput(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureSourceInputIsMutable();
        sourceInput_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param value The sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addSourceInput(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureSourceInputIsMutable();
        sourceInput_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param values The sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addAllSourceInput(
          java.lang.Iterable<java.lang.String> values) {
        ensureSourceInputIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, sourceInput_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearSourceInput() {
        sourceInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;       The backend derives the value for each batch input from one or
       *&#64;&#64;       more other inputs. 'source_input' gives the names of those
       *&#64;&#64;       inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 4;</code>
       * @param value The bytes of the sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addSourceInputBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        ensureSourceInputIsMutable();
        sourceInput_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.BatchInput)
    }

    // @@protoc_insertion_point(class_scope:inference.BatchInput)
    private static final inference.ModelConfigOuterClass.BatchInput DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.BatchInput();
    }

    public static inference.ModelConfigOuterClass.BatchInput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<BatchInput>
        PARSER = new com.google.protobuf.AbstractParser<BatchInput>() {
      @java.lang.Override
      public BatchInput parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BatchInput(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BatchInput> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BatchInput> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchInput getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface BatchOutputOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.BatchOutput)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @return A list containing the targetName.
     */
    java.util.List<java.lang.String>
        getTargetNameList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @return The count of targetName.
     */
    int getTargetNameCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    java.lang.String getTargetName(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param index The index of the value to return.
     * @return The bytes of the targetName at the given index.
     */
    com.google.protobuf.ByteString
        getTargetNameBytes(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @return The enum numeric value on the wire for kind.
     */
    int getKindValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @return The kind.
     */
    inference.ModelConfigOuterClass.BatchOutput.Kind getKind();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @return A list containing the sourceInput.
     */
    java.util.List<java.lang.String>
        getSourceInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @return The count of sourceInput.
     */
    int getSourceInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    java.lang.String getSourceInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param index The index of the value to return.
     * @return The bytes of the sourceInput at the given index.
     */
    com.google.protobuf.ByteString
        getSourceInputBytes(int index);
  }
  /**
   * <pre>
   *&#64;&#64;.. cpp:var:: message BatchOutput
   *&#64;&#64;
   *&#64;&#64;   A batch output is an output produced by the model that must be handled
   *&#64;&#64;   differently by the backend based on all the requests in a batch.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.BatchOutput}
   */
  public static final class BatchOutput extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.BatchOutput)
      BatchOutputOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use BatchOutput.newBuilder() to construct.
    private BatchOutput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private BatchOutput() {
      targetName_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      kind_ = 0;
      sourceInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new BatchOutput();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private BatchOutput(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                targetName_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000001;
              }
              targetName_.add(s);
              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              kind_ = rawValue;
              break;
            }
            case 26: {
              java.lang.String s = input.readStringRequireUtf8();
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                sourceInput_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000002;
              }
              sourceInput_.add(s);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          targetName_ = targetName_.getUnmodifiableView();
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          sourceInput_ = sourceInput_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_BatchOutput_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_BatchOutput_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.BatchOutput.class, inference.ModelConfigOuterClass.BatchOutput.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: Kind
     *&#64;&#64;
     *&#64;&#64;     The kind of the batch output.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.BatchOutput.Kind}
     */
    public enum Kind
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::BATCH_SCATTER_WITH_INPUT_SHAPE = 0
       *&#64;&#64;
       *&#64;&#64;       The output should be scattered according to the shape of
       *&#64;&#64;       'source_input'. The dynamic dimension of the output will
       *&#64;&#64;       be set to the value of the same dimension in the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_SCATTER_WITH_INPUT_SHAPE = 0;</code>
       */
      BATCH_SCATTER_WITH_INPUT_SHAPE(0),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Kind::BATCH_SCATTER_WITH_INPUT_SHAPE = 0
       *&#64;&#64;
       *&#64;&#64;       The output should be scattered according to the shape of
       *&#64;&#64;       'source_input'. The dynamic dimension of the output will
       *&#64;&#64;       be set to the value of the same dimension in the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>BATCH_SCATTER_WITH_INPUT_SHAPE = 0;</code>
       */
      public static final int BATCH_SCATTER_WITH_INPUT_SHAPE_VALUE = 0;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static Kind valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static Kind forNumber(int value) {
        switch (value) {
          case 0: return BATCH_SCATTER_WITH_INPUT_SHAPE;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<Kind>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          Kind> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
              public Kind findValueByNumber(int number) {
                return Kind.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalStateException(
              "Can't get the descriptor of an unrecognized enum value.");
        }
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.BatchOutput.getDescriptor().getEnumTypes().get(0);
      }

      private static final Kind[] VALUES = values();

      public static Kind valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private Kind(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.BatchOutput.Kind)
    }

    public static final int TARGET_NAME_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList targetName_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @return A list containing the targetName.
     */
    public com.google.protobuf.ProtocolStringList
        getTargetNameList() {
      return targetName_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @return The count of targetName.
     */
    public int getTargetNameCount() {
      return targetName_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param index The index of the element to return.
     * @return The targetName at the given index.
     */
    public java.lang.String getTargetName(int index) {
      return targetName_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string target_name (repeated)
     *&#64;&#64;
     *&#64;&#64;     The name of the outputs to be produced by this batch output
     *&#64;&#64;     specification.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string target_name = 1;</code>
     * @param index The index of the value to return.
     * @return The bytes of the targetName at the given index.
     */
    public com.google.protobuf.ByteString
        getTargetNameBytes(int index) {
      return targetName_.getByteString(index);
    }

    public static final int KIND_FIELD_NUMBER = 2;
    private int kind_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @return The enum numeric value on the wire for kind.
     */
    @java.lang.Override public int getKindValue() {
      return kind_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Kind kind
     *&#64;&#64;
     *&#64;&#64;     The kind of this batch output.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.BatchOutput.Kind kind = 2;</code>
     * @return The kind.
     */
    @java.lang.Override public inference.ModelConfigOuterClass.BatchOutput.Kind getKind() {
      @SuppressWarnings("deprecation")
      inference.ModelConfigOuterClass.BatchOutput.Kind result = inference.ModelConfigOuterClass.BatchOutput.Kind.valueOf(kind_);
      return result == null ? inference.ModelConfigOuterClass.BatchOutput.Kind.UNRECOGNIZED : result;
    }

    public static final int SOURCE_INPUT_FIELD_NUMBER = 3;
    private com.google.protobuf.LazyStringList sourceInput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @return A list containing the sourceInput.
     */
    public com.google.protobuf.ProtocolStringList
        getSourceInputList() {
      return sourceInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @return The count of sourceInput.
     */
    public int getSourceInputCount() {
      return sourceInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param index The index of the element to return.
     * @return The sourceInput at the given index.
     */
    public java.lang.String getSourceInput(int index) {
      return sourceInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string source_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The backend derives each batch output from one or more inputs.
     *&#64;&#64;     'source_input' gives the names of those inputs.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string source_input = 3;</code>
     * @param index The index of the value to return.
     * @return The bytes of the sourceInput at the given index.
     */
    public com.google.protobuf.ByteString
        getSourceInputBytes(int index) {
      return sourceInput_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < targetName_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, targetName_.getRaw(i));
      }
      if (kind_ != inference.ModelConfigOuterClass.BatchOutput.Kind.BATCH_SCATTER_WITH_INPUT_SHAPE.getNumber()) {
        output.writeEnum(2, kind_);
      }
      for (int i = 0; i < sourceInput_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 3, sourceInput_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < targetName_.size(); i++) {
          dataSize += computeStringSizeNoTag(targetName_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getTargetNameList().size();
      }
      if (kind_ != inference.ModelConfigOuterClass.BatchOutput.Kind.BATCH_SCATTER_WITH_INPUT_SHAPE.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, kind_);
      }
      {
        int dataSize = 0;
        for (int i = 0; i < sourceInput_.size(); i++) {
          dataSize += computeStringSizeNoTag(sourceInput_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getSourceInputList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.BatchOutput)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.BatchOutput other = (inference.ModelConfigOuterClass.BatchOutput) obj;

      if (!getTargetNameList()
          .equals(other.getTargetNameList())) return false;
      if (kind_ != other.kind_) return false;
      if (!getSourceInputList()
          .equals(other.getSourceInputList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getTargetNameCount() > 0) {
        hash = (37 * hash) + TARGET_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getTargetNameList().hashCode();
      }
      hash = (37 * hash) + KIND_FIELD_NUMBER;
      hash = (53 * hash) + kind_;
      if (getSourceInputCount() > 0) {
        hash = (37 * hash) + SOURCE_INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getSourceInputList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.BatchOutput parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.BatchOutput prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;.. cpp:var:: message BatchOutput
     *&#64;&#64;
     *&#64;&#64;   A batch output is an output produced by the model that must be handled
     *&#64;&#64;   differently by the backend based on all the requests in a batch.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.BatchOutput}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.BatchOutput)
        inference.ModelConfigOuterClass.BatchOutputOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_BatchOutput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_BatchOutput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.BatchOutput.class, inference.ModelConfigOuterClass.BatchOutput.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.BatchOutput.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        targetName_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        kind_ = 0;

        sourceInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_BatchOutput_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchOutput getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.BatchOutput.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchOutput build() {
        inference.ModelConfigOuterClass.BatchOutput result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchOutput buildPartial() {
        inference.ModelConfigOuterClass.BatchOutput result = new inference.ModelConfigOuterClass.BatchOutput(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          targetName_ = targetName_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.targetName_ = targetName_;
        result.kind_ = kind_;
        if (((bitField0_ & 0x00000002) != 0)) {
          sourceInput_ = sourceInput_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000002);
        }
        result.sourceInput_ = sourceInput_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.BatchOutput) {
          return mergeFrom((inference.ModelConfigOuterClass.BatchOutput)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.BatchOutput other) {
        if (other == inference.ModelConfigOuterClass.BatchOutput.getDefaultInstance()) return this;
        if (!other.targetName_.isEmpty()) {
          if (targetName_.isEmpty()) {
            targetName_ = other.targetName_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureTargetNameIsMutable();
            targetName_.addAll(other.targetName_);
          }
          onChanged();
        }
        if (other.kind_ != 0) {
          setKindValue(other.getKindValue());
        }
        if (!other.sourceInput_.isEmpty()) {
          if (sourceInput_.isEmpty()) {
            sourceInput_ = other.sourceInput_;
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            ensureSourceInputIsMutable();
            sourceInput_.addAll(other.sourceInput_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.BatchOutput parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.BatchOutput) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.LazyStringList targetName_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureTargetNameIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          targetName_ = new com.google.protobuf.LazyStringArrayList(targetName_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @return A list containing the targetName.
       */
      public com.google.protobuf.ProtocolStringList
          getTargetNameList() {
        return targetName_.getUnmodifiableView();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @return The count of targetName.
       */
      public int getTargetNameCount() {
        return targetName_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param index The index of the element to return.
       * @return The targetName at the given index.
       */
      public java.lang.String getTargetName(int index) {
        return targetName_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param index The index of the value to return.
       * @return The bytes of the targetName at the given index.
       */
      public com.google.protobuf.ByteString
          getTargetNameBytes(int index) {
        return targetName_.getByteString(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param index The index to set the value at.
       * @param value The targetName to set.
       * @return This builder for chaining.
       */
      public Builder setTargetName(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureTargetNameIsMutable();
        targetName_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param value The targetName to add.
       * @return This builder for chaining.
       */
      public Builder addTargetName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureTargetNameIsMutable();
        targetName_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param values The targetName to add.
       * @return This builder for chaining.
       */
      public Builder addAllTargetName(
          java.lang.Iterable<java.lang.String> values) {
        ensureTargetNameIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, targetName_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTargetName() {
        targetName_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string target_name (repeated)
       *&#64;&#64;
       *&#64;&#64;     The name of the outputs to be produced by this batch output
       *&#64;&#64;     specification.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string target_name = 1;</code>
       * @param value The bytes of the targetName to add.
       * @return This builder for chaining.
       */
      public Builder addTargetNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        ensureTargetNameIsMutable();
        targetName_.add(value);
        onChanged();
        return this;
      }

      private int kind_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @return The enum numeric value on the wire for kind.
       */
      @java.lang.Override public int getKindValue() {
        return kind_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @param value The enum numeric value on the wire for kind to set.
       * @return This builder for chaining.
       */
      public Builder setKindValue(int value) {
        
        kind_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @return The kind.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.BatchOutput.Kind getKind() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.BatchOutput.Kind result = inference.ModelConfigOuterClass.BatchOutput.Kind.valueOf(kind_);
        return result == null ? inference.ModelConfigOuterClass.BatchOutput.Kind.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @param value The kind to set.
       * @return This builder for chaining.
       */
      public Builder setKind(inference.ModelConfigOuterClass.BatchOutput.Kind value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        kind_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;     The kind of this batch output.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.BatchOutput.Kind kind = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearKind() {
        
        kind_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.LazyStringList sourceInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureSourceInputIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          sourceInput_ = new com.google.protobuf.LazyStringArrayList(sourceInput_);
          bitField0_ |= 0x00000002;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @return A list containing the sourceInput.
       */
      public com.google.protobuf.ProtocolStringList
          getSourceInputList() {
        return sourceInput_.getUnmodifiableView();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @return The count of sourceInput.
       */
      public int getSourceInputCount() {
        return sourceInput_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param index The index of the element to return.
       * @return The sourceInput at the given index.
       */
      public java.lang.String getSourceInput(int index) {
        return sourceInput_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param index The index of the value to return.
       * @return The bytes of the sourceInput at the given index.
       */
      public com.google.protobuf.ByteString
          getSourceInputBytes(int index) {
        return sourceInput_.getByteString(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param index The index to set the value at.
       * @param value The sourceInput to set.
       * @return This builder for chaining.
       */
      public Builder setSourceInput(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureSourceInputIsMutable();
        sourceInput_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param value The sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addSourceInput(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureSourceInputIsMutable();
        sourceInput_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param values The sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addAllSourceInput(
          java.lang.Iterable<java.lang.String> values) {
        ensureSourceInputIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, sourceInput_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearSourceInput() {
        sourceInput_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string source_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The backend derives each batch output from one or more inputs.
       *&#64;&#64;     'source_input' gives the names of those inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string source_input = 3;</code>
       * @param value The bytes of the sourceInput to add.
       * @return This builder for chaining.
       */
      public Builder addSourceInputBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        ensureSourceInputIsMutable();
        sourceInput_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.BatchOutput)
    }

    // @@protoc_insertion_point(class_scope:inference.BatchOutput)
    private static final inference.ModelConfigOuterClass.BatchOutput DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.BatchOutput();
    }

    public static inference.ModelConfigOuterClass.BatchOutput getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<BatchOutput>
        PARSER = new com.google.protobuf.AbstractParser<BatchOutput>() {
      @java.lang.Override
      public BatchOutput parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new BatchOutput(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<BatchOutput> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<BatchOutput> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchOutput getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelVersionPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelVersionPolicy)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     * @return Whether the latest field is set.
     */
    boolean hasLatest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     * @return The latest.
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder getLatestOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     * @return Whether the all field is set.
     */
    boolean hasAll();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     * @return The all.
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy.All getAll();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder getAllOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     * @return Whether the specific field is set.
     */
    boolean hasSpecific();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     * @return The specific.
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder getSpecificOrBuilder();

    public inference.ModelConfigOuterClass.ModelVersionPolicy.PolicyChoiceCase getPolicyChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelVersionPolicy
   *&#64;&#64;
   *&#64;&#64;   Policy indicating which versions of a model should be made
   *&#64;&#64;   available by the inference server.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelVersionPolicy}
   */
  public static final class ModelVersionPolicy extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelVersionPolicy)
      ModelVersionPolicyOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelVersionPolicy.newBuilder() to construct.
    private ModelVersionPolicy(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelVersionPolicy() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelVersionPolicy();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelVersionPolicy(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder subBuilder = null;
              if (policyChoiceCase_ == 1) {
                subBuilder = ((inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_).toBuilder();
              }
              policyChoice_ =
                  input.readMessage(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_);
                policyChoice_ = subBuilder.buildPartial();
              }
              policyChoiceCase_ = 1;
              break;
            }
            case 18: {
              inference.ModelConfigOuterClass.ModelVersionPolicy.All.Builder subBuilder = null;
              if (policyChoiceCase_ == 2) {
                subBuilder = ((inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_).toBuilder();
              }
              policyChoice_ =
                  input.readMessage(inference.ModelConfigOuterClass.ModelVersionPolicy.All.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_);
                policyChoice_ = subBuilder.buildPartial();
              }
              policyChoiceCase_ = 2;
              break;
            }
            case 26: {
              inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder subBuilder = null;
              if (policyChoiceCase_ == 3) {
                subBuilder = ((inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_).toBuilder();
              }
              policyChoice_ =
                  input.readMessage(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_);
                policyChoice_ = subBuilder.buildPartial();
              }
              policyChoiceCase_ = 3;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelVersionPolicy.class, inference.ModelConfigOuterClass.ModelVersionPolicy.Builder.class);
    }

    public interface LatestOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelVersionPolicy.Latest)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       * @return The numVersions.
       */
      int getNumVersions();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Latest
     *&#64;&#64;
     *&#64;&#64;     Serve only the latest version(s) of a model. This is
     *&#64;&#64;     the default policy.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelVersionPolicy.Latest}
     */
    public static final class Latest extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelVersionPolicy.Latest)
        LatestOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Latest.newBuilder() to construct.
      private Latest(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Latest() {
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Latest();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Latest(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                numVersions_ = input.readUInt32();
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Latest_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Latest_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.class, inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder.class);
      }

      public static final int NUM_VERSIONS_FIELD_NUMBER = 1;
      private int numVersions_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint32 num_versions
       *&#64;&#64;
       *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
       *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
       *&#64;&#64;       default only the single highest-number version of a
       *&#64;&#64;       model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 num_versions = 1;</code>
       * @return The numVersions.
       */
      @java.lang.Override
      public int getNumVersions() {
        return numVersions_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (numVersions_ != 0) {
          output.writeUInt32(1, numVersions_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (numVersions_ != 0) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt32Size(1, numVersions_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelVersionPolicy.Latest)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelVersionPolicy.Latest other = (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) obj;

        if (getNumVersions()
            != other.getNumVersions()) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NUM_VERSIONS_FIELD_NUMBER;
        hash = (53 * hash) + getNumVersions();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Latest
       *&#64;&#64;
       *&#64;&#64;     Serve only the latest version(s) of a model. This is
       *&#64;&#64;     the default policy.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelVersionPolicy.Latest}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelVersionPolicy.Latest)
          inference.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Latest_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Latest_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.class, inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          numVersions_ = 0;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Latest_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelVersionPolicy.Latest build() {
          inference.ModelConfigOuterClass.ModelVersionPolicy.Latest result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelVersionPolicy.Latest buildPartial() {
          inference.ModelConfigOuterClass.ModelVersionPolicy.Latest result = new inference.ModelConfigOuterClass.ModelVersionPolicy.Latest(this);
          result.numVersions_ = numVersions_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelVersionPolicy.Latest)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest other) {
          if (other == inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance()) return this;
          if (other.getNumVersions() != 0) {
            setNumVersions(other.getNumVersions());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelVersionPolicy.Latest parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private int numVersions_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         * @return The numVersions.
         */
        @java.lang.Override
        public int getNumVersions() {
          return numVersions_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         * @param value The numVersions to set.
         * @return This builder for chaining.
         */
        public Builder setNumVersions(int value) {
          
          numVersions_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint32 num_versions
         *&#64;&#64;
         *&#64;&#64;       Serve only the 'num_versions' highest-numbered versions. T
         *&#64;&#64;       The default value of 'num_versions' is 1, indicating that by
         *&#64;&#64;       default only the single highest-number version of a
         *&#64;&#64;       model will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint32 num_versions = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearNumVersions() {
          
          numVersions_ = 0;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelVersionPolicy.Latest)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.Latest)
      private static final inference.ModelConfigOuterClass.ModelVersionPolicy.Latest DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelVersionPolicy.Latest();
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Latest>
          PARSER = new com.google.protobuf.AbstractParser<Latest>() {
        @java.lang.Override
        public Latest parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Latest(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Latest> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Latest> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface AllOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelVersionPolicy.All)
        com.google.protobuf.MessageOrBuilder {
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message All
     *&#64;&#64;
     *&#64;&#64;     Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelVersionPolicy.All}
     */
    public static final class All extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelVersionPolicy.All)
        AllOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use All.newBuilder() to construct.
      private All(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private All() {
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new All();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private All(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_All_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_All_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelVersionPolicy.All.class, inference.ModelConfigOuterClass.ModelVersionPolicy.All.Builder.class);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelVersionPolicy.All)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelVersionPolicy.All other = (inference.ModelConfigOuterClass.ModelVersionPolicy.All) obj;

        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelVersionPolicy.All prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message All
       *&#64;&#64;
       *&#64;&#64;     Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelVersionPolicy.All}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelVersionPolicy.All)
          inference.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_All_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_All_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelVersionPolicy.All.class, inference.ModelConfigOuterClass.ModelVersionPolicy.All.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelVersionPolicy.All.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_All_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelVersionPolicy.All build() {
          inference.ModelConfigOuterClass.ModelVersionPolicy.All result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelVersionPolicy.All buildPartial() {
          inference.ModelConfigOuterClass.ModelVersionPolicy.All result = new inference.ModelConfigOuterClass.ModelVersionPolicy.All(this);
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelVersionPolicy.All) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelVersionPolicy.All)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelVersionPolicy.All other) {
          if (other == inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance()) return this;
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelVersionPolicy.All parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelVersionPolicy.All) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelVersionPolicy.All)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.All)
      private static final inference.ModelConfigOuterClass.ModelVersionPolicy.All DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelVersionPolicy.All();
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<All>
          PARSER = new com.google.protobuf.AbstractParser<All>() {
        @java.lang.Override
        public All parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new All(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<All> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<All> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.All getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface SpecificOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelVersionPolicy.Specific)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @return A list containing the versions.
       */
      java.util.List<java.lang.Long> getVersionsList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @return The count of versions.
       */
      int getVersionsCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @param index The index of the element to return.
       * @return The versions at the given index.
       */
      long getVersions(int index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Specific
     *&#64;&#64;
     *&#64;&#64;     Serve only specific versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelVersionPolicy.Specific}
     */
    public static final class Specific extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelVersionPolicy.Specific)
        SpecificOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Specific.newBuilder() to construct.
      private Specific(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Specific() {
        versions_ = emptyLongList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Specific();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Specific(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  versions_ = newLongList();
                  mutable_bitField0_ |= 0x00000001;
                }
                versions_.addLong(input.readInt64());
                break;
              }
              case 10: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                  versions_ = newLongList();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  versions_.addLong(input.readInt64());
                }
                input.popLimit(limit);
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            versions_.makeImmutable(); // C
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Specific_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Specific_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.class, inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder.class);
      }

      public static final int VERSIONS_FIELD_NUMBER = 1;
      private com.google.protobuf.Internal.LongList versions_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @return A list containing the versions.
       */
      @java.lang.Override
      public java.util.List<java.lang.Long>
          getVersionsList() {
        return versions_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @return The count of versions.
       */
      public int getVersionsCount() {
        return versions_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
       *&#64;&#64;
       *&#64;&#64;       The specific versions of the model that will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 versions = 1;</code>
       * @param index The index of the element to return.
       * @return The versions at the given index.
       */
      public long getVersions(int index) {
        return versions_.getLong(index);
      }
      private int versionsMemoizedSerializedSize = -1;

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (getVersionsList().size() > 0) {
          output.writeUInt32NoTag(10);
          output.writeUInt32NoTag(versionsMemoizedSerializedSize);
        }
        for (int i = 0; i < versions_.size(); i++) {
          output.writeInt64NoTag(versions_.getLong(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        {
          int dataSize = 0;
          for (int i = 0; i < versions_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt64SizeNoTag(versions_.getLong(i));
          }
          size += dataSize;
          if (!getVersionsList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          versionsMemoizedSerializedSize = dataSize;
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelVersionPolicy.Specific)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelVersionPolicy.Specific other = (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) obj;

        if (!getVersionsList()
            .equals(other.getVersionsList())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        if (getVersionsCount() > 0) {
          hash = (37 * hash) + VERSIONS_FIELD_NUMBER;
          hash = (53 * hash) + getVersionsList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Specific
       *&#64;&#64;
       *&#64;&#64;     Serve only specific versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelVersionPolicy.Specific}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelVersionPolicy.Specific)
          inference.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Specific_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Specific_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.class, inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          versions_ = emptyLongList();
          bitField0_ = (bitField0_ & ~0x00000001);
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_Specific_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelVersionPolicy.Specific build() {
          inference.ModelConfigOuterClass.ModelVersionPolicy.Specific result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelVersionPolicy.Specific buildPartial() {
          inference.ModelConfigOuterClass.ModelVersionPolicy.Specific result = new inference.ModelConfigOuterClass.ModelVersionPolicy.Specific(this);
          int from_bitField0_ = bitField0_;
          if (((bitField0_ & 0x00000001) != 0)) {
            versions_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.versions_ = versions_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelVersionPolicy.Specific)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific other) {
          if (other == inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance()) return this;
          if (!other.versions_.isEmpty()) {
            if (versions_.isEmpty()) {
              versions_ = other.versions_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureVersionsIsMutable();
              versions_.addAll(other.versions_);
            }
            onChanged();
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelVersionPolicy.Specific parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private com.google.protobuf.Internal.LongList versions_ = emptyLongList();
        private void ensureVersionsIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            versions_ = mutableCopy(versions_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @return A list containing the versions.
         */
        public java.util.List<java.lang.Long>
            getVersionsList() {
          return ((bitField0_ & 0x00000001) != 0) ?
                   java.util.Collections.unmodifiableList(versions_) : versions_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @return The count of versions.
         */
        public int getVersionsCount() {
          return versions_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @param index The index of the element to return.
         * @return The versions at the given index.
         */
        public long getVersions(int index) {
          return versions_.getLong(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @param index The index to set the value at.
         * @param value The versions to set.
         * @return This builder for chaining.
         */
        public Builder setVersions(
            int index, long value) {
          ensureVersionsIsMutable();
          versions_.setLong(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @param value The versions to add.
         * @return This builder for chaining.
         */
        public Builder addVersions(long value) {
          ensureVersionsIsMutable();
          versions_.addLong(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @param values The versions to add.
         * @return This builder for chaining.
         */
        public Builder addAllVersions(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureVersionsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, versions_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 versions (repeated)
         *&#64;&#64;
         *&#64;&#64;       The specific versions of the model that will be served.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 versions = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearVersions() {
          versions_ = emptyLongList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelVersionPolicy.Specific)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy.Specific)
      private static final inference.ModelConfigOuterClass.ModelVersionPolicy.Specific DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelVersionPolicy.Specific();
      }

      public static inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Specific>
          PARSER = new com.google.protobuf.AbstractParser<Specific>() {
        @java.lang.Override
        public Specific parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Specific(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Specific> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Specific> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    private int policyChoiceCase_ = 0;
    private java.lang.Object policyChoice_;
    public enum PolicyChoiceCase
        implements com.google.protobuf.Internal.EnumLite,
            com.google.protobuf.AbstractMessage.InternalOneOfEnum {
      LATEST(1),
      ALL(2),
      SPECIFIC(3),
      POLICYCHOICE_NOT_SET(0);
      private final int value;
      private PolicyChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static PolicyChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static PolicyChoiceCase forNumber(int value) {
        switch (value) {
          case 1: return LATEST;
          case 2: return ALL;
          case 3: return SPECIFIC;
          case 0: return POLICYCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public PolicyChoiceCase
    getPolicyChoiceCase() {
      return PolicyChoiceCase.forNumber(
          policyChoiceCase_);
    }

    public static final int LATEST_FIELD_NUMBER = 1;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     * @return Whether the latest field is set.
     */
    @java.lang.Override
    public boolean hasLatest() {
      return policyChoiceCase_ == 1;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     * @return The latest.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest() {
      if (policyChoiceCase_ == 1) {
         return (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Latest latest
     *&#64;&#64;
     *&#64;&#64;       Serve only latest version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder getLatestOrBuilder() {
      if (policyChoiceCase_ == 1) {
         return (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
    }

    public static final int ALL_FIELD_NUMBER = 2;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     * @return Whether the all field is set.
     */
    @java.lang.Override
    public boolean hasAll() {
      return policyChoiceCase_ == 2;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     * @return The all.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy.All getAll() {
      if (policyChoiceCase_ == 2) {
         return (inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: All all
     *&#64;&#64;
     *&#64;&#64;       Serve all versions of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.All all = 2;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder getAllOrBuilder() {
      if (policyChoiceCase_ == 2) {
         return (inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
    }

    public static final int SPECIFIC_FIELD_NUMBER = 3;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     * @return Whether the specific field is set.
     */
    @java.lang.Override
    public boolean hasSpecific() {
      return policyChoiceCase_ == 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     * @return The specific.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific() {
      if (policyChoiceCase_ == 3) {
         return (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: Specific specific
     *&#64;&#64;
     *&#64;&#64;       Serve only specific version(s) of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder getSpecificOrBuilder() {
      if (policyChoiceCase_ == 3) {
         return (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (policyChoiceCase_ == 1) {
        output.writeMessage(1, (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_);
      }
      if (policyChoiceCase_ == 2) {
        output.writeMessage(2, (inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_);
      }
      if (policyChoiceCase_ == 3) {
        output.writeMessage(3, (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (policyChoiceCase_ == 1) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_);
      }
      if (policyChoiceCase_ == 2) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, (inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_);
      }
      if (policyChoiceCase_ == 3) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelVersionPolicy)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelVersionPolicy other = (inference.ModelConfigOuterClass.ModelVersionPolicy) obj;

      if (!getPolicyChoiceCase().equals(other.getPolicyChoiceCase())) return false;
      switch (policyChoiceCase_) {
        case 1:
          if (!getLatest()
              .equals(other.getLatest())) return false;
          break;
        case 2:
          if (!getAll()
              .equals(other.getAll())) return false;
          break;
        case 3:
          if (!getSpecific()
              .equals(other.getSpecific())) return false;
          break;
        case 0:
        default:
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      switch (policyChoiceCase_) {
        case 1:
          hash = (37 * hash) + LATEST_FIELD_NUMBER;
          hash = (53 * hash) + getLatest().hashCode();
          break;
        case 2:
          hash = (37 * hash) + ALL_FIELD_NUMBER;
          hash = (53 * hash) + getAll().hashCode();
          break;
        case 3:
          hash = (37 * hash) + SPECIFIC_FIELD_NUMBER;
          hash = (53 * hash) + getSpecific().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelVersionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelVersionPolicy prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelVersionPolicy
     *&#64;&#64;
     *&#64;&#64;   Policy indicating which versions of a model should be made
     *&#64;&#64;   available by the inference server.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelVersionPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelVersionPolicy)
        inference.ModelConfigOuterClass.ModelVersionPolicyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelVersionPolicy.class, inference.ModelConfigOuterClass.ModelVersionPolicy.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelVersionPolicy.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        policyChoiceCase_ = 0;
        policyChoice_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelVersionPolicy_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy build() {
        inference.ModelConfigOuterClass.ModelVersionPolicy result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy buildPartial() {
        inference.ModelConfigOuterClass.ModelVersionPolicy result = new inference.ModelConfigOuterClass.ModelVersionPolicy(this);
        if (policyChoiceCase_ == 1) {
          if (latestBuilder_ == null) {
            result.policyChoice_ = policyChoice_;
          } else {
            result.policyChoice_ = latestBuilder_.build();
          }
        }
        if (policyChoiceCase_ == 2) {
          if (allBuilder_ == null) {
            result.policyChoice_ = policyChoice_;
          } else {
            result.policyChoice_ = allBuilder_.build();
          }
        }
        if (policyChoiceCase_ == 3) {
          if (specificBuilder_ == null) {
            result.policyChoice_ = policyChoice_;
          } else {
            result.policyChoice_ = specificBuilder_.build();
          }
        }
        result.policyChoiceCase_ = policyChoiceCase_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelVersionPolicy) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelVersionPolicy)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelVersionPolicy other) {
        if (other == inference.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance()) return this;
        switch (other.getPolicyChoiceCase()) {
          case LATEST: {
            mergeLatest(other.getLatest());
            break;
          }
          case ALL: {
            mergeAll(other.getAll());
            break;
          }
          case SPECIFIC: {
            mergeSpecific(other.getSpecific());
            break;
          }
          case POLICYCHOICE_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelVersionPolicy parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelVersionPolicy) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int policyChoiceCase_ = 0;
      private java.lang.Object policyChoice_;
      public PolicyChoiceCase
          getPolicyChoiceCase() {
        return PolicyChoiceCase.forNumber(
            policyChoiceCase_);
      }

      public Builder clearPolicyChoice() {
        policyChoiceCase_ = 0;
        policyChoice_ = null;
        onChanged();
        return this;
      }


      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelVersionPolicy.Latest, inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder, inference.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder> latestBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       * @return Whether the latest field is set.
       */
      @java.lang.Override
      public boolean hasLatest() {
        return policyChoiceCase_ == 1;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       * @return The latest.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.Latest getLatest() {
        if (latestBuilder_ == null) {
          if (policyChoiceCase_ == 1) {
            return (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        } else {
          if (policyChoiceCase_ == 1) {
            return latestBuilder_.getMessage();
          }
          return inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder setLatest(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
        if (latestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          policyChoice_ = value;
          onChanged();
        } else {
          latestBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 1;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder setLatest(
          inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder builderForValue) {
        if (latestBuilder_ == null) {
          policyChoice_ = builderForValue.build();
          onChanged();
        } else {
          latestBuilder_.setMessage(builderForValue.build());
        }
        policyChoiceCase_ = 1;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder mergeLatest(inference.ModelConfigOuterClass.ModelVersionPolicy.Latest value) {
        if (latestBuilder_ == null) {
          if (policyChoiceCase_ == 1 &&
              policyChoice_ != inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance()) {
            policyChoice_ = inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.newBuilder((inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            policyChoice_ = value;
          }
          onChanged();
        } else {
          if (policyChoiceCase_ == 1) {
            latestBuilder_.mergeFrom(value);
          }
          latestBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 1;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public Builder clearLatest() {
        if (latestBuilder_ == null) {
          if (policyChoiceCase_ == 1) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
            onChanged();
          }
        } else {
          if (policyChoiceCase_ == 1) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
          }
          latestBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder getLatestBuilder() {
        return getLatestFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder getLatestOrBuilder() {
        if ((policyChoiceCase_ == 1) && (latestBuilder_ != null)) {
          return latestBuilder_.getMessageOrBuilder();
        } else {
          if (policyChoiceCase_ == 1) {
            return (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Latest latest
       *&#64;&#64;
       *&#64;&#64;       Serve only latest version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Latest latest = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelVersionPolicy.Latest, inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder, inference.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder> 
          getLatestFieldBuilder() {
        if (latestBuilder_ == null) {
          if (!(policyChoiceCase_ == 1)) {
            policyChoice_ = inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.getDefaultInstance();
          }
          latestBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelVersionPolicy.Latest, inference.ModelConfigOuterClass.ModelVersionPolicy.Latest.Builder, inference.ModelConfigOuterClass.ModelVersionPolicy.LatestOrBuilder>(
                  (inference.ModelConfigOuterClass.ModelVersionPolicy.Latest) policyChoice_,
                  getParentForChildren(),
                  isClean());
          policyChoice_ = null;
        }
        policyChoiceCase_ = 1;
        onChanged();;
        return latestBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelVersionPolicy.All, inference.ModelConfigOuterClass.ModelVersionPolicy.All.Builder, inference.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder> allBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       * @return Whether the all field is set.
       */
      @java.lang.Override
      public boolean hasAll() {
        return policyChoiceCase_ == 2;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       * @return The all.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.All getAll() {
        if (allBuilder_ == null) {
          if (policyChoiceCase_ == 2) {
            return (inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        } else {
          if (policyChoiceCase_ == 2) {
            return allBuilder_.getMessage();
          }
          return inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder setAll(inference.ModelConfigOuterClass.ModelVersionPolicy.All value) {
        if (allBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          policyChoice_ = value;
          onChanged();
        } else {
          allBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 2;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder setAll(
          inference.ModelConfigOuterClass.ModelVersionPolicy.All.Builder builderForValue) {
        if (allBuilder_ == null) {
          policyChoice_ = builderForValue.build();
          onChanged();
        } else {
          allBuilder_.setMessage(builderForValue.build());
        }
        policyChoiceCase_ = 2;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder mergeAll(inference.ModelConfigOuterClass.ModelVersionPolicy.All value) {
        if (allBuilder_ == null) {
          if (policyChoiceCase_ == 2 &&
              policyChoice_ != inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance()) {
            policyChoice_ = inference.ModelConfigOuterClass.ModelVersionPolicy.All.newBuilder((inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            policyChoice_ = value;
          }
          onChanged();
        } else {
          if (policyChoiceCase_ == 2) {
            allBuilder_.mergeFrom(value);
          }
          allBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 2;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      public Builder clearAll() {
        if (allBuilder_ == null) {
          if (policyChoiceCase_ == 2) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
            onChanged();
          }
        } else {
          if (policyChoiceCase_ == 2) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
          }
          allBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      public inference.ModelConfigOuterClass.ModelVersionPolicy.All.Builder getAllBuilder() {
        return getAllFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder getAllOrBuilder() {
        if ((policyChoiceCase_ == 2) && (allBuilder_ != null)) {
          return allBuilder_.getMessageOrBuilder();
        } else {
          if (policyChoiceCase_ == 2) {
            return (inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: All all
       *&#64;&#64;
       *&#64;&#64;       Serve all versions of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.All all = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelVersionPolicy.All, inference.ModelConfigOuterClass.ModelVersionPolicy.All.Builder, inference.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder> 
          getAllFieldBuilder() {
        if (allBuilder_ == null) {
          if (!(policyChoiceCase_ == 2)) {
            policyChoice_ = inference.ModelConfigOuterClass.ModelVersionPolicy.All.getDefaultInstance();
          }
          allBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelVersionPolicy.All, inference.ModelConfigOuterClass.ModelVersionPolicy.All.Builder, inference.ModelConfigOuterClass.ModelVersionPolicy.AllOrBuilder>(
                  (inference.ModelConfigOuterClass.ModelVersionPolicy.All) policyChoice_,
                  getParentForChildren(),
                  isClean());
          policyChoice_ = null;
        }
        policyChoiceCase_ = 2;
        onChanged();;
        return allBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelVersionPolicy.Specific, inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder, inference.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder> specificBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       * @return Whether the specific field is set.
       */
      @java.lang.Override
      public boolean hasSpecific() {
        return policyChoiceCase_ == 3;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       * @return The specific.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.Specific getSpecific() {
        if (specificBuilder_ == null) {
          if (policyChoiceCase_ == 3) {
            return (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        } else {
          if (policyChoiceCase_ == 3) {
            return specificBuilder_.getMessage();
          }
          return inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder setSpecific(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
        if (specificBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          policyChoice_ = value;
          onChanged();
        } else {
          specificBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder setSpecific(
          inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder builderForValue) {
        if (specificBuilder_ == null) {
          policyChoice_ = builderForValue.build();
          onChanged();
        } else {
          specificBuilder_.setMessage(builderForValue.build());
        }
        policyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder mergeSpecific(inference.ModelConfigOuterClass.ModelVersionPolicy.Specific value) {
        if (specificBuilder_ == null) {
          if (policyChoiceCase_ == 3 &&
              policyChoice_ != inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance()) {
            policyChoice_ = inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.newBuilder((inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            policyChoice_ = value;
          }
          onChanged();
        } else {
          if (policyChoiceCase_ == 3) {
            specificBuilder_.mergeFrom(value);
          }
          specificBuilder_.setMessage(value);
        }
        policyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public Builder clearSpecific() {
        if (specificBuilder_ == null) {
          if (policyChoiceCase_ == 3) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
            onChanged();
          }
        } else {
          if (policyChoiceCase_ == 3) {
            policyChoiceCase_ = 0;
            policyChoice_ = null;
          }
          specificBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      public inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder getSpecificBuilder() {
        return getSpecificFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder getSpecificOrBuilder() {
        if ((policyChoiceCase_ == 3) && (specificBuilder_ != null)) {
          return specificBuilder_.getMessageOrBuilder();
        } else {
          if (policyChoiceCase_ == 3) {
            return (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Specific specific
       *&#64;&#64;
       *&#64;&#64;       Serve only specific version(s) of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy.Specific specific = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelVersionPolicy.Specific, inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder, inference.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder> 
          getSpecificFieldBuilder() {
        if (specificBuilder_ == null) {
          if (!(policyChoiceCase_ == 3)) {
            policyChoice_ = inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.getDefaultInstance();
          }
          specificBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelVersionPolicy.Specific, inference.ModelConfigOuterClass.ModelVersionPolicy.Specific.Builder, inference.ModelConfigOuterClass.ModelVersionPolicy.SpecificOrBuilder>(
                  (inference.ModelConfigOuterClass.ModelVersionPolicy.Specific) policyChoice_,
                  getParentForChildren(),
                  isClean());
          policyChoice_ = null;
        }
        policyChoiceCase_ = 3;
        onChanged();;
        return specificBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelVersionPolicy)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelVersionPolicy)
    private static final inference.ModelConfigOuterClass.ModelVersionPolicy DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelVersionPolicy();
    }

    public static inference.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelVersionPolicy>
        PARSER = new com.google.protobuf.AbstractParser<ModelVersionPolicy>() {
      @java.lang.Override
      public ModelVersionPolicy parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelVersionPolicy(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelVersionPolicy> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelVersionPolicy> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelOptimizationPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @return Whether the graph field is set.
     */
    boolean hasGraph();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @return The graph.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder getGraphOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return The enum numeric value on the wire for priority.
     */
    int getPriorityValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return The priority.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @return Whether the cuda field is set.
     */
    boolean hasCuda();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @return The cuda.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder getCudaOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @return Whether the executionAccelerators field is set.
     */
    boolean hasExecutionAccelerators();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @return The executionAccelerators.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getExecutionAccelerators();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder getExecutionAcceleratorsOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @return Whether the inputPinnedMemory field is set.
     */
    boolean hasInputPinnedMemory();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @return The inputPinnedMemory.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getInputPinnedMemory();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder getInputPinnedMemoryOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @return Whether the outputPinnedMemory field is set.
     */
    boolean hasOutputPinnedMemory();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @return The outputPinnedMemory.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getOutputPinnedMemory();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder getOutputPinnedMemoryOrBuilder();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelOptimizationPolicy
   *&#64;&#64;
   *&#64;&#64;   Optimization settings for a model. These settings control if/how a
   *&#64;&#64;   model is optimized and prioritized by the backend framework when
   *&#64;&#64;   it is loaded.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelOptimizationPolicy}
   */
  public static final class ModelOptimizationPolicy extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy)
      ModelOptimizationPolicyOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelOptimizationPolicy.newBuilder() to construct.
    private ModelOptimizationPolicy(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelOptimizationPolicy() {
      priority_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelOptimizationPolicy();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelOptimizationPolicy(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder subBuilder = null;
              if (graph_ != null) {
                subBuilder = graph_.toBuilder();
              }
              graph_ = input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(graph_);
                graph_ = subBuilder.buildPartial();
              }

              break;
            }
            case 16: {
              int rawValue = input.readEnum();

              priority_ = rawValue;
              break;
            }
            case 26: {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder subBuilder = null;
              if (cuda_ != null) {
                subBuilder = cuda_.toBuilder();
              }
              cuda_ = input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(cuda_);
                cuda_ = subBuilder.buildPartial();
              }

              break;
            }
            case 34: {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder subBuilder = null;
              if (executionAccelerators_ != null) {
                subBuilder = executionAccelerators_.toBuilder();
              }
              executionAccelerators_ = input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(executionAccelerators_);
                executionAccelerators_ = subBuilder.buildPartial();
              }

              break;
            }
            case 42: {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder subBuilder = null;
              if (inputPinnedMemory_ != null) {
                subBuilder = inputPinnedMemory_.toBuilder();
              }
              inputPinnedMemory_ = input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(inputPinnedMemory_);
                inputPinnedMemory_ = subBuilder.buildPartial();
              }

              break;
            }
            case 50: {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder subBuilder = null;
              if (outputPinnedMemory_ != null) {
                subBuilder = outputPinnedMemory_.toBuilder();
              }
              outputPinnedMemory_ = input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(outputPinnedMemory_);
                outputPinnedMemory_ = subBuilder.buildPartial();
              }

              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: ModelPriority
     *&#64;&#64;
     *&#64;&#64;     Model priorities. A model will be given scheduling and execution
     *&#64;&#64;     preference over models at lower priorities. Current model
     *&#64;&#64;     priorities only work for TensorRT models.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.ModelOptimizationPolicy.ModelPriority}
     */
    public enum ModelPriority
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
       *&#64;&#64;
       *&#64;&#64;       The default model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_DEFAULT = 0;</code>
       */
      PRIORITY_DEFAULT(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
       *&#64;&#64;
       *&#64;&#64;       The maximum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MAX = 1;</code>
       */
      PRIORITY_MAX(1),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
       *&#64;&#64;
       *&#64;&#64;       The minimum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MIN = 2;</code>
       */
      PRIORITY_MIN(2),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_DEFAULT = 0
       *&#64;&#64;
       *&#64;&#64;       The default model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_DEFAULT = 0;</code>
       */
      public static final int PRIORITY_DEFAULT_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MAX = 1
       *&#64;&#64;
       *&#64;&#64;       The maximum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MAX = 1;</code>
       */
      public static final int PRIORITY_MAX_VALUE = 1;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: ModelPriority::PRIORITY_MIN = 2
       *&#64;&#64;
       *&#64;&#64;       The minimum model priority.
       *&#64;&#64;
       * </pre>
       *
       * <code>PRIORITY_MIN = 2;</code>
       */
      public static final int PRIORITY_MIN_VALUE = 2;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static ModelPriority valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static ModelPriority forNumber(int value) {
        switch (value) {
          case 0: return PRIORITY_DEFAULT;
          case 1: return PRIORITY_MAX;
          case 2: return PRIORITY_MIN;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<ModelPriority>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          ModelPriority> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<ModelPriority>() {
              public ModelPriority findValueByNumber(int number) {
                return ModelPriority.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalStateException(
              "Can't get the descriptor of an unrecognized enum value.");
        }
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.ModelOptimizationPolicy.getDescriptor().getEnumTypes().get(0);
      }

      private static final ModelPriority[] VALUES = values();

      public static ModelPriority valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private ModelPriority(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.ModelOptimizationPolicy.ModelPriority)
    }

    public interface GraphOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Graph)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       * @return The level.
       */
      int getLevel();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Graph
     *&#64;&#64;
     *&#64;&#64;     Enable generic graph optimization of the model. If not specified
     *&#64;&#64;     the framework's default level of optimization is used. Supports
     *&#64;&#64;     TensorFlow graphdef and savedmodel and Onnx models. For TensorFlow
     *&#64;&#64;     causes XLA to be enabled/disabled for the model. For Onnx defaults
     *&#64;&#64;     to enabling all optimizations, -1 enables only basic optimizations,
     *&#64;&#64;     +1 enables only basic and extended optimizations.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy.Graph}
     */
    public static final class Graph extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Graph)
        GraphOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Graph.newBuilder() to construct.
      private Graph(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Graph() {
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Graph();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Graph(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                level_ = input.readInt32();
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Graph_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Graph_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder.class);
      }

      public static final int LEVEL_FIELD_NUMBER = 1;
      private int level_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 level
       *&#64;&#64;
       *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
       *&#64;&#64;
       *&#64;&#64;         - -1: Disabled
       *&#64;&#64;         -  0: Framework default
       *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
       *&#64;&#64;            higher optimization levels)
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 level = 1;</code>
       * @return The level.
       */
      @java.lang.Override
      public int getLevel() {
        return level_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (level_ != 0) {
          output.writeInt32(1, level_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (level_ != 0) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt32Size(1, level_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph other = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph) obj;

        if (getLevel()
            != other.getLevel()) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + LEVEL_FIELD_NUMBER;
        hash = (53 * hash) + getLevel();
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Graph
       *&#64;&#64;
       *&#64;&#64;     Enable generic graph optimization of the model. If not specified
       *&#64;&#64;     the framework's default level of optimization is used. Supports
       *&#64;&#64;     TensorFlow graphdef and savedmodel and Onnx models. For TensorFlow
       *&#64;&#64;     causes XLA to be enabled/disabled for the model. For Onnx defaults
       *&#64;&#64;     to enabling all optimizations, -1 enables only basic optimizations,
       *&#64;&#64;     +1 enables only basic and extended optimizations.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.Graph}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Graph)
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Graph_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Graph_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          level_ = 0;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Graph_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph build() {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph buildPartial() {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph result = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph(this);
          result.level_ = level_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph other) {
          if (other == inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance()) return this;
          if (other.getLevel() != 0) {
            setLevel(other.getLevel());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private int level_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         * @return The level.
         */
        @java.lang.Override
        public int getLevel() {
          return level_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         * @param value The level to set.
         * @return This builder for chaining.
         */
        public Builder setLevel(int value) {
          
          level_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 level
         *&#64;&#64;
         *&#64;&#64;       The optimization level. Defaults to 0 (zero) if not specified.
         *&#64;&#64;
         *&#64;&#64;         - -1: Disabled
         *&#64;&#64;         -  0: Framework default
         *&#64;&#64;         -  1+: Enable optimization level (greater values indicate
         *&#64;&#64;            higher optimization levels)
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 level = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearLevel() {
          
          level_ = 0;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Graph)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Graph)
      private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph();
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Graph>
          PARSER = new com.google.protobuf.AbstractParser<Graph>() {
        @java.lang.Override
        public Graph parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Graph(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Graph> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Graph> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface CudaOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Cuda)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       * @return The graphs.
       */
      boolean getGraphs();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool busy_wait_events
       *&#64;&#64;
       *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
       *&#64;&#64;       latency from event complete to host thread to be notified, with
       *&#64;&#64;       the cost of high CPU load. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool busy_wait_events = 2;</code>
       * @return The busyWaitEvents.
       */
      boolean getBusyWaitEvents();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> 
          getGraphSpecList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getGraphSpec(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      int getGraphSpecCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder> 
          getGraphSpecOrBuilderList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder getGraphSpecOrBuilder(
          int index);
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda}
     */
    public static final class Cuda extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Cuda)
        CudaOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Cuda.newBuilder() to construct.
      private Cuda(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Cuda() {
        graphSpec_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Cuda();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Cuda(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                graphs_ = input.readBool();
                break;
              }
              case 16: {

                busyWaitEvents_ = input.readBool();
                break;
              }
              case 26: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  graphSpec_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec>();
                  mutable_bitField0_ |= 0x00000001;
                }
                graphSpec_.add(
                    input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.parser(), extensionRegistry));
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            graphSpec_ = java.util.Collections.unmodifiableList(graphSpec_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder.class);
      }

      public interface GraphSpecOrBuilder extends
          // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
          com.google.protobuf.MessageOrBuilder {

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int32 batch_size
         *&#64;&#64;
         *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
         *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
         *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 batch_size = 1;</code>
         * @return The batchSize.
         */
        int getBatchSize();

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        int getInputCount();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        boolean containsInput(
            java.lang.String key);
        /**
         * Use {@link #getInputMap()} instead.
         */
        @java.lang.Deprecated
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
        getInput();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
        getInputMap();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */

        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
            java.lang.String key,
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue);
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */

        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
            java.lang.String key);

        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         * @return Whether the graphLowerBound field is set.
         */
        boolean hasGraphLowerBound();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         * @return The graphLowerBound.
         */
        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getGraphLowerBound();
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         */
        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBoundOrBuilder getGraphLowerBoundOrBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: message GraphSpec
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec}
       */
      public static final class GraphSpec extends
          com.google.protobuf.GeneratedMessageV3 implements
          // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
          GraphSpecOrBuilder {
      private static final long serialVersionUID = 0L;
        // Use GraphSpec.newBuilder() to construct.
        private GraphSpec(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
          super(builder);
        }
        private GraphSpec() {
        }

        @java.lang.Override
        @SuppressWarnings({"unused"})
        protected java.lang.Object newInstance(
            UnusedPrivateParameter unused) {
          return new GraphSpec();
        }

        @java.lang.Override
        public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
          return this.unknownFields;
        }
        private GraphSpec(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          this();
          if (extensionRegistry == null) {
            throw new java.lang.NullPointerException();
          }
          int mutable_bitField0_ = 0;
          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
              com.google.protobuf.UnknownFieldSet.newBuilder();
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 8: {

                  batchSize_ = input.readInt32();
                  break;
                }
                case 18: {
                  if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                    input_ = com.google.protobuf.MapField.newMapField(
                        InputDefaultEntryHolder.defaultEntry);
                    mutable_bitField0_ |= 0x00000001;
                  }
                  com.google.protobuf.MapEntry<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
                  input__ = input.readMessage(
                      InputDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                  input_.getMutableMap().put(
                      input__.getKey(), input__.getValue());
                  break;
                }
                case 26: {
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.Builder subBuilder = null;
                  if (graphLowerBound_ != null) {
                    subBuilder = graphLowerBound_.toBuilder();
                  }
                  graphLowerBound_ = input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.parser(), extensionRegistry);
                  if (subBuilder != null) {
                    subBuilder.mergeFrom(graphLowerBound_);
                    graphLowerBound_ = subBuilder.buildPartial();
                  }

                  break;
                }
                default: {
                  if (!parseUnknownField(
                      input, unknownFields, extensionRegistry, tag)) {
                    done = true;
                  }
                  break;
                }
              }
            }
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(this);
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(
                e).setUnfinishedMessage(this);
          } finally {
            this.unknownFields = unknownFields.build();
            makeExtensionsImmutable();
          }
        }
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_descriptor;
        }

        @SuppressWarnings({"rawtypes"})
        @java.lang.Override
        protected com.google.protobuf.MapField internalGetMapField(
            int number) {
          switch (number) {
            case 2:
              return internalGetInput();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder.class);
        }

        public interface ShapeOrBuilder extends
            // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
            com.google.protobuf.MessageOrBuilder {

          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @return A list containing the dim.
           */
          java.util.List<java.lang.Long> getDimList();
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @return The count of dim.
           */
          int getDimCount();
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @param index The index of the element to return.
           * @return The dim at the given index.
           */
          long getDim(int index);
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: message Dims
         *&#64;&#64;
         *&#64;&#64;         Specification of tensor dimension.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape}
         */
        public static final class Shape extends
            com.google.protobuf.GeneratedMessageV3 implements
            // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
            ShapeOrBuilder {
        private static final long serialVersionUID = 0L;
          // Use Shape.newBuilder() to construct.
          private Shape(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
            super(builder);
          }
          private Shape() {
            dim_ = emptyLongList();
          }

          @java.lang.Override
          @SuppressWarnings({"unused"})
          protected java.lang.Object newInstance(
              UnusedPrivateParameter unused) {
            return new Shape();
          }

          @java.lang.Override
          public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
            return this.unknownFields;
          }
          private Shape(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            this();
            if (extensionRegistry == null) {
              throw new java.lang.NullPointerException();
            }
            int mutable_bitField0_ = 0;
            com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                com.google.protobuf.UnknownFieldSet.newBuilder();
            try {
              boolean done = false;
              while (!done) {
                int tag = input.readTag();
                switch (tag) {
                  case 0:
                    done = true;
                    break;
                  case 8: {
                    if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                      dim_ = newLongList();
                      mutable_bitField0_ |= 0x00000001;
                    }
                    dim_.addLong(input.readInt64());
                    break;
                  }
                  case 10: {
                    int length = input.readRawVarint32();
                    int limit = input.pushLimit(length);
                    if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                      dim_ = newLongList();
                      mutable_bitField0_ |= 0x00000001;
                    }
                    while (input.getBytesUntilLimit() > 0) {
                      dim_.addLong(input.readInt64());
                    }
                    input.popLimit(limit);
                    break;
                  }
                  default: {
                    if (!parseUnknownField(
                        input, unknownFields, extensionRegistry, tag)) {
                      done = true;
                    }
                    break;
                  }
                }
              }
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              throw e.setUnfinishedMessage(this);
            } catch (java.io.IOException e) {
              throw new com.google.protobuf.InvalidProtocolBufferException(
                  e).setUnfinishedMessage(this);
            } finally {
              if (((mutable_bitField0_ & 0x00000001) != 0)) {
                dim_.makeImmutable(); // C
              }
              this.unknownFields = unknownFields.build();
              makeExtensionsImmutable();
            }
          }
          public static final com.google.protobuf.Descriptors.Descriptor
              getDescriptor() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_descriptor;
          }

          @java.lang.Override
          protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
              internalGetFieldAccessorTable() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_fieldAccessorTable
                .ensureFieldAccessorsInitialized(
                    inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.Builder.class);
          }

          public static final int DIM_FIELD_NUMBER = 1;
          private com.google.protobuf.Internal.LongList dim_;
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @return A list containing the dim.
           */
          @java.lang.Override
          public java.util.List<java.lang.Long>
              getDimList() {
            return dim_;
          }
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @return The count of dim.
           */
          public int getDimCount() {
            return dim_.size();
          }
          /**
           * <pre>
           *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
           *&#64;&#64;
           *&#64;&#64;           The dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>repeated int64 dim = 1;</code>
           * @param index The index of the element to return.
           * @return The dim at the given index.
           */
          public long getDim(int index) {
            return dim_.getLong(index);
          }
          private int dimMemoizedSerializedSize = -1;

          private byte memoizedIsInitialized = -1;
          @java.lang.Override
          public final boolean isInitialized() {
            byte isInitialized = memoizedIsInitialized;
            if (isInitialized == 1) return true;
            if (isInitialized == 0) return false;

            memoizedIsInitialized = 1;
            return true;
          }

          @java.lang.Override
          public void writeTo(com.google.protobuf.CodedOutputStream output)
                              throws java.io.IOException {
            getSerializedSize();
            if (getDimList().size() > 0) {
              output.writeUInt32NoTag(10);
              output.writeUInt32NoTag(dimMemoizedSerializedSize);
            }
            for (int i = 0; i < dim_.size(); i++) {
              output.writeInt64NoTag(dim_.getLong(i));
            }
            unknownFields.writeTo(output);
          }

          @java.lang.Override
          public int getSerializedSize() {
            int size = memoizedSize;
            if (size != -1) return size;

            size = 0;
            {
              int dataSize = 0;
              for (int i = 0; i < dim_.size(); i++) {
                dataSize += com.google.protobuf.CodedOutputStream
                  .computeInt64SizeNoTag(dim_.getLong(i));
              }
              size += dataSize;
              if (!getDimList().isEmpty()) {
                size += 1;
                size += com.google.protobuf.CodedOutputStream
                    .computeInt32SizeNoTag(dataSize);
              }
              dimMemoizedSerializedSize = dataSize;
            }
            size += unknownFields.getSerializedSize();
            memoizedSize = size;
            return size;
          }

          @java.lang.Override
          public boolean equals(final java.lang.Object obj) {
            if (obj == this) {
             return true;
            }
            if (!(obj instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)) {
              return super.equals(obj);
            }
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape other = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape) obj;

            if (!getDimList()
                .equals(other.getDimList())) return false;
            if (!unknownFields.equals(other.unknownFields)) return false;
            return true;
          }

          @java.lang.Override
          public int hashCode() {
            if (memoizedHashCode != 0) {
              return memoizedHashCode;
            }
            int hash = 41;
            hash = (19 * hash) + getDescriptor().hashCode();
            if (getDimCount() > 0) {
              hash = (37 * hash) + DIM_FIELD_NUMBER;
              hash = (53 * hash) + getDimList().hashCode();
            }
            hash = (29 * hash) + unknownFields.hashCode();
            memoizedHashCode = hash;
            return hash;
          }

          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              java.nio.ByteBuffer data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              java.nio.ByteBuffer data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              com.google.protobuf.ByteString data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              com.google.protobuf.ByteString data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(byte[] data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              byte[] data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(java.io.InputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseWithIOException(PARSER, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              java.io.InputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseWithIOException(PARSER, input, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseDelimitedFrom(java.io.InputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseDelimitedWithIOException(PARSER, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseDelimitedFrom(
              java.io.InputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              com.google.protobuf.CodedInputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseWithIOException(PARSER, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parseFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseWithIOException(PARSER, input, extensionRegistry);
          }

          @java.lang.Override
          public Builder newBuilderForType() { return newBuilder(); }
          public static Builder newBuilder() {
            return DEFAULT_INSTANCE.toBuilder();
          }
          public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape prototype) {
            return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
          }
          @java.lang.Override
          public Builder toBuilder() {
            return this == DEFAULT_INSTANCE
                ? new Builder() : new Builder().mergeFrom(this);
          }

          @java.lang.Override
          protected Builder newBuilderForType(
              com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
            Builder builder = new Builder(parent);
            return builder;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: message Dims
           *&#64;&#64;
           *&#64;&#64;         Specification of tensor dimension.
           *&#64;&#64;
           * </pre>
           *
           * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape}
           */
          public static final class Builder extends
              com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
              // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.ShapeOrBuilder {
            public static final com.google.protobuf.Descriptors.Descriptor
                getDescriptor() {
              return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_descriptor;
            }

            @java.lang.Override
            protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
                internalGetFieldAccessorTable() {
              return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_fieldAccessorTable
                  .ensureFieldAccessorsInitialized(
                      inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.Builder.class);
            }

            // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.newBuilder()
            private Builder() {
              maybeForceBuilderInitialization();
            }

            private Builder(
                com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
              super(parent);
              maybeForceBuilderInitialization();
            }
            private void maybeForceBuilderInitialization() {
              if (com.google.protobuf.GeneratedMessageV3
                      .alwaysUseFieldBuilders) {
              }
            }
            @java.lang.Override
            public Builder clear() {
              super.clear();
              dim_ = emptyLongList();
              bitField0_ = (bitField0_ & ~0x00000001);
              return this;
            }

            @java.lang.Override
            public com.google.protobuf.Descriptors.Descriptor
                getDescriptorForType() {
              return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_descriptor;
            }

            @java.lang.Override
            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getDefaultInstanceForType() {
              return inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.getDefaultInstance();
            }

            @java.lang.Override
            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape build() {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape result = buildPartial();
              if (!result.isInitialized()) {
                throw newUninitializedMessageException(result);
              }
              return result;
            }

            @java.lang.Override
            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape buildPartial() {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape result = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape(this);
              int from_bitField0_ = bitField0_;
              if (((bitField0_ & 0x00000001) != 0)) {
                dim_.makeImmutable();
                bitField0_ = (bitField0_ & ~0x00000001);
              }
              result.dim_ = dim_;
              onBuilt();
              return result;
            }

            @java.lang.Override
            public Builder clone() {
              return super.clone();
            }
            @java.lang.Override
            public Builder setField(
                com.google.protobuf.Descriptors.FieldDescriptor field,
                java.lang.Object value) {
              return super.setField(field, value);
            }
            @java.lang.Override
            public Builder clearField(
                com.google.protobuf.Descriptors.FieldDescriptor field) {
              return super.clearField(field);
            }
            @java.lang.Override
            public Builder clearOneof(
                com.google.protobuf.Descriptors.OneofDescriptor oneof) {
              return super.clearOneof(oneof);
            }
            @java.lang.Override
            public Builder setRepeatedField(
                com.google.protobuf.Descriptors.FieldDescriptor field,
                int index, java.lang.Object value) {
              return super.setRepeatedField(field, index, value);
            }
            @java.lang.Override
            public Builder addRepeatedField(
                com.google.protobuf.Descriptors.FieldDescriptor field,
                java.lang.Object value) {
              return super.addRepeatedField(field, value);
            }
            @java.lang.Override
            public Builder mergeFrom(com.google.protobuf.Message other) {
              if (other instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape) {
                return mergeFrom((inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)other);
              } else {
                super.mergeFrom(other);
                return this;
              }
            }

            public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape other) {
              if (other == inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.getDefaultInstance()) return this;
              if (!other.dim_.isEmpty()) {
                if (dim_.isEmpty()) {
                  dim_ = other.dim_;
                  bitField0_ = (bitField0_ & ~0x00000001);
                } else {
                  ensureDimIsMutable();
                  dim_.addAll(other.dim_);
                }
                onChanged();
              }
              this.mergeUnknownFields(other.unknownFields);
              onChanged();
              return this;
            }

            @java.lang.Override
            public final boolean isInitialized() {
              return true;
            }

            @java.lang.Override
            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape parsedMessage = null;
              try {
                parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
              } catch (com.google.protobuf.InvalidProtocolBufferException e) {
                parsedMessage = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape) e.getUnfinishedMessage();
                throw e.unwrapIOException();
              } finally {
                if (parsedMessage != null) {
                  mergeFrom(parsedMessage);
                }
              }
              return this;
            }
            private int bitField0_;

            private com.google.protobuf.Internal.LongList dim_ = emptyLongList();
            private void ensureDimIsMutable() {
              if (!((bitField0_ & 0x00000001) != 0)) {
                dim_ = mutableCopy(dim_);
                bitField0_ |= 0x00000001;
               }
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @return A list containing the dim.
             */
            public java.util.List<java.lang.Long>
                getDimList() {
              return ((bitField0_ & 0x00000001) != 0) ?
                       java.util.Collections.unmodifiableList(dim_) : dim_;
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @return The count of dim.
             */
            public int getDimCount() {
              return dim_.size();
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @param index The index of the element to return.
             * @return The dim at the given index.
             */
            public long getDim(int index) {
              return dim_.getLong(index);
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @param index The index to set the value at.
             * @param value The dim to set.
             * @return This builder for chaining.
             */
            public Builder setDim(
                int index, long value) {
              ensureDimIsMutable();
              dim_.setLong(index, value);
              onChanged();
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @param value The dim to add.
             * @return This builder for chaining.
             */
            public Builder addDim(long value) {
              ensureDimIsMutable();
              dim_.addLong(value);
              onChanged();
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @param values The dim to add.
             * @return This builder for chaining.
             */
            public Builder addAllDim(
                java.lang.Iterable<? extends java.lang.Long> values) {
              ensureDimIsMutable();
              com.google.protobuf.AbstractMessageLite.Builder.addAll(
                  values, dim_);
              onChanged();
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;        .. cpp:var:: int64 dim (repeated)
             *&#64;&#64;
             *&#64;&#64;           The dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>repeated int64 dim = 1;</code>
             * @return This builder for chaining.
             */
            public Builder clearDim() {
              dim_ = emptyLongList();
              bitField0_ = (bitField0_ & ~0x00000001);
              onChanged();
              return this;
            }
            @java.lang.Override
            public final Builder setUnknownFields(
                final com.google.protobuf.UnknownFieldSet unknownFields) {
              return super.setUnknownFields(unknownFields);
            }

            @java.lang.Override
            public final Builder mergeUnknownFields(
                final com.google.protobuf.UnknownFieldSet unknownFields) {
              return super.mergeUnknownFields(unknownFields);
            }


            // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
          }

          // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape)
          private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape DEFAULT_INSTANCE;
          static {
            DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape();
          }

          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getDefaultInstance() {
            return DEFAULT_INSTANCE;
          }

          private static final com.google.protobuf.Parser<Shape>
              PARSER = new com.google.protobuf.AbstractParser<Shape>() {
            @java.lang.Override
            public Shape parsePartialFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
              return new Shape(input, extensionRegistry);
            }
          };

          public static com.google.protobuf.Parser<Shape> parser() {
            return PARSER;
          }

          @java.lang.Override
          public com.google.protobuf.Parser<Shape> getParserForType() {
            return PARSER;
          }

          @java.lang.Override
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getDefaultInstanceForType() {
            return DEFAULT_INSTANCE;
          }

        }

        public interface LowerBoundOrBuilder extends
            // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
            com.google.protobuf.MessageOrBuilder {

          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @return The batchSize.
           */
          int getBatchSize();

          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          int getInputCount();
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          boolean containsInput(
              java.lang.String key);
          /**
           * Use {@link #getInputMap()} instead.
           */
          @java.lang.Deprecated
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          getInput();
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          getInputMap();
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */

          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
              java.lang.String key,
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue);
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */

          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
              java.lang.String key);
        }
        /**
         * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound}
         */
        public static final class LowerBound extends
            com.google.protobuf.GeneratedMessageV3 implements
            // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
            LowerBoundOrBuilder {
        private static final long serialVersionUID = 0L;
          // Use LowerBound.newBuilder() to construct.
          private LowerBound(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
            super(builder);
          }
          private LowerBound() {
          }

          @java.lang.Override
          @SuppressWarnings({"unused"})
          protected java.lang.Object newInstance(
              UnusedPrivateParameter unused) {
            return new LowerBound();
          }

          @java.lang.Override
          public final com.google.protobuf.UnknownFieldSet
          getUnknownFields() {
            return this.unknownFields;
          }
          private LowerBound(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            this();
            if (extensionRegistry == null) {
              throw new java.lang.NullPointerException();
            }
            int mutable_bitField0_ = 0;
            com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                com.google.protobuf.UnknownFieldSet.newBuilder();
            try {
              boolean done = false;
              while (!done) {
                int tag = input.readTag();
                switch (tag) {
                  case 0:
                    done = true;
                    break;
                  case 8: {

                    batchSize_ = input.readInt32();
                    break;
                  }
                  case 18: {
                    if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                      input_ = com.google.protobuf.MapField.newMapField(
                          InputDefaultEntryHolder.defaultEntry);
                      mutable_bitField0_ |= 0x00000001;
                    }
                    com.google.protobuf.MapEntry<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
                    input__ = input.readMessage(
                        InputDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                    input_.getMutableMap().put(
                        input__.getKey(), input__.getValue());
                    break;
                  }
                  default: {
                    if (!parseUnknownField(
                        input, unknownFields, extensionRegistry, tag)) {
                      done = true;
                    }
                    break;
                  }
                }
              }
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              throw e.setUnfinishedMessage(this);
            } catch (java.io.IOException e) {
              throw new com.google.protobuf.InvalidProtocolBufferException(
                  e).setUnfinishedMessage(this);
            } finally {
              this.unknownFields = unknownFields.build();
              makeExtensionsImmutable();
            }
          }
          public static final com.google.protobuf.Descriptors.Descriptor
              getDescriptor() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_descriptor;
          }

          @SuppressWarnings({"rawtypes"})
          @java.lang.Override
          protected com.google.protobuf.MapField internalGetMapField(
              int number) {
            switch (number) {
              case 2:
                return internalGetInput();
              default:
                throw new RuntimeException(
                    "Invalid map field number: " + number);
            }
          }
          @java.lang.Override
          protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
              internalGetFieldAccessorTable() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_fieldAccessorTable
                .ensureFieldAccessorsInitialized(
                    inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.Builder.class);
          }

          public static final int BATCH_SIZE_FIELD_NUMBER = 1;
          private int batchSize_;
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @return The batchSize.
           */
          @java.lang.Override
          public int getBatchSize() {
            return batchSize_;
          }

          public static final int INPUT_FIELD_NUMBER = 2;
          private static final class InputDefaultEntryHolder {
            static final com.google.protobuf.MapEntry<
                java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> defaultEntry =
                    com.google.protobuf.MapEntry
                    .<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>newDefaultInstance(
                        inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_InputEntry_descriptor, 
                        com.google.protobuf.WireFormat.FieldType.STRING,
                        "",
                        com.google.protobuf.WireFormat.FieldType.MESSAGE,
                        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.getDefaultInstance());
          }
          private com.google.protobuf.MapField<
              java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> input_;
          private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          internalGetInput() {
            if (input_ == null) {
              return com.google.protobuf.MapField.emptyMapField(
                  InputDefaultEntryHolder.defaultEntry);
            }
            return input_;
          }

          public int getInputCount() {
            return internalGetInput().getMap().size();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */

          @java.lang.Override
          public boolean containsInput(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            return internalGetInput().getMap().containsKey(key);
          }
          /**
           * Use {@link #getInputMap()} instead.
           */
          @java.lang.Override
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInput() {
            return getInputMap();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInputMap() {
            return internalGetInput().getMap();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
              java.lang.String key,
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                internalGetInput().getMap();
            return map.containsKey(key) ? map.get(key) : defaultValue;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
           *&#64;&#64;         the input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                internalGetInput().getMap();
            if (!map.containsKey(key)) {
              throw new java.lang.IllegalArgumentException();
            }
            return map.get(key);
          }

          private byte memoizedIsInitialized = -1;
          @java.lang.Override
          public final boolean isInitialized() {
            byte isInitialized = memoizedIsInitialized;
            if (isInitialized == 1) return true;
            if (isInitialized == 0) return false;

            memoizedIsInitialized = 1;
            return true;
          }

          @java.lang.Override
          public void writeTo(com.google.protobuf.CodedOutputStream output)
                              throws java.io.IOException {
            if (batchSize_ != 0) {
              output.writeInt32(1, batchSize_);
            }
            com.google.protobuf.GeneratedMessageV3
              .serializeStringMapTo(
                output,
                internalGetInput(),
                InputDefaultEntryHolder.defaultEntry,
                2);
            unknownFields.writeTo(output);
          }

          @java.lang.Override
          public int getSerializedSize() {
            int size = memoizedSize;
            if (size != -1) return size;

            size = 0;
            if (batchSize_ != 0) {
              size += com.google.protobuf.CodedOutputStream
                .computeInt32Size(1, batchSize_);
            }
            for (java.util.Map.Entry<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> entry
                 : internalGetInput().getMap().entrySet()) {
              com.google.protobuf.MapEntry<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
              input__ = InputDefaultEntryHolder.defaultEntry.newBuilderForType()
                  .setKey(entry.getKey())
                  .setValue(entry.getValue())
                  .build();
              size += com.google.protobuf.CodedOutputStream
                  .computeMessageSize(2, input__);
            }
            size += unknownFields.getSerializedSize();
            memoizedSize = size;
            return size;
          }

          @java.lang.Override
          public boolean equals(final java.lang.Object obj) {
            if (obj == this) {
             return true;
            }
            if (!(obj instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)) {
              return super.equals(obj);
            }
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound other = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound) obj;

            if (getBatchSize()
                != other.getBatchSize()) return false;
            if (!internalGetInput().equals(
                other.internalGetInput())) return false;
            if (!unknownFields.equals(other.unknownFields)) return false;
            return true;
          }

          @java.lang.Override
          public int hashCode() {
            if (memoizedHashCode != 0) {
              return memoizedHashCode;
            }
            int hash = 41;
            hash = (19 * hash) + getDescriptor().hashCode();
            hash = (37 * hash) + BATCH_SIZE_FIELD_NUMBER;
            hash = (53 * hash) + getBatchSize();
            if (!internalGetInput().getMap().isEmpty()) {
              hash = (37 * hash) + INPUT_FIELD_NUMBER;
              hash = (53 * hash) + internalGetInput().hashCode();
            }
            hash = (29 * hash) + unknownFields.hashCode();
            memoizedHashCode = hash;
            return hash;
          }

          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              java.nio.ByteBuffer data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              java.nio.ByteBuffer data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              com.google.protobuf.ByteString data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              com.google.protobuf.ByteString data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(byte[] data)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              byte[] data,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return PARSER.parseFrom(data, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(java.io.InputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseWithIOException(PARSER, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              java.io.InputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseWithIOException(PARSER, input, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseDelimitedFrom(java.io.InputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseDelimitedWithIOException(PARSER, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseDelimitedFrom(
              java.io.InputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              com.google.protobuf.CodedInputStream input)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseWithIOException(PARSER, input);
          }
          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parseFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            return com.google.protobuf.GeneratedMessageV3
                .parseWithIOException(PARSER, input, extensionRegistry);
          }

          @java.lang.Override
          public Builder newBuilderForType() { return newBuilder(); }
          public static Builder newBuilder() {
            return DEFAULT_INSTANCE.toBuilder();
          }
          public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound prototype) {
            return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
          }
          @java.lang.Override
          public Builder toBuilder() {
            return this == DEFAULT_INSTANCE
                ? new Builder() : new Builder().mergeFrom(this);
          }

          @java.lang.Override
          protected Builder newBuilderForType(
              com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
            Builder builder = new Builder(parent);
            return builder;
          }
          /**
           * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound}
           */
          public static final class Builder extends
              com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
              // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBoundOrBuilder {
            public static final com.google.protobuf.Descriptors.Descriptor
                getDescriptor() {
              return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_descriptor;
            }

            @SuppressWarnings({"rawtypes"})
            protected com.google.protobuf.MapField internalGetMapField(
                int number) {
              switch (number) {
                case 2:
                  return internalGetInput();
                default:
                  throw new RuntimeException(
                      "Invalid map field number: " + number);
              }
            }
            @SuppressWarnings({"rawtypes"})
            protected com.google.protobuf.MapField internalGetMutableMapField(
                int number) {
              switch (number) {
                case 2:
                  return internalGetMutableInput();
                default:
                  throw new RuntimeException(
                      "Invalid map field number: " + number);
              }
            }
            @java.lang.Override
            protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
                internalGetFieldAccessorTable() {
              return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_fieldAccessorTable
                  .ensureFieldAccessorsInitialized(
                      inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.Builder.class);
            }

            // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.newBuilder()
            private Builder() {
              maybeForceBuilderInitialization();
            }

            private Builder(
                com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
              super(parent);
              maybeForceBuilderInitialization();
            }
            private void maybeForceBuilderInitialization() {
              if (com.google.protobuf.GeneratedMessageV3
                      .alwaysUseFieldBuilders) {
              }
            }
            @java.lang.Override
            public Builder clear() {
              super.clear();
              batchSize_ = 0;

              internalGetMutableInput().clear();
              return this;
            }

            @java.lang.Override
            public com.google.protobuf.Descriptors.Descriptor
                getDescriptorForType() {
              return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_descriptor;
            }

            @java.lang.Override
            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getDefaultInstanceForType() {
              return inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.getDefaultInstance();
            }

            @java.lang.Override
            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound build() {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound result = buildPartial();
              if (!result.isInitialized()) {
                throw newUninitializedMessageException(result);
              }
              return result;
            }

            @java.lang.Override
            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound buildPartial() {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound result = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound(this);
              int from_bitField0_ = bitField0_;
              result.batchSize_ = batchSize_;
              result.input_ = internalGetInput();
              result.input_.makeImmutable();
              onBuilt();
              return result;
            }

            @java.lang.Override
            public Builder clone() {
              return super.clone();
            }
            @java.lang.Override
            public Builder setField(
                com.google.protobuf.Descriptors.FieldDescriptor field,
                java.lang.Object value) {
              return super.setField(field, value);
            }
            @java.lang.Override
            public Builder clearField(
                com.google.protobuf.Descriptors.FieldDescriptor field) {
              return super.clearField(field);
            }
            @java.lang.Override
            public Builder clearOneof(
                com.google.protobuf.Descriptors.OneofDescriptor oneof) {
              return super.clearOneof(oneof);
            }
            @java.lang.Override
            public Builder setRepeatedField(
                com.google.protobuf.Descriptors.FieldDescriptor field,
                int index, java.lang.Object value) {
              return super.setRepeatedField(field, index, value);
            }
            @java.lang.Override
            public Builder addRepeatedField(
                com.google.protobuf.Descriptors.FieldDescriptor field,
                java.lang.Object value) {
              return super.addRepeatedField(field, value);
            }
            @java.lang.Override
            public Builder mergeFrom(com.google.protobuf.Message other) {
              if (other instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound) {
                return mergeFrom((inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)other);
              } else {
                super.mergeFrom(other);
                return this;
              }
            }

            public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound other) {
              if (other == inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.getDefaultInstance()) return this;
              if (other.getBatchSize() != 0) {
                setBatchSize(other.getBatchSize());
              }
              internalGetMutableInput().mergeFrom(
                  other.internalGetInput());
              this.mergeUnknownFields(other.unknownFields);
              onChanged();
              return this;
            }

            @java.lang.Override
            public final boolean isInitialized() {
              return true;
            }

            @java.lang.Override
            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound parsedMessage = null;
              try {
                parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
              } catch (com.google.protobuf.InvalidProtocolBufferException e) {
                parsedMessage = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound) e.getUnfinishedMessage();
                throw e.unwrapIOException();
              } finally {
                if (parsedMessage != null) {
                  mergeFrom(parsedMessage);
                }
              }
              return this;
            }
            private int bitField0_;

            private int batchSize_ ;
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: int32 batch_size
             *&#64;&#64;
             *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
             *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
             *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
             *&#64;&#64;
             * </pre>
             *
             * <code>int32 batch_size = 1;</code>
             * @return The batchSize.
             */
            @java.lang.Override
            public int getBatchSize() {
              return batchSize_;
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: int32 batch_size
             *&#64;&#64;
             *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
             *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
             *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
             *&#64;&#64;
             * </pre>
             *
             * <code>int32 batch_size = 1;</code>
             * @param value The batchSize to set.
             * @return This builder for chaining.
             */
            public Builder setBatchSize(int value) {
              
              batchSize_ = value;
              onChanged();
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: int32 batch_size
             *&#64;&#64;
             *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
             *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
             *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
             *&#64;&#64;
             * </pre>
             *
             * <code>int32 batch_size = 1;</code>
             * @return This builder for chaining.
             */
            public Builder clearBatchSize() {
              
              batchSize_ = 0;
              onChanged();
              return this;
            }

            private com.google.protobuf.MapField<
                java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> input_;
            private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
            internalGetInput() {
              if (input_ == null) {
                return com.google.protobuf.MapField.emptyMapField(
                    InputDefaultEntryHolder.defaultEntry);
              }
              return input_;
            }
            private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
            internalGetMutableInput() {
              onChanged();;
              if (input_ == null) {
                input_ = com.google.protobuf.MapField.newMapField(
                    InputDefaultEntryHolder.defaultEntry);
              }
              if (!input_.isMutable()) {
                input_ = input_.copy();
              }
              return input_;
            }

            public int getInputCount() {
              return internalGetInput().getMap().size();
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */

            @java.lang.Override
            public boolean containsInput(
                java.lang.String key) {
              if (key == null) { throw new java.lang.NullPointerException(); }
              return internalGetInput().getMap().containsKey(key);
            }
            /**
             * Use {@link #getInputMap()} instead.
             */
            @java.lang.Override
            @java.lang.Deprecated
            public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInput() {
              return getInputMap();
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            @java.lang.Override

            public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInputMap() {
              return internalGetInput().getMap();
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            @java.lang.Override

            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
                java.lang.String key,
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue) {
              if (key == null) { throw new java.lang.NullPointerException(); }
              java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                  internalGetInput().getMap();
              return map.containsKey(key) ? map.get(key) : defaultValue;
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            @java.lang.Override

            public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
                java.lang.String key) {
              if (key == null) { throw new java.lang.NullPointerException(); }
              java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                  internalGetInput().getMap();
              if (!map.containsKey(key)) {
                throw new java.lang.IllegalArgumentException();
              }
              return map.get(key);
            }

            public Builder clearInput() {
              internalGetMutableInput().getMutableMap()
                  .clear();
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */

            public Builder removeInput(
                java.lang.String key) {
              if (key == null) { throw new java.lang.NullPointerException(); }
              internalGetMutableInput().getMutableMap()
                  .remove(key);
              return this;
            }
            /**
             * Use alternate mutation accessors instead.
             */
            @java.lang.Deprecated
            public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
            getMutableInput() {
              return internalGetMutableInput().getMutableMap();
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */
            public Builder putInput(
                java.lang.String key,
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape value) {
              if (key == null) { throw new java.lang.NullPointerException(); }
              if (value == null) { throw new java.lang.NullPointerException(); }
              internalGetMutableInput().getMutableMap()
                  .put(key, value);
              return this;
            }
            /**
             * <pre>
             *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
             *&#64;&#64;
             *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of
             *&#64;&#64;         the input without batching dimension.
             *&#64;&#64;
             * </pre>
             *
             * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
             */

            public Builder putAllInput(
                java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> values) {
              internalGetMutableInput().getMutableMap()
                  .putAll(values);
              return this;
            }
            @java.lang.Override
            public final Builder setUnknownFields(
                final com.google.protobuf.UnknownFieldSet unknownFields) {
              return super.setUnknownFields(unknownFields);
            }

            @java.lang.Override
            public final Builder mergeUnknownFields(
                final com.google.protobuf.UnknownFieldSet unknownFields) {
              return super.mergeUnknownFields(unknownFields);
            }


            // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
          }

          // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound)
          private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound DEFAULT_INSTANCE;
          static {
            DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound();
          }

          public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getDefaultInstance() {
            return DEFAULT_INSTANCE;
          }

          private static final com.google.protobuf.Parser<LowerBound>
              PARSER = new com.google.protobuf.AbstractParser<LowerBound>() {
            @java.lang.Override
            public LowerBound parsePartialFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
              return new LowerBound(input, extensionRegistry);
            }
          };

          public static com.google.protobuf.Parser<LowerBound> parser() {
            return PARSER;
          }

          @java.lang.Override
          public com.google.protobuf.Parser<LowerBound> getParserForType() {
            return PARSER;
          }

          @java.lang.Override
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getDefaultInstanceForType() {
            return DEFAULT_INSTANCE;
          }

        }

        public static final int BATCH_SIZE_FIELD_NUMBER = 1;
        private int batchSize_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: int32 batch_size
         *&#64;&#64;
         *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
         *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
         *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 batch_size = 1;</code>
         * @return The batchSize.
         */
        @java.lang.Override
        public int getBatchSize() {
          return batchSize_;
        }

        public static final int INPUT_FIELD_NUMBER = 2;
        private static final class InputDefaultEntryHolder {
          static final com.google.protobuf.MapEntry<
              java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> defaultEntry =
                  com.google.protobuf.MapEntry
                  .<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>newDefaultInstance(
                      inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_InputEntry_descriptor, 
                      com.google.protobuf.WireFormat.FieldType.STRING,
                      "",
                      com.google.protobuf.WireFormat.FieldType.MESSAGE,
                      inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape.getDefaultInstance());
        }
        private com.google.protobuf.MapField<
            java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> input_;
        private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
        internalGetInput() {
          if (input_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                InputDefaultEntryHolder.defaultEntry);
          }
          return input_;
        }

        public int getInputCount() {
          return internalGetInput().getMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */

        @java.lang.Override
        public boolean containsInput(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          return internalGetInput().getMap().containsKey(key);
        }
        /**
         * Use {@link #getInputMap()} instead.
         */
        @java.lang.Override
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInput() {
          return getInputMap();
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        @java.lang.Override

        public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInputMap() {
          return internalGetInput().getMap();
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        @java.lang.Override

        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
            java.lang.String key,
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
              internalGetInput().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
         *&#64;&#64;
         *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
         *&#64;&#64;         input without batching dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
         */
        @java.lang.Override

        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
              internalGetInput().getMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }

        public static final int GRAPH_LOWER_BOUND_FIELD_NUMBER = 3;
        private inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graphLowerBound_;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         * @return Whether the graphLowerBound field is set.
         */
        @java.lang.Override
        public boolean hasGraphLowerBound() {
          return graphLowerBound_ != null;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         * @return The graphLowerBound.
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getGraphLowerBound() {
          return graphLowerBound_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.getDefaultInstance() : graphLowerBound_;
        }
        /**
         * <pre>
         *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
         *&#64;&#64;
         *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
         *&#64;&#64;         If specified, the graph can be used for input shapes and
         *&#64;&#64;         batch sizes that are in closed interval between the lower
         *&#64;&#64;         bound specification and graph specification. For dynamic
         *&#64;&#64;         shape model, this allows CUDA graphs to be launched
         *&#64;&#64;         frequently without capturing all possible shape combinations.
         *&#64;&#64;         However, using graph for shape combinations different from
         *&#64;&#64;         the one used for capturing introduces uninitialized data for
         *&#64;&#64;         execution and it may distort the inference result if
         *&#64;&#64;         the model is sensitive to uninitialized data.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBoundOrBuilder getGraphLowerBoundOrBuilder() {
          return getGraphLowerBound();
        }

        private byte memoizedIsInitialized = -1;
        @java.lang.Override
        public final boolean isInitialized() {
          byte isInitialized = memoizedIsInitialized;
          if (isInitialized == 1) return true;
          if (isInitialized == 0) return false;

          memoizedIsInitialized = 1;
          return true;
        }

        @java.lang.Override
        public void writeTo(com.google.protobuf.CodedOutputStream output)
                            throws java.io.IOException {
          if (batchSize_ != 0) {
            output.writeInt32(1, batchSize_);
          }
          com.google.protobuf.GeneratedMessageV3
            .serializeStringMapTo(
              output,
              internalGetInput(),
              InputDefaultEntryHolder.defaultEntry,
              2);
          if (graphLowerBound_ != null) {
            output.writeMessage(3, getGraphLowerBound());
          }
          unknownFields.writeTo(output);
        }

        @java.lang.Override
        public int getSerializedSize() {
          int size = memoizedSize;
          if (size != -1) return size;

          size = 0;
          if (batchSize_ != 0) {
            size += com.google.protobuf.CodedOutputStream
              .computeInt32Size(1, batchSize_);
          }
          for (java.util.Map.Entry<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> entry
               : internalGetInput().getMap().entrySet()) {
            com.google.protobuf.MapEntry<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
            input__ = InputDefaultEntryHolder.defaultEntry.newBuilderForType()
                .setKey(entry.getKey())
                .setValue(entry.getValue())
                .build();
            size += com.google.protobuf.CodedOutputStream
                .computeMessageSize(2, input__);
          }
          if (graphLowerBound_ != null) {
            size += com.google.protobuf.CodedOutputStream
              .computeMessageSize(3, getGraphLowerBound());
          }
          size += unknownFields.getSerializedSize();
          memoizedSize = size;
          return size;
        }

        @java.lang.Override
        public boolean equals(final java.lang.Object obj) {
          if (obj == this) {
           return true;
          }
          if (!(obj instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec)) {
            return super.equals(obj);
          }
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec other = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec) obj;

          if (getBatchSize()
              != other.getBatchSize()) return false;
          if (!internalGetInput().equals(
              other.internalGetInput())) return false;
          if (hasGraphLowerBound() != other.hasGraphLowerBound()) return false;
          if (hasGraphLowerBound()) {
            if (!getGraphLowerBound()
                .equals(other.getGraphLowerBound())) return false;
          }
          if (!unknownFields.equals(other.unknownFields)) return false;
          return true;
        }

        @java.lang.Override
        public int hashCode() {
          if (memoizedHashCode != 0) {
            return memoizedHashCode;
          }
          int hash = 41;
          hash = (19 * hash) + getDescriptor().hashCode();
          hash = (37 * hash) + BATCH_SIZE_FIELD_NUMBER;
          hash = (53 * hash) + getBatchSize();
          if (!internalGetInput().getMap().isEmpty()) {
            hash = (37 * hash) + INPUT_FIELD_NUMBER;
            hash = (53 * hash) + internalGetInput().hashCode();
          }
          if (hasGraphLowerBound()) {
            hash = (37 * hash) + GRAPH_LOWER_BOUND_FIELD_NUMBER;
            hash = (53 * hash) + getGraphLowerBound().hashCode();
          }
          hash = (29 * hash) + unknownFields.hashCode();
          memoizedHashCode = hash;
          return hash;
        }

        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            java.nio.ByteBuffer data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            java.nio.ByteBuffer data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            com.google.protobuf.ByteString data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(byte[] data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseDelimitedFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            com.google.protobuf.CodedInputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }

        @java.lang.Override
        public Builder newBuilderForType() { return newBuilder(); }
        public static Builder newBuilder() {
          return DEFAULT_INSTANCE.toBuilder();
        }
        public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec prototype) {
          return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
        }
        @java.lang.Override
        public Builder toBuilder() {
          return this == DEFAULT_INSTANCE
              ? new Builder() : new Builder().mergeFrom(this);
        }

        @java.lang.Override
        protected Builder newBuilderForType(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          Builder builder = new Builder(parent);
          return builder;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: message GraphSpec
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda.GraphSpec}
         */
        public static final class Builder extends
            com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
            // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder {
          public static final com.google.protobuf.Descriptors.Descriptor
              getDescriptor() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_descriptor;
          }

          @SuppressWarnings({"rawtypes"})
          protected com.google.protobuf.MapField internalGetMapField(
              int number) {
            switch (number) {
              case 2:
                return internalGetInput();
              default:
                throw new RuntimeException(
                    "Invalid map field number: " + number);
            }
          }
          @SuppressWarnings({"rawtypes"})
          protected com.google.protobuf.MapField internalGetMutableMapField(
              int number) {
            switch (number) {
              case 2:
                return internalGetMutableInput();
              default:
                throw new RuntimeException(
                    "Invalid map field number: " + number);
            }
          }
          @java.lang.Override
          protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
              internalGetFieldAccessorTable() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_fieldAccessorTable
                .ensureFieldAccessorsInitialized(
                    inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder.class);
          }

          // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.newBuilder()
          private Builder() {
            maybeForceBuilderInitialization();
          }

          private Builder(
              com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
            super(parent);
            maybeForceBuilderInitialization();
          }
          private void maybeForceBuilderInitialization() {
            if (com.google.protobuf.GeneratedMessageV3
                    .alwaysUseFieldBuilders) {
            }
          }
          @java.lang.Override
          public Builder clear() {
            super.clear();
            batchSize_ = 0;

            internalGetMutableInput().clear();
            if (graphLowerBoundBuilder_ == null) {
              graphLowerBound_ = null;
            } else {
              graphLowerBound_ = null;
              graphLowerBoundBuilder_ = null;
            }
            return this;
          }

          @java.lang.Override
          public com.google.protobuf.Descriptors.Descriptor
              getDescriptorForType() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_descriptor;
          }

          @java.lang.Override
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getDefaultInstanceForType() {
            return inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.getDefaultInstance();
          }

          @java.lang.Override
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec build() {
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec result = buildPartial();
            if (!result.isInitialized()) {
              throw newUninitializedMessageException(result);
            }
            return result;
          }

          @java.lang.Override
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec buildPartial() {
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec result = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec(this);
            int from_bitField0_ = bitField0_;
            result.batchSize_ = batchSize_;
            result.input_ = internalGetInput();
            result.input_.makeImmutable();
            if (graphLowerBoundBuilder_ == null) {
              result.graphLowerBound_ = graphLowerBound_;
            } else {
              result.graphLowerBound_ = graphLowerBoundBuilder_.build();
            }
            onBuilt();
            return result;
          }

          @java.lang.Override
          public Builder clone() {
            return super.clone();
          }
          @java.lang.Override
          public Builder setField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.setField(field, value);
          }
          @java.lang.Override
          public Builder clearField(
              com.google.protobuf.Descriptors.FieldDescriptor field) {
            return super.clearField(field);
          }
          @java.lang.Override
          public Builder clearOneof(
              com.google.protobuf.Descriptors.OneofDescriptor oneof) {
            return super.clearOneof(oneof);
          }
          @java.lang.Override
          public Builder setRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              int index, java.lang.Object value) {
            return super.setRepeatedField(field, index, value);
          }
          @java.lang.Override
          public Builder addRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.addRepeatedField(field, value);
          }
          @java.lang.Override
          public Builder mergeFrom(com.google.protobuf.Message other) {
            if (other instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec) {
              return mergeFrom((inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec)other);
            } else {
              super.mergeFrom(other);
              return this;
            }
          }

          public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec other) {
            if (other == inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.getDefaultInstance()) return this;
            if (other.getBatchSize() != 0) {
              setBatchSize(other.getBatchSize());
            }
            internalGetMutableInput().mergeFrom(
                other.internalGetInput());
            if (other.hasGraphLowerBound()) {
              mergeGraphLowerBound(other.getGraphLowerBound());
            }
            this.mergeUnknownFields(other.unknownFields);
            onChanged();
            return this;
          }

          @java.lang.Override
          public final boolean isInitialized() {
            return true;
          }

          @java.lang.Override
          public Builder mergeFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec parsedMessage = null;
            try {
              parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              parsedMessage = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec) e.getUnfinishedMessage();
              throw e.unwrapIOException();
            } finally {
              if (parsedMessage != null) {
                mergeFrom(parsedMessage);
              }
            }
            return this;
          }
          private int bitField0_;

          private int batchSize_ ;
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @return The batchSize.
           */
          @java.lang.Override
          public int getBatchSize() {
            return batchSize_;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @param value The batchSize to set.
           * @return This builder for chaining.
           */
          public Builder setBatchSize(int value) {
            
            batchSize_ = value;
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: int32 batch_size
           *&#64;&#64;
           *&#64;&#64;         The batch size of the CUDA graph. If 'max_batch_size' is 0,
           *&#64;&#64;         'batch_size' must be set to 0. Otherwise, 'batch_size' must
           *&#64;&#64;         be set to value between 1 and 'max_batch_size'.
           *&#64;&#64;
           * </pre>
           *
           * <code>int32 batch_size = 1;</code>
           * @return This builder for chaining.
           */
          public Builder clearBatchSize() {
            
            batchSize_ = 0;
            onChanged();
            return this;
          }

          private com.google.protobuf.MapField<
              java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> input_;
          private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          internalGetInput() {
            if (input_ == null) {
              return com.google.protobuf.MapField.emptyMapField(
                  InputDefaultEntryHolder.defaultEntry);
            }
            return input_;
          }
          private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          internalGetMutableInput() {
            onChanged();;
            if (input_ == null) {
              input_ = com.google.protobuf.MapField.newMapField(
                  InputDefaultEntryHolder.defaultEntry);
            }
            if (!input_.isMutable()) {
              input_ = input_.copy();
            }
            return input_;
          }

          public int getInputCount() {
            return internalGetInput().getMap().size();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */

          @java.lang.Override
          public boolean containsInput(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            return internalGetInput().getMap().containsKey(key);
          }
          /**
           * Use {@link #getInputMap()} instead.
           */
          @java.lang.Override
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInput() {
            return getInputMap();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> getInputMap() {
            return internalGetInput().getMap();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrDefault(
              java.lang.String key,
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape defaultValue) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                internalGetInput().getMap();
            return map.containsKey(key) ? map.get(key) : defaultValue;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          @java.lang.Override

          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape getInputOrThrow(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> map =
                internalGetInput().getMap();
            if (!map.containsKey(key)) {
              throw new java.lang.IllegalArgumentException();
            }
            return map.get(key);
          }

          public Builder clearInput() {
            internalGetMutableInput().getMutableMap()
                .clear();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */

          public Builder removeInput(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            internalGetMutableInput().getMutableMap()
                .remove(key);
            return this;
          }
          /**
           * Use alternate mutation accessors instead.
           */
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape>
          getMutableInput() {
            return internalGetMutableInput().getMutableMap();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */
          public Builder putInput(
              java.lang.String key,
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape value) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            if (value == null) { throw new java.lang.NullPointerException(); }
            internalGetMutableInput().getMutableMap()
                .put(key, value);
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: map&lt;string, Shape&gt; input
           *&#64;&#64;
           *&#64;&#64;         The specification of the inputs. 'Shape' is the shape of the
           *&#64;&#64;         input without batching dimension.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, .inference.ModelOptimizationPolicy.Cuda.GraphSpec.Shape&gt; input = 2;</code>
           */

          public Builder putAllInput(
              java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Shape> values) {
            internalGetMutableInput().getMutableMap()
                .putAll(values);
            return this;
          }

          private inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graphLowerBound_;
          private com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBoundOrBuilder> graphLowerBoundBuilder_;
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           * @return Whether the graphLowerBound field is set.
           */
          public boolean hasGraphLowerBound() {
            return graphLowerBoundBuilder_ != null || graphLowerBound_ != null;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           * @return The graphLowerBound.
           */
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound getGraphLowerBound() {
            if (graphLowerBoundBuilder_ == null) {
              return graphLowerBound_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.getDefaultInstance() : graphLowerBound_;
            } else {
              return graphLowerBoundBuilder_.getMessage();
            }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public Builder setGraphLowerBound(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound value) {
            if (graphLowerBoundBuilder_ == null) {
              if (value == null) {
                throw new NullPointerException();
              }
              graphLowerBound_ = value;
              onChanged();
            } else {
              graphLowerBoundBuilder_.setMessage(value);
            }

            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public Builder setGraphLowerBound(
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.Builder builderForValue) {
            if (graphLowerBoundBuilder_ == null) {
              graphLowerBound_ = builderForValue.build();
              onChanged();
            } else {
              graphLowerBoundBuilder_.setMessage(builderForValue.build());
            }

            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public Builder mergeGraphLowerBound(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound value) {
            if (graphLowerBoundBuilder_ == null) {
              if (graphLowerBound_ != null) {
                graphLowerBound_ =
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.newBuilder(graphLowerBound_).mergeFrom(value).buildPartial();
              } else {
                graphLowerBound_ = value;
              }
              onChanged();
            } else {
              graphLowerBoundBuilder_.mergeFrom(value);
            }

            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public Builder clearGraphLowerBound() {
            if (graphLowerBoundBuilder_ == null) {
              graphLowerBound_ = null;
              onChanged();
            } else {
              graphLowerBound_ = null;
              graphLowerBoundBuilder_ = null;
            }

            return this;
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.Builder getGraphLowerBoundBuilder() {
            
            onChanged();
            return getGraphLowerBoundFieldBuilder().getBuilder();
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBoundOrBuilder getGraphLowerBoundOrBuilder() {
            if (graphLowerBoundBuilder_ != null) {
              return graphLowerBoundBuilder_.getMessageOrBuilder();
            } else {
              return graphLowerBound_ == null ?
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.getDefaultInstance() : graphLowerBound_;
            }
          }
          /**
           * <pre>
           *&#64;&#64;      .. cpp:var:: LowerBound graph_lower_bound
           *&#64;&#64;
           *&#64;&#64;         Specify the lower bound of the CUDA graph. Optional.
           *&#64;&#64;         If specified, the graph can be used for input shapes and
           *&#64;&#64;         batch sizes that are in closed interval between the lower
           *&#64;&#64;         bound specification and graph specification. For dynamic
           *&#64;&#64;         shape model, this allows CUDA graphs to be launched
           *&#64;&#64;         frequently without capturing all possible shape combinations.
           *&#64;&#64;         However, using graph for shape combinations different from
           *&#64;&#64;         the one used for capturing introduces uninitialized data for
           *&#64;&#64;         execution and it may distort the inference result if
           *&#64;&#64;         the model is sensitive to uninitialized data.
           *&#64;&#64;
           * </pre>
           *
           * <code>.inference.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound graph_lower_bound = 3;</code>
           */
          private com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBoundOrBuilder> 
              getGraphLowerBoundFieldBuilder() {
            if (graphLowerBoundBuilder_ == null) {
              graphLowerBoundBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBound.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.LowerBoundOrBuilder>(
                      getGraphLowerBound(),
                      getParentForChildren(),
                      isClean());
              graphLowerBound_ = null;
            }
            return graphLowerBoundBuilder_;
          }
          @java.lang.Override
          public final Builder setUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.setUnknownFields(unknownFields);
          }

          @java.lang.Override
          public final Builder mergeUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.mergeUnknownFields(unknownFields);
          }


          // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
        }

        // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda.GraphSpec)
        private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec DEFAULT_INSTANCE;
        static {
          DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec();
        }

        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getDefaultInstance() {
          return DEFAULT_INSTANCE;
        }

        private static final com.google.protobuf.Parser<GraphSpec>
            PARSER = new com.google.protobuf.AbstractParser<GraphSpec>() {
          @java.lang.Override
          public GraphSpec parsePartialFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return new GraphSpec(input, extensionRegistry);
          }
        };

        public static com.google.protobuf.Parser<GraphSpec> parser() {
          return PARSER;
        }

        @java.lang.Override
        public com.google.protobuf.Parser<GraphSpec> getParserForType() {
          return PARSER;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getDefaultInstanceForType() {
          return DEFAULT_INSTANCE;
        }

      }

      public static final int GRAPHS_FIELD_NUMBER = 1;
      private boolean graphs_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool graphs
       *&#64;&#64;
       *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
       *&#64;&#64;       them more efficiently. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool graphs = 1;</code>
       * @return The graphs.
       */
      @java.lang.Override
      public boolean getGraphs() {
        return graphs_;
      }

      public static final int BUSY_WAIT_EVENTS_FIELD_NUMBER = 2;
      private boolean busyWaitEvents_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool busy_wait_events
       *&#64;&#64;
       *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
       *&#64;&#64;       latency from event complete to host thread to be notified, with
       *&#64;&#64;       the cost of high CPU load. Default value is false.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool busy_wait_events = 2;</code>
       * @return The busyWaitEvents.
       */
      @java.lang.Override
      public boolean getBusyWaitEvents() {
        return busyWaitEvents_;
      }

      public static final int GRAPH_SPEC_FIELD_NUMBER = 3;
      private java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> graphSpec_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> getGraphSpecList() {
        return graphSpec_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      @java.lang.Override
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder> 
          getGraphSpecOrBuilderList() {
        return graphSpec_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      @java.lang.Override
      public int getGraphSpecCount() {
        return graphSpec_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getGraphSpec(int index) {
        return graphSpec_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
       *&#64;&#64;
       *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
       *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
       *&#64;&#64;       based on model settings.
       *&#64;&#64;       Currently only recognized by TensorRT backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder getGraphSpecOrBuilder(
          int index) {
        return graphSpec_.get(index);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (graphs_ != false) {
          output.writeBool(1, graphs_);
        }
        if (busyWaitEvents_ != false) {
          output.writeBool(2, busyWaitEvents_);
        }
        for (int i = 0; i < graphSpec_.size(); i++) {
          output.writeMessage(3, graphSpec_.get(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (graphs_ != false) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(1, graphs_);
        }
        if (busyWaitEvents_ != false) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(2, busyWaitEvents_);
        }
        for (int i = 0; i < graphSpec_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(3, graphSpec_.get(i));
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda other = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda) obj;

        if (getGraphs()
            != other.getGraphs()) return false;
        if (getBusyWaitEvents()
            != other.getBusyWaitEvents()) return false;
        if (!getGraphSpecList()
            .equals(other.getGraphSpecList())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + GRAPHS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getGraphs());
        hash = (37 * hash) + BUSY_WAIT_EVENTS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getBusyWaitEvents());
        if (getGraphSpecCount() > 0) {
          hash = (37 * hash) + GRAPH_SPEC_FIELD_NUMBER;
          hash = (53 * hash) + getGraphSpecList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.Cuda}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.Cuda)
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
            getGraphSpecFieldBuilder();
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          graphs_ = false;

          busyWaitEvents_ = false;

          if (graphSpecBuilder_ == null) {
            graphSpec_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            graphSpecBuilder_.clear();
          }
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_Cuda_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda build() {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda buildPartial() {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda result = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda(this);
          int from_bitField0_ = bitField0_;
          result.graphs_ = graphs_;
          result.busyWaitEvents_ = busyWaitEvents_;
          if (graphSpecBuilder_ == null) {
            if (((bitField0_ & 0x00000001) != 0)) {
              graphSpec_ = java.util.Collections.unmodifiableList(graphSpec_);
              bitField0_ = (bitField0_ & ~0x00000001);
            }
            result.graphSpec_ = graphSpec_;
          } else {
            result.graphSpec_ = graphSpecBuilder_.build();
          }
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda other) {
          if (other == inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance()) return this;
          if (other.getGraphs() != false) {
            setGraphs(other.getGraphs());
          }
          if (other.getBusyWaitEvents() != false) {
            setBusyWaitEvents(other.getBusyWaitEvents());
          }
          if (graphSpecBuilder_ == null) {
            if (!other.graphSpec_.isEmpty()) {
              if (graphSpec_.isEmpty()) {
                graphSpec_ = other.graphSpec_;
                bitField0_ = (bitField0_ & ~0x00000001);
              } else {
                ensureGraphSpecIsMutable();
                graphSpec_.addAll(other.graphSpec_);
              }
              onChanged();
            }
          } else {
            if (!other.graphSpec_.isEmpty()) {
              if (graphSpecBuilder_.isEmpty()) {
                graphSpecBuilder_.dispose();
                graphSpecBuilder_ = null;
                graphSpec_ = other.graphSpec_;
                bitField0_ = (bitField0_ & ~0x00000001);
                graphSpecBuilder_ = 
                  com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                     getGraphSpecFieldBuilder() : null;
              } else {
                graphSpecBuilder_.addAllMessages(other.graphSpec_);
              }
            }
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private boolean graphs_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         * @return The graphs.
         */
        @java.lang.Override
        public boolean getGraphs() {
          return graphs_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         * @param value The graphs to set.
         * @return This builder for chaining.
         */
        public Builder setGraphs(boolean value) {
          
          graphs_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool graphs
         *&#64;&#64;
         *&#64;&#64;       Use CUDA graphs API to capture model operations and execute
         *&#64;&#64;       them more efficiently. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool graphs = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearGraphs() {
          
          graphs_ = false;
          onChanged();
          return this;
        }

        private boolean busyWaitEvents_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool busy_wait_events
         *&#64;&#64;
         *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
         *&#64;&#64;       latency from event complete to host thread to be notified, with
         *&#64;&#64;       the cost of high CPU load. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool busy_wait_events = 2;</code>
         * @return The busyWaitEvents.
         */
        @java.lang.Override
        public boolean getBusyWaitEvents() {
          return busyWaitEvents_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool busy_wait_events
         *&#64;&#64;
         *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
         *&#64;&#64;       latency from event complete to host thread to be notified, with
         *&#64;&#64;       the cost of high CPU load. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool busy_wait_events = 2;</code>
         * @param value The busyWaitEvents to set.
         * @return This builder for chaining.
         */
        public Builder setBusyWaitEvents(boolean value) {
          
          busyWaitEvents_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool busy_wait_events
         *&#64;&#64;
         *&#64;&#64;       Use busy-waiting to synchronize CUDA events to achieve minimum
         *&#64;&#64;       latency from event complete to host thread to be notified, with
         *&#64;&#64;       the cost of high CPU load. Default value is false.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool busy_wait_events = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearBusyWaitEvents() {
          
          busyWaitEvents_ = false;
          onChanged();
          return this;
        }

        private java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> graphSpec_ =
          java.util.Collections.emptyList();
        private void ensureGraphSpecIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            graphSpec_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec>(graphSpec_);
            bitField0_ |= 0x00000001;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilderV3<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder> graphSpecBuilder_;

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> getGraphSpecList() {
          if (graphSpecBuilder_ == null) {
            return java.util.Collections.unmodifiableList(graphSpec_);
          } else {
            return graphSpecBuilder_.getMessageList();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public int getGraphSpecCount() {
          if (graphSpecBuilder_ == null) {
            return graphSpec_.size();
          } else {
            return graphSpecBuilder_.getCount();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec getGraphSpec(int index) {
          if (graphSpecBuilder_ == null) {
            return graphSpec_.get(index);
          } else {
            return graphSpecBuilder_.getMessage(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder setGraphSpec(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec value) {
          if (graphSpecBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureGraphSpecIsMutable();
            graphSpec_.set(index, value);
            onChanged();
          } else {
            graphSpecBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder setGraphSpec(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder builderForValue) {
          if (graphSpecBuilder_ == null) {
            ensureGraphSpecIsMutable();
            graphSpec_.set(index, builderForValue.build());
            onChanged();
          } else {
            graphSpecBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addGraphSpec(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec value) {
          if (graphSpecBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureGraphSpecIsMutable();
            graphSpec_.add(value);
            onChanged();
          } else {
            graphSpecBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addGraphSpec(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec value) {
          if (graphSpecBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureGraphSpecIsMutable();
            graphSpec_.add(index, value);
            onChanged();
          } else {
            graphSpecBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addGraphSpec(
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder builderForValue) {
          if (graphSpecBuilder_ == null) {
            ensureGraphSpecIsMutable();
            graphSpec_.add(builderForValue.build());
            onChanged();
          } else {
            graphSpecBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addGraphSpec(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder builderForValue) {
          if (graphSpecBuilder_ == null) {
            ensureGraphSpecIsMutable();
            graphSpec_.add(index, builderForValue.build());
            onChanged();
          } else {
            graphSpecBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder addAllGraphSpec(
            java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec> values) {
          if (graphSpecBuilder_ == null) {
            ensureGraphSpecIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, graphSpec_);
            onChanged();
          } else {
            graphSpecBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder clearGraphSpec() {
          if (graphSpecBuilder_ == null) {
            graphSpec_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
            onChanged();
          } else {
            graphSpecBuilder_.clear();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public Builder removeGraphSpec(int index) {
          if (graphSpecBuilder_ == null) {
            ensureGraphSpecIsMutable();
            graphSpec_.remove(index);
            onChanged();
          } else {
            graphSpecBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder getGraphSpecBuilder(
            int index) {
          return getGraphSpecFieldBuilder().getBuilder(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder getGraphSpecOrBuilder(
            int index) {
          if (graphSpecBuilder_ == null) {
            return graphSpec_.get(index);  } else {
            return graphSpecBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder> 
             getGraphSpecOrBuilderList() {
          if (graphSpecBuilder_ != null) {
            return graphSpecBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(graphSpec_);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder addGraphSpecBuilder() {
          return getGraphSpecFieldBuilder().addBuilder(
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder addGraphSpecBuilder(
            int index) {
          return getGraphSpecFieldBuilder().addBuilder(
              index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: GraphSpec graph_spec (repeated)
         *&#64;&#64;
         *&#64;&#64;       Specification of the CUDA graph to be captured. If not specified
         *&#64;&#64;       and 'graphs' is true, the default CUDA graphs will be captured
         *&#64;&#64;       based on model settings.
         *&#64;&#64;       Currently only recognized by TensorRT backend.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.Cuda.GraphSpec graph_spec = 3;</code>
         */
        public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder> 
             getGraphSpecBuilderList() {
          return getGraphSpecFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilderV3<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder> 
            getGraphSpecFieldBuilder() {
          if (graphSpecBuilder_ == null) {
            graphSpecBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpec.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.GraphSpecOrBuilder>(
                    graphSpec_,
                    ((bitField0_ & 0x00000001) != 0),
                    getParentForChildren(),
                    isClean());
            graphSpec_ = null;
          }
          return graphSpecBuilder_;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.Cuda)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.Cuda)
      private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda();
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Cuda>
          PARSER = new com.google.protobuf.AbstractParser<Cuda>() {
        @java.lang.Override
        public Cuda parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Cuda(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Cuda> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Cuda> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface ExecutionAcceleratorsOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.ExecutionAccelerators)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> 
          getGpuExecutionAcceleratorList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getGpuExecutionAccelerator(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      int getGpuExecutionAcceleratorCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getGpuExecutionAcceleratorOrBuilderList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getGpuExecutionAcceleratorOrBuilder(
          int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> 
          getCpuExecutionAcceleratorList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getCpuExecutionAccelerator(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      int getCpuExecutionAcceleratorCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getCpuExecutionAcceleratorOrBuilderList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getCpuExecutionAcceleratorOrBuilder(
          int index);
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message ExecutionAccelerators
     *&#64;&#64;
     *&#64;&#64;     Specify the preferred execution accelerators to be used to execute
     *&#64;&#64;     the model. Currently only recognized by ONNX Runtime backend and
     *&#64;&#64;     TensorFlow backend.
     *&#64;&#64;
     *&#64;&#64;     For ONNX Runtime backend, it will deploy the model with the execution
     *&#64;&#64;     accelerators by priority, the priority is determined based on the
     *&#64;&#64;     order that they are set, i.e. the provider at the front has highest
     *&#64;&#64;     priority. Overall, the priority will be in the following order:
     *&#64;&#64;         &lt;gpu_execution_accelerator&gt; (if instance is on GPU)
     *&#64;&#64;         CUDA Execution Provider     (if instance is on GPU)
     *&#64;&#64;         &lt;cpu_execution_accelerator&gt;
     *&#64;&#64;         Default CPU Execution Provider
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy.ExecutionAccelerators}
     */
    public static final class ExecutionAccelerators extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.ExecutionAccelerators)
        ExecutionAcceleratorsOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use ExecutionAccelerators.newBuilder() to construct.
      private ExecutionAccelerators(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private ExecutionAccelerators() {
        gpuExecutionAccelerator_ = java.util.Collections.emptyList();
        cpuExecutionAccelerator_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new ExecutionAccelerators();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private ExecutionAccelerators(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  gpuExecutionAccelerator_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator>();
                  mutable_bitField0_ |= 0x00000001;
                }
                gpuExecutionAccelerator_.add(
                    input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.parser(), extensionRegistry));
                break;
              }
              case 18: {
                if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                  cpuExecutionAccelerator_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator>();
                  mutable_bitField0_ |= 0x00000002;
                }
                cpuExecutionAccelerator_.add(
                    input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.parser(), extensionRegistry));
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            gpuExecutionAccelerator_ = java.util.Collections.unmodifiableList(gpuExecutionAccelerator_);
          }
          if (((mutable_bitField0_ & 0x00000002) != 0)) {
            cpuExecutionAccelerator_ = java.util.Collections.unmodifiableList(cpuExecutionAccelerator_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder.class);
      }

      public interface AcceleratorOrBuilder extends
          // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
          com.google.protobuf.MessageOrBuilder {

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The name.
         */
        java.lang.String getName();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        com.google.protobuf.ByteString
            getNameBytes();

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        int getParametersCount();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        boolean containsParameters(
            java.lang.String key);
        /**
         * Use {@link #getParametersMap()} instead.
         */
        @java.lang.Deprecated
        java.util.Map<java.lang.String, java.lang.String>
        getParameters();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        java.util.Map<java.lang.String, java.lang.String>
        getParametersMap();
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        java.lang.String getParametersOrDefault(
            java.lang.String key,
            java.lang.String defaultValue);
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        java.lang.String getParametersOrThrow(
            java.lang.String key);
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Accelerator
       *&#64;&#64;
       *&#64;&#64;     Specify the accelerator to be used to execute the model.
       *&#64;&#64;     Accelerator with the same name may accept different parameters
       *&#64;&#64;     depending on the backends.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator}
       */
      public static final class Accelerator extends
          com.google.protobuf.GeneratedMessageV3 implements
          // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
          AcceleratorOrBuilder {
      private static final long serialVersionUID = 0L;
        // Use Accelerator.newBuilder() to construct.
        private Accelerator(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
          super(builder);
        }
        private Accelerator() {
          name_ = "";
        }

        @java.lang.Override
        @SuppressWarnings({"unused"})
        protected java.lang.Object newInstance(
            UnusedPrivateParameter unused) {
          return new Accelerator();
        }

        @java.lang.Override
        public final com.google.protobuf.UnknownFieldSet
        getUnknownFields() {
          return this.unknownFields;
        }
        private Accelerator(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          this();
          if (extensionRegistry == null) {
            throw new java.lang.NullPointerException();
          }
          int mutable_bitField0_ = 0;
          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
              com.google.protobuf.UnknownFieldSet.newBuilder();
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 10: {
                  java.lang.String s = input.readStringRequireUtf8();

                  name_ = s;
                  break;
                }
                case 18: {
                  if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                    parameters_ = com.google.protobuf.MapField.newMapField(
                        ParametersDefaultEntryHolder.defaultEntry);
                    mutable_bitField0_ |= 0x00000001;
                  }
                  com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
                  parameters__ = input.readMessage(
                      ParametersDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                  parameters_.getMutableMap().put(
                      parameters__.getKey(), parameters__.getValue());
                  break;
                }
                default: {
                  if (!parseUnknownField(
                      input, unknownFields, extensionRegistry, tag)) {
                    done = true;
                  }
                  break;
                }
              }
            }
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(this);
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(
                e).setUnfinishedMessage(this);
          } finally {
            this.unknownFields = unknownFields.build();
            makeExtensionsImmutable();
          }
        }
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor;
        }

        @SuppressWarnings({"rawtypes"})
        @java.lang.Override
        protected com.google.protobuf.MapField internalGetMapField(
            int number) {
          switch (number) {
            case 2:
              return internalGetParameters();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder.class);
        }

        public static final int NAME_FIELD_NUMBER = 1;
        private volatile java.lang.Object name_;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The name.
         */
        @java.lang.Override
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (ref instanceof java.lang.String) {
            return (java.lang.String) ref;
          } else {
            com.google.protobuf.ByteString bs = 
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            name_ = s;
            return s;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the execution accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        @java.lang.Override
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof java.lang.String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }

        public static final int PARAMETERS_FIELD_NUMBER = 2;
        private static final class ParametersDefaultEntryHolder {
          static final com.google.protobuf.MapEntry<
              java.lang.String, java.lang.String> defaultEntry =
                  com.google.protobuf.MapEntry
                  .<java.lang.String, java.lang.String>newDefaultInstance(
                      inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_descriptor, 
                      com.google.protobuf.WireFormat.FieldType.STRING,
                      "",
                      com.google.protobuf.WireFormat.FieldType.STRING,
                      "");
        }
        private com.google.protobuf.MapField<
            java.lang.String, java.lang.String> parameters_;
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetParameters() {
          if (parameters_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                ParametersDefaultEntryHolder.defaultEntry);
          }
          return parameters_;
        }

        public int getParametersCount() {
          return internalGetParameters().getMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */

        @java.lang.Override
        public boolean containsParameters(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          return internalGetParameters().getMap().containsKey(key);
        }
        /**
         * Use {@link #getParametersMap()} instead.
         */
        @java.lang.Override
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getParameters() {
          return getParametersMap();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public java.util.Map<java.lang.String, java.lang.String> getParametersMap() {
          return internalGetParameters().getMap();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public java.lang.String getParametersOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetParameters().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
         *&#64;&#64;
         *&#64;&#64;       Additional paremeters used to configure the accelerator.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; parameters = 2;</code>
         */
        @java.lang.Override

        public java.lang.String getParametersOrThrow(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetParameters().getMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }

        private byte memoizedIsInitialized = -1;
        @java.lang.Override
        public final boolean isInitialized() {
          byte isInitialized = memoizedIsInitialized;
          if (isInitialized == 1) return true;
          if (isInitialized == 0) return false;

          memoizedIsInitialized = 1;
          return true;
        }

        @java.lang.Override
        public void writeTo(com.google.protobuf.CodedOutputStream output)
                            throws java.io.IOException {
          if (!getNameBytes().isEmpty()) {
            com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
          }
          com.google.protobuf.GeneratedMessageV3
            .serializeStringMapTo(
              output,
              internalGetParameters(),
              ParametersDefaultEntryHolder.defaultEntry,
              2);
          unknownFields.writeTo(output);
        }

        @java.lang.Override
        public int getSerializedSize() {
          int size = memoizedSize;
          if (size != -1) return size;

          size = 0;
          if (!getNameBytes().isEmpty()) {
            size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
          }
          for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
               : internalGetParameters().getMap().entrySet()) {
            com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
            parameters__ = ParametersDefaultEntryHolder.defaultEntry.newBuilderForType()
                .setKey(entry.getKey())
                .setValue(entry.getValue())
                .build();
            size += com.google.protobuf.CodedOutputStream
                .computeMessageSize(2, parameters__);
          }
          size += unknownFields.getSerializedSize();
          memoizedSize = size;
          return size;
        }

        @java.lang.Override
        public boolean equals(final java.lang.Object obj) {
          if (obj == this) {
           return true;
          }
          if (!(obj instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)) {
            return super.equals(obj);
          }
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator other = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator) obj;

          if (!getName()
              .equals(other.getName())) return false;
          if (!internalGetParameters().equals(
              other.internalGetParameters())) return false;
          if (!unknownFields.equals(other.unknownFields)) return false;
          return true;
        }

        @java.lang.Override
        public int hashCode() {
          if (memoizedHashCode != 0) {
            return memoizedHashCode;
          }
          int hash = 41;
          hash = (19 * hash) + getDescriptor().hashCode();
          hash = (37 * hash) + NAME_FIELD_NUMBER;
          hash = (53 * hash) + getName().hashCode();
          if (!internalGetParameters().getMap().isEmpty()) {
            hash = (37 * hash) + PARAMETERS_FIELD_NUMBER;
            hash = (53 * hash) + internalGetParameters().hashCode();
          }
          hash = (29 * hash) + unknownFields.hashCode();
          memoizedHashCode = hash;
          return hash;
        }

        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            java.nio.ByteBuffer data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            java.nio.ByteBuffer data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.ByteString data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(byte[] data)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return PARSER.parseFrom(data, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseDelimitedFrom(java.io.InputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.CodedInputStream input)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input);
        }
        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          return com.google.protobuf.GeneratedMessageV3
              .parseWithIOException(PARSER, input, extensionRegistry);
        }

        @java.lang.Override
        public Builder newBuilderForType() { return newBuilder(); }
        public static Builder newBuilder() {
          return DEFAULT_INSTANCE.toBuilder();
        }
        public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator prototype) {
          return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
        }
        @java.lang.Override
        public Builder toBuilder() {
          return this == DEFAULT_INSTANCE
              ? new Builder() : new Builder().mergeFrom(this);
        }

        @java.lang.Override
        protected Builder newBuilderForType(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          Builder builder = new Builder(parent);
          return builder;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;  .. cpp:var:: message Accelerator
         *&#64;&#64;
         *&#64;&#64;     Specify the accelerator to be used to execute the model.
         *&#64;&#64;     Accelerator with the same name may accept different parameters
         *&#64;&#64;     depending on the backends.
         *&#64;&#64;
         * </pre>
         *
         * Protobuf type {@code inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator}
         */
        public static final class Builder extends
            com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
            // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder {
          public static final com.google.protobuf.Descriptors.Descriptor
              getDescriptor() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor;
          }

          @SuppressWarnings({"rawtypes"})
          protected com.google.protobuf.MapField internalGetMapField(
              int number) {
            switch (number) {
              case 2:
                return internalGetParameters();
              default:
                throw new RuntimeException(
                    "Invalid map field number: " + number);
            }
          }
          @SuppressWarnings({"rawtypes"})
          protected com.google.protobuf.MapField internalGetMutableMapField(
              int number) {
            switch (number) {
              case 2:
                return internalGetMutableParameters();
              default:
                throw new RuntimeException(
                    "Invalid map field number: " + number);
            }
          }
          @java.lang.Override
          protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
              internalGetFieldAccessorTable() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_fieldAccessorTable
                .ensureFieldAccessorsInitialized(
                    inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder.class);
          }

          // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.newBuilder()
          private Builder() {
            maybeForceBuilderInitialization();
          }

          private Builder(
              com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
            super(parent);
            maybeForceBuilderInitialization();
          }
          private void maybeForceBuilderInitialization() {
            if (com.google.protobuf.GeneratedMessageV3
                    .alwaysUseFieldBuilders) {
            }
          }
          @java.lang.Override
          public Builder clear() {
            super.clear();
            name_ = "";

            internalGetMutableParameters().clear();
            return this;
          }

          @java.lang.Override
          public com.google.protobuf.Descriptors.Descriptor
              getDescriptorForType() {
            return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor;
          }

          @java.lang.Override
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getDefaultInstanceForType() {
            return inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance();
          }

          @java.lang.Override
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator build() {
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator result = buildPartial();
            if (!result.isInitialized()) {
              throw newUninitializedMessageException(result);
            }
            return result;
          }

          @java.lang.Override
          public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator buildPartial() {
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator result = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator(this);
            int from_bitField0_ = bitField0_;
            result.name_ = name_;
            result.parameters_ = internalGetParameters();
            result.parameters_.makeImmutable();
            onBuilt();
            return result;
          }

          @java.lang.Override
          public Builder clone() {
            return super.clone();
          }
          @java.lang.Override
          public Builder setField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.setField(field, value);
          }
          @java.lang.Override
          public Builder clearField(
              com.google.protobuf.Descriptors.FieldDescriptor field) {
            return super.clearField(field);
          }
          @java.lang.Override
          public Builder clearOneof(
              com.google.protobuf.Descriptors.OneofDescriptor oneof) {
            return super.clearOneof(oneof);
          }
          @java.lang.Override
          public Builder setRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              int index, java.lang.Object value) {
            return super.setRepeatedField(field, index, value);
          }
          @java.lang.Override
          public Builder addRepeatedField(
              com.google.protobuf.Descriptors.FieldDescriptor field,
              java.lang.Object value) {
            return super.addRepeatedField(field, value);
          }
          @java.lang.Override
          public Builder mergeFrom(com.google.protobuf.Message other) {
            if (other instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator) {
              return mergeFrom((inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)other);
            } else {
              super.mergeFrom(other);
              return this;
            }
          }

          public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator other) {
            if (other == inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance()) return this;
            if (!other.getName().isEmpty()) {
              name_ = other.name_;
              onChanged();
            }
            internalGetMutableParameters().mergeFrom(
                other.internalGetParameters());
            this.mergeUnknownFields(other.unknownFields);
            onChanged();
            return this;
          }

          @java.lang.Override
          public final boolean isInitialized() {
            return true;
          }

          @java.lang.Override
          public Builder mergeFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws java.io.IOException {
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator parsedMessage = null;
            try {
              parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
            } catch (com.google.protobuf.InvalidProtocolBufferException e) {
              parsedMessage = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator) e.getUnfinishedMessage();
              throw e.unwrapIOException();
            } finally {
              if (parsedMessage != null) {
                mergeFrom(parsedMessage);
              }
            }
            return this;
          }
          private int bitField0_;

          private java.lang.Object name_ = "";
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @return The name.
           */
          public java.lang.String getName() {
            java.lang.Object ref = name_;
            if (!(ref instanceof java.lang.String)) {
              com.google.protobuf.ByteString bs =
                  (com.google.protobuf.ByteString) ref;
              java.lang.String s = bs.toStringUtf8();
              name_ = s;
              return s;
            } else {
              return (java.lang.String) ref;
            }
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @return The bytes for name.
           */
          public com.google.protobuf.ByteString
              getNameBytes() {
            java.lang.Object ref = name_;
            if (ref instanceof String) {
              com.google.protobuf.ByteString b = 
                  com.google.protobuf.ByteString.copyFromUtf8(
                      (java.lang.String) ref);
              name_ = b;
              return b;
            } else {
              return (com.google.protobuf.ByteString) ref;
            }
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @param value The name to set.
           * @return This builder for chaining.
           */
          public Builder setName(
              java.lang.String value) {
            if (value == null) {
    throw new NullPointerException();
  }
  
            name_ = value;
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @return This builder for chaining.
           */
          public Builder clearName() {
            
            name_ = getDefaultInstance().getName();
            onChanged();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: string name
           *&#64;&#64;
           *&#64;&#64;       The name of the execution accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>string name = 1;</code>
           * @param value The bytes for name to set.
           * @return This builder for chaining.
           */
          public Builder setNameBytes(
              com.google.protobuf.ByteString value) {
            if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
            
            name_ = value;
            onChanged();
            return this;
          }

          private com.google.protobuf.MapField<
              java.lang.String, java.lang.String> parameters_;
          private com.google.protobuf.MapField<java.lang.String, java.lang.String>
          internalGetParameters() {
            if (parameters_ == null) {
              return com.google.protobuf.MapField.emptyMapField(
                  ParametersDefaultEntryHolder.defaultEntry);
            }
            return parameters_;
          }
          private com.google.protobuf.MapField<java.lang.String, java.lang.String>
          internalGetMutableParameters() {
            onChanged();;
            if (parameters_ == null) {
              parameters_ = com.google.protobuf.MapField.newMapField(
                  ParametersDefaultEntryHolder.defaultEntry);
            }
            if (!parameters_.isMutable()) {
              parameters_ = parameters_.copy();
            }
            return parameters_;
          }

          public int getParametersCount() {
            return internalGetParameters().getMap().size();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          @java.lang.Override
          public boolean containsParameters(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            return internalGetParameters().getMap().containsKey(key);
          }
          /**
           * Use {@link #getParametersMap()} instead.
           */
          @java.lang.Override
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, java.lang.String> getParameters() {
            return getParametersMap();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          @java.lang.Override

          public java.util.Map<java.lang.String, java.lang.String> getParametersMap() {
            return internalGetParameters().getMap();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          @java.lang.Override

          public java.lang.String getParametersOrDefault(
              java.lang.String key,
              java.lang.String defaultValue) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            java.util.Map<java.lang.String, java.lang.String> map =
                internalGetParameters().getMap();
            return map.containsKey(key) ? map.get(key) : defaultValue;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          @java.lang.Override

          public java.lang.String getParametersOrThrow(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            java.util.Map<java.lang.String, java.lang.String> map =
                internalGetParameters().getMap();
            if (!map.containsKey(key)) {
              throw new java.lang.IllegalArgumentException();
            }
            return map.get(key);
          }

          public Builder clearParameters() {
            internalGetMutableParameters().getMutableMap()
                .clear();
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          public Builder removeParameters(
              java.lang.String key) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            internalGetMutableParameters().getMutableMap()
                .remove(key);
            return this;
          }
          /**
           * Use alternate mutation accessors instead.
           */
          @java.lang.Deprecated
          public java.util.Map<java.lang.String, java.lang.String>
          getMutableParameters() {
            return internalGetMutableParameters().getMutableMap();
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */
          public Builder putParameters(
              java.lang.String key,
              java.lang.String value) {
            if (key == null) { throw new java.lang.NullPointerException(); }
            if (value == null) { throw new java.lang.NullPointerException(); }
            internalGetMutableParameters().getMutableMap()
                .put(key, value);
            return this;
          }
          /**
           * <pre>
           *&#64;&#64;    .. cpp:var:: map&lt;string, string&gt; parameters
           *&#64;&#64;
           *&#64;&#64;       Additional paremeters used to configure the accelerator.
           *&#64;&#64;
           * </pre>
           *
           * <code>map&lt;string, string&gt; parameters = 2;</code>
           */

          public Builder putAllParameters(
              java.util.Map<java.lang.String, java.lang.String> values) {
            internalGetMutableParameters().getMutableMap()
                .putAll(values);
            return this;
          }
          @java.lang.Override
          public final Builder setUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.setUnknownFields(unknownFields);
          }

          @java.lang.Override
          public final Builder mergeUnknownFields(
              final com.google.protobuf.UnknownFieldSet unknownFields) {
            return super.mergeUnknownFields(unknownFields);
          }


          // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
        }

        // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
        private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator DEFAULT_INSTANCE;
        static {
          DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator();
        }

        public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getDefaultInstance() {
          return DEFAULT_INSTANCE;
        }

        private static final com.google.protobuf.Parser<Accelerator>
            PARSER = new com.google.protobuf.AbstractParser<Accelerator>() {
          @java.lang.Override
          public Accelerator parsePartialFrom(
              com.google.protobuf.CodedInputStream input,
              com.google.protobuf.ExtensionRegistryLite extensionRegistry)
              throws com.google.protobuf.InvalidProtocolBufferException {
            return new Accelerator(input, extensionRegistry);
          }
        };

        public static com.google.protobuf.Parser<Accelerator> parser() {
          return PARSER;
        }

        @java.lang.Override
        public com.google.protobuf.Parser<Accelerator> getParserForType() {
          return PARSER;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getDefaultInstanceForType() {
          return DEFAULT_INSTANCE;
        }

      }

      public static final int GPU_EXECUTION_ACCELERATOR_FIELD_NUMBER = 1;
      private java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> gpuExecutionAccelerator_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getGpuExecutionAcceleratorList() {
        return gpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      @java.lang.Override
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getGpuExecutionAcceleratorOrBuilderList() {
        return gpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      @java.lang.Override
      public int getGpuExecutionAcceleratorCount() {
        return gpuExecutionAccelerator_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getGpuExecutionAccelerator(int index) {
        return gpuExecutionAccelerator_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on GPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
       *&#64;&#64;       "auto_mixed_precision", "gpu_io".
       *&#64;&#64;
       *&#64;&#64;       For "tensorrt", the following parameters can be specified:
       *&#64;&#64;         "precision_mode": The precision used for optimization.
       *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
       *&#64;&#64;
       *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
       *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
       *&#64;&#64;
       *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
       *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
       *&#64;&#64;
       *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
       *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
       *&#64;&#64;
       *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
       *&#64;&#64;       the model will try to use FP16 for better performance.
       *&#64;&#64;       This optimization can not be set with "tensorrt".
       *&#64;&#64;
       *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
       *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
       *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
       *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
       *&#64;&#64;       object will be created on model creation and it will request all
       *&#64;&#64;       outputs for every model execution, which may impact the
       *&#64;&#64;       performance if a request does not require all outputs. This
       *&#64;&#64;       optimization will only take affect if the model instance is
       *&#64;&#64;       created with KIND_GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getGpuExecutionAcceleratorOrBuilder(
          int index) {
        return gpuExecutionAccelerator_.get(index);
      }

      public static final int CPU_EXECUTION_ACCELERATOR_FIELD_NUMBER = 2;
      private java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> cpuExecutionAccelerator_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getCpuExecutionAcceleratorList() {
        return cpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      @java.lang.Override
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
          getCpuExecutionAcceleratorOrBuilderList() {
        return cpuExecutionAccelerator_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      @java.lang.Override
      public int getCpuExecutionAcceleratorCount() {
        return cpuExecutionAccelerator_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getCpuExecutionAccelerator(int index) {
        return cpuExecutionAccelerator_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
       *&#64;&#64;
       *&#64;&#64;       The preferred execution provider to be used if the model instance
       *&#64;&#64;       is deployed on CPU.
       *&#64;&#64;
       *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
       *&#64;&#64;       and no parameters are required.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getCpuExecutionAcceleratorOrBuilder(
          int index) {
        return cpuExecutionAccelerator_.get(index);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        for (int i = 0; i < gpuExecutionAccelerator_.size(); i++) {
          output.writeMessage(1, gpuExecutionAccelerator_.get(i));
        }
        for (int i = 0; i < cpuExecutionAccelerator_.size(); i++) {
          output.writeMessage(2, cpuExecutionAccelerator_.get(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        for (int i = 0; i < gpuExecutionAccelerator_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(1, gpuExecutionAccelerator_.get(i));
        }
        for (int i = 0; i < cpuExecutionAccelerator_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, cpuExecutionAccelerator_.get(i));
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators other = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators) obj;

        if (!getGpuExecutionAcceleratorList()
            .equals(other.getGpuExecutionAcceleratorList())) return false;
        if (!getCpuExecutionAcceleratorList()
            .equals(other.getCpuExecutionAcceleratorList())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        if (getGpuExecutionAcceleratorCount() > 0) {
          hash = (37 * hash) + GPU_EXECUTION_ACCELERATOR_FIELD_NUMBER;
          hash = (53 * hash) + getGpuExecutionAcceleratorList().hashCode();
        }
        if (getCpuExecutionAcceleratorCount() > 0) {
          hash = (37 * hash) + CPU_EXECUTION_ACCELERATOR_FIELD_NUMBER;
          hash = (53 * hash) + getCpuExecutionAcceleratorList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message ExecutionAccelerators
       *&#64;&#64;
       *&#64;&#64;     Specify the preferred execution accelerators to be used to execute
       *&#64;&#64;     the model. Currently only recognized by ONNX Runtime backend and
       *&#64;&#64;     TensorFlow backend.
       *&#64;&#64;
       *&#64;&#64;     For ONNX Runtime backend, it will deploy the model with the execution
       *&#64;&#64;     accelerators by priority, the priority is determined based on the
       *&#64;&#64;     order that they are set, i.e. the provider at the front has highest
       *&#64;&#64;     priority. Overall, the priority will be in the following order:
       *&#64;&#64;         &lt;gpu_execution_accelerator&gt; (if instance is on GPU)
       *&#64;&#64;         CUDA Execution Provider     (if instance is on GPU)
       *&#64;&#64;         &lt;cpu_execution_accelerator&gt;
       *&#64;&#64;         Default CPU Execution Provider
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.ExecutionAccelerators}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.ExecutionAccelerators)
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
            getGpuExecutionAcceleratorFieldBuilder();
            getCpuExecutionAcceleratorFieldBuilder();
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          if (gpuExecutionAcceleratorBuilder_ == null) {
            gpuExecutionAccelerator_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            gpuExecutionAcceleratorBuilder_.clear();
          }
          if (cpuExecutionAcceleratorBuilder_ == null) {
            cpuExecutionAccelerator_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000002);
          } else {
            cpuExecutionAcceleratorBuilder_.clear();
          }
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators build() {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators buildPartial() {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators result = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators(this);
          int from_bitField0_ = bitField0_;
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (((bitField0_ & 0x00000001) != 0)) {
              gpuExecutionAccelerator_ = java.util.Collections.unmodifiableList(gpuExecutionAccelerator_);
              bitField0_ = (bitField0_ & ~0x00000001);
            }
            result.gpuExecutionAccelerator_ = gpuExecutionAccelerator_;
          } else {
            result.gpuExecutionAccelerator_ = gpuExecutionAcceleratorBuilder_.build();
          }
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (((bitField0_ & 0x00000002) != 0)) {
              cpuExecutionAccelerator_ = java.util.Collections.unmodifiableList(cpuExecutionAccelerator_);
              bitField0_ = (bitField0_ & ~0x00000002);
            }
            result.cpuExecutionAccelerator_ = cpuExecutionAccelerator_;
          } else {
            result.cpuExecutionAccelerator_ = cpuExecutionAcceleratorBuilder_.build();
          }
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators other) {
          if (other == inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance()) return this;
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (!other.gpuExecutionAccelerator_.isEmpty()) {
              if (gpuExecutionAccelerator_.isEmpty()) {
                gpuExecutionAccelerator_ = other.gpuExecutionAccelerator_;
                bitField0_ = (bitField0_ & ~0x00000001);
              } else {
                ensureGpuExecutionAcceleratorIsMutable();
                gpuExecutionAccelerator_.addAll(other.gpuExecutionAccelerator_);
              }
              onChanged();
            }
          } else {
            if (!other.gpuExecutionAccelerator_.isEmpty()) {
              if (gpuExecutionAcceleratorBuilder_.isEmpty()) {
                gpuExecutionAcceleratorBuilder_.dispose();
                gpuExecutionAcceleratorBuilder_ = null;
                gpuExecutionAccelerator_ = other.gpuExecutionAccelerator_;
                bitField0_ = (bitField0_ & ~0x00000001);
                gpuExecutionAcceleratorBuilder_ = 
                  com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                     getGpuExecutionAcceleratorFieldBuilder() : null;
              } else {
                gpuExecutionAcceleratorBuilder_.addAllMessages(other.gpuExecutionAccelerator_);
              }
            }
          }
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (!other.cpuExecutionAccelerator_.isEmpty()) {
              if (cpuExecutionAccelerator_.isEmpty()) {
                cpuExecutionAccelerator_ = other.cpuExecutionAccelerator_;
                bitField0_ = (bitField0_ & ~0x00000002);
              } else {
                ensureCpuExecutionAcceleratorIsMutable();
                cpuExecutionAccelerator_.addAll(other.cpuExecutionAccelerator_);
              }
              onChanged();
            }
          } else {
            if (!other.cpuExecutionAccelerator_.isEmpty()) {
              if (cpuExecutionAcceleratorBuilder_.isEmpty()) {
                cpuExecutionAcceleratorBuilder_.dispose();
                cpuExecutionAcceleratorBuilder_ = null;
                cpuExecutionAccelerator_ = other.cpuExecutionAccelerator_;
                bitField0_ = (bitField0_ & ~0x00000002);
                cpuExecutionAcceleratorBuilder_ = 
                  com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                     getCpuExecutionAcceleratorFieldBuilder() : null;
              } else {
                cpuExecutionAcceleratorBuilder_.addAllMessages(other.cpuExecutionAccelerator_);
              }
            }
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> gpuExecutionAccelerator_ =
          java.util.Collections.emptyList();
        private void ensureGpuExecutionAcceleratorIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            gpuExecutionAccelerator_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator>(gpuExecutionAccelerator_);
            bitField0_ |= 0x00000001;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilderV3<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> gpuExecutionAcceleratorBuilder_;

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getGpuExecutionAcceleratorList() {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            return java.util.Collections.unmodifiableList(gpuExecutionAccelerator_);
          } else {
            return gpuExecutionAcceleratorBuilder_.getMessageList();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public int getGpuExecutionAcceleratorCount() {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            return gpuExecutionAccelerator_.size();
          } else {
            return gpuExecutionAcceleratorBuilder_.getCount();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getGpuExecutionAccelerator(int index) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            return gpuExecutionAccelerator_.get(index);
          } else {
            return gpuExecutionAcceleratorBuilder_.getMessage(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder setGpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.set(index, value);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder setGpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.set(index, builderForValue.build());
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.add(value);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.add(index, value);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.add(builderForValue.build());
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addGpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.add(index, builderForValue.build());
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder addAllGpuExecutionAccelerator(
            java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> values) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, gpuExecutionAccelerator_);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder clearGpuExecutionAccelerator() {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            gpuExecutionAccelerator_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.clear();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public Builder removeGpuExecutionAccelerator(int index) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            ensureGpuExecutionAcceleratorIsMutable();
            gpuExecutionAccelerator_.remove(index);
            onChanged();
          } else {
            gpuExecutionAcceleratorBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder getGpuExecutionAcceleratorBuilder(
            int index) {
          return getGpuExecutionAcceleratorFieldBuilder().getBuilder(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getGpuExecutionAcceleratorOrBuilder(
            int index) {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            return gpuExecutionAccelerator_.get(index);  } else {
            return gpuExecutionAcceleratorBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
             getGpuExecutionAcceleratorOrBuilderList() {
          if (gpuExecutionAcceleratorBuilder_ != null) {
            return gpuExecutionAcceleratorBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(gpuExecutionAccelerator_);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder addGpuExecutionAcceleratorBuilder() {
          return getGpuExecutionAcceleratorFieldBuilder().addBuilder(
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder addGpuExecutionAcceleratorBuilder(
            int index) {
          return getGpuExecutionAcceleratorFieldBuilder().addBuilder(
              index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator gpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on GPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "tensorrt" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         *&#64;&#64;       For TensorFlow backend, possible values are "tensorrt",
         *&#64;&#64;       "auto_mixed_precision", "gpu_io".
         *&#64;&#64;
         *&#64;&#64;       For "tensorrt", the following parameters can be specified:
         *&#64;&#64;         "precision_mode": The precision used for optimization.
         *&#64;&#64;         Allowed values are "FP32" and "FP16". Default value is "FP32".
         *&#64;&#64;
         *&#64;&#64;         "max_cached_engines": The maximum number of cached TensorRT
         *&#64;&#64;         engines in dynamic TensorRT ops. Default value is 100.
         *&#64;&#64;
         *&#64;&#64;         "minimum_segment_size": The smallest model subgraph that will
         *&#64;&#64;         be considered for optimization by TensorRT. Default value is 3.
         *&#64;&#64;
         *&#64;&#64;         "max_workspace_size_bytes": The maximum GPU memory the model
         *&#64;&#64;         can use temporarily during execution. Default value is 1GB.
         *&#64;&#64;
         *&#64;&#64;       For "auto_mixed_precision", no parameters are required. If set,
         *&#64;&#64;       the model will try to use FP16 for better performance.
         *&#64;&#64;       This optimization can not be set with "tensorrt".
         *&#64;&#64;
         *&#64;&#64;       For "gpu_io", no parameters are required. If set, the model will
         *&#64;&#64;       be executed using TensorFlow Callable API to set input and output
         *&#64;&#64;       tensors in GPU memory if possible, which can reduce data transfer
         *&#64;&#64;       overhead if the model is used in ensemble. However, the Callable
         *&#64;&#64;       object will be created on model creation and it will request all
         *&#64;&#64;       outputs for every model execution, which may impact the
         *&#64;&#64;       performance if a request does not require all outputs. This
         *&#64;&#64;       optimization will only take affect if the model instance is
         *&#64;&#64;       created with KIND_GPU.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator gpu_execution_accelerator = 1;</code>
         */
        public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder> 
             getGpuExecutionAcceleratorBuilderList() {
          return getGpuExecutionAcceleratorFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilderV3<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
            getGpuExecutionAcceleratorFieldBuilder() {
          if (gpuExecutionAcceleratorBuilder_ == null) {
            gpuExecutionAcceleratorBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder>(
                    gpuExecutionAccelerator_,
                    ((bitField0_ & 0x00000001) != 0),
                    getParentForChildren(),
                    isClean());
            gpuExecutionAccelerator_ = null;
          }
          return gpuExecutionAcceleratorBuilder_;
        }

        private java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> cpuExecutionAccelerator_ =
          java.util.Collections.emptyList();
        private void ensureCpuExecutionAcceleratorIsMutable() {
          if (!((bitField0_ & 0x00000002) != 0)) {
            cpuExecutionAccelerator_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator>(cpuExecutionAccelerator_);
            bitField0_ |= 0x00000002;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilderV3<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> cpuExecutionAcceleratorBuilder_;

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> getCpuExecutionAcceleratorList() {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            return java.util.Collections.unmodifiableList(cpuExecutionAccelerator_);
          } else {
            return cpuExecutionAcceleratorBuilder_.getMessageList();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public int getCpuExecutionAcceleratorCount() {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            return cpuExecutionAccelerator_.size();
          } else {
            return cpuExecutionAcceleratorBuilder_.getCount();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator getCpuExecutionAccelerator(int index) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            return cpuExecutionAccelerator_.get(index);
          } else {
            return cpuExecutionAcceleratorBuilder_.getMessage(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder setCpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.set(index, value);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder setCpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.set(index, builderForValue.build());
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.add(value);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator value) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.add(index, value);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.add(builderForValue.build());
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addCpuExecutionAccelerator(
            int index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder builderForValue) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.add(index, builderForValue.build());
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder addAllCpuExecutionAccelerator(
            java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator> values) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, cpuExecutionAccelerator_);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder clearCpuExecutionAccelerator() {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            cpuExecutionAccelerator_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000002);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.clear();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public Builder removeCpuExecutionAccelerator(int index) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            ensureCpuExecutionAcceleratorIsMutable();
            cpuExecutionAccelerator_.remove(index);
            onChanged();
          } else {
            cpuExecutionAcceleratorBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder getCpuExecutionAcceleratorBuilder(
            int index) {
          return getCpuExecutionAcceleratorFieldBuilder().getBuilder(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder getCpuExecutionAcceleratorOrBuilder(
            int index) {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            return cpuExecutionAccelerator_.get(index);  } else {
            return cpuExecutionAcceleratorBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public java.util.List<? extends inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
             getCpuExecutionAcceleratorOrBuilderList() {
          if (cpuExecutionAcceleratorBuilder_ != null) {
            return cpuExecutionAcceleratorBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(cpuExecutionAccelerator_);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder addCpuExecutionAcceleratorBuilder() {
          return getCpuExecutionAcceleratorFieldBuilder().addBuilder(
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder addCpuExecutionAcceleratorBuilder(
            int index) {
          return getCpuExecutionAcceleratorFieldBuilder().addBuilder(
              index, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Accelerator cpu_execution_accelerator (repeated)
         *&#64;&#64;
         *&#64;&#64;       The preferred execution provider to be used if the model instance
         *&#64;&#64;       is deployed on CPU.
         *&#64;&#64;
         *&#64;&#64;       For ONNX Runtime backend, possible value is "openvino" as name,
         *&#64;&#64;       and no parameters are required.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator cpu_execution_accelerator = 2;</code>
         */
        public java.util.List<inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder> 
             getCpuExecutionAcceleratorBuilderList() {
          return getCpuExecutionAcceleratorFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilderV3<
            inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder> 
            getCpuExecutionAcceleratorFieldBuilder() {
          if (cpuExecutionAcceleratorBuilder_ == null) {
            cpuExecutionAcceleratorBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.AcceleratorOrBuilder>(
                    cpuExecutionAccelerator_,
                    ((bitField0_ & 0x00000002) != 0),
                    getParentForChildren(),
                    isClean());
            cpuExecutionAccelerator_ = null;
          }
          return cpuExecutionAcceleratorBuilder_;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.ExecutionAccelerators)
      private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators();
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<ExecutionAccelerators>
          PARSER = new com.google.protobuf.AbstractParser<ExecutionAccelerators>() {
        @java.lang.Override
        public ExecutionAccelerators parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new ExecutionAccelerators(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<ExecutionAccelerators> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<ExecutionAccelerators> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface PinnedMemoryBufferOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool enable
       *&#64;&#64;
       *&#64;&#64;       Use pinned memory buffer. Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool enable = 1;</code>
       * @return The enable.
       */
      boolean getEnable();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message PinnedMemoryBuffer
     *&#64;&#64;
     *&#64;&#64;     Specify whether to use a pinned memory buffer when transferring data
     *&#64;&#64;     between non-pinned system memory and GPU memory. Using a pinned
     *&#64;&#64;     memory buffer for system from/to GPU transfers will typically provide
     *&#64;&#64;     increased performance. For example, in the common use case where the
     *&#64;&#64;     request provides inputs and delivers outputs via non-pinned system
     *&#64;&#64;     memory, if the model instance accepts GPU IOs, the inputs will be
     *&#64;&#64;     processed by two copies: from non-pinned system memory to pinned
     *&#64;&#64;     memory, and from pinned memory to GPU memory. Similarly, pinned
     *&#64;&#64;     memory will be used for delivering the outputs.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy.PinnedMemoryBuffer}
     */
    public static final class PinnedMemoryBuffer extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
        PinnedMemoryBufferOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use PinnedMemoryBuffer.newBuilder() to construct.
      private PinnedMemoryBuffer(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private PinnedMemoryBuffer() {
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new PinnedMemoryBuffer();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private PinnedMemoryBuffer(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                enable_ = input.readBool();
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder.class);
      }

      public static final int ENABLE_FIELD_NUMBER = 1;
      private boolean enable_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: bool enable
       *&#64;&#64;
       *&#64;&#64;       Use pinned memory buffer. Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool enable = 1;</code>
       * @return The enable.
       */
      @java.lang.Override
      public boolean getEnable() {
        return enable_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (enable_ != false) {
          output.writeBool(1, enable_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (enable_ != false) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(1, enable_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer other = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer) obj;

        if (getEnable()
            != other.getEnable()) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + ENABLE_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getEnable());
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message PinnedMemoryBuffer
       *&#64;&#64;
       *&#64;&#64;     Specify whether to use a pinned memory buffer when transferring data
       *&#64;&#64;     between non-pinned system memory and GPU memory. Using a pinned
       *&#64;&#64;     memory buffer for system from/to GPU transfers will typically provide
       *&#64;&#64;     increased performance. For example, in the common use case where the
       *&#64;&#64;     request provides inputs and delivers outputs via non-pinned system
       *&#64;&#64;     memory, if the model instance accepts GPU IOs, the inputs will be
       *&#64;&#64;     processed by two copies: from non-pinned system memory to pinned
       *&#64;&#64;     memory, and from pinned memory to GPU memory. Similarly, pinned
       *&#64;&#64;     memory will be used for delivering the outputs.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelOptimizationPolicy.PinnedMemoryBuffer}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          enable_ = false;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer build() {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer buildPartial() {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer result = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer(this);
          result.enable_ = enable_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer other) {
          if (other == inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance()) return this;
          if (other.getEnable() != false) {
            setEnable(other.getEnable());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private boolean enable_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool enable
         *&#64;&#64;
         *&#64;&#64;       Use pinned memory buffer. Default is true.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool enable = 1;</code>
         * @return The enable.
         */
        @java.lang.Override
        public boolean getEnable() {
          return enable_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool enable
         *&#64;&#64;
         *&#64;&#64;       Use pinned memory buffer. Default is true.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool enable = 1;</code>
         * @param value The enable to set.
         * @return This builder for chaining.
         */
        public Builder setEnable(boolean value) {
          
          enable_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: bool enable
         *&#64;&#64;
         *&#64;&#64;       Use pinned memory buffer. Default is true.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool enable = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearEnable() {
          
          enable_ = false;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy.PinnedMemoryBuffer)
      private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer();
      }

      public static inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<PinnedMemoryBuffer>
          PARSER = new com.google.protobuf.AbstractParser<PinnedMemoryBuffer>() {
        @java.lang.Override
        public PinnedMemoryBuffer parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new PinnedMemoryBuffer(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<PinnedMemoryBuffer> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<PinnedMemoryBuffer> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int GRAPH_FIELD_NUMBER = 1;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph graph_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @return Whether the graph field is set.
     */
    @java.lang.Override
    public boolean hasGraph() {
      return graph_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     * @return The graph.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph() {
      return graph_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Graph graph
     *&#64;&#64;
     *&#64;&#64;     The graph optimization setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder getGraphOrBuilder() {
      return getGraph();
    }

    public static final int PRIORITY_FIELD_NUMBER = 2;
    private int priority_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return The enum numeric value on the wire for priority.
     */
    @java.lang.Override public int getPriorityValue() {
      return priority_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelPriority priority
     *&#64;&#64;
     *&#64;&#64;     The priority setting for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
     * @return The priority.
     */
    @java.lang.Override public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority() {
      @SuppressWarnings("deprecation")
      inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority result = inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.valueOf(priority_);
      return result == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.UNRECOGNIZED : result;
    }

    public static final int CUDA_FIELD_NUMBER = 3;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda cuda_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @return Whether the cuda field is set.
     */
    @java.lang.Override
    public boolean hasCuda() {
      return cuda_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     * @return The cuda.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda() {
      return cuda_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Cuda cuda
     *&#64;&#64;
     *&#64;&#64;     CUDA-specific optimization settings. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder getCudaOrBuilder() {
      return getCuda();
    }

    public static final int EXECUTION_ACCELERATORS_FIELD_NUMBER = 4;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators executionAccelerators_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @return Whether the executionAccelerators field is set.
     */
    @java.lang.Override
    public boolean hasExecutionAccelerators() {
      return executionAccelerators_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     * @return The executionAccelerators.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getExecutionAccelerators() {
      return executionAccelerators_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance() : executionAccelerators_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
     *&#64;&#64;
     *&#64;&#64;     The accelerators used for the model. Optional.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder getExecutionAcceleratorsOrBuilder() {
      return getExecutionAccelerators();
    }

    public static final int INPUT_PINNED_MEMORY_FIELD_NUMBER = 5;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer inputPinnedMemory_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @return Whether the inputPinnedMemory field is set.
     */
    @java.lang.Override
    public boolean hasInputPinnedMemory() {
      return inputPinnedMemory_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     * @return The inputPinnedMemory.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getInputPinnedMemory() {
      return inputPinnedMemory_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance() : inputPinnedMemory_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder getInputPinnedMemoryOrBuilder() {
      return getInputPinnedMemory();
    }

    public static final int OUTPUT_PINNED_MEMORY_FIELD_NUMBER = 6;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer outputPinnedMemory_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @return Whether the outputPinnedMemory field is set.
     */
    @java.lang.Override
    public boolean hasOutputPinnedMemory() {
      return outputPinnedMemory_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     * @return The outputPinnedMemory.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getOutputPinnedMemory() {
      return outputPinnedMemory_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance() : outputPinnedMemory_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
     *&#64;&#64;
     *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
     *&#64;&#64;     is between GPU memory and non-pinned system memory.
     *&#64;&#64;     Default is true.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder getOutputPinnedMemoryOrBuilder() {
      return getOutputPinnedMemory();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (graph_ != null) {
        output.writeMessage(1, getGraph());
      }
      if (priority_ != inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.PRIORITY_DEFAULT.getNumber()) {
        output.writeEnum(2, priority_);
      }
      if (cuda_ != null) {
        output.writeMessage(3, getCuda());
      }
      if (executionAccelerators_ != null) {
        output.writeMessage(4, getExecutionAccelerators());
      }
      if (inputPinnedMemory_ != null) {
        output.writeMessage(5, getInputPinnedMemory());
      }
      if (outputPinnedMemory_ != null) {
        output.writeMessage(6, getOutputPinnedMemory());
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (graph_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getGraph());
      }
      if (priority_ != inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.PRIORITY_DEFAULT.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, priority_);
      }
      if (cuda_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getCuda());
      }
      if (executionAccelerators_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, getExecutionAccelerators());
      }
      if (inputPinnedMemory_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, getInputPinnedMemory());
      }
      if (outputPinnedMemory_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, getOutputPinnedMemory());
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelOptimizationPolicy other = (inference.ModelConfigOuterClass.ModelOptimizationPolicy) obj;

      if (hasGraph() != other.hasGraph()) return false;
      if (hasGraph()) {
        if (!getGraph()
            .equals(other.getGraph())) return false;
      }
      if (priority_ != other.priority_) return false;
      if (hasCuda() != other.hasCuda()) return false;
      if (hasCuda()) {
        if (!getCuda()
            .equals(other.getCuda())) return false;
      }
      if (hasExecutionAccelerators() != other.hasExecutionAccelerators()) return false;
      if (hasExecutionAccelerators()) {
        if (!getExecutionAccelerators()
            .equals(other.getExecutionAccelerators())) return false;
      }
      if (hasInputPinnedMemory() != other.hasInputPinnedMemory()) return false;
      if (hasInputPinnedMemory()) {
        if (!getInputPinnedMemory()
            .equals(other.getInputPinnedMemory())) return false;
      }
      if (hasOutputPinnedMemory() != other.hasOutputPinnedMemory()) return false;
      if (hasOutputPinnedMemory()) {
        if (!getOutputPinnedMemory()
            .equals(other.getOutputPinnedMemory())) return false;
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasGraph()) {
        hash = (37 * hash) + GRAPH_FIELD_NUMBER;
        hash = (53 * hash) + getGraph().hashCode();
      }
      hash = (37 * hash) + PRIORITY_FIELD_NUMBER;
      hash = (53 * hash) + priority_;
      if (hasCuda()) {
        hash = (37 * hash) + CUDA_FIELD_NUMBER;
        hash = (53 * hash) + getCuda().hashCode();
      }
      if (hasExecutionAccelerators()) {
        hash = (37 * hash) + EXECUTION_ACCELERATORS_FIELD_NUMBER;
        hash = (53 * hash) + getExecutionAccelerators().hashCode();
      }
      if (hasInputPinnedMemory()) {
        hash = (37 * hash) + INPUT_PINNED_MEMORY_FIELD_NUMBER;
        hash = (53 * hash) + getInputPinnedMemory().hashCode();
      }
      if (hasOutputPinnedMemory()) {
        hash = (37 * hash) + OUTPUT_PINNED_MEMORY_FIELD_NUMBER;
        hash = (53 * hash) + getOutputPinnedMemory().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOptimizationPolicy prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelOptimizationPolicy
     *&#64;&#64;
     *&#64;&#64;   Optimization settings for a model. These settings control if/how a
     *&#64;&#64;   model is optimized and prioritized by the backend framework when
     *&#64;&#64;   it is loaded.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOptimizationPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelOptimizationPolicy)
        inference.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelOptimizationPolicy.class, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelOptimizationPolicy.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (graphBuilder_ == null) {
          graph_ = null;
        } else {
          graph_ = null;
          graphBuilder_ = null;
        }
        priority_ = 0;

        if (cudaBuilder_ == null) {
          cuda_ = null;
        } else {
          cuda_ = null;
          cudaBuilder_ = null;
        }
        if (executionAcceleratorsBuilder_ == null) {
          executionAccelerators_ = null;
        } else {
          executionAccelerators_ = null;
          executionAcceleratorsBuilder_ = null;
        }
        if (inputPinnedMemoryBuilder_ == null) {
          inputPinnedMemory_ = null;
        } else {
          inputPinnedMemory_ = null;
          inputPinnedMemoryBuilder_ = null;
        }
        if (outputPinnedMemoryBuilder_ == null) {
          outputPinnedMemory_ = null;
        } else {
          outputPinnedMemory_ = null;
          outputPinnedMemoryBuilder_ = null;
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOptimizationPolicy_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy build() {
        inference.ModelConfigOuterClass.ModelOptimizationPolicy result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy buildPartial() {
        inference.ModelConfigOuterClass.ModelOptimizationPolicy result = new inference.ModelConfigOuterClass.ModelOptimizationPolicy(this);
        if (graphBuilder_ == null) {
          result.graph_ = graph_;
        } else {
          result.graph_ = graphBuilder_.build();
        }
        result.priority_ = priority_;
        if (cudaBuilder_ == null) {
          result.cuda_ = cuda_;
        } else {
          result.cuda_ = cudaBuilder_.build();
        }
        if (executionAcceleratorsBuilder_ == null) {
          result.executionAccelerators_ = executionAccelerators_;
        } else {
          result.executionAccelerators_ = executionAcceleratorsBuilder_.build();
        }
        if (inputPinnedMemoryBuilder_ == null) {
          result.inputPinnedMemory_ = inputPinnedMemory_;
        } else {
          result.inputPinnedMemory_ = inputPinnedMemoryBuilder_.build();
        }
        if (outputPinnedMemoryBuilder_ == null) {
          result.outputPinnedMemory_ = outputPinnedMemory_;
        } else {
          result.outputPinnedMemory_ = outputPinnedMemoryBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelOptimizationPolicy) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelOptimizationPolicy)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOptimizationPolicy other) {
        if (other == inference.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance()) return this;
        if (other.hasGraph()) {
          mergeGraph(other.getGraph());
        }
        if (other.priority_ != 0) {
          setPriorityValue(other.getPriorityValue());
        }
        if (other.hasCuda()) {
          mergeCuda(other.getCuda());
        }
        if (other.hasExecutionAccelerators()) {
          mergeExecutionAccelerators(other.getExecutionAccelerators());
        }
        if (other.hasInputPinnedMemory()) {
          mergeInputPinnedMemory(other.getInputPinnedMemory());
        }
        if (other.hasOutputPinnedMemory()) {
          mergeOutputPinnedMemory(other.getOutputPinnedMemory());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelOptimizationPolicy parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelOptimizationPolicy) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph graph_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder> graphBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       * @return Whether the graph field is set.
       */
      public boolean hasGraph() {
        return graphBuilder_ != null || graph_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       * @return The graph.
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph getGraph() {
        if (graphBuilder_ == null) {
          return graph_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
        } else {
          return graphBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder setGraph(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
        if (graphBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          graph_ = value;
          onChanged();
        } else {
          graphBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder setGraph(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder builderForValue) {
        if (graphBuilder_ == null) {
          graph_ = builderForValue.build();
          onChanged();
        } else {
          graphBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder mergeGraph(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph value) {
        if (graphBuilder_ == null) {
          if (graph_ != null) {
            graph_ =
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.newBuilder(graph_).mergeFrom(value).buildPartial();
          } else {
            graph_ = value;
          }
          onChanged();
        } else {
          graphBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public Builder clearGraph() {
        if (graphBuilder_ == null) {
          graph_ = null;
          onChanged();
        } else {
          graph_ = null;
          graphBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder getGraphBuilder() {
        
        onChanged();
        return getGraphFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder getGraphOrBuilder() {
        if (graphBuilder_ != null) {
          return graphBuilder_.getMessageOrBuilder();
        } else {
          return graph_ == null ?
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.getDefaultInstance() : graph_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Graph graph
       *&#64;&#64;
       *&#64;&#64;     The graph optimization setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Graph graph = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder> 
          getGraphFieldBuilder() {
        if (graphBuilder_ == null) {
          graphBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Graph.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.GraphOrBuilder>(
                  getGraph(),
                  getParentForChildren(),
                  isClean());
          graph_ = null;
        }
        return graphBuilder_;
      }

      private int priority_ = 0;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @return The enum numeric value on the wire for priority.
       */
      @java.lang.Override public int getPriorityValue() {
        return priority_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @param value The enum numeric value on the wire for priority to set.
       * @return This builder for chaining.
       */
      public Builder setPriorityValue(int value) {
        
        priority_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @return The priority.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority getPriority() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority result = inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.valueOf(priority_);
        return result == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @param value The priority to set.
       * @return This builder for chaining.
       */
      public Builder setPriority(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ModelPriority value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        priority_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelPriority priority
       *&#64;&#64;
       *&#64;&#64;     The priority setting for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ModelPriority priority = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearPriority() {
        
        priority_ = 0;
        onChanged();
        return this;
      }

      private inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda cuda_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder> cudaBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       * @return Whether the cuda field is set.
       */
      public boolean hasCuda() {
        return cudaBuilder_ != null || cuda_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       * @return The cuda.
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda getCuda() {
        if (cudaBuilder_ == null) {
          return cuda_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
        } else {
          return cudaBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder setCuda(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
        if (cudaBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          cuda_ = value;
          onChanged();
        } else {
          cudaBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder setCuda(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder builderForValue) {
        if (cudaBuilder_ == null) {
          cuda_ = builderForValue.build();
          onChanged();
        } else {
          cudaBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder mergeCuda(inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda value) {
        if (cudaBuilder_ == null) {
          if (cuda_ != null) {
            cuda_ =
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.newBuilder(cuda_).mergeFrom(value).buildPartial();
          } else {
            cuda_ = value;
          }
          onChanged();
        } else {
          cudaBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public Builder clearCuda() {
        if (cudaBuilder_ == null) {
          cuda_ = null;
          onChanged();
        } else {
          cuda_ = null;
          cudaBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder getCudaBuilder() {
        
        onChanged();
        return getCudaFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder getCudaOrBuilder() {
        if (cudaBuilder_ != null) {
          return cudaBuilder_.getMessageOrBuilder();
        } else {
          return cuda_ == null ?
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.getDefaultInstance() : cuda_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Cuda cuda
       *&#64;&#64;
       *&#64;&#64;     CUDA-specific optimization settings. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.Cuda cuda = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder> 
          getCudaFieldBuilder() {
        if (cudaBuilder_ == null) {
          cudaBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Cuda.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.CudaOrBuilder>(
                  getCuda(),
                  getParentForChildren(),
                  isClean());
          cuda_ = null;
        }
        return cudaBuilder_;
      }

      private inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators executionAccelerators_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder> executionAcceleratorsBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       * @return Whether the executionAccelerators field is set.
       */
      public boolean hasExecutionAccelerators() {
        return executionAcceleratorsBuilder_ != null || executionAccelerators_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       * @return The executionAccelerators.
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators getExecutionAccelerators() {
        if (executionAcceleratorsBuilder_ == null) {
          return executionAccelerators_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance() : executionAccelerators_;
        } else {
          return executionAcceleratorsBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder setExecutionAccelerators(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators value) {
        if (executionAcceleratorsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          executionAccelerators_ = value;
          onChanged();
        } else {
          executionAcceleratorsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder setExecutionAccelerators(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder builderForValue) {
        if (executionAcceleratorsBuilder_ == null) {
          executionAccelerators_ = builderForValue.build();
          onChanged();
        } else {
          executionAcceleratorsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder mergeExecutionAccelerators(inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators value) {
        if (executionAcceleratorsBuilder_ == null) {
          if (executionAccelerators_ != null) {
            executionAccelerators_ =
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.newBuilder(executionAccelerators_).mergeFrom(value).buildPartial();
          } else {
            executionAccelerators_ = value;
          }
          onChanged();
        } else {
          executionAcceleratorsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public Builder clearExecutionAccelerators() {
        if (executionAcceleratorsBuilder_ == null) {
          executionAccelerators_ = null;
          onChanged();
        } else {
          executionAccelerators_ = null;
          executionAcceleratorsBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder getExecutionAcceleratorsBuilder() {
        
        onChanged();
        return getExecutionAcceleratorsFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder getExecutionAcceleratorsOrBuilder() {
        if (executionAcceleratorsBuilder_ != null) {
          return executionAcceleratorsBuilder_.getMessageOrBuilder();
        } else {
          return executionAccelerators_ == null ?
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.getDefaultInstance() : executionAccelerators_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ExecutionAccelerators execution_accelerators
       *&#64;&#64;
       *&#64;&#64;     The accelerators used for the model. Optional.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.ExecutionAccelerators execution_accelerators = 4;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder> 
          getExecutionAcceleratorsFieldBuilder() {
        if (executionAcceleratorsBuilder_ == null) {
          executionAcceleratorsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAccelerators.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.ExecutionAcceleratorsOrBuilder>(
                  getExecutionAccelerators(),
                  getParentForChildren(),
                  isClean());
          executionAccelerators_ = null;
        }
        return executionAcceleratorsBuilder_;
      }

      private inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer inputPinnedMemory_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder> inputPinnedMemoryBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       * @return Whether the inputPinnedMemory field is set.
       */
      public boolean hasInputPinnedMemory() {
        return inputPinnedMemoryBuilder_ != null || inputPinnedMemory_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       * @return The inputPinnedMemory.
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getInputPinnedMemory() {
        if (inputPinnedMemoryBuilder_ == null) {
          return inputPinnedMemory_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance() : inputPinnedMemory_;
        } else {
          return inputPinnedMemoryBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public Builder setInputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
        if (inputPinnedMemoryBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          inputPinnedMemory_ = value;
          onChanged();
        } else {
          inputPinnedMemoryBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public Builder setInputPinnedMemory(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder builderForValue) {
        if (inputPinnedMemoryBuilder_ == null) {
          inputPinnedMemory_ = builderForValue.build();
          onChanged();
        } else {
          inputPinnedMemoryBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public Builder mergeInputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
        if (inputPinnedMemoryBuilder_ == null) {
          if (inputPinnedMemory_ != null) {
            inputPinnedMemory_ =
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.newBuilder(inputPinnedMemory_).mergeFrom(value).buildPartial();
          } else {
            inputPinnedMemory_ = value;
          }
          onChanged();
        } else {
          inputPinnedMemoryBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public Builder clearInputPinnedMemory() {
        if (inputPinnedMemoryBuilder_ == null) {
          inputPinnedMemory_ = null;
          onChanged();
        } else {
          inputPinnedMemory_ = null;
          inputPinnedMemoryBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder getInputPinnedMemoryBuilder() {
        
        onChanged();
        return getInputPinnedMemoryFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder getInputPinnedMemoryOrBuilder() {
        if (inputPinnedMemoryBuilder_ != null) {
          return inputPinnedMemoryBuilder_.getMessageOrBuilder();
        } else {
          return inputPinnedMemory_ == null ?
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance() : inputPinnedMemory_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer input_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for inputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer input_pinned_memory = 5;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder> 
          getInputPinnedMemoryFieldBuilder() {
        if (inputPinnedMemoryBuilder_ == null) {
          inputPinnedMemoryBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder>(
                  getInputPinnedMemory(),
                  getParentForChildren(),
                  isClean());
          inputPinnedMemory_ = null;
        }
        return inputPinnedMemoryBuilder_;
      }

      private inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer outputPinnedMemory_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder> outputPinnedMemoryBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       * @return Whether the outputPinnedMemory field is set.
       */
      public boolean hasOutputPinnedMemory() {
        return outputPinnedMemoryBuilder_ != null || outputPinnedMemory_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       * @return The outputPinnedMemory.
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer getOutputPinnedMemory() {
        if (outputPinnedMemoryBuilder_ == null) {
          return outputPinnedMemory_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance() : outputPinnedMemory_;
        } else {
          return outputPinnedMemoryBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public Builder setOutputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
        if (outputPinnedMemoryBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          outputPinnedMemory_ = value;
          onChanged();
        } else {
          outputPinnedMemoryBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public Builder setOutputPinnedMemory(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder builderForValue) {
        if (outputPinnedMemoryBuilder_ == null) {
          outputPinnedMemory_ = builderForValue.build();
          onChanged();
        } else {
          outputPinnedMemoryBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public Builder mergeOutputPinnedMemory(inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer value) {
        if (outputPinnedMemoryBuilder_ == null) {
          if (outputPinnedMemory_ != null) {
            outputPinnedMemory_ =
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.newBuilder(outputPinnedMemory_).mergeFrom(value).buildPartial();
          } else {
            outputPinnedMemory_ = value;
          }
          onChanged();
        } else {
          outputPinnedMemoryBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public Builder clearOutputPinnedMemory() {
        if (outputPinnedMemoryBuilder_ == null) {
          outputPinnedMemory_ = null;
          onChanged();
        } else {
          outputPinnedMemory_ = null;
          outputPinnedMemoryBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder getOutputPinnedMemoryBuilder() {
        
        onChanged();
        return getOutputPinnedMemoryFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder getOutputPinnedMemoryOrBuilder() {
        if (outputPinnedMemoryBuilder_ != null) {
          return outputPinnedMemoryBuilder_.getMessageOrBuilder();
        } else {
          return outputPinnedMemory_ == null ?
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.getDefaultInstance() : outputPinnedMemory_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: PinnedMemoryBuffer output_pinned_memory
       *&#64;&#64;
       *&#64;&#64;     Use pinned memory buffer when the data transfer for outputs
       *&#64;&#64;     is between GPU memory and non-pinned system memory.
       *&#64;&#64;     Default is true.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy.PinnedMemoryBuffer output_pinned_memory = 6;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder> 
          getOutputPinnedMemoryFieldBuilder() {
        if (outputPinnedMemoryBuilder_ == null) {
          outputPinnedMemoryBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBuffer.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicy.PinnedMemoryBufferOrBuilder>(
                  getOutputPinnedMemory(),
                  getParentForChildren(),
                  isClean());
          outputPinnedMemory_ = null;
        }
        return outputPinnedMemoryBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelOptimizationPolicy)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelOptimizationPolicy)
    private static final inference.ModelConfigOuterClass.ModelOptimizationPolicy DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOptimizationPolicy();
    }

    public static inference.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelOptimizationPolicy>
        PARSER = new com.google.protobuf.AbstractParser<ModelOptimizationPolicy>() {
      @java.lang.Override
      public ModelOptimizationPolicy parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelOptimizationPolicy(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelOptimizationPolicy> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelOptimizationPolicy> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelQueuePolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelQueuePolicy)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @return The enum numeric value on the wire for timeoutAction.
     */
    int getTimeoutActionValue();
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @return The timeoutAction.
     */
    inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction getTimeoutAction();

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
     *&#64;&#64;
     *&#64;&#64;     The default timeout for every request, in microseconds.
     *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 default_timeout_microseconds = 2;</code>
     * @return The defaultTimeoutMicroseconds.
     */
    long getDefaultTimeoutMicroseconds();

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
     *&#64;&#64;
     *&#64;&#64;     Whether individual request can override the default timeout value.
     *&#64;&#64;     When true, individual requests can set a timeout that is less than
     *&#64;&#64;     the default timeout value but may not increase the timeout.
     *&#64;&#64;     The default value is false.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_timeout_override = 3;</code>
     * @return The allowTimeoutOverride.
     */
    boolean getAllowTimeoutOverride();

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
     *&#64;&#64;
     *&#64;&#64;     The maximum queue size for holding requests. A request will be
     *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
     *&#64;&#64;     full. The default value is 0 which indicates that no maximum
     *&#64;&#64;     queue size is enforced.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 max_queue_size = 4;</code>
     * @return The maxQueueSize.
     */
    int getMaxQueueSize();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelQueuePolicy
   *&#64;&#64;
   *&#64;&#64;   Queue policy for inference requests.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelQueuePolicy}
   */
  public static final class ModelQueuePolicy extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelQueuePolicy)
      ModelQueuePolicyOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelQueuePolicy.newBuilder() to construct.
    private ModelQueuePolicy(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelQueuePolicy() {
      timeoutAction_ = 0;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelQueuePolicy();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelQueuePolicy(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              int rawValue = input.readEnum();

              timeoutAction_ = rawValue;
              break;
            }
            case 16: {

              defaultTimeoutMicroseconds_ = input.readUInt64();
              break;
            }
            case 24: {

              allowTimeoutOverride_ = input.readBool();
              break;
            }
            case 32: {

              maxQueueSize_ = input.readUInt32();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelQueuePolicy_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelQueuePolicy_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelQueuePolicy.class, inference.ModelConfigOuterClass.ModelQueuePolicy.Builder.class);
    }

    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:enum:: TimeoutAction
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf enum {@code inference.ModelQueuePolicy.TimeoutAction}
     */
    public enum TimeoutAction
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Action::REJECT = 0
       *&#64;&#64;
       *&#64;&#64;       Reject the request and return error message accordingly.
       *&#64;&#64;
       * </pre>
       *
       * <code>REJECT = 0;</code>
       */
      REJECT(0),
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Action::DELAY = 1
       *&#64;&#64;
       *&#64;&#64;       Delay the request until all other requests at the same
       *&#64;&#64;       (or higher) priority levels that have not reached their timeouts
       *&#64;&#64;       are processed. A delayed request will eventually be processed,
       *&#64;&#64;       but may be delayed indefinitely due to newly arriving requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>DELAY = 1;</code>
       */
      DELAY(1),
      UNRECOGNIZED(-1),
      ;

      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Action::REJECT = 0
       *&#64;&#64;
       *&#64;&#64;       Reject the request and return error message accordingly.
       *&#64;&#64;
       * </pre>
       *
       * <code>REJECT = 0;</code>
       */
      public static final int REJECT_VALUE = 0;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:enumerator:: Action::DELAY = 1
       *&#64;&#64;
       *&#64;&#64;       Delay the request until all other requests at the same
       *&#64;&#64;       (or higher) priority levels that have not reached their timeouts
       *&#64;&#64;       are processed. A delayed request will eventually be processed,
       *&#64;&#64;       but may be delayed indefinitely due to newly arriving requests.
       *&#64;&#64;
       * </pre>
       *
       * <code>DELAY = 1;</code>
       */
      public static final int DELAY_VALUE = 1;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static TimeoutAction valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static TimeoutAction forNumber(int value) {
        switch (value) {
          case 0: return REJECT;
          case 1: return DELAY;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<TimeoutAction>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          TimeoutAction> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<TimeoutAction>() {
              public TimeoutAction findValueByNumber(int number) {
                return TimeoutAction.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        if (this == UNRECOGNIZED) {
          throw new java.lang.IllegalStateException(
              "Can't get the descriptor of an unrecognized enum value.");
        }
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.ModelQueuePolicy.getDescriptor().getEnumTypes().get(0);
      }

      private static final TimeoutAction[] VALUES = values();

      public static TimeoutAction valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private TimeoutAction(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:inference.ModelQueuePolicy.TimeoutAction)
    }

    public static final int TIMEOUT_ACTION_FIELD_NUMBER = 1;
    private int timeoutAction_;
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @return The enum numeric value on the wire for timeoutAction.
     */
    @java.lang.Override public int getTimeoutActionValue() {
      return timeoutAction_;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
     *&#64;&#64;
     *&#64;&#64;     The action applied to timed-out request.
     *&#64;&#64;     The default action is REJECT.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
     * @return The timeoutAction.
     */
    @java.lang.Override public inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction getTimeoutAction() {
      @SuppressWarnings("deprecation")
      inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction result = inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction.valueOf(timeoutAction_);
      return result == null ? inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction.UNRECOGNIZED : result;
    }

    public static final int DEFAULT_TIMEOUT_MICROSECONDS_FIELD_NUMBER = 2;
    private long defaultTimeoutMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
     *&#64;&#64;
     *&#64;&#64;     The default timeout for every request, in microseconds.
     *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 default_timeout_microseconds = 2;</code>
     * @return The defaultTimeoutMicroseconds.
     */
    @java.lang.Override
    public long getDefaultTimeoutMicroseconds() {
      return defaultTimeoutMicroseconds_;
    }

    public static final int ALLOW_TIMEOUT_OVERRIDE_FIELD_NUMBER = 3;
    private boolean allowTimeoutOverride_;
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
     *&#64;&#64;
     *&#64;&#64;     Whether individual request can override the default timeout value.
     *&#64;&#64;     When true, individual requests can set a timeout that is less than
     *&#64;&#64;     the default timeout value but may not increase the timeout.
     *&#64;&#64;     The default value is false.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool allow_timeout_override = 3;</code>
     * @return The allowTimeoutOverride.
     */
    @java.lang.Override
    public boolean getAllowTimeoutOverride() {
      return allowTimeoutOverride_;
    }

    public static final int MAX_QUEUE_SIZE_FIELD_NUMBER = 4;
    private int maxQueueSize_;
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
     *&#64;&#64;
     *&#64;&#64;     The maximum queue size for holding requests. A request will be
     *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
     *&#64;&#64;     full. The default value is 0 which indicates that no maximum
     *&#64;&#64;     queue size is enforced.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 max_queue_size = 4;</code>
     * @return The maxQueueSize.
     */
    @java.lang.Override
    public int getMaxQueueSize() {
      return maxQueueSize_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (timeoutAction_ != inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction.REJECT.getNumber()) {
        output.writeEnum(1, timeoutAction_);
      }
      if (defaultTimeoutMicroseconds_ != 0L) {
        output.writeUInt64(2, defaultTimeoutMicroseconds_);
      }
      if (allowTimeoutOverride_ != false) {
        output.writeBool(3, allowTimeoutOverride_);
      }
      if (maxQueueSize_ != 0) {
        output.writeUInt32(4, maxQueueSize_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (timeoutAction_ != inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction.REJECT.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, timeoutAction_);
      }
      if (defaultTimeoutMicroseconds_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, defaultTimeoutMicroseconds_);
      }
      if (allowTimeoutOverride_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, allowTimeoutOverride_);
      }
      if (maxQueueSize_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(4, maxQueueSize_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelQueuePolicy)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelQueuePolicy other = (inference.ModelConfigOuterClass.ModelQueuePolicy) obj;

      if (timeoutAction_ != other.timeoutAction_) return false;
      if (getDefaultTimeoutMicroseconds()
          != other.getDefaultTimeoutMicroseconds()) return false;
      if (getAllowTimeoutOverride()
          != other.getAllowTimeoutOverride()) return false;
      if (getMaxQueueSize()
          != other.getMaxQueueSize()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + TIMEOUT_ACTION_FIELD_NUMBER;
      hash = (53 * hash) + timeoutAction_;
      hash = (37 * hash) + DEFAULT_TIMEOUT_MICROSECONDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getDefaultTimeoutMicroseconds());
      hash = (37 * hash) + ALLOW_TIMEOUT_OVERRIDE_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getAllowTimeoutOverride());
      hash = (37 * hash) + MAX_QUEUE_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + getMaxQueueSize();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelQueuePolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelQueuePolicy prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelQueuePolicy
     *&#64;&#64;
     *&#64;&#64;   Queue policy for inference requests.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelQueuePolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelQueuePolicy)
        inference.ModelConfigOuterClass.ModelQueuePolicyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelQueuePolicy_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelQueuePolicy_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelQueuePolicy.class, inference.ModelConfigOuterClass.ModelQueuePolicy.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelQueuePolicy.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        timeoutAction_ = 0;

        defaultTimeoutMicroseconds_ = 0L;

        allowTimeoutOverride_ = false;

        maxQueueSize_ = 0;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelQueuePolicy_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelQueuePolicy.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelQueuePolicy build() {
        inference.ModelConfigOuterClass.ModelQueuePolicy result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelQueuePolicy buildPartial() {
        inference.ModelConfigOuterClass.ModelQueuePolicy result = new inference.ModelConfigOuterClass.ModelQueuePolicy(this);
        result.timeoutAction_ = timeoutAction_;
        result.defaultTimeoutMicroseconds_ = defaultTimeoutMicroseconds_;
        result.allowTimeoutOverride_ = allowTimeoutOverride_;
        result.maxQueueSize_ = maxQueueSize_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelQueuePolicy) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelQueuePolicy)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelQueuePolicy other) {
        if (other == inference.ModelConfigOuterClass.ModelQueuePolicy.getDefaultInstance()) return this;
        if (other.timeoutAction_ != 0) {
          setTimeoutActionValue(other.getTimeoutActionValue());
        }
        if (other.getDefaultTimeoutMicroseconds() != 0L) {
          setDefaultTimeoutMicroseconds(other.getDefaultTimeoutMicroseconds());
        }
        if (other.getAllowTimeoutOverride() != false) {
          setAllowTimeoutOverride(other.getAllowTimeoutOverride());
        }
        if (other.getMaxQueueSize() != 0) {
          setMaxQueueSize(other.getMaxQueueSize());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelQueuePolicy parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelQueuePolicy) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private int timeoutAction_ = 0;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @return The enum numeric value on the wire for timeoutAction.
       */
      @java.lang.Override public int getTimeoutActionValue() {
        return timeoutAction_;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @param value The enum numeric value on the wire for timeoutAction to set.
       * @return This builder for chaining.
       */
      public Builder setTimeoutActionValue(int value) {
        
        timeoutAction_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @return The timeoutAction.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction getTimeoutAction() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction result = inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction.valueOf(timeoutAction_);
        return result == null ? inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @param value The timeoutAction to set.
       * @return This builder for chaining.
       */
      public Builder setTimeoutAction(inference.ModelConfigOuterClass.ModelQueuePolicy.TimeoutAction value) {
        if (value == null) {
          throw new NullPointerException();
        }
        
        timeoutAction_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: TimeoutAction timeout_action
       *&#64;&#64;
       *&#64;&#64;     The action applied to timed-out request.
       *&#64;&#64;     The default action is REJECT.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy.TimeoutAction timeout_action = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearTimeoutAction() {
        
        timeoutAction_ = 0;
        onChanged();
        return this;
      }

      private long defaultTimeoutMicroseconds_ ;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
       *&#64;&#64;
       *&#64;&#64;     The default timeout for every request, in microseconds.
       *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 default_timeout_microseconds = 2;</code>
       * @return The defaultTimeoutMicroseconds.
       */
      @java.lang.Override
      public long getDefaultTimeoutMicroseconds() {
        return defaultTimeoutMicroseconds_;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
       *&#64;&#64;
       *&#64;&#64;     The default timeout for every request, in microseconds.
       *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 default_timeout_microseconds = 2;</code>
       * @param value The defaultTimeoutMicroseconds to set.
       * @return This builder for chaining.
       */
      public Builder setDefaultTimeoutMicroseconds(long value) {
        
        defaultTimeoutMicroseconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint64 default_timeout_microseconds
       *&#64;&#64;
       *&#64;&#64;     The default timeout for every request, in microseconds.
       *&#64;&#64;     The default value is 0 which indicates that no timeout is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 default_timeout_microseconds = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearDefaultTimeoutMicroseconds() {
        
        defaultTimeoutMicroseconds_ = 0L;
        onChanged();
        return this;
      }

      private boolean allowTimeoutOverride_ ;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
       *&#64;&#64;
       *&#64;&#64;     Whether individual request can override the default timeout value.
       *&#64;&#64;     When true, individual requests can set a timeout that is less than
       *&#64;&#64;     the default timeout value but may not increase the timeout.
       *&#64;&#64;     The default value is false.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_timeout_override = 3;</code>
       * @return The allowTimeoutOverride.
       */
      @java.lang.Override
      public boolean getAllowTimeoutOverride() {
        return allowTimeoutOverride_;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
       *&#64;&#64;
       *&#64;&#64;     Whether individual request can override the default timeout value.
       *&#64;&#64;     When true, individual requests can set a timeout that is less than
       *&#64;&#64;     the default timeout value but may not increase the timeout.
       *&#64;&#64;     The default value is false.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_timeout_override = 3;</code>
       * @param value The allowTimeoutOverride to set.
       * @return This builder for chaining.
       */
      public Builder setAllowTimeoutOverride(boolean value) {
        
        allowTimeoutOverride_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: bool allow_timeout_override
       *&#64;&#64;
       *&#64;&#64;     Whether individual request can override the default timeout value.
       *&#64;&#64;     When true, individual requests can set a timeout that is less than
       *&#64;&#64;     the default timeout value but may not increase the timeout.
       *&#64;&#64;     The default value is false.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool allow_timeout_override = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearAllowTimeoutOverride() {
        
        allowTimeoutOverride_ = false;
        onChanged();
        return this;
      }

      private int maxQueueSize_ ;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
       *&#64;&#64;
       *&#64;&#64;     The maximum queue size for holding requests. A request will be
       *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
       *&#64;&#64;     full. The default value is 0 which indicates that no maximum
       *&#64;&#64;     queue size is enforced.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 max_queue_size = 4;</code>
       * @return The maxQueueSize.
       */
      @java.lang.Override
      public int getMaxQueueSize() {
        return maxQueueSize_;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
       *&#64;&#64;
       *&#64;&#64;     The maximum queue size for holding requests. A request will be
       *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
       *&#64;&#64;     full. The default value is 0 which indicates that no maximum
       *&#64;&#64;     queue size is enforced.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 max_queue_size = 4;</code>
       * @param value The maxQueueSize to set.
       * @return This builder for chaining.
       */
      public Builder setMaxQueueSize(int value) {
        
        maxQueueSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: uint32 max_queue_size
       *&#64;&#64;
       *&#64;&#64;     The maximum queue size for holding requests. A request will be
       *&#64;&#64;     rejected immediately if it can't be enqueued because the queue is
       *&#64;&#64;     full. The default value is 0 which indicates that no maximum
       *&#64;&#64;     queue size is enforced.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 max_queue_size = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxQueueSize() {
        
        maxQueueSize_ = 0;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelQueuePolicy)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelQueuePolicy)
    private static final inference.ModelConfigOuterClass.ModelQueuePolicy DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelQueuePolicy();
    }

    public static inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelQueuePolicy>
        PARSER = new com.google.protobuf.AbstractParser<ModelQueuePolicy>() {
      @java.lang.Override
      public ModelQueuePolicy parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelQueuePolicy(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelQueuePolicy> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelQueuePolicy> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelDynamicBatchingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelDynamicBatching)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @return A list containing the preferredBatchSize.
     */
    java.util.List<java.lang.Integer> getPreferredBatchSizeList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @return The count of preferredBatchSize.
     */
    int getPreferredBatchSizeCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @param index The index of the element to return.
     * @return The preferredBatchSize at the given index.
     */
    int getPreferredBatchSize(int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     * @return The maxQueueDelayMicroseconds.
     */
    long getMaxQueueDelayMicroseconds();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool preserve_ordering
     *&#64;&#64;
     *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
     *&#64;&#64;     match the order of requests received by the scheduler. Default is
     *&#64;&#64;     false. If true, the responses will be returned in the same order as
     *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
     *&#64;&#64;     may be returned in arbitrary order. This option is specifically
     *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
     *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
     *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
     *&#64;&#64;     order.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool preserve_ordering = 3;</code>
     * @return The preserveOrdering.
     */
    boolean getPreserveOrdering();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority_levels
     *&#64;&#64;
     *&#64;&#64;     The number of priority levels to be enabled for the model,
     *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
     *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
     *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
     *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
     *&#64;&#64;     handled in the order that they are received.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority_levels = 4;</code>
     * @return The priorityLevels.
     */
    int getPriorityLevels();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
     *&#64;&#64;
     *&#64;&#64;     The priority level used for requests that don't specify their
     *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 default_priority_level = 5;</code>
     * @return The defaultPriorityLevel.
     */
    int getDefaultPriorityLevel();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     * @return Whether the defaultQueuePolicy field is set.
     */
    boolean hasDefaultQueuePolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     * @return The defaultQueuePolicy.
     */
    inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultQueuePolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     */
    inference.ModelConfigOuterClass.ModelQueuePolicyOrBuilder getDefaultQueuePolicyOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    int getPriorityQueuePolicyCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    boolean containsPriorityQueuePolicy(
        int key);
    /**
     * Use {@link #getPriorityQueuePolicyMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
    getPriorityQueuePolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
    getPriorityQueuePolicyMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */

    inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrDefault(
        int key,
        inference.ModelConfigOuterClass.ModelQueuePolicy defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */

    inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrThrow(
        int key);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelDynamicBatching
   *&#64;&#64;
   *&#64;&#64;   Dynamic batching configuration. These settings control how dynamic
   *&#64;&#64;   batching operates for the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelDynamicBatching}
   */
  public static final class ModelDynamicBatching extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelDynamicBatching)
      ModelDynamicBatchingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelDynamicBatching.newBuilder() to construct.
    private ModelDynamicBatching(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelDynamicBatching() {
      preferredBatchSize_ = emptyIntList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelDynamicBatching();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelDynamicBatching(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                preferredBatchSize_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              preferredBatchSize_.addInt(input.readInt32());
              break;
            }
            case 10: {
              int length = input.readRawVarint32();
              int limit = input.pushLimit(length);
              if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                preferredBatchSize_ = newIntList();
                mutable_bitField0_ |= 0x00000001;
              }
              while (input.getBytesUntilLimit() > 0) {
                preferredBatchSize_.addInt(input.readInt32());
              }
              input.popLimit(limit);
              break;
            }
            case 16: {

              maxQueueDelayMicroseconds_ = input.readUInt64();
              break;
            }
            case 24: {

              preserveOrdering_ = input.readBool();
              break;
            }
            case 32: {

              priorityLevels_ = input.readUInt32();
              break;
            }
            case 40: {

              defaultPriorityLevel_ = input.readUInt32();
              break;
            }
            case 50: {
              inference.ModelConfigOuterClass.ModelQueuePolicy.Builder subBuilder = null;
              if (defaultQueuePolicy_ != null) {
                subBuilder = defaultQueuePolicy_.toBuilder();
              }
              defaultQueuePolicy_ = input.readMessage(inference.ModelConfigOuterClass.ModelQueuePolicy.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(defaultQueuePolicy_);
                defaultQueuePolicy_ = subBuilder.buildPartial();
              }

              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                priorityQueuePolicy_ = com.google.protobuf.MapField.newMapField(
                    PriorityQueuePolicyDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000002;
              }
              com.google.protobuf.MapEntry<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
              priorityQueuePolicy__ = input.readMessage(
                  PriorityQueuePolicyDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              priorityQueuePolicy_.getMutableMap().put(
                  priorityQueuePolicy__.getKey(), priorityQueuePolicy__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          preferredBatchSize_.makeImmutable(); // C
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelDynamicBatching_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 7:
          return internalGetPriorityQueuePolicy();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelDynamicBatching_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelDynamicBatching.class, inference.ModelConfigOuterClass.ModelDynamicBatching.Builder.class);
    }

    public static final int PREFERRED_BATCH_SIZE_FIELD_NUMBER = 1;
    private com.google.protobuf.Internal.IntList preferredBatchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @return A list containing the preferredBatchSize.
     */
    @java.lang.Override
    public java.util.List<java.lang.Integer>
        getPreferredBatchSizeList() {
      return preferredBatchSize_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @return The count of preferredBatchSize.
     */
    public int getPreferredBatchSizeCount() {
      return preferredBatchSize_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
     *&#64;&#64;
     *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
     *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
     *&#64;&#64;     not specified a preferred batch size will be chosen automatically
     *&#64;&#64;     based on model and GPU characteristics.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated int32 preferred_batch_size = 1;</code>
     * @param index The index of the element to return.
     * @return The preferredBatchSize at the given index.
     */
    public int getPreferredBatchSize(int index) {
      return preferredBatchSize_.getInt(index);
    }
    private int preferredBatchSizeMemoizedSerializedSize = -1;

    public static final int MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER = 2;
    private long maxQueueDelayMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
     *&#64;&#64;     the scheduling queue to wait for additional requests for
     *&#64;&#64;     batching. Default is 0.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_queue_delay_microseconds = 2;</code>
     * @return The maxQueueDelayMicroseconds.
     */
    @java.lang.Override
    public long getMaxQueueDelayMicroseconds() {
      return maxQueueDelayMicroseconds_;
    }

    public static final int PRESERVE_ORDERING_FIELD_NUMBER = 3;
    private boolean preserveOrdering_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool preserve_ordering
     *&#64;&#64;
     *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
     *&#64;&#64;     match the order of requests received by the scheduler. Default is
     *&#64;&#64;     false. If true, the responses will be returned in the same order as
     *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
     *&#64;&#64;     may be returned in arbitrary order. This option is specifically
     *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
     *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
     *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
     *&#64;&#64;     order.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool preserve_ordering = 3;</code>
     * @return The preserveOrdering.
     */
    @java.lang.Override
    public boolean getPreserveOrdering() {
      return preserveOrdering_;
    }

    public static final int PRIORITY_LEVELS_FIELD_NUMBER = 4;
    private int priorityLevels_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 priority_levels
     *&#64;&#64;
     *&#64;&#64;     The number of priority levels to be enabled for the model,
     *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
     *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
     *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
     *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
     *&#64;&#64;     handled in the order that they are received.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 priority_levels = 4;</code>
     * @return The priorityLevels.
     */
    @java.lang.Override
    public int getPriorityLevels() {
      return priorityLevels_;
    }

    public static final int DEFAULT_PRIORITY_LEVEL_FIELD_NUMBER = 5;
    private int defaultPriorityLevel_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
     *&#64;&#64;
     *&#64;&#64;     The priority level used for requests that don't specify their
     *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 default_priority_level = 5;</code>
     * @return The defaultPriorityLevel.
     */
    @java.lang.Override
    public int getDefaultPriorityLevel() {
      return defaultPriorityLevel_;
    }

    public static final int DEFAULT_QUEUE_POLICY_FIELD_NUMBER = 6;
    private inference.ModelConfigOuterClass.ModelQueuePolicy defaultQueuePolicy_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     * @return Whether the defaultQueuePolicy field is set.
     */
    @java.lang.Override
    public boolean hasDefaultQueuePolicy() {
      return defaultQueuePolicy_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     * @return The defaultQueuePolicy.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultQueuePolicy() {
      return defaultQueuePolicy_ == null ? inference.ModelConfigOuterClass.ModelQueuePolicy.getDefaultInstance() : defaultQueuePolicy_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
     *&#64;&#64;
     *&#64;&#64;     The default queue policy used for requests that don't require
     *&#64;&#64;     priority handling and requests that specify priority levels where
     *&#64;&#64;     there is no specific policy given. If not specified, a policy with
     *&#64;&#64;     default field values will be used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelQueuePolicyOrBuilder getDefaultQueuePolicyOrBuilder() {
      return getDefaultQueuePolicy();
    }

    public static final int PRIORITY_QUEUE_POLICY_FIELD_NUMBER = 7;
    private static final class PriorityQueuePolicyDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>newDefaultInstance(
                  inference.ModelConfigOuterClass.internal_static_inference_ModelDynamicBatching_PriorityQueuePolicyEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.UINT32,
                  0,
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  inference.ModelConfigOuterClass.ModelQueuePolicy.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> priorityQueuePolicy_;
    private com.google.protobuf.MapField<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
    internalGetPriorityQueuePolicy() {
      if (priorityQueuePolicy_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            PriorityQueuePolicyDefaultEntryHolder.defaultEntry);
      }
      return priorityQueuePolicy_;
    }

    public int getPriorityQueuePolicyCount() {
      return internalGetPriorityQueuePolicy().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */

    @java.lang.Override
    public boolean containsPriorityQueuePolicy(
        int key) {
      
      return internalGetPriorityQueuePolicy().getMap().containsKey(key);
    }
    /**
     * Use {@link #getPriorityQueuePolicyMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> getPriorityQueuePolicy() {
      return getPriorityQueuePolicyMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> getPriorityQueuePolicyMap() {
      return internalGetPriorityQueuePolicy().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrDefault(
        int key,
        inference.ModelConfigOuterClass.ModelQueuePolicy defaultValue) {
      
      java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> map =
          internalGetPriorityQueuePolicy().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
     *&#64;&#64;
     *&#64;&#64;     Specify the queue policy for the priority level. The default queue
     *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
     *&#64;&#64;     policy.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrThrow(
        int key) {
      
      java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> map =
          internalGetPriorityQueuePolicy().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (getPreferredBatchSizeList().size() > 0) {
        output.writeUInt32NoTag(10);
        output.writeUInt32NoTag(preferredBatchSizeMemoizedSerializedSize);
      }
      for (int i = 0; i < preferredBatchSize_.size(); i++) {
        output.writeInt32NoTag(preferredBatchSize_.getInt(i));
      }
      if (maxQueueDelayMicroseconds_ != 0L) {
        output.writeUInt64(2, maxQueueDelayMicroseconds_);
      }
      if (preserveOrdering_ != false) {
        output.writeBool(3, preserveOrdering_);
      }
      if (priorityLevels_ != 0) {
        output.writeUInt32(4, priorityLevels_);
      }
      if (defaultPriorityLevel_ != 0) {
        output.writeUInt32(5, defaultPriorityLevel_);
      }
      if (defaultQueuePolicy_ != null) {
        output.writeMessage(6, getDefaultQueuePolicy());
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeIntegerMapTo(
          output,
          internalGetPriorityQueuePolicy(),
          PriorityQueuePolicyDefaultEntryHolder.defaultEntry,
          7);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < preferredBatchSize_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeInt32SizeNoTag(preferredBatchSize_.getInt(i));
        }
        size += dataSize;
        if (!getPreferredBatchSizeList().isEmpty()) {
          size += 1;
          size += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(dataSize);
        }
        preferredBatchSizeMemoizedSerializedSize = dataSize;
      }
      if (maxQueueDelayMicroseconds_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, maxQueueDelayMicroseconds_);
      }
      if (preserveOrdering_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(3, preserveOrdering_);
      }
      if (priorityLevels_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(4, priorityLevels_);
      }
      if (defaultPriorityLevel_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(5, defaultPriorityLevel_);
      }
      if (defaultQueuePolicy_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, getDefaultQueuePolicy());
      }
      for (java.util.Map.Entry<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> entry
           : internalGetPriorityQueuePolicy().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
        priorityQueuePolicy__ = PriorityQueuePolicyDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(7, priorityQueuePolicy__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelDynamicBatching)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelDynamicBatching other = (inference.ModelConfigOuterClass.ModelDynamicBatching) obj;

      if (!getPreferredBatchSizeList()
          .equals(other.getPreferredBatchSizeList())) return false;
      if (getMaxQueueDelayMicroseconds()
          != other.getMaxQueueDelayMicroseconds()) return false;
      if (getPreserveOrdering()
          != other.getPreserveOrdering()) return false;
      if (getPriorityLevels()
          != other.getPriorityLevels()) return false;
      if (getDefaultPriorityLevel()
          != other.getDefaultPriorityLevel()) return false;
      if (hasDefaultQueuePolicy() != other.hasDefaultQueuePolicy()) return false;
      if (hasDefaultQueuePolicy()) {
        if (!getDefaultQueuePolicy()
            .equals(other.getDefaultQueuePolicy())) return false;
      }
      if (!internalGetPriorityQueuePolicy().equals(
          other.internalGetPriorityQueuePolicy())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getPreferredBatchSizeCount() > 0) {
        hash = (37 * hash) + PREFERRED_BATCH_SIZE_FIELD_NUMBER;
        hash = (53 * hash) + getPreferredBatchSizeList().hashCode();
      }
      hash = (37 * hash) + MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMaxQueueDelayMicroseconds());
      hash = (37 * hash) + PRESERVE_ORDERING_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getPreserveOrdering());
      hash = (37 * hash) + PRIORITY_LEVELS_FIELD_NUMBER;
      hash = (53 * hash) + getPriorityLevels();
      hash = (37 * hash) + DEFAULT_PRIORITY_LEVEL_FIELD_NUMBER;
      hash = (53 * hash) + getDefaultPriorityLevel();
      if (hasDefaultQueuePolicy()) {
        hash = (37 * hash) + DEFAULT_QUEUE_POLICY_FIELD_NUMBER;
        hash = (53 * hash) + getDefaultQueuePolicy().hashCode();
      }
      if (!internalGetPriorityQueuePolicy().getMap().isEmpty()) {
        hash = (37 * hash) + PRIORITY_QUEUE_POLICY_FIELD_NUMBER;
        hash = (53 * hash) + internalGetPriorityQueuePolicy().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelDynamicBatching parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelDynamicBatching prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelDynamicBatching
     *&#64;&#64;
     *&#64;&#64;   Dynamic batching configuration. These settings control how dynamic
     *&#64;&#64;   batching operates for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelDynamicBatching}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelDynamicBatching)
        inference.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelDynamicBatching_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 7:
            return internalGetPriorityQueuePolicy();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 7:
            return internalGetMutablePriorityQueuePolicy();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelDynamicBatching_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelDynamicBatching.class, inference.ModelConfigOuterClass.ModelDynamicBatching.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelDynamicBatching.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        preferredBatchSize_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        maxQueueDelayMicroseconds_ = 0L;

        preserveOrdering_ = false;

        priorityLevels_ = 0;

        defaultPriorityLevel_ = 0;

        if (defaultQueuePolicyBuilder_ == null) {
          defaultQueuePolicy_ = null;
        } else {
          defaultQueuePolicy_ = null;
          defaultQueuePolicyBuilder_ = null;
        }
        internalGetMutablePriorityQueuePolicy().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelDynamicBatching_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelDynamicBatching build() {
        inference.ModelConfigOuterClass.ModelDynamicBatching result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelDynamicBatching buildPartial() {
        inference.ModelConfigOuterClass.ModelDynamicBatching result = new inference.ModelConfigOuterClass.ModelDynamicBatching(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          preferredBatchSize_.makeImmutable();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.preferredBatchSize_ = preferredBatchSize_;
        result.maxQueueDelayMicroseconds_ = maxQueueDelayMicroseconds_;
        result.preserveOrdering_ = preserveOrdering_;
        result.priorityLevels_ = priorityLevels_;
        result.defaultPriorityLevel_ = defaultPriorityLevel_;
        if (defaultQueuePolicyBuilder_ == null) {
          result.defaultQueuePolicy_ = defaultQueuePolicy_;
        } else {
          result.defaultQueuePolicy_ = defaultQueuePolicyBuilder_.build();
        }
        result.priorityQueuePolicy_ = internalGetPriorityQueuePolicy();
        result.priorityQueuePolicy_.makeImmutable();
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelDynamicBatching) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelDynamicBatching)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelDynamicBatching other) {
        if (other == inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance()) return this;
        if (!other.preferredBatchSize_.isEmpty()) {
          if (preferredBatchSize_.isEmpty()) {
            preferredBatchSize_ = other.preferredBatchSize_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensurePreferredBatchSizeIsMutable();
            preferredBatchSize_.addAll(other.preferredBatchSize_);
          }
          onChanged();
        }
        if (other.getMaxQueueDelayMicroseconds() != 0L) {
          setMaxQueueDelayMicroseconds(other.getMaxQueueDelayMicroseconds());
        }
        if (other.getPreserveOrdering() != false) {
          setPreserveOrdering(other.getPreserveOrdering());
        }
        if (other.getPriorityLevels() != 0) {
          setPriorityLevels(other.getPriorityLevels());
        }
        if (other.getDefaultPriorityLevel() != 0) {
          setDefaultPriorityLevel(other.getDefaultPriorityLevel());
        }
        if (other.hasDefaultQueuePolicy()) {
          mergeDefaultQueuePolicy(other.getDefaultQueuePolicy());
        }
        internalGetMutablePriorityQueuePolicy().mergeFrom(
            other.internalGetPriorityQueuePolicy());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelDynamicBatching parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelDynamicBatching) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.Internal.IntList preferredBatchSize_ = emptyIntList();
      private void ensurePreferredBatchSizeIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          preferredBatchSize_ = mutableCopy(preferredBatchSize_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @return A list containing the preferredBatchSize.
       */
      public java.util.List<java.lang.Integer>
          getPreferredBatchSizeList() {
        return ((bitField0_ & 0x00000001) != 0) ?
                 java.util.Collections.unmodifiableList(preferredBatchSize_) : preferredBatchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @return The count of preferredBatchSize.
       */
      public int getPreferredBatchSizeCount() {
        return preferredBatchSize_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @param index The index of the element to return.
       * @return The preferredBatchSize at the given index.
       */
      public int getPreferredBatchSize(int index) {
        return preferredBatchSize_.getInt(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @param index The index to set the value at.
       * @param value The preferredBatchSize to set.
       * @return This builder for chaining.
       */
      public Builder setPreferredBatchSize(
          int index, int value) {
        ensurePreferredBatchSizeIsMutable();
        preferredBatchSize_.setInt(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @param value The preferredBatchSize to add.
       * @return This builder for chaining.
       */
      public Builder addPreferredBatchSize(int value) {
        ensurePreferredBatchSizeIsMutable();
        preferredBatchSize_.addInt(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @param values The preferredBatchSize to add.
       * @return This builder for chaining.
       */
      public Builder addAllPreferredBatchSize(
          java.lang.Iterable<? extends java.lang.Integer> values) {
        ensurePreferredBatchSizeIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, preferredBatchSize_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;     Preferred batch sizes for dynamic batching. If a batch of one of
       *&#64;&#64;     these sizes can be formed it will be executed immediately.  If
       *&#64;&#64;     not specified a preferred batch size will be chosen automatically
       *&#64;&#64;     based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearPreferredBatchSize() {
        preferredBatchSize_ = emptyIntList();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }

      private long maxQueueDelayMicroseconds_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      @java.lang.Override
      public long getMaxQueueDelayMicroseconds() {
        return maxQueueDelayMicroseconds_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       * @param value The maxQueueDelayMicroseconds to set.
       * @return This builder for chaining.
       */
      public Builder setMaxQueueDelayMicroseconds(long value) {
        
        maxQueueDelayMicroseconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, a request will be delayed in
       *&#64;&#64;     the scheduling queue to wait for additional requests for
       *&#64;&#64;     batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxQueueDelayMicroseconds() {
        
        maxQueueDelayMicroseconds_ = 0L;
        onChanged();
        return this;
      }

      private boolean preserveOrdering_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool preserve_ordering
       *&#64;&#64;
       *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
       *&#64;&#64;     match the order of requests received by the scheduler. Default is
       *&#64;&#64;     false. If true, the responses will be returned in the same order as
       *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
       *&#64;&#64;     may be returned in arbitrary order. This option is specifically
       *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
       *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
       *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
       *&#64;&#64;     order.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool preserve_ordering = 3;</code>
       * @return The preserveOrdering.
       */
      @java.lang.Override
      public boolean getPreserveOrdering() {
        return preserveOrdering_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool preserve_ordering
       *&#64;&#64;
       *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
       *&#64;&#64;     match the order of requests received by the scheduler. Default is
       *&#64;&#64;     false. If true, the responses will be returned in the same order as
       *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
       *&#64;&#64;     may be returned in arbitrary order. This option is specifically
       *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
       *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
       *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
       *&#64;&#64;     order.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool preserve_ordering = 3;</code>
       * @param value The preserveOrdering to set.
       * @return This builder for chaining.
       */
      public Builder setPreserveOrdering(boolean value) {
        
        preserveOrdering_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool preserve_ordering
       *&#64;&#64;
       *&#64;&#64;     Should the dynamic batcher preserve the ordering of responses to
       *&#64;&#64;     match the order of requests received by the scheduler. Default is
       *&#64;&#64;     false. If true, the responses will be returned in the same order as
       *&#64;&#64;     the order of requests sent to the scheduler. If false, the responses
       *&#64;&#64;     may be returned in arbitrary order. This option is specifically
       *&#64;&#64;     needed when a sequence of related inference requests (i.e. inference
       *&#64;&#64;     requests with the same correlation ID) are sent to the dynamic
       *&#64;&#64;     batcher to ensure that the sequence responses are in the correct
       *&#64;&#64;     order.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool preserve_ordering = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearPreserveOrdering() {
        
        preserveOrdering_ = false;
        onChanged();
        return this;
      }

      private int priorityLevels_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority_levels
       *&#64;&#64;
       *&#64;&#64;     The number of priority levels to be enabled for the model,
       *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
       *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
       *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
       *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
       *&#64;&#64;     handled in the order that they are received.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority_levels = 4;</code>
       * @return The priorityLevels.
       */
      @java.lang.Override
      public int getPriorityLevels() {
        return priorityLevels_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority_levels
       *&#64;&#64;
       *&#64;&#64;     The number of priority levels to be enabled for the model,
       *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
       *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
       *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
       *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
       *&#64;&#64;     handled in the order that they are received.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority_levels = 4;</code>
       * @param value The priorityLevels to set.
       * @return This builder for chaining.
       */
      public Builder setPriorityLevels(int value) {
        
        priorityLevels_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 priority_levels
       *&#64;&#64;
       *&#64;&#64;     The number of priority levels to be enabled for the model,
       *&#64;&#64;     the priority level starts from 1 and 1 is the highest priority.
       *&#64;&#64;     Requests are handled in priority order with all priority 1 requests
       *&#64;&#64;     processed before priority 2, all priority 2 requests processed before
       *&#64;&#64;     priority 3, etc. Requests with the same priority level will be
       *&#64;&#64;     handled in the order that they are received.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 priority_levels = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearPriorityLevels() {
        
        priorityLevels_ = 0;
        onChanged();
        return this;
      }

      private int defaultPriorityLevel_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
       *&#64;&#64;
       *&#64;&#64;     The priority level used for requests that don't specify their
       *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 default_priority_level = 5;</code>
       * @return The defaultPriorityLevel.
       */
      @java.lang.Override
      public int getDefaultPriorityLevel() {
        return defaultPriorityLevel_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
       *&#64;&#64;
       *&#64;&#64;     The priority level used for requests that don't specify their
       *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 default_priority_level = 5;</code>
       * @param value The defaultPriorityLevel to set.
       * @return This builder for chaining.
       */
      public Builder setDefaultPriorityLevel(int value) {
        
        defaultPriorityLevel_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 default_priority_level
       *&#64;&#64;
       *&#64;&#64;     The priority level used for requests that don't specify their
       *&#64;&#64;     priority. The value must be in the range [ 1, 'priority_levels' ].
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 default_priority_level = 5;</code>
       * @return This builder for chaining.
       */
      public Builder clearDefaultPriorityLevel() {
        
        defaultPriorityLevel_ = 0;
        onChanged();
        return this;
      }

      private inference.ModelConfigOuterClass.ModelQueuePolicy defaultQueuePolicy_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelQueuePolicy, inference.ModelConfigOuterClass.ModelQueuePolicy.Builder, inference.ModelConfigOuterClass.ModelQueuePolicyOrBuilder> defaultQueuePolicyBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       * @return Whether the defaultQueuePolicy field is set.
       */
      public boolean hasDefaultQueuePolicy() {
        return defaultQueuePolicyBuilder_ != null || defaultQueuePolicy_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       * @return The defaultQueuePolicy.
       */
      public inference.ModelConfigOuterClass.ModelQueuePolicy getDefaultQueuePolicy() {
        if (defaultQueuePolicyBuilder_ == null) {
          return defaultQueuePolicy_ == null ? inference.ModelConfigOuterClass.ModelQueuePolicy.getDefaultInstance() : defaultQueuePolicy_;
        } else {
          return defaultQueuePolicyBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public Builder setDefaultQueuePolicy(inference.ModelConfigOuterClass.ModelQueuePolicy value) {
        if (defaultQueuePolicyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          defaultQueuePolicy_ = value;
          onChanged();
        } else {
          defaultQueuePolicyBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public Builder setDefaultQueuePolicy(
          inference.ModelConfigOuterClass.ModelQueuePolicy.Builder builderForValue) {
        if (defaultQueuePolicyBuilder_ == null) {
          defaultQueuePolicy_ = builderForValue.build();
          onChanged();
        } else {
          defaultQueuePolicyBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public Builder mergeDefaultQueuePolicy(inference.ModelConfigOuterClass.ModelQueuePolicy value) {
        if (defaultQueuePolicyBuilder_ == null) {
          if (defaultQueuePolicy_ != null) {
            defaultQueuePolicy_ =
              inference.ModelConfigOuterClass.ModelQueuePolicy.newBuilder(defaultQueuePolicy_).mergeFrom(value).buildPartial();
          } else {
            defaultQueuePolicy_ = value;
          }
          onChanged();
        } else {
          defaultQueuePolicyBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public Builder clearDefaultQueuePolicy() {
        if (defaultQueuePolicyBuilder_ == null) {
          defaultQueuePolicy_ = null;
          onChanged();
        } else {
          defaultQueuePolicy_ = null;
          defaultQueuePolicyBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelQueuePolicy.Builder getDefaultQueuePolicyBuilder() {
        
        onChanged();
        return getDefaultQueuePolicyFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelQueuePolicyOrBuilder getDefaultQueuePolicyOrBuilder() {
        if (defaultQueuePolicyBuilder_ != null) {
          return defaultQueuePolicyBuilder_.getMessageOrBuilder();
        } else {
          return defaultQueuePolicy_ == null ?
              inference.ModelConfigOuterClass.ModelQueuePolicy.getDefaultInstance() : defaultQueuePolicy_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelQueuePolicy default_queue_policy
       *&#64;&#64;
       *&#64;&#64;     The default queue policy used for requests that don't require
       *&#64;&#64;     priority handling and requests that specify priority levels where
       *&#64;&#64;     there is no specific policy given. If not specified, a policy with
       *&#64;&#64;     default field values will be used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelQueuePolicy default_queue_policy = 6;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelQueuePolicy, inference.ModelConfigOuterClass.ModelQueuePolicy.Builder, inference.ModelConfigOuterClass.ModelQueuePolicyOrBuilder> 
          getDefaultQueuePolicyFieldBuilder() {
        if (defaultQueuePolicyBuilder_ == null) {
          defaultQueuePolicyBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelQueuePolicy, inference.ModelConfigOuterClass.ModelQueuePolicy.Builder, inference.ModelConfigOuterClass.ModelQueuePolicyOrBuilder>(
                  getDefaultQueuePolicy(),
                  getParentForChildren(),
                  isClean());
          defaultQueuePolicy_ = null;
        }
        return defaultQueuePolicyBuilder_;
      }

      private com.google.protobuf.MapField<
          java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> priorityQueuePolicy_;
      private com.google.protobuf.MapField<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
      internalGetPriorityQueuePolicy() {
        if (priorityQueuePolicy_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              PriorityQueuePolicyDefaultEntryHolder.defaultEntry);
        }
        return priorityQueuePolicy_;
      }
      private com.google.protobuf.MapField<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
      internalGetMutablePriorityQueuePolicy() {
        onChanged();;
        if (priorityQueuePolicy_ == null) {
          priorityQueuePolicy_ = com.google.protobuf.MapField.newMapField(
              PriorityQueuePolicyDefaultEntryHolder.defaultEntry);
        }
        if (!priorityQueuePolicy_.isMutable()) {
          priorityQueuePolicy_ = priorityQueuePolicy_.copy();
        }
        return priorityQueuePolicy_;
      }

      public int getPriorityQueuePolicyCount() {
        return internalGetPriorityQueuePolicy().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */

      @java.lang.Override
      public boolean containsPriorityQueuePolicy(
          int key) {
        
        return internalGetPriorityQueuePolicy().getMap().containsKey(key);
      }
      /**
       * Use {@link #getPriorityQueuePolicyMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> getPriorityQueuePolicy() {
        return getPriorityQueuePolicyMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> getPriorityQueuePolicyMap() {
        return internalGetPriorityQueuePolicy().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrDefault(
          int key,
          inference.ModelConfigOuterClass.ModelQueuePolicy defaultValue) {
        
        java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> map =
            internalGetPriorityQueuePolicy().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelQueuePolicy getPriorityQueuePolicyOrThrow(
          int key) {
        
        java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> map =
            internalGetPriorityQueuePolicy().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearPriorityQueuePolicy() {
        internalGetMutablePriorityQueuePolicy().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */

      public Builder removePriorityQueuePolicy(
          int key) {
        
        internalGetMutablePriorityQueuePolicy().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy>
      getMutablePriorityQueuePolicy() {
        return internalGetMutablePriorityQueuePolicy().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */
      public Builder putPriorityQueuePolicy(
          int key,
          inference.ModelConfigOuterClass.ModelQueuePolicy value) {
        
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutablePriorityQueuePolicy().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;uint32, ModelQueuePolicy&gt; priority_queue_policy
       *&#64;&#64;
       *&#64;&#64;     Specify the queue policy for the priority level. The default queue
       *&#64;&#64;     policy will be used if a priority level doesn't specify a queue
       *&#64;&#64;     policy.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;uint32, .inference.ModelQueuePolicy&gt; priority_queue_policy = 7;</code>
       */

      public Builder putAllPriorityQueuePolicy(
          java.util.Map<java.lang.Integer, inference.ModelConfigOuterClass.ModelQueuePolicy> values) {
        internalGetMutablePriorityQueuePolicy().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelDynamicBatching)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelDynamicBatching)
    private static final inference.ModelConfigOuterClass.ModelDynamicBatching DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelDynamicBatching();
    }

    public static inference.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelDynamicBatching>
        PARSER = new com.google.protobuf.AbstractParser<ModelDynamicBatching>() {
      @java.lang.Override
      public ModelDynamicBatching parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelDynamicBatching(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelDynamicBatching> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelDynamicBatching> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelDynamicBatching getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelSequenceBatchingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     * @return Whether the direct field is set.
     */
    boolean hasDirect();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     * @return The direct.
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDirect();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder getDirectOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     * @return Whether the oldest field is set.
     */
    boolean hasOldest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     * @return The oldest.
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getOldest();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder getOldestOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     * @return The maxSequenceIdleMicroseconds.
     */
    long getMaxSequenceIdleMicroseconds();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> 
        getControlInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    int getControlInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    java.util.List<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
        getControlInputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
        int index);

    public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyChoiceCase getStrategyChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelSequenceBatching
   *&#64;&#64;
   *&#64;&#64;   Sequence batching configuration. These settings control how sequence
   *&#64;&#64;   batching operates for the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelSequenceBatching}
   */
  public static final class ModelSequenceBatching extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching)
      ModelSequenceBatchingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelSequenceBatching.newBuilder() to construct.
    private ModelSequenceBatching(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelSequenceBatching() {
      controlInput_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelSequenceBatching();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelSequenceBatching(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              maxSequenceIdleMicroseconds_ = input.readUInt64();
              break;
            }
            case 18: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                controlInput_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput>();
                mutable_bitField0_ |= 0x00000001;
              }
              controlInput_.add(
                  input.readMessage(inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.parser(), extensionRegistry));
              break;
            }
            case 26: {
              inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder subBuilder = null;
              if (strategyChoiceCase_ == 3) {
                subBuilder = ((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_).toBuilder();
              }
              strategyChoice_ =
                  input.readMessage(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_);
                strategyChoice_ = subBuilder.buildPartial();
              }
              strategyChoiceCase_ = 3;
              break;
            }
            case 34: {
              inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder subBuilder = null;
              if (strategyChoiceCase_ == 4) {
                subBuilder = ((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_).toBuilder();
              }
              strategyChoice_ =
                  input.readMessage(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_);
                strategyChoice_ = subBuilder.buildPartial();
              }
              strategyChoiceCase_ = 4;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          controlInput_ = java.util.Collections.unmodifiableList(controlInput_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelSequenceBatching.class, inference.ModelConfigOuterClass.ModelSequenceBatching.Builder.class);
    }

    public interface ControlOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching.Control)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @return The enum numeric value on the wire for kind.
       */
      int getKindValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @return The kind.
       */
      inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @return A list containing the int32FalseTrue.
       */
      java.util.List<java.lang.Integer> getInt32FalseTrueList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @return The count of int32FalseTrue.
       */
      int getInt32FalseTrueCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @param index The index of the element to return.
       * @return The int32FalseTrue at the given index.
       */
      int getInt32FalseTrue(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @return A list containing the fp32FalseTrue.
       */
      java.util.List<java.lang.Float> getFp32FalseTrueList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @return The count of fp32FalseTrue.
       */
      int getFp32FalseTrueCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @param index The index of the element to return.
       * @return The fp32FalseTrue at the given index.
       */
      float getFp32FalseTrue(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      int getDataTypeValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @return The dataType.
       */
      inference.ModelConfigOuterClass.DataType getDataType();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Control
     *&#64;&#64;
     *&#64;&#64;     A control is a signal that the sequence batcher uses to
     *&#64;&#64;     communicate with a backend.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching.Control}
     */
    public static final class Control extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching.Control)
        ControlOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Control.newBuilder() to construct.
      private Control(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Control() {
        kind_ = 0;
        int32FalseTrue_ = emptyIntList();
        fp32FalseTrue_ = emptyFloatList();
        dataType_ = 0;
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Control();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Control(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                int rawValue = input.readEnum();

                kind_ = rawValue;
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  int32FalseTrue_ = newIntList();
                  mutable_bitField0_ |= 0x00000001;
                }
                int32FalseTrue_.addInt(input.readInt32());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                  int32FalseTrue_ = newIntList();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  int32FalseTrue_.addInt(input.readInt32());
                }
                input.popLimit(limit);
                break;
              }
              case 29: {
                if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                  fp32FalseTrue_ = newFloatList();
                  mutable_bitField0_ |= 0x00000002;
                }
                fp32FalseTrue_.addFloat(input.readFloat());
                break;
              }
              case 26: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000002) != 0) && input.getBytesUntilLimit() > 0) {
                  fp32FalseTrue_ = newFloatList();
                  mutable_bitField0_ |= 0x00000002;
                }
                while (input.getBytesUntilLimit() > 0) {
                  fp32FalseTrue_.addFloat(input.readFloat());
                }
                input.popLimit(limit);
                break;
              }
              case 32: {
                int rawValue = input.readEnum();

                dataType_ = rawValue;
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            int32FalseTrue_.makeImmutable(); // C
          }
          if (((mutable_bitField0_ & 0x00000002) != 0)) {
            fp32FalseTrue_.makeImmutable(); // C
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_Control_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_Control_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelSequenceBatching.Control.class, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder.class);
      }

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:enum:: Kind
       *&#64;&#64;
       *&#64;&#64;       The kind of the control.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf enum {@code inference.ModelSequenceBatching.Control.Kind}
       */
      public enum Kind
          implements com.google.protobuf.ProtocolMessageEnum {
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
         *&#64;&#64;
         *&#64;&#64;         A new sequence is/is-not starting. If true a sequence is
         *&#64;&#64;         starting, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_START = 0;</code>
         */
        CONTROL_SEQUENCE_START(0),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ready for inference. If true the
         *&#64;&#64;         input tensor data is valid and should be used. If false
         *&#64;&#64;         the input tensor data is invalid and inferencing should
         *&#64;&#64;         be "skipped".  Must specify either int32_false_true or
         *&#64;&#64;         fp32_false_true for this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_READY = 1;</code>
         */
        CONTROL_SEQUENCE_READY(1),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_END = 2
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ending. If true a sequence is
         *&#64;&#64;         ending, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_END = 2;</code>
         */
        CONTROL_SEQUENCE_END(2),
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_CORRID = 3
         *&#64;&#64;
         *&#64;&#64;         The correlation ID of the sequence. The correlation ID
         *&#64;&#64;         is an uint64_t value that is communicated in whole or
         *&#64;&#64;         in part by the tensor. The tensor's datatype must be
         *&#64;&#64;         specified by data_type and must be TYPE_UINT64, TYPE_INT64,
         *&#64;&#64;         TYPE_UINT32 or TYPE_INT32. If a 32-bit datatype is specified
         *&#64;&#64;         the correlation ID will be truncated to the low-order 32
         *&#64;&#64;         bits. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_CORRID = 3;</code>
         */
        CONTROL_SEQUENCE_CORRID(3),
        UNRECOGNIZED(-1),
        ;

        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_START = 0
         *&#64;&#64;
         *&#64;&#64;         A new sequence is/is-not starting. If true a sequence is
         *&#64;&#64;         starting, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_START = 0;</code>
         */
        public static final int CONTROL_SEQUENCE_START_VALUE = 0;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_READY = 1
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ready for inference. If true the
         *&#64;&#64;         input tensor data is valid and should be used. If false
         *&#64;&#64;         the input tensor data is invalid and inferencing should
         *&#64;&#64;         be "skipped".  Must specify either int32_false_true or
         *&#64;&#64;         fp32_false_true for this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_READY = 1;</code>
         */
        public static final int CONTROL_SEQUENCE_READY_VALUE = 1;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_END = 2
         *&#64;&#64;
         *&#64;&#64;         A sequence is/is-not ending. If true a sequence is
         *&#64;&#64;         ending, if false a sequence is continuing. Must
         *&#64;&#64;         specify either int32_false_true or fp32_false_true for
         *&#64;&#64;         this control. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_END = 2;</code>
         */
        public static final int CONTROL_SEQUENCE_END_VALUE = 2;
        /**
         * <pre>
         *&#64;&#64;      .. cpp:enumerator:: Kind::CONTROL_SEQUENCE_CORRID = 3
         *&#64;&#64;
         *&#64;&#64;         The correlation ID of the sequence. The correlation ID
         *&#64;&#64;         is an uint64_t value that is communicated in whole or
         *&#64;&#64;         in part by the tensor. The tensor's datatype must be
         *&#64;&#64;         specified by data_type and must be TYPE_UINT64, TYPE_INT64,
         *&#64;&#64;         TYPE_UINT32 or TYPE_INT32. If a 32-bit datatype is specified
         *&#64;&#64;         the correlation ID will be truncated to the low-order 32
         *&#64;&#64;         bits. This control is optional.
         *&#64;&#64;
         * </pre>
         *
         * <code>CONTROL_SEQUENCE_CORRID = 3;</code>
         */
        public static final int CONTROL_SEQUENCE_CORRID_VALUE = 3;


        public final int getNumber() {
          if (this == UNRECOGNIZED) {
            throw new java.lang.IllegalArgumentException(
                "Can't get the number of an unknown enum value.");
          }
          return value;
        }

        /**
         * @param value The numeric wire value of the corresponding enum entry.
         * @return The enum associated with the given numeric wire value.
         * @deprecated Use {@link #forNumber(int)} instead.
         */
        @java.lang.Deprecated
        public static Kind valueOf(int value) {
          return forNumber(value);
        }

        /**
         * @param value The numeric wire value of the corresponding enum entry.
         * @return The enum associated with the given numeric wire value.
         */
        public static Kind forNumber(int value) {
          switch (value) {
            case 0: return CONTROL_SEQUENCE_START;
            case 1: return CONTROL_SEQUENCE_READY;
            case 2: return CONTROL_SEQUENCE_END;
            case 3: return CONTROL_SEQUENCE_CORRID;
            default: return null;
          }
        }

        public static com.google.protobuf.Internal.EnumLiteMap<Kind>
            internalGetValueMap() {
          return internalValueMap;
        }
        private static final com.google.protobuf.Internal.EnumLiteMap<
            Kind> internalValueMap =
              new com.google.protobuf.Internal.EnumLiteMap<Kind>() {
                public Kind findValueByNumber(int number) {
                  return Kind.forNumber(number);
                }
              };

        public final com.google.protobuf.Descriptors.EnumValueDescriptor
            getValueDescriptor() {
          if (this == UNRECOGNIZED) {
            throw new java.lang.IllegalStateException(
                "Can't get the descriptor of an unrecognized enum value.");
          }
          return getDescriptor().getValues().get(ordinal());
        }
        public final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptorForType() {
          return getDescriptor();
        }
        public static final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.ModelSequenceBatching.Control.getDescriptor().getEnumTypes().get(0);
        }

        private static final Kind[] VALUES = values();

        public static Kind valueOf(
            com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
          if (desc.getType() != getDescriptor()) {
            throw new java.lang.IllegalArgumentException(
              "EnumValueDescriptor is not for this type.");
          }
          if (desc.getIndex() == -1) {
            return UNRECOGNIZED;
          }
          return VALUES[desc.getIndex()];
        }

        private final int value;

        private Kind(int value) {
          this.value = value;
        }

        // @@protoc_insertion_point(enum_scope:inference.ModelSequenceBatching.Control.Kind)
      }

      public static final int KIND_FIELD_NUMBER = 1;
      private int kind_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @return The enum numeric value on the wire for kind.
       */
      @java.lang.Override public int getKindValue() {
        return kind_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Kind kind
       *&#64;&#64;
       *&#64;&#64;       The kind of this control.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
       * @return The kind.
       */
      @java.lang.Override public inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind result = inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.valueOf(kind_);
        return result == null ? inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.UNRECOGNIZED : result;
      }

      public static final int INT32_FALSE_TRUE_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.IntList int32FalseTrue_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @return A list containing the int32FalseTrue.
       */
      @java.lang.Override
      public java.util.List<java.lang.Integer>
          getInt32FalseTrueList() {
        return int32FalseTrue_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @return The count of int32FalseTrue.
       */
      public int getInt32FalseTrueCount() {
        return int32FalseTrue_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in an int32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 int32_false_true = 2;</code>
       * @param index The index of the element to return.
       * @return The int32FalseTrue at the given index.
       */
      public int getInt32FalseTrue(int index) {
        return int32FalseTrue_.getInt(index);
      }
      private int int32FalseTrueMemoizedSerializedSize = -1;

      public static final int FP32_FALSE_TRUE_FIELD_NUMBER = 3;
      private com.google.protobuf.Internal.FloatList fp32FalseTrue_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @return A list containing the fp32FalseTrue.
       */
      @java.lang.Override
      public java.util.List<java.lang.Float>
          getFp32FalseTrueList() {
        return fp32FalseTrue_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @return The count of fp32FalseTrue.
       */
      public int getFp32FalseTrueCount() {
        return fp32FalseTrue_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control's true and false setting is indicated by setting
       *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
       *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
       *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
       *&#64;&#64;       first the false value and the second the true value.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated float fp32_false_true = 3;</code>
       * @param index The index of the element to return.
       * @return The fp32FalseTrue at the given index.
       */
      public float getFp32FalseTrue(int index) {
        return fp32FalseTrue_.getFloat(index);
      }
      private int fp32FalseTrueMemoizedSerializedSize = -1;

      public static final int DATA_TYPE_FIELD_NUMBER = 4;
      private int dataType_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The control's datatype.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 4;</code>
       * @return The dataType.
       */
      @java.lang.Override public inference.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (kind_ != inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.CONTROL_SEQUENCE_START.getNumber()) {
          output.writeEnum(1, kind_);
        }
        if (getInt32FalseTrueList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(int32FalseTrueMemoizedSerializedSize);
        }
        for (int i = 0; i < int32FalseTrue_.size(); i++) {
          output.writeInt32NoTag(int32FalseTrue_.getInt(i));
        }
        if (getFp32FalseTrueList().size() > 0) {
          output.writeUInt32NoTag(26);
          output.writeUInt32NoTag(fp32FalseTrueMemoizedSerializedSize);
        }
        for (int i = 0; i < fp32FalseTrue_.size(); i++) {
          output.writeFloatNoTag(fp32FalseTrue_.getFloat(i));
        }
        if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
          output.writeEnum(4, dataType_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (kind_ != inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.CONTROL_SEQUENCE_START.getNumber()) {
          size += com.google.protobuf.CodedOutputStream
            .computeEnumSize(1, kind_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < int32FalseTrue_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(int32FalseTrue_.getInt(i));
          }
          size += dataSize;
          if (!getInt32FalseTrueList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          int32FalseTrueMemoizedSerializedSize = dataSize;
        }
        {
          int dataSize = 0;
          dataSize = 4 * getFp32FalseTrueList().size();
          size += dataSize;
          if (!getFp32FalseTrueList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          fp32FalseTrueMemoizedSerializedSize = dataSize;
        }
        if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
          size += com.google.protobuf.CodedOutputStream
            .computeEnumSize(4, dataType_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelSequenceBatching.Control)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelSequenceBatching.Control other = (inference.ModelConfigOuterClass.ModelSequenceBatching.Control) obj;

        if (kind_ != other.kind_) return false;
        if (!getInt32FalseTrueList()
            .equals(other.getInt32FalseTrueList())) return false;
        if (!getFp32FalseTrueList()
            .equals(other.getFp32FalseTrueList())) return false;
        if (dataType_ != other.dataType_) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + KIND_FIELD_NUMBER;
        hash = (53 * hash) + kind_;
        if (getInt32FalseTrueCount() > 0) {
          hash = (37 * hash) + INT32_FALSE_TRUE_FIELD_NUMBER;
          hash = (53 * hash) + getInt32FalseTrueList().hashCode();
        }
        if (getFp32FalseTrueCount() > 0) {
          hash = (37 * hash) + FP32_FALSE_TRUE_FIELD_NUMBER;
          hash = (53 * hash) + getFp32FalseTrueList().hashCode();
        }
        hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + dataType_;
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching.Control prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Control
       *&#64;&#64;
       *&#64;&#64;     A control is a signal that the sequence batcher uses to
       *&#64;&#64;     communicate with a backend.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelSequenceBatching.Control}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching.Control)
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_Control_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_Control_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelSequenceBatching.Control.class, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.Control.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          kind_ = 0;

          int32FalseTrue_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000001);
          fp32FalseTrue_ = emptyFloatList();
          bitField0_ = (bitField0_ & ~0x00000002);
          dataType_ = 0;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_Control_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control build() {
          inference.ModelConfigOuterClass.ModelSequenceBatching.Control result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control buildPartial() {
          inference.ModelConfigOuterClass.ModelSequenceBatching.Control result = new inference.ModelConfigOuterClass.ModelSequenceBatching.Control(this);
          int from_bitField0_ = bitField0_;
          result.kind_ = kind_;
          if (((bitField0_ & 0x00000001) != 0)) {
            int32FalseTrue_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.int32FalseTrue_ = int32FalseTrue_;
          if (((bitField0_ & 0x00000002) != 0)) {
            fp32FalseTrue_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.fp32FalseTrue_ = fp32FalseTrue_;
          result.dataType_ = dataType_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelSequenceBatching.Control) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelSequenceBatching.Control)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelSequenceBatching.Control other) {
          if (other == inference.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance()) return this;
          if (other.kind_ != 0) {
            setKindValue(other.getKindValue());
          }
          if (!other.int32FalseTrue_.isEmpty()) {
            if (int32FalseTrue_.isEmpty()) {
              int32FalseTrue_ = other.int32FalseTrue_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureInt32FalseTrueIsMutable();
              int32FalseTrue_.addAll(other.int32FalseTrue_);
            }
            onChanged();
          }
          if (!other.fp32FalseTrue_.isEmpty()) {
            if (fp32FalseTrue_.isEmpty()) {
              fp32FalseTrue_ = other.fp32FalseTrue_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureFp32FalseTrueIsMutable();
              fp32FalseTrue_.addAll(other.fp32FalseTrue_);
            }
            onChanged();
          }
          if (other.dataType_ != 0) {
            setDataTypeValue(other.getDataTypeValue());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelSequenceBatching.Control parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelSequenceBatching.Control) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private int kind_ = 0;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @return The enum numeric value on the wire for kind.
         */
        @java.lang.Override public int getKindValue() {
          return kind_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @param value The enum numeric value on the wire for kind to set.
         * @return This builder for chaining.
         */
        public Builder setKindValue(int value) {
          
          kind_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @return The kind.
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind getKind() {
          @SuppressWarnings("deprecation")
          inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind result = inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.valueOf(kind_);
          return result == null ? inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind.UNRECOGNIZED : result;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @param value The kind to set.
         * @return This builder for chaining.
         */
        public Builder setKind(inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Kind value) {
          if (value == null) {
            throw new NullPointerException();
          }
          
          kind_ = value.getNumber();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Kind kind
         *&#64;&#64;
         *&#64;&#64;       The kind of this control.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.ModelSequenceBatching.Control.Kind kind = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearKind() {
          
          kind_ = 0;
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.IntList int32FalseTrue_ = emptyIntList();
        private void ensureInt32FalseTrueIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            int32FalseTrue_ = mutableCopy(int32FalseTrue_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @return A list containing the int32FalseTrue.
         */
        public java.util.List<java.lang.Integer>
            getInt32FalseTrueList() {
          return ((bitField0_ & 0x00000001) != 0) ?
                   java.util.Collections.unmodifiableList(int32FalseTrue_) : int32FalseTrue_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @return The count of int32FalseTrue.
         */
        public int getInt32FalseTrueCount() {
          return int32FalseTrue_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @param index The index of the element to return.
         * @return The int32FalseTrue at the given index.
         */
        public int getInt32FalseTrue(int index) {
          return int32FalseTrue_.getInt(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @param index The index to set the value at.
         * @param value The int32FalseTrue to set.
         * @return This builder for chaining.
         */
        public Builder setInt32FalseTrue(
            int index, int value) {
          ensureInt32FalseTrueIsMutable();
          int32FalseTrue_.setInt(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @param value The int32FalseTrue to add.
         * @return This builder for chaining.
         */
        public Builder addInt32FalseTrue(int value) {
          ensureInt32FalseTrueIsMutable();
          int32FalseTrue_.addInt(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @param values The int32FalseTrue to add.
         * @return This builder for chaining.
         */
        public Builder addAllInt32FalseTrue(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensureInt32FalseTrueIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, int32FalseTrue_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 int32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in an int32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'int32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 int32_false_true = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearInt32FalseTrue() {
          int32FalseTrue_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.FloatList fp32FalseTrue_ = emptyFloatList();
        private void ensureFp32FalseTrueIsMutable() {
          if (!((bitField0_ & 0x00000002) != 0)) {
            fp32FalseTrue_ = mutableCopy(fp32FalseTrue_);
            bitField0_ |= 0x00000002;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @return A list containing the fp32FalseTrue.
         */
        public java.util.List<java.lang.Float>
            getFp32FalseTrueList() {
          return ((bitField0_ & 0x00000002) != 0) ?
                   java.util.Collections.unmodifiableList(fp32FalseTrue_) : fp32FalseTrue_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @return The count of fp32FalseTrue.
         */
        public int getFp32FalseTrueCount() {
          return fp32FalseTrue_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @param index The index of the element to return.
         * @return The fp32FalseTrue at the given index.
         */
        public float getFp32FalseTrue(int index) {
          return fp32FalseTrue_.getFloat(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @param index The index to set the value at.
         * @param value The fp32FalseTrue to set.
         * @return This builder for chaining.
         */
        public Builder setFp32FalseTrue(
            int index, float value) {
          ensureFp32FalseTrueIsMutable();
          fp32FalseTrue_.setFloat(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @param value The fp32FalseTrue to add.
         * @return This builder for chaining.
         */
        public Builder addFp32FalseTrue(float value) {
          ensureFp32FalseTrueIsMutable();
          fp32FalseTrue_.addFloat(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @param values The fp32FalseTrue to add.
         * @return This builder for chaining.
         */
        public Builder addAllFp32FalseTrue(
            java.lang.Iterable<? extends java.lang.Float> values) {
          ensureFp32FalseTrueIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, fp32FalseTrue_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float fp32_false_true (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control's true and false setting is indicated by setting
         *&#64;&#64;       a value in a fp32 tensor. The tensor must be a
         *&#64;&#64;       1-dimensional tensor with size equal to the batch size of
         *&#64;&#64;       the request. 'fp32_false_true' must have two entries: the
         *&#64;&#64;       first the false value and the second the true value.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated float fp32_false_true = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearFp32FalseTrue() {
          fp32FalseTrue_ = emptyFloatList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
          return this;
        }

        private int dataType_ = 0;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @return The enum numeric value on the wire for dataType.
         */
        @java.lang.Override public int getDataTypeValue() {
          return dataType_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @param value The enum numeric value on the wire for dataType to set.
         * @return This builder for chaining.
         */
        public Builder setDataTypeValue(int value) {
          
          dataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @return The dataType.
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.DataType getDataType() {
          @SuppressWarnings("deprecation")
          inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
          return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @param value The dataType to set.
         * @return This builder for chaining.
         */
        public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
          if (value == null) {
            throw new NullPointerException();
          }
          
          dataType_ = value.getNumber();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The control's datatype.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 4;</code>
         * @return This builder for chaining.
         */
        public Builder clearDataType() {
          
          dataType_ = 0;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching.Control)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.Control)
      private static final inference.ModelConfigOuterClass.ModelSequenceBatching.Control DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelSequenceBatching.Control();
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Control>
          PARSER = new com.google.protobuf.AbstractParser<Control>() {
        @java.lang.Override
        public Control parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Control(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Control> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Control> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.Control getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface ControlInputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching.ControlInput)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      java.lang.String getName();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> 
          getControlList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      inference.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index);
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      int getControlCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      java.util.List<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
          getControlOrBuilderList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
          int index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message ControlInput
     *&#64;&#64;
     *&#64;&#64;     The sequence control values to communicate by a model input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching.ControlInput}
     */
    public static final class ControlInput extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching.ControlInput)
        ControlInputOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use ControlInput.newBuilder() to construct.
      private ControlInput(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private ControlInput() {
        name_ = "";
        control_ = java.util.Collections.emptyList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new ControlInput();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private ControlInput(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                name_ = s;
                break;
              }
              case 18: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  control_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelSequenceBatching.Control>();
                  mutable_bitField0_ |= 0x00000001;
                }
                control_.add(
                    input.readMessage(inference.ModelConfigOuterClass.ModelSequenceBatching.Control.parser(), extensionRegistry));
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            control_ = java.util.Collections.unmodifiableList(control_);
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_ControlInput_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_ControlInput_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.class, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder.class);
      }

      public static final int NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object name_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      @java.lang.Override
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;       The name of the model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int CONTROL_FIELD_NUMBER = 2;
      private java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> control_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      @java.lang.Override
      public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> getControlList() {
        return control_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      @java.lang.Override
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
          getControlOrBuilderList() {
        return control_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      @java.lang.Override
      public int getControlCount() {
        return control_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index) {
        return control_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: Control control (repeated)
       *&#64;&#64;
       *&#64;&#64;       The control value(s) that should be communicated to the
       *&#64;&#64;       model using this model input.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
          int index) {
        return control_.get(index);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!getNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
        }
        for (int i = 0; i < control_.size(); i++) {
          output.writeMessage(2, control_.get(i));
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
        }
        for (int i = 0; i < control_.size(); i++) {
          size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(2, control_.get(i));
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput other = (inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput) obj;

        if (!getName()
            .equals(other.getName())) return false;
        if (!getControlList()
            .equals(other.getControlList())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
        if (getControlCount() > 0) {
          hash = (37 * hash) + CONTROL_FIELD_NUMBER;
          hash = (53 * hash) + getControlList().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message ControlInput
       *&#64;&#64;
       *&#64;&#64;     The sequence control values to communicate by a model input.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelSequenceBatching.ControlInput}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching.ControlInput)
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_ControlInput_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_ControlInput_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.class, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
            getControlFieldBuilder();
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          name_ = "";

          if (controlBuilder_ == null) {
            control_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            controlBuilder_.clear();
          }
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_ControlInput_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput build() {
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput buildPartial() {
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput result = new inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput(this);
          int from_bitField0_ = bitField0_;
          result.name_ = name_;
          if (controlBuilder_ == null) {
            if (((bitField0_ & 0x00000001) != 0)) {
              control_ = java.util.Collections.unmodifiableList(control_);
              bitField0_ = (bitField0_ & ~0x00000001);
            }
            result.control_ = control_;
          } else {
            result.control_ = controlBuilder_.build();
          }
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput other) {
          if (other == inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance()) return this;
          if (!other.getName().isEmpty()) {
            name_ = other.name_;
            onChanged();
          }
          if (controlBuilder_ == null) {
            if (!other.control_.isEmpty()) {
              if (control_.isEmpty()) {
                control_ = other.control_;
                bitField0_ = (bitField0_ & ~0x00000001);
              } else {
                ensureControlIsMutable();
                control_.addAll(other.control_);
              }
              onChanged();
            }
          } else {
            if (!other.control_.isEmpty()) {
              if (controlBuilder_.isEmpty()) {
                controlBuilder_.dispose();
                controlBuilder_ = null;
                control_ = other.control_;
                bitField0_ = (bitField0_ & ~0x00000001);
                controlBuilder_ = 
                  com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                     getControlFieldBuilder() : null;
              } else {
                controlBuilder_.addAllMessages(other.control_);
              }
            }
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.lang.Object name_ = "";
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The name.
         */
        public java.lang.String getName() {
          java.lang.Object ref = name_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          java.lang.Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The name to set.
         * @return This builder for chaining.
         */
        public Builder setName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          name_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearName() {
          
          name_ = getDefaultInstance().getName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string name
         *&#64;&#64;
         *&#64;&#64;       The name of the model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>string name = 1;</code>
         * @param value The bytes for name to set.
         * @return This builder for chaining.
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          name_ = value;
          onChanged();
          return this;
        }

        private java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> control_ =
          java.util.Collections.emptyList();
        private void ensureControlIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            control_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelSequenceBatching.Control>(control_);
            bitField0_ |= 0x00000001;
           }
        }

        private com.google.protobuf.RepeatedFieldBuilderV3<
            inference.ModelConfigOuterClass.ModelSequenceBatching.Control, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> controlBuilder_;

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.Control> getControlList() {
          if (controlBuilder_ == null) {
            return java.util.Collections.unmodifiableList(control_);
          } else {
            return controlBuilder_.getMessageList();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public int getControlCount() {
          if (controlBuilder_ == null) {
            return control_.size();
          } else {
            return controlBuilder_.getCount();
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control getControl(int index) {
          if (controlBuilder_ == null) {
            return control_.get(index);
          } else {
            return controlBuilder_.getMessage(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder setControl(
            int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          if (controlBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureControlIsMutable();
            control_.set(index, value);
            onChanged();
          } else {
            controlBuilder_.setMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder setControl(
            int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.set(index, builderForValue.build());
            onChanged();
          } else {
            controlBuilder_.setMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(inference.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          if (controlBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureControlIsMutable();
            control_.add(value);
            onChanged();
          } else {
            controlBuilder_.addMessage(value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control value) {
          if (controlBuilder_ == null) {
            if (value == null) {
              throw new NullPointerException();
            }
            ensureControlIsMutable();
            control_.add(index, value);
            onChanged();
          } else {
            controlBuilder_.addMessage(index, value);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.add(builderForValue.build());
            onChanged();
          } else {
            controlBuilder_.addMessage(builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addControl(
            int index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder builderForValue) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.add(index, builderForValue.build());
            onChanged();
          } else {
            controlBuilder_.addMessage(index, builderForValue.build());
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder addAllControl(
            java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.Control> values) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            com.google.protobuf.AbstractMessageLite.Builder.addAll(
                values, control_);
            onChanged();
          } else {
            controlBuilder_.addAllMessages(values);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder clearControl() {
          if (controlBuilder_ == null) {
            control_ = java.util.Collections.emptyList();
            bitField0_ = (bitField0_ & ~0x00000001);
            onChanged();
          } else {
            controlBuilder_.clear();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public Builder removeControl(int index) {
          if (controlBuilder_ == null) {
            ensureControlIsMutable();
            control_.remove(index);
            onChanged();
          } else {
            controlBuilder_.remove(index);
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder getControlBuilder(
            int index) {
          return getControlFieldBuilder().getBuilder(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder getControlOrBuilder(
            int index) {
          if (controlBuilder_ == null) {
            return control_.get(index);  } else {
            return controlBuilder_.getMessageOrBuilder(index);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public java.util.List<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
             getControlOrBuilderList() {
          if (controlBuilder_ != null) {
            return controlBuilder_.getMessageOrBuilderList();
          } else {
            return java.util.Collections.unmodifiableList(control_);
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder addControlBuilder() {
          return getControlFieldBuilder().addBuilder(
              inference.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder addControlBuilder(
            int index) {
          return getControlFieldBuilder().addBuilder(
              index, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.getDefaultInstance());
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: Control control (repeated)
         *&#64;&#64;
         *&#64;&#64;       The control value(s) that should be communicated to the
         *&#64;&#64;       model using this model input.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated .inference.ModelSequenceBatching.Control control = 2;</code>
         */
        public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder> 
             getControlBuilderList() {
          return getControlFieldBuilder().getBuilderList();
        }
        private com.google.protobuf.RepeatedFieldBuilderV3<
            inference.ModelConfigOuterClass.ModelSequenceBatching.Control, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder> 
            getControlFieldBuilder() {
          if (controlBuilder_ == null) {
            controlBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
                inference.ModelConfigOuterClass.ModelSequenceBatching.Control, inference.ModelConfigOuterClass.ModelSequenceBatching.Control.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlOrBuilder>(
                    control_,
                    ((bitField0_ & 0x00000001) != 0),
                    getParentForChildren(),
                    isClean());
            control_ = null;
          }
          return controlBuilder_;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching.ControlInput)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.ControlInput)
      private static final inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput();
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<ControlInput>
          PARSER = new com.google.protobuf.AbstractParser<ControlInput>() {
        @java.lang.Override
        public ControlInput parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new ControlInput(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<ControlInput> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<ControlInput> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface StrategyDirectOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching.StrategyDirect)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 1;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      long getMaxQueueDelayMicroseconds();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
       *&#64;&#64;
       *&#64;&#64;       The minimum slot utilization that must be satisfied to
       *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
       *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
       *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
       *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
       *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
       *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
       *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
       *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
       *&#64;&#64;       value.
       *&#64;&#64;
       * </pre>
       *
       * <code>float minimum_slot_utilization = 2;</code>
       * @return The minimumSlotUtilization.
       */
      float getMinimumSlotUtilization();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message StrategyDirect
     *&#64;&#64;
     *&#64;&#64;     The sequence batcher uses a specific, unique batch
     *&#64;&#64;     slot for each sequence. All inference requests in a
     *&#64;&#64;     sequence are directed to the same batch slot in the same
     *&#64;&#64;     model instance over the lifetime of the sequence. This
     *&#64;&#64;     is the default strategy.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching.StrategyDirect}
     */
    public static final class StrategyDirect extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching.StrategyDirect)
        StrategyDirectOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use StrategyDirect.newBuilder() to construct.
      private StrategyDirect(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private StrategyDirect() {
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new StrategyDirect();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private StrategyDirect(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                maxQueueDelayMicroseconds_ = input.readUInt64();
                break;
              }
              case 21: {

                minimumSlotUtilization_ = input.readFloat();
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyDirect_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyDirect_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.class, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder.class);
      }

      public static final int MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER = 1;
      private long maxQueueDelayMicroseconds_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 1;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      @java.lang.Override
      public long getMaxQueueDelayMicroseconds() {
        return maxQueueDelayMicroseconds_;
      }

      public static final int MINIMUM_SLOT_UTILIZATION_FIELD_NUMBER = 2;
      private float minimumSlotUtilization_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
       *&#64;&#64;
       *&#64;&#64;       The minimum slot utilization that must be satisfied to
       *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
       *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
       *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
       *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
       *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
       *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
       *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
       *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
       *&#64;&#64;       value.
       *&#64;&#64;
       * </pre>
       *
       * <code>float minimum_slot_utilization = 2;</code>
       * @return The minimumSlotUtilization.
       */
      @java.lang.Override
      public float getMinimumSlotUtilization() {
        return minimumSlotUtilization_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (maxQueueDelayMicroseconds_ != 0L) {
          output.writeUInt64(1, maxQueueDelayMicroseconds_);
        }
        if (minimumSlotUtilization_ != 0F) {
          output.writeFloat(2, minimumSlotUtilization_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (maxQueueDelayMicroseconds_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(1, maxQueueDelayMicroseconds_);
        }
        if (minimumSlotUtilization_ != 0F) {
          size += com.google.protobuf.CodedOutputStream
            .computeFloatSize(2, minimumSlotUtilization_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect other = (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) obj;

        if (getMaxQueueDelayMicroseconds()
            != other.getMaxQueueDelayMicroseconds()) return false;
        if (java.lang.Float.floatToIntBits(getMinimumSlotUtilization())
            != java.lang.Float.floatToIntBits(
                other.getMinimumSlotUtilization())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getMaxQueueDelayMicroseconds());
        hash = (37 * hash) + MINIMUM_SLOT_UTILIZATION_FIELD_NUMBER;
        hash = (53 * hash) + java.lang.Float.floatToIntBits(
            getMinimumSlotUtilization());
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message StrategyDirect
       *&#64;&#64;
       *&#64;&#64;     The sequence batcher uses a specific, unique batch
       *&#64;&#64;     slot for each sequence. All inference requests in a
       *&#64;&#64;     sequence are directed to the same batch slot in the same
       *&#64;&#64;     model instance over the lifetime of the sequence. This
       *&#64;&#64;     is the default strategy.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelSequenceBatching.StrategyDirect}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching.StrategyDirect)
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyDirect_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyDirect_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.class, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          maxQueueDelayMicroseconds_ = 0L;

          minimumSlotUtilization_ = 0F;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyDirect_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect build() {
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect buildPartial() {
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect result = new inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect(this);
          result.maxQueueDelayMicroseconds_ = maxQueueDelayMicroseconds_;
          result.minimumSlotUtilization_ = minimumSlotUtilization_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect other) {
          if (other == inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance()) return this;
          if (other.getMaxQueueDelayMicroseconds() != 0L) {
            setMaxQueueDelayMicroseconds(other.getMaxQueueDelayMicroseconds());
          }
          if (other.getMinimumSlotUtilization() != 0F) {
            setMinimumSlotUtilization(other.getMinimumSlotUtilization());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }

        private long maxQueueDelayMicroseconds_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 1;</code>
         * @return The maxQueueDelayMicroseconds.
         */
        @java.lang.Override
        public long getMaxQueueDelayMicroseconds() {
          return maxQueueDelayMicroseconds_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 1;</code>
         * @param value The maxQueueDelayMicroseconds to set.
         * @return This builder for chaining.
         */
        public Builder setMaxQueueDelayMicroseconds(long value) {
          
          maxQueueDelayMicroseconds_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the sequence batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearMaxQueueDelayMicroseconds() {
          
          maxQueueDelayMicroseconds_ = 0L;
          onChanged();
          return this;
        }

        private float minimumSlotUtilization_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
         *&#64;&#64;
         *&#64;&#64;       The minimum slot utilization that must be satisfied to
         *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
         *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
         *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
         *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
         *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
         *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
         *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
         *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
         *&#64;&#64;       value.
         *&#64;&#64;
         * </pre>
         *
         * <code>float minimum_slot_utilization = 2;</code>
         * @return The minimumSlotUtilization.
         */
        @java.lang.Override
        public float getMinimumSlotUtilization() {
          return minimumSlotUtilization_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
         *&#64;&#64;
         *&#64;&#64;       The minimum slot utilization that must be satisfied to
         *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
         *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
         *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
         *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
         *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
         *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
         *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
         *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
         *&#64;&#64;       value.
         *&#64;&#64;
         * </pre>
         *
         * <code>float minimum_slot_utilization = 2;</code>
         * @param value The minimumSlotUtilization to set.
         * @return This builder for chaining.
         */
        public Builder setMinimumSlotUtilization(float value) {
          
          minimumSlotUtilization_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: float minimum_slot_utilization
         *&#64;&#64;
         *&#64;&#64;       The minimum slot utilization that must be satisfied to
         *&#64;&#64;       execute the batch before 'max_queue_delay_microseconds' expires.
         *&#64;&#64;       For example, a value of 0.5 indicates that the batch should be
         *&#64;&#64;       executed as soon as 50% or more of the slots are ready even if
         *&#64;&#64;       the 'max_queue_delay_microseconds' timeout has not expired.
         *&#64;&#64;       The default is 0.0, indicating that a batch will be executed
         *&#64;&#64;       before 'max_queue_delay_microseconds' timeout expires if at least
         *&#64;&#64;       one batch slot is ready. 'max_queue_delay_microseconds' will be
         *&#64;&#64;       ignored unless minimum_slot_utilization is set to a non-zero
         *&#64;&#64;       value.
         *&#64;&#64;
         * </pre>
         *
         * <code>float minimum_slot_utilization = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearMinimumSlotUtilization() {
          
          minimumSlotUtilization_ = 0F;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching.StrategyDirect)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.StrategyDirect)
      private static final inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect();
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<StrategyDirect>
          PARSER = new com.google.protobuf.AbstractParser<StrategyDirect>() {
        @java.lang.Override
        public StrategyDirect parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new StrategyDirect(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<StrategyDirect> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<StrategyDirect> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface StrategyOldestOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelSequenceBatching.StrategyOldest)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
       *&#64;&#64;
       *&#64;&#64;       Maximum number of candidate sequences that the batcher
       *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
       *&#64;&#64;       and become candidates when existing candidate sequences
       *&#64;&#64;       complete.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_candidate_sequences = 1;</code>
       * @return The maxCandidateSequences.
       */
      int getMaxCandidateSequences();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @return A list containing the preferredBatchSize.
       */
      java.util.List<java.lang.Integer> getPreferredBatchSizeList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @return The count of preferredBatchSize.
       */
      int getPreferredBatchSizeCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @param index The index of the element to return.
       * @return The preferredBatchSize at the given index.
       */
      int getPreferredBatchSize(int index);

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 3;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      long getMaxQueueDelayMicroseconds();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message StrategyOldest
     *&#64;&#64;
     *&#64;&#64;     The sequence batcher maintains up to 'max_candidate_sequences'
     *&#64;&#64;     candidate sequences. 'max_candidate_sequences' can be greater
     *&#64;&#64;     than the model's 'max_batch_size'. For inferencing the batcher
     *&#64;&#64;     chooses from the candidate sequences up to 'max_batch_size'
     *&#64;&#64;     inference requests. Requests are chosen in an oldest-first
     *&#64;&#64;     manner across all candidate sequences. A given sequence is
     *&#64;&#64;     not guaranteed to be assigned to the same batch slot for
     *&#64;&#64;     all inference requests of that sequence.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching.StrategyOldest}
     */
    public static final class StrategyOldest extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelSequenceBatching.StrategyOldest)
        StrategyOldestOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use StrategyOldest.newBuilder() to construct.
      private StrategyOldest(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private StrategyOldest() {
        preferredBatchSize_ = emptyIntList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new StrategyOldest();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private StrategyOldest(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {

                maxCandidateSequences_ = input.readInt32();
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  preferredBatchSize_ = newIntList();
                  mutable_bitField0_ |= 0x00000001;
                }
                preferredBatchSize_.addInt(input.readInt32());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                  preferredBatchSize_ = newIntList();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  preferredBatchSize_.addInt(input.readInt32());
                }
                input.popLimit(limit);
                break;
              }
              case 24: {

                maxQueueDelayMicroseconds_ = input.readUInt64();
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            preferredBatchSize_.makeImmutable(); // C
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyOldest_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyOldest_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.class, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder.class);
      }

      public static final int MAX_CANDIDATE_SEQUENCES_FIELD_NUMBER = 1;
      private int maxCandidateSequences_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
       *&#64;&#64;
       *&#64;&#64;       Maximum number of candidate sequences that the batcher
       *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
       *&#64;&#64;       and become candidates when existing candidate sequences
       *&#64;&#64;       complete.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_candidate_sequences = 1;</code>
       * @return The maxCandidateSequences.
       */
      @java.lang.Override
      public int getMaxCandidateSequences() {
        return maxCandidateSequences_;
      }

      public static final int PREFERRED_BATCH_SIZE_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.IntList preferredBatchSize_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @return A list containing the preferredBatchSize.
       */
      @java.lang.Override
      public java.util.List<java.lang.Integer>
          getPreferredBatchSizeList() {
        return preferredBatchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @return The count of preferredBatchSize.
       */
      public int getPreferredBatchSizeCount() {
        return preferredBatchSize_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
       *&#64;&#64;
       *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
       *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
       *&#64;&#64;       it will be executed immediately. If not specified a
       *&#64;&#64;       preferred batch size will be chosen automatically
       *&#64;&#64;       based on model and GPU characteristics.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int32 preferred_batch_size = 2;</code>
       * @param index The index of the element to return.
       * @return The preferredBatchSize at the given index.
       */
      public int getPreferredBatchSize(int index) {
        return preferredBatchSize_.getInt(index);
      }
      private int preferredBatchSizeMemoizedSerializedSize = -1;

      public static final int MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER = 3;
      private long maxQueueDelayMicroseconds_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
       *&#64;&#64;
       *&#64;&#64;       The maximum time, in microseconds, a candidate request
       *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
       *&#64;&#64;       wait for additional requests for batching. Default is 0.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_queue_delay_microseconds = 3;</code>
       * @return The maxQueueDelayMicroseconds.
       */
      @java.lang.Override
      public long getMaxQueueDelayMicroseconds() {
        return maxQueueDelayMicroseconds_;
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (maxCandidateSequences_ != 0) {
          output.writeInt32(1, maxCandidateSequences_);
        }
        if (getPreferredBatchSizeList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(preferredBatchSizeMemoizedSerializedSize);
        }
        for (int i = 0; i < preferredBatchSize_.size(); i++) {
          output.writeInt32NoTag(preferredBatchSize_.getInt(i));
        }
        if (maxQueueDelayMicroseconds_ != 0L) {
          output.writeUInt64(3, maxQueueDelayMicroseconds_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (maxCandidateSequences_ != 0) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt32Size(1, maxCandidateSequences_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < preferredBatchSize_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt32SizeNoTag(preferredBatchSize_.getInt(i));
          }
          size += dataSize;
          if (!getPreferredBatchSizeList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          preferredBatchSizeMemoizedSerializedSize = dataSize;
        }
        if (maxQueueDelayMicroseconds_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeUInt64Size(3, maxQueueDelayMicroseconds_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest other = (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) obj;

        if (getMaxCandidateSequences()
            != other.getMaxCandidateSequences()) return false;
        if (!getPreferredBatchSizeList()
            .equals(other.getPreferredBatchSizeList())) return false;
        if (getMaxQueueDelayMicroseconds()
            != other.getMaxQueueDelayMicroseconds()) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + MAX_CANDIDATE_SEQUENCES_FIELD_NUMBER;
        hash = (53 * hash) + getMaxCandidateSequences();
        if (getPreferredBatchSizeCount() > 0) {
          hash = (37 * hash) + PREFERRED_BATCH_SIZE_FIELD_NUMBER;
          hash = (53 * hash) + getPreferredBatchSizeList().hashCode();
        }
        hash = (37 * hash) + MAX_QUEUE_DELAY_MICROSECONDS_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getMaxQueueDelayMicroseconds());
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message StrategyOldest
       *&#64;&#64;
       *&#64;&#64;     The sequence batcher maintains up to 'max_candidate_sequences'
       *&#64;&#64;     candidate sequences. 'max_candidate_sequences' can be greater
       *&#64;&#64;     than the model's 'max_batch_size'. For inferencing the batcher
       *&#64;&#64;     chooses from the candidate sequences up to 'max_batch_size'
       *&#64;&#64;     inference requests. Requests are chosen in an oldest-first
       *&#64;&#64;     manner across all candidate sequences. A given sequence is
       *&#64;&#64;     not guaranteed to be assigned to the same batch slot for
       *&#64;&#64;     all inference requests of that sequence.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelSequenceBatching.StrategyOldest}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching.StrategyOldest)
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyOldest_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyOldest_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.class, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          maxCandidateSequences_ = 0;

          preferredBatchSize_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000001);
          maxQueueDelayMicroseconds_ = 0L;

          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_StrategyOldest_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest build() {
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest buildPartial() {
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest result = new inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest(this);
          int from_bitField0_ = bitField0_;
          result.maxCandidateSequences_ = maxCandidateSequences_;
          if (((bitField0_ & 0x00000001) != 0)) {
            preferredBatchSize_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.preferredBatchSize_ = preferredBatchSize_;
          result.maxQueueDelayMicroseconds_ = maxQueueDelayMicroseconds_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest other) {
          if (other == inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance()) return this;
          if (other.getMaxCandidateSequences() != 0) {
            setMaxCandidateSequences(other.getMaxCandidateSequences());
          }
          if (!other.preferredBatchSize_.isEmpty()) {
            if (preferredBatchSize_.isEmpty()) {
              preferredBatchSize_ = other.preferredBatchSize_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensurePreferredBatchSizeIsMutable();
              preferredBatchSize_.addAll(other.preferredBatchSize_);
            }
            onChanged();
          }
          if (other.getMaxQueueDelayMicroseconds() != 0L) {
            setMaxQueueDelayMicroseconds(other.getMaxQueueDelayMicroseconds());
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private int maxCandidateSequences_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
         *&#64;&#64;
         *&#64;&#64;       Maximum number of candidate sequences that the batcher
         *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
         *&#64;&#64;       and become candidates when existing candidate sequences
         *&#64;&#64;       complete.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 max_candidate_sequences = 1;</code>
         * @return The maxCandidateSequences.
         */
        @java.lang.Override
        public int getMaxCandidateSequences() {
          return maxCandidateSequences_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
         *&#64;&#64;
         *&#64;&#64;       Maximum number of candidate sequences that the batcher
         *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
         *&#64;&#64;       and become candidates when existing candidate sequences
         *&#64;&#64;       complete.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 max_candidate_sequences = 1;</code>
         * @param value The maxCandidateSequences to set.
         * @return This builder for chaining.
         */
        public Builder setMaxCandidateSequences(int value) {
          
          maxCandidateSequences_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 max_candidate_sequences
         *&#64;&#64;
         *&#64;&#64;       Maximum number of candidate sequences that the batcher
         *&#64;&#64;       maintains. Excess seqences are kept in an ordered backlog
         *&#64;&#64;       and become candidates when existing candidate sequences
         *&#64;&#64;       complete.
         *&#64;&#64;
         * </pre>
         *
         * <code>int32 max_candidate_sequences = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearMaxCandidateSequences() {
          
          maxCandidateSequences_ = 0;
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.IntList preferredBatchSize_ = emptyIntList();
        private void ensurePreferredBatchSizeIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            preferredBatchSize_ = mutableCopy(preferredBatchSize_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @return A list containing the preferredBatchSize.
         */
        public java.util.List<java.lang.Integer>
            getPreferredBatchSizeList() {
          return ((bitField0_ & 0x00000001) != 0) ?
                   java.util.Collections.unmodifiableList(preferredBatchSize_) : preferredBatchSize_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @return The count of preferredBatchSize.
         */
        public int getPreferredBatchSizeCount() {
          return preferredBatchSize_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @param index The index of the element to return.
         * @return The preferredBatchSize at the given index.
         */
        public int getPreferredBatchSize(int index) {
          return preferredBatchSize_.getInt(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @param index The index to set the value at.
         * @param value The preferredBatchSize to set.
         * @return This builder for chaining.
         */
        public Builder setPreferredBatchSize(
            int index, int value) {
          ensurePreferredBatchSizeIsMutable();
          preferredBatchSize_.setInt(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @param value The preferredBatchSize to add.
         * @return This builder for chaining.
         */
        public Builder addPreferredBatchSize(int value) {
          ensurePreferredBatchSizeIsMutable();
          preferredBatchSize_.addInt(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @param values The preferredBatchSize to add.
         * @return This builder for chaining.
         */
        public Builder addAllPreferredBatchSize(
            java.lang.Iterable<? extends java.lang.Integer> values) {
          ensurePreferredBatchSizeIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, preferredBatchSize_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int32 preferred_batch_size (repeated)
         *&#64;&#64;
         *&#64;&#64;       Preferred batch sizes for dynamic batching of candidate
         *&#64;&#64;       sequences. If a batch of one of these sizes can be formed
         *&#64;&#64;       it will be executed immediately. If not specified a
         *&#64;&#64;       preferred batch size will be chosen automatically
         *&#64;&#64;       based on model and GPU characteristics.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int32 preferred_batch_size = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearPreferredBatchSize() {
          preferredBatchSize_ = emptyIntList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }

        private long maxQueueDelayMicroseconds_ ;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 3;</code>
         * @return The maxQueueDelayMicroseconds.
         */
        @java.lang.Override
        public long getMaxQueueDelayMicroseconds() {
          return maxQueueDelayMicroseconds_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 3;</code>
         * @param value The maxQueueDelayMicroseconds to set.
         * @return This builder for chaining.
         */
        public Builder setMaxQueueDelayMicroseconds(long value) {
          
          maxQueueDelayMicroseconds_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: uint64 max_queue_delay_microseconds
         *&#64;&#64;
         *&#64;&#64;       The maximum time, in microseconds, a candidate request
         *&#64;&#64;       will be delayed in the dynamic batch scheduling queue to
         *&#64;&#64;       wait for additional requests for batching. Default is 0.
         *&#64;&#64;
         * </pre>
         *
         * <code>uint64 max_queue_delay_microseconds = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearMaxQueueDelayMicroseconds() {
          
          maxQueueDelayMicroseconds_ = 0L;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching.StrategyOldest)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching.StrategyOldest)
      private static final inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest();
      }

      public static inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<StrategyOldest>
          PARSER = new com.google.protobuf.AbstractParser<StrategyOldest>() {
        @java.lang.Override
        public StrategyOldest parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new StrategyOldest(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<StrategyOldest> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<StrategyOldest> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    private int strategyChoiceCase_ = 0;
    private java.lang.Object strategyChoice_;
    public enum StrategyChoiceCase
        implements com.google.protobuf.Internal.EnumLite,
            com.google.protobuf.AbstractMessage.InternalOneOfEnum {
      DIRECT(3),
      OLDEST(4),
      STRATEGYCHOICE_NOT_SET(0);
      private final int value;
      private StrategyChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static StrategyChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static StrategyChoiceCase forNumber(int value) {
        switch (value) {
          case 3: return DIRECT;
          case 4: return OLDEST;
          case 0: return STRATEGYCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public StrategyChoiceCase
    getStrategyChoiceCase() {
      return StrategyChoiceCase.forNumber(
          strategyChoiceCase_);
    }

    public static final int DIRECT_FIELD_NUMBER = 3;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     * @return Whether the direct field is set.
     */
    @java.lang.Override
    public boolean hasDirect() {
      return strategyChoiceCase_ == 3;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     * @return The direct.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDirect() {
      if (strategyChoiceCase_ == 3) {
         return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyDirect direct
     *&#64;&#64;
     *&#64;&#64;       StrategyDirect scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder getDirectOrBuilder() {
      if (strategyChoiceCase_ == 3) {
         return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
    }

    public static final int OLDEST_FIELD_NUMBER = 4;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     * @return Whether the oldest field is set.
     */
    @java.lang.Override
    public boolean hasOldest() {
      return strategyChoiceCase_ == 4;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     * @return The oldest.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getOldest() {
      if (strategyChoiceCase_ == 4) {
         return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
     *&#64;&#64;
     *&#64;&#64;       StrategyOldest scheduling strategy.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder getOldestOrBuilder() {
      if (strategyChoiceCase_ == 4) {
         return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_;
      }
      return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
    }

    public static final int MAX_SEQUENCE_IDLE_MICROSECONDS_FIELD_NUMBER = 1;
    private long maxSequenceIdleMicroseconds_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
     *&#64;&#64;
     *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
     *&#64;&#64;     be idle before it is aborted. The inference server considers a
     *&#64;&#64;     sequence idle when it does not have any inference request queued
     *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
     *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
     *&#64;&#64;     available for another sequence. If not specified (or specified as
     *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint64 max_sequence_idle_microseconds = 1;</code>
     * @return The maxSequenceIdleMicroseconds.
     */
    @java.lang.Override
    public long getMaxSequenceIdleMicroseconds() {
      return maxSequenceIdleMicroseconds_;
    }

    public static final int CONTROL_INPUT_FIELD_NUMBER = 2;
    private java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> controlInput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> getControlInputList() {
      return controlInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    @java.lang.Override
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
        getControlInputOrBuilderList() {
      return controlInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    @java.lang.Override
    public int getControlInputCount() {
      return controlInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index) {
      return controlInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     sequence start, stop, ready and similar control values to the
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
        int index) {
      return controlInput_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (maxSequenceIdleMicroseconds_ != 0L) {
        output.writeUInt64(1, maxSequenceIdleMicroseconds_);
      }
      for (int i = 0; i < controlInput_.size(); i++) {
        output.writeMessage(2, controlInput_.get(i));
      }
      if (strategyChoiceCase_ == 3) {
        output.writeMessage(3, (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_);
      }
      if (strategyChoiceCase_ == 4) {
        output.writeMessage(4, (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (maxSequenceIdleMicroseconds_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, maxSequenceIdleMicroseconds_);
      }
      for (int i = 0; i < controlInput_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, controlInput_.get(i));
      }
      if (strategyChoiceCase_ == 3) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_);
      }
      if (strategyChoiceCase_ == 4) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelSequenceBatching)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelSequenceBatching other = (inference.ModelConfigOuterClass.ModelSequenceBatching) obj;

      if (getMaxSequenceIdleMicroseconds()
          != other.getMaxSequenceIdleMicroseconds()) return false;
      if (!getControlInputList()
          .equals(other.getControlInputList())) return false;
      if (!getStrategyChoiceCase().equals(other.getStrategyChoiceCase())) return false;
      switch (strategyChoiceCase_) {
        case 3:
          if (!getDirect()
              .equals(other.getDirect())) return false;
          break;
        case 4:
          if (!getOldest()
              .equals(other.getOldest())) return false;
          break;
        case 0:
        default:
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + MAX_SEQUENCE_IDLE_MICROSECONDS_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getMaxSequenceIdleMicroseconds());
      if (getControlInputCount() > 0) {
        hash = (37 * hash) + CONTROL_INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getControlInputList().hashCode();
      }
      switch (strategyChoiceCase_) {
        case 3:
          hash = (37 * hash) + DIRECT_FIELD_NUMBER;
          hash = (53 * hash) + getDirect().hashCode();
          break;
        case 4:
          hash = (37 * hash) + OLDEST_FIELD_NUMBER;
          hash = (53 * hash) + getOldest().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelSequenceBatching parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelSequenceBatching prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelSequenceBatching
     *&#64;&#64;
     *&#64;&#64;   Sequence batching configuration. These settings control how sequence
     *&#64;&#64;   batching operates for the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelSequenceBatching}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelSequenceBatching)
        inference.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelSequenceBatching.class, inference.ModelConfigOuterClass.ModelSequenceBatching.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelSequenceBatching.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getControlInputFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        maxSequenceIdleMicroseconds_ = 0L;

        if (controlInputBuilder_ == null) {
          controlInput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          controlInputBuilder_.clear();
        }
        strategyChoiceCase_ = 0;
        strategyChoice_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelSequenceBatching_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching build() {
        inference.ModelConfigOuterClass.ModelSequenceBatching result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching buildPartial() {
        inference.ModelConfigOuterClass.ModelSequenceBatching result = new inference.ModelConfigOuterClass.ModelSequenceBatching(this);
        int from_bitField0_ = bitField0_;
        if (strategyChoiceCase_ == 3) {
          if (directBuilder_ == null) {
            result.strategyChoice_ = strategyChoice_;
          } else {
            result.strategyChoice_ = directBuilder_.build();
          }
        }
        if (strategyChoiceCase_ == 4) {
          if (oldestBuilder_ == null) {
            result.strategyChoice_ = strategyChoice_;
          } else {
            result.strategyChoice_ = oldestBuilder_.build();
          }
        }
        result.maxSequenceIdleMicroseconds_ = maxSequenceIdleMicroseconds_;
        if (controlInputBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            controlInput_ = java.util.Collections.unmodifiableList(controlInput_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.controlInput_ = controlInput_;
        } else {
          result.controlInput_ = controlInputBuilder_.build();
        }
        result.strategyChoiceCase_ = strategyChoiceCase_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelSequenceBatching) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelSequenceBatching)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelSequenceBatching other) {
        if (other == inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance()) return this;
        if (other.getMaxSequenceIdleMicroseconds() != 0L) {
          setMaxSequenceIdleMicroseconds(other.getMaxSequenceIdleMicroseconds());
        }
        if (controlInputBuilder_ == null) {
          if (!other.controlInput_.isEmpty()) {
            if (controlInput_.isEmpty()) {
              controlInput_ = other.controlInput_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureControlInputIsMutable();
              controlInput_.addAll(other.controlInput_);
            }
            onChanged();
          }
        } else {
          if (!other.controlInput_.isEmpty()) {
            if (controlInputBuilder_.isEmpty()) {
              controlInputBuilder_.dispose();
              controlInputBuilder_ = null;
              controlInput_ = other.controlInput_;
              bitField0_ = (bitField0_ & ~0x00000001);
              controlInputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getControlInputFieldBuilder() : null;
            } else {
              controlInputBuilder_.addAllMessages(other.controlInput_);
            }
          }
        }
        switch (other.getStrategyChoiceCase()) {
          case DIRECT: {
            mergeDirect(other.getDirect());
            break;
          }
          case OLDEST: {
            mergeOldest(other.getOldest());
            break;
          }
          case STRATEGYCHOICE_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelSequenceBatching parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelSequenceBatching) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int strategyChoiceCase_ = 0;
      private java.lang.Object strategyChoice_;
      public StrategyChoiceCase
          getStrategyChoiceCase() {
        return StrategyChoiceCase.forNumber(
            strategyChoiceCase_);
      }

      public Builder clearStrategyChoice() {
        strategyChoiceCase_ = 0;
        strategyChoice_ = null;
        onChanged();
        return this;
      }

      private int bitField0_;

      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder> directBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       * @return Whether the direct field is set.
       */
      @java.lang.Override
      public boolean hasDirect() {
        return strategyChoiceCase_ == 3;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       * @return The direct.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect getDirect() {
        if (directBuilder_ == null) {
          if (strategyChoiceCase_ == 3) {
            return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
        } else {
          if (strategyChoiceCase_ == 3) {
            return directBuilder_.getMessage();
          }
          return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder setDirect(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect value) {
        if (directBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          strategyChoice_ = value;
          onChanged();
        } else {
          directBuilder_.setMessage(value);
        }
        strategyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder setDirect(
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder builderForValue) {
        if (directBuilder_ == null) {
          strategyChoice_ = builderForValue.build();
          onChanged();
        } else {
          directBuilder_.setMessage(builderForValue.build());
        }
        strategyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder mergeDirect(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect value) {
        if (directBuilder_ == null) {
          if (strategyChoiceCase_ == 3 &&
              strategyChoice_ != inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance()) {
            strategyChoice_ = inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.newBuilder((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            strategyChoice_ = value;
          }
          onChanged();
        } else {
          if (strategyChoiceCase_ == 3) {
            directBuilder_.mergeFrom(value);
          }
          directBuilder_.setMessage(value);
        }
        strategyChoiceCase_ = 3;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public Builder clearDirect() {
        if (directBuilder_ == null) {
          if (strategyChoiceCase_ == 3) {
            strategyChoiceCase_ = 0;
            strategyChoice_ = null;
            onChanged();
          }
        } else {
          if (strategyChoiceCase_ == 3) {
            strategyChoiceCase_ = 0;
            strategyChoice_ = null;
          }
          directBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder getDirectBuilder() {
        return getDirectFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder getDirectOrBuilder() {
        if ((strategyChoiceCase_ == 3) && (directBuilder_ != null)) {
          return directBuilder_.getMessageOrBuilder();
        } else {
          if (strategyChoiceCase_ == 3) {
            return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyDirect direct
       *&#64;&#64;
       *&#64;&#64;       StrategyDirect scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyDirect direct = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder> 
          getDirectFieldBuilder() {
        if (directBuilder_ == null) {
          if (!(strategyChoiceCase_ == 3)) {
            strategyChoice_ = inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.getDefaultInstance();
          }
          directBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirectOrBuilder>(
                  (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyDirect) strategyChoice_,
                  getParentForChildren(),
                  isClean());
          strategyChoice_ = null;
        }
        strategyChoiceCase_ = 3;
        onChanged();;
        return directBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder> oldestBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       * @return Whether the oldest field is set.
       */
      @java.lang.Override
      public boolean hasOldest() {
        return strategyChoiceCase_ == 4;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       * @return The oldest.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest getOldest() {
        if (oldestBuilder_ == null) {
          if (strategyChoiceCase_ == 4) {
            return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
        } else {
          if (strategyChoiceCase_ == 4) {
            return oldestBuilder_.getMessage();
          }
          return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder setOldest(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest value) {
        if (oldestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          strategyChoice_ = value;
          onChanged();
        } else {
          oldestBuilder_.setMessage(value);
        }
        strategyChoiceCase_ = 4;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder setOldest(
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder builderForValue) {
        if (oldestBuilder_ == null) {
          strategyChoice_ = builderForValue.build();
          onChanged();
        } else {
          oldestBuilder_.setMessage(builderForValue.build());
        }
        strategyChoiceCase_ = 4;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder mergeOldest(inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest value) {
        if (oldestBuilder_ == null) {
          if (strategyChoiceCase_ == 4 &&
              strategyChoice_ != inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance()) {
            strategyChoice_ = inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.newBuilder((inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            strategyChoice_ = value;
          }
          onChanged();
        } else {
          if (strategyChoiceCase_ == 4) {
            oldestBuilder_.mergeFrom(value);
          }
          oldestBuilder_.setMessage(value);
        }
        strategyChoiceCase_ = 4;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public Builder clearOldest() {
        if (oldestBuilder_ == null) {
          if (strategyChoiceCase_ == 4) {
            strategyChoiceCase_ = 0;
            strategyChoice_ = null;
            onChanged();
          }
        } else {
          if (strategyChoiceCase_ == 4) {
            strategyChoiceCase_ = 0;
            strategyChoice_ = null;
          }
          oldestBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder getOldestBuilder() {
        return getOldestFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder getOldestOrBuilder() {
        if ((strategyChoiceCase_ == 4) && (oldestBuilder_ != null)) {
          return oldestBuilder_.getMessageOrBuilder();
        } else {
          if (strategyChoiceCase_ == 4) {
            return (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_;
          }
          return inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: StrategyOldest oldest
       *&#64;&#64;
       *&#64;&#64;       StrategyOldest scheduling strategy.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching.StrategyOldest oldest = 4;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder> 
          getOldestFieldBuilder() {
        if (oldestBuilder_ == null) {
          if (!(strategyChoiceCase_ == 4)) {
            strategyChoice_ = inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.getDefaultInstance();
          }
          oldestBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldestOrBuilder>(
                  (inference.ModelConfigOuterClass.ModelSequenceBatching.StrategyOldest) strategyChoice_,
                  getParentForChildren(),
                  isClean());
          strategyChoice_ = null;
        }
        strategyChoiceCase_ = 4;
        onChanged();;
        return oldestBuilder_;
      }

      private long maxSequenceIdleMicroseconds_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       * @return The maxSequenceIdleMicroseconds.
       */
      @java.lang.Override
      public long getMaxSequenceIdleMicroseconds() {
        return maxSequenceIdleMicroseconds_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       * @param value The maxSequenceIdleMicroseconds to set.
       * @return This builder for chaining.
       */
      public Builder setMaxSequenceIdleMicroseconds(long value) {
        
        maxSequenceIdleMicroseconds_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint64 max_sequence_idle_microseconds
       *&#64;&#64;
       *&#64;&#64;     The maximum time, in microseconds, that a sequence is allowed to
       *&#64;&#64;     be idle before it is aborted. The inference server considers a
       *&#64;&#64;     sequence idle when it does not have any inference request queued
       *&#64;&#64;     for the sequence. If this limit is exceeded, the inference server
       *&#64;&#64;     will free the sequence slot allocated by the sequence and make it
       *&#64;&#64;     available for another sequence. If not specified (or specified as
       *&#64;&#64;     zero) a default value of 1000000 (1 second) is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint64 max_sequence_idle_microseconds = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxSequenceIdleMicroseconds() {
        
        maxSequenceIdleMicroseconds_ = 0L;
        onChanged();
        return this;
      }

      private java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> controlInput_ =
        java.util.Collections.emptyList();
      private void ensureControlInputIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          controlInput_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput>(controlInput_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> controlInputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> getControlInputList() {
        if (controlInputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(controlInput_);
        } else {
          return controlInputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public int getControlInputCount() {
        if (controlInputBuilder_ == null) {
          return controlInput_.size();
        } else {
          return controlInputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput getControlInput(int index) {
        if (controlInputBuilder_ == null) {
          return controlInput_.get(index);
        } else {
          return controlInputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder setControlInput(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        if (controlInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureControlInputIsMutable();
          controlInput_.set(index, value);
          onChanged();
        } else {
          controlInputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder setControlInput(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.set(index, builderForValue.build());
          onChanged();
        } else {
          controlInputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        if (controlInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureControlInputIsMutable();
          controlInput_.add(value);
          onChanged();
        } else {
          controlInputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput value) {
        if (controlInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureControlInputIsMutable();
          controlInput_.add(index, value);
          onChanged();
        } else {
          controlInputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.add(builderForValue.build());
          onChanged();
        } else {
          controlInputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addControlInput(
          int index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder builderForValue) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.add(index, builderForValue.build());
          onChanged();
        } else {
          controlInputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder addAllControlInput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput> values) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, controlInput_);
          onChanged();
        } else {
          controlInputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder clearControlInput() {
        if (controlInputBuilder_ == null) {
          controlInput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          controlInputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public Builder removeControlInput(int index) {
        if (controlInputBuilder_ == null) {
          ensureControlInputIsMutable();
          controlInput_.remove(index);
          onChanged();
        } else {
          controlInputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder getControlInputBuilder(
          int index) {
        return getControlInputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder getControlInputOrBuilder(
          int index) {
        if (controlInputBuilder_ == null) {
          return controlInput_.get(index);  } else {
          return controlInputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
           getControlInputOrBuilderList() {
        if (controlInputBuilder_ != null) {
          return controlInputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(controlInput_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder addControlInputBuilder() {
        return getControlInputFieldBuilder().addBuilder(
            inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder addControlInputBuilder(
          int index) {
        return getControlInputFieldBuilder().addBuilder(
            index, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ControlInput control_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     sequence start, stop, ready and similar control values to the
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelSequenceBatching.ControlInput control_input = 2;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder> 
           getControlInputBuilderList() {
        return getControlInputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder> 
          getControlInputFieldBuilder() {
        if (controlInputBuilder_ == null) {
          controlInputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInput.Builder, inference.ModelConfigOuterClass.ModelSequenceBatching.ControlInputOrBuilder>(
                  controlInput_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          controlInput_ = null;
        }
        return controlInputBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelSequenceBatching)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelSequenceBatching)
    private static final inference.ModelConfigOuterClass.ModelSequenceBatching DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelSequenceBatching();
    }

    public static inference.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelSequenceBatching>
        PARSER = new com.google.protobuf.AbstractParser<ModelSequenceBatching>() {
      @java.lang.Override
      public ModelSequenceBatching parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelSequenceBatching(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelSequenceBatching> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelSequenceBatching> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelEnsemblingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelEnsembling)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelEnsembling.Step> 
        getStepList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    inference.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    int getStepCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    java.util.List<? extends inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
        getStepOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
        int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelEnsembling
   *&#64;&#64;
   *&#64;&#64;   Model ensembling configuration. These settings specify the models that
   *&#64;&#64;   compose the ensemble and how data flows between the models.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelEnsembling}
   */
  public static final class ModelEnsembling extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelEnsembling)
      ModelEnsemblingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelEnsembling.newBuilder() to construct.
    private ModelEnsembling(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelEnsembling() {
      step_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelEnsembling();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelEnsembling(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                step_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelEnsembling.Step>();
                mutable_bitField0_ |= 0x00000001;
              }
              step_.add(
                  input.readMessage(inference.ModelConfigOuterClass.ModelEnsembling.Step.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          step_ = java.util.Collections.unmodifiableList(step_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelEnsembling.class, inference.ModelConfigOuterClass.ModelEnsembling.Builder.class);
    }

    public interface StepOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelEnsembling.Step)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @return The modelName.
       */
      java.lang.String getModelName();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @return The bytes for modelName.
       */
      com.google.protobuf.ByteString
          getModelNameBytes();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       * @return The modelVersion.
       */
      long getModelVersion();

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      int getInputMapCount();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      boolean containsInputMap(
          java.lang.String key);
      /**
       * Use {@link #getInputMapMap()} instead.
       */
      @java.lang.Deprecated
      java.util.Map<java.lang.String, java.lang.String>
      getInputMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      java.util.Map<java.lang.String, java.lang.String>
      getInputMapMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      java.lang.String getInputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue);
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      java.lang.String getInputMapOrThrow(
          java.lang.String key);

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      int getOutputMapCount();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      boolean containsOutputMap(
          java.lang.String key);
      /**
       * Use {@link #getOutputMapMap()} instead.
       */
      @java.lang.Deprecated
      java.util.Map<java.lang.String, java.lang.String>
      getOutputMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      java.util.Map<java.lang.String, java.lang.String>
      getOutputMapMap();
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      java.lang.String getOutputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue);
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      java.lang.String getOutputMapOrThrow(
          java.lang.String key);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: message Step
     *&#64;&#64;
     *&#64;&#64;     Each step specifies a model included in the ensemble,
     *&#64;&#64;     maps ensemble tensor names to the model input tensors,
     *&#64;&#64;     and maps model output tensors to ensemble tensor names
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelEnsembling.Step}
     */
    public static final class Step extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelEnsembling.Step)
        StepOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Step.newBuilder() to construct.
      private Step(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Step() {
        modelName_ = "";
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Step();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Step(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                java.lang.String s = input.readStringRequireUtf8();

                modelName_ = s;
                break;
              }
              case 16: {

                modelVersion_ = input.readInt64();
                break;
              }
              case 26: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  inputMap_ = com.google.protobuf.MapField.newMapField(
                      InputMapDefaultEntryHolder.defaultEntry);
                  mutable_bitField0_ |= 0x00000001;
                }
                com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
                inputMap__ = input.readMessage(
                    InputMapDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                inputMap_.getMutableMap().put(
                    inputMap__.getKey(), inputMap__.getValue());
                break;
              }
              case 34: {
                if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                  outputMap_ = com.google.protobuf.MapField.newMapField(
                      OutputMapDefaultEntryHolder.defaultEntry);
                  mutable_bitField0_ |= 0x00000002;
                }
                com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
                outputMap__ = input.readMessage(
                    OutputMapDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                outputMap_.getMutableMap().put(
                    outputMap__.getKey(), outputMap__.getValue());
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_Step_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      @java.lang.Override
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetInputMap();
          case 4:
            return internalGetOutputMap();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_Step_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelEnsembling.Step.class, inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder.class);
      }

      public static final int MODEL_NAME_FIELD_NUMBER = 1;
      private volatile java.lang.Object modelName_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @return The modelName.
       */
      @java.lang.Override
      public java.lang.String getModelName() {
        java.lang.Object ref = modelName_;
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          modelName_ = s;
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string model_name
       *&#64;&#64;
       *&#64;&#64;     The name of the model to execute for this step of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>string model_name = 1;</code>
       * @return The bytes for modelName.
       */
      @java.lang.Override
      public com.google.protobuf.ByteString
          getModelNameBytes() {
        java.lang.Object ref = modelName_;
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          modelName_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int MODEL_VERSION_FIELD_NUMBER = 2;
      private long modelVersion_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int64 model_version
       *&#64;&#64;
       *&#64;&#64;     The version of the model to use for inference. If -1
       *&#64;&#64;     the latest/most-recent version of the model is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>int64 model_version = 2;</code>
       * @return The modelVersion.
       */
      @java.lang.Override
      public long getModelVersion() {
        return modelVersion_;
      }

      public static final int INPUT_MAP_FIELD_NUMBER = 3;
      private static final class InputMapDefaultEntryHolder {
        static final com.google.protobuf.MapEntry<
            java.lang.String, java.lang.String> defaultEntry =
                com.google.protobuf.MapEntry
                .<java.lang.String, java.lang.String>newDefaultInstance(
                    inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_Step_InputMapEntry_descriptor, 
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "");
      }
      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> inputMap_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetInputMap() {
        if (inputMap_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              InputMapDefaultEntryHolder.defaultEntry);
        }
        return inputMap_;
      }

      public int getInputMapCount() {
        return internalGetInputMap().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */

      @java.lang.Override
      public boolean containsInputMap(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetInputMap().getMap().containsKey(key);
      }
      /**
       * Use {@link #getInputMapMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getInputMap() {
        return getInputMapMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getInputMapMap() {
        return internalGetInputMap().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      @java.lang.Override

      public java.lang.String getInputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetInputMap().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
       *&#64;&#64;     shape as the model input. Each model input must be assigned to
       *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
       *&#64;&#64;     to multiple model inputs.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; input_map = 3;</code>
       */
      @java.lang.Override

      public java.lang.String getInputMapOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetInputMap().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public static final int OUTPUT_MAP_FIELD_NUMBER = 4;
      private static final class OutputMapDefaultEntryHolder {
        static final com.google.protobuf.MapEntry<
            java.lang.String, java.lang.String> defaultEntry =
                com.google.protobuf.MapEntry
                .<java.lang.String, java.lang.String>newDefaultInstance(
                    inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_Step_OutputMapEntry_descriptor, 
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "");
      }
      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> outputMap_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetOutputMap() {
        if (outputMap_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              OutputMapDefaultEntryHolder.defaultEntry);
        }
        return outputMap_;
      }

      public int getOutputMapCount() {
        return internalGetOutputMap().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */

      @java.lang.Override
      public boolean containsOutputMap(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetOutputMap().getMap().containsKey(key);
      }
      /**
       * Use {@link #getOutputMapMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getOutputMap() {
        return getOutputMapMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getOutputMapMap() {
        return internalGetOutputMap().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      @java.lang.Override

      public java.lang.String getOutputMapOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetOutputMap().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
       *&#64;&#64;
       *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
       *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
       *&#64;&#64;     be inferred from the model output. It is optional to assign all
       *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
       *&#64;&#64;     can appear in an output map only once.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; output_map = 4;</code>
       */
      @java.lang.Override

      public java.lang.String getOutputMapOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetOutputMap().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!getModelNameBytes().isEmpty()) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, modelName_);
        }
        if (modelVersion_ != 0L) {
          output.writeInt64(2, modelVersion_);
        }
        com.google.protobuf.GeneratedMessageV3
          .serializeStringMapTo(
            output,
            internalGetInputMap(),
            InputMapDefaultEntryHolder.defaultEntry,
            3);
        com.google.protobuf.GeneratedMessageV3
          .serializeStringMapTo(
            output,
            internalGetOutputMap(),
            OutputMapDefaultEntryHolder.defaultEntry,
            4);
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!getModelNameBytes().isEmpty()) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, modelName_);
        }
        if (modelVersion_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt64Size(2, modelVersion_);
        }
        for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
             : internalGetInputMap().getMap().entrySet()) {
          com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
          inputMap__ = InputMapDefaultEntryHolder.defaultEntry.newBuilderForType()
              .setKey(entry.getKey())
              .setValue(entry.getValue())
              .build();
          size += com.google.protobuf.CodedOutputStream
              .computeMessageSize(3, inputMap__);
        }
        for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
             : internalGetOutputMap().getMap().entrySet()) {
          com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
          outputMap__ = OutputMapDefaultEntryHolder.defaultEntry.newBuilderForType()
              .setKey(entry.getKey())
              .setValue(entry.getValue())
              .build();
          size += com.google.protobuf.CodedOutputStream
              .computeMessageSize(4, outputMap__);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelEnsembling.Step)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelEnsembling.Step other = (inference.ModelConfigOuterClass.ModelEnsembling.Step) obj;

        if (!getModelName()
            .equals(other.getModelName())) return false;
        if (getModelVersion()
            != other.getModelVersion()) return false;
        if (!internalGetInputMap().equals(
            other.internalGetInputMap())) return false;
        if (!internalGetOutputMap().equals(
            other.internalGetOutputMap())) return false;
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + MODEL_NAME_FIELD_NUMBER;
        hash = (53 * hash) + getModelName().hashCode();
        hash = (37 * hash) + MODEL_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getModelVersion());
        if (!internalGetInputMap().getMap().isEmpty()) {
          hash = (37 * hash) + INPUT_MAP_FIELD_NUMBER;
          hash = (53 * hash) + internalGetInputMap().hashCode();
        }
        if (!internalGetOutputMap().getMap().isEmpty()) {
          hash = (37 * hash) + OUTPUT_MAP_FIELD_NUMBER;
          hash = (53 * hash) + internalGetOutputMap().hashCode();
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelEnsembling.Step parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelEnsembling.Step prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: message Step
       *&#64;&#64;
       *&#64;&#64;     Each step specifies a model included in the ensemble,
       *&#64;&#64;     maps ensemble tensor names to the model input tensors,
       *&#64;&#64;     and maps model output tensors to ensemble tensor names
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelEnsembling.Step}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelEnsembling.Step)
          inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_Step_descriptor;
        }

        @SuppressWarnings({"rawtypes"})
        protected com.google.protobuf.MapField internalGetMapField(
            int number) {
          switch (number) {
            case 3:
              return internalGetInputMap();
            case 4:
              return internalGetOutputMap();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @SuppressWarnings({"rawtypes"})
        protected com.google.protobuf.MapField internalGetMutableMapField(
            int number) {
          switch (number) {
            case 3:
              return internalGetMutableInputMap();
            case 4:
              return internalGetMutableOutputMap();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_Step_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelEnsembling.Step.class, inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelEnsembling.Step.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          modelName_ = "";

          modelVersion_ = 0L;

          internalGetMutableInputMap().clear();
          internalGetMutableOutputMap().clear();
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_Step_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelEnsembling.Step build() {
          inference.ModelConfigOuterClass.ModelEnsembling.Step result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelEnsembling.Step buildPartial() {
          inference.ModelConfigOuterClass.ModelEnsembling.Step result = new inference.ModelConfigOuterClass.ModelEnsembling.Step(this);
          int from_bitField0_ = bitField0_;
          result.modelName_ = modelName_;
          result.modelVersion_ = modelVersion_;
          result.inputMap_ = internalGetInputMap();
          result.inputMap_.makeImmutable();
          result.outputMap_ = internalGetOutputMap();
          result.outputMap_.makeImmutable();
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelEnsembling.Step) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelEnsembling.Step)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelEnsembling.Step other) {
          if (other == inference.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance()) return this;
          if (!other.getModelName().isEmpty()) {
            modelName_ = other.modelName_;
            onChanged();
          }
          if (other.getModelVersion() != 0L) {
            setModelVersion(other.getModelVersion());
          }
          internalGetMutableInputMap().mergeFrom(
              other.internalGetInputMap());
          internalGetMutableOutputMap().mergeFrom(
              other.internalGetOutputMap());
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelEnsembling.Step parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelEnsembling.Step) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int bitField0_;

        private java.lang.Object modelName_ = "";
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @return The modelName.
         */
        public java.lang.String getModelName() {
          java.lang.Object ref = modelName_;
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            modelName_ = s;
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @return The bytes for modelName.
         */
        public com.google.protobuf.ByteString
            getModelNameBytes() {
          java.lang.Object ref = modelName_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            modelName_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @param value The modelName to set.
         * @return This builder for chaining.
         */
        public Builder setModelName(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  
          modelName_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearModelName() {
          
          modelName_ = getDefaultInstance().getModelName();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: string model_name
         *&#64;&#64;
         *&#64;&#64;     The name of the model to execute for this step of the ensemble.
         *&#64;&#64;
         * </pre>
         *
         * <code>string model_name = 1;</code>
         * @param value The bytes for modelName to set.
         * @return This builder for chaining.
         */
        public Builder setModelNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          
          modelName_ = value;
          onChanged();
          return this;
        }

        private long modelVersion_ ;
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         * @return The modelVersion.
         */
        @java.lang.Override
        public long getModelVersion() {
          return modelVersion_;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         * @param value The modelVersion to set.
         * @return This builder for chaining.
         */
        public Builder setModelVersion(long value) {
          
          modelVersion_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: int64 model_version
         *&#64;&#64;
         *&#64;&#64;     The version of the model to use for inference. If -1
         *&#64;&#64;     the latest/most-recent version of the model is used.
         *&#64;&#64;
         * </pre>
         *
         * <code>int64 model_version = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearModelVersion() {
          
          modelVersion_ = 0L;
          onChanged();
          return this;
        }

        private com.google.protobuf.MapField<
            java.lang.String, java.lang.String> inputMap_;
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetInputMap() {
          if (inputMap_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                InputMapDefaultEntryHolder.defaultEntry);
          }
          return inputMap_;
        }
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetMutableInputMap() {
          onChanged();;
          if (inputMap_ == null) {
            inputMap_ = com.google.protobuf.MapField.newMapField(
                InputMapDefaultEntryHolder.defaultEntry);
          }
          if (!inputMap_.isMutable()) {
            inputMap_ = inputMap_.copy();
          }
          return inputMap_;
        }

        public int getInputMapCount() {
          return internalGetInputMap().getMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        @java.lang.Override
        public boolean containsInputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          return internalGetInputMap().getMap().containsKey(key);
        }
        /**
         * Use {@link #getInputMapMap()} instead.
         */
        @java.lang.Override
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getInputMap() {
          return getInputMapMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        @java.lang.Override

        public java.util.Map<java.lang.String, java.lang.String> getInputMapMap() {
          return internalGetInputMap().getMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        @java.lang.Override

        public java.lang.String getInputMapOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetInputMap().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        @java.lang.Override

        public java.lang.String getInputMapOrThrow(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetInputMap().getMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }

        public Builder clearInputMap() {
          internalGetMutableInputMap().getMutableMap()
              .clear();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public Builder removeInputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableInputMap().getMutableMap()
              .remove(key);
          return this;
        }
        /**
         * Use alternate mutation accessors instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String>
        getMutableInputMap() {
          return internalGetMutableInputMap().getMutableMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */
        public Builder putInputMap(
            java.lang.String key,
            java.lang.String value) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          if (value == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableInputMap().getMutableMap()
              .put(key, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; input_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an input tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The ensemble tensor must have the same data type and
         *&#64;&#64;     shape as the model input. Each model input must be assigned to
         *&#64;&#64;     one ensemble tensor, but the same ensemble tensor can be assigned
         *&#64;&#64;     to multiple model inputs.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; input_map = 3;</code>
         */

        public Builder putAllInputMap(
            java.util.Map<java.lang.String, java.lang.String> values) {
          internalGetMutableInputMap().getMutableMap()
              .putAll(values);
          return this;
        }

        private com.google.protobuf.MapField<
            java.lang.String, java.lang.String> outputMap_;
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetOutputMap() {
          if (outputMap_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                OutputMapDefaultEntryHolder.defaultEntry);
          }
          return outputMap_;
        }
        private com.google.protobuf.MapField<java.lang.String, java.lang.String>
        internalGetMutableOutputMap() {
          onChanged();;
          if (outputMap_ == null) {
            outputMap_ = com.google.protobuf.MapField.newMapField(
                OutputMapDefaultEntryHolder.defaultEntry);
          }
          if (!outputMap_.isMutable()) {
            outputMap_ = outputMap_.copy();
          }
          return outputMap_;
        }

        public int getOutputMapCount() {
          return internalGetOutputMap().getMap().size();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        @java.lang.Override
        public boolean containsOutputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          return internalGetOutputMap().getMap().containsKey(key);
        }
        /**
         * Use {@link #getOutputMapMap()} instead.
         */
        @java.lang.Override
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String> getOutputMap() {
          return getOutputMapMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        @java.lang.Override

        public java.util.Map<java.lang.String, java.lang.String> getOutputMapMap() {
          return internalGetOutputMap().getMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        @java.lang.Override

        public java.lang.String getOutputMapOrDefault(
            java.lang.String key,
            java.lang.String defaultValue) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetOutputMap().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        @java.lang.Override

        public java.lang.String getOutputMapOrThrow(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          java.util.Map<java.lang.String, java.lang.String> map =
              internalGetOutputMap().getMap();
          if (!map.containsKey(key)) {
            throw new java.lang.IllegalArgumentException();
          }
          return map.get(key);
        }

        public Builder clearOutputMap() {
          internalGetMutableOutputMap().getMutableMap()
              .clear();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public Builder removeOutputMap(
            java.lang.String key) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableOutputMap().getMutableMap()
              .remove(key);
          return this;
        }
        /**
         * Use alternate mutation accessors instead.
         */
        @java.lang.Deprecated
        public java.util.Map<java.lang.String, java.lang.String>
        getMutableOutputMap() {
          return internalGetMutableOutputMap().getMutableMap();
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */
        public Builder putOutputMap(
            java.lang.String key,
            java.lang.String value) {
          if (key == null) { throw new java.lang.NullPointerException(); }
          if (value == null) { throw new java.lang.NullPointerException(); }
          internalGetMutableOutputMap().getMutableMap()
              .put(key, value);
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; output_map
         *&#64;&#64;
         *&#64;&#64;     Map from name of an output tensor on this step's model to ensemble
         *&#64;&#64;     tensor name. The data type and shape of the ensemble tensor will
         *&#64;&#64;     be inferred from the model output. It is optional to assign all
         *&#64;&#64;     model outputs to ensemble tensors. One ensemble tensor name
         *&#64;&#64;     can appear in an output map only once.
         *&#64;&#64;
         * </pre>
         *
         * <code>map&lt;string, string&gt; output_map = 4;</code>
         */

        public Builder putAllOutputMap(
            java.util.Map<java.lang.String, java.lang.String> values) {
          internalGetMutableOutputMap().getMutableMap()
              .putAll(values);
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelEnsembling.Step)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelEnsembling.Step)
      private static final inference.ModelConfigOuterClass.ModelEnsembling.Step DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelEnsembling.Step();
      }

      public static inference.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Step>
          PARSER = new com.google.protobuf.AbstractParser<Step>() {
        @java.lang.Override
        public Step parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Step(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Step> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Step> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelEnsembling.Step getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int STEP_FIELD_NUMBER = 1;
    private java.util.List<inference.ModelConfigOuterClass.ModelEnsembling.Step> step_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelEnsembling.Step> getStepList() {
      return step_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    @java.lang.Override
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
        getStepOrBuilderList() {
      return step_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    @java.lang.Override
    public int getStepCount() {
      return step_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index) {
      return step_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: Step step (repeated)
     *&#64;&#64;
     *&#64;&#64;     The models and the input / output mappings used within the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
        int index) {
      return step_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < step_.size(); i++) {
        output.writeMessage(1, step_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < step_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, step_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelEnsembling)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelEnsembling other = (inference.ModelConfigOuterClass.ModelEnsembling) obj;

      if (!getStepList()
          .equals(other.getStepList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getStepCount() > 0) {
        hash = (37 * hash) + STEP_FIELD_NUMBER;
        hash = (53 * hash) + getStepList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelEnsembling parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelEnsembling prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelEnsembling
     *&#64;&#64;
     *&#64;&#64;   Model ensembling configuration. These settings specify the models that
     *&#64;&#64;   compose the ensemble and how data flows between the models.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelEnsembling}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelEnsembling)
        inference.ModelConfigOuterClass.ModelEnsemblingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelEnsembling.class, inference.ModelConfigOuterClass.ModelEnsembling.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelEnsembling.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getStepFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        if (stepBuilder_ == null) {
          step_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          stepBuilder_.clear();
        }
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelEnsembling_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelEnsembling getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelEnsembling build() {
        inference.ModelConfigOuterClass.ModelEnsembling result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelEnsembling buildPartial() {
        inference.ModelConfigOuterClass.ModelEnsembling result = new inference.ModelConfigOuterClass.ModelEnsembling(this);
        int from_bitField0_ = bitField0_;
        if (stepBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            step_ = java.util.Collections.unmodifiableList(step_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.step_ = step_;
        } else {
          result.step_ = stepBuilder_.build();
        }
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelEnsembling) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelEnsembling)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelEnsembling other) {
        if (other == inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance()) return this;
        if (stepBuilder_ == null) {
          if (!other.step_.isEmpty()) {
            if (step_.isEmpty()) {
              step_ = other.step_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureStepIsMutable();
              step_.addAll(other.step_);
            }
            onChanged();
          }
        } else {
          if (!other.step_.isEmpty()) {
            if (stepBuilder_.isEmpty()) {
              stepBuilder_.dispose();
              stepBuilder_ = null;
              step_ = other.step_;
              bitField0_ = (bitField0_ & ~0x00000001);
              stepBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getStepFieldBuilder() : null;
            } else {
              stepBuilder_.addAllMessages(other.step_);
            }
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelEnsembling parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelEnsembling) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.util.List<inference.ModelConfigOuterClass.ModelEnsembling.Step> step_ =
        java.util.Collections.emptyList();
      private void ensureStepIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          step_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelEnsembling.Step>(step_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelEnsembling.Step, inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder, inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> stepBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelEnsembling.Step> getStepList() {
        if (stepBuilder_ == null) {
          return java.util.Collections.unmodifiableList(step_);
        } else {
          return stepBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public int getStepCount() {
        if (stepBuilder_ == null) {
          return step_.size();
        } else {
          return stepBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelEnsembling.Step getStep(int index) {
        if (stepBuilder_ == null) {
          return step_.get(index);
        } else {
          return stepBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder setStep(
          int index, inference.ModelConfigOuterClass.ModelEnsembling.Step value) {
        if (stepBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStepIsMutable();
          step_.set(index, value);
          onChanged();
        } else {
          stepBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder setStep(
          int index, inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.set(index, builderForValue.build());
          onChanged();
        } else {
          stepBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(inference.ModelConfigOuterClass.ModelEnsembling.Step value) {
        if (stepBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStepIsMutable();
          step_.add(value);
          onChanged();
        } else {
          stepBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          int index, inference.ModelConfigOuterClass.ModelEnsembling.Step value) {
        if (stepBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureStepIsMutable();
          step_.add(index, value);
          onChanged();
        } else {
          stepBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.add(builderForValue.build());
          onChanged();
        } else {
          stepBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addStep(
          int index, inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder builderForValue) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.add(index, builderForValue.build());
          onChanged();
        } else {
          stepBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder addAllStep(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelEnsembling.Step> values) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, step_);
          onChanged();
        } else {
          stepBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder clearStep() {
        if (stepBuilder_ == null) {
          step_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          stepBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public Builder removeStep(int index) {
        if (stepBuilder_ == null) {
          ensureStepIsMutable();
          step_.remove(index);
          onChanged();
        } else {
          stepBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder getStepBuilder(
          int index) {
        return getStepFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder getStepOrBuilder(
          int index) {
        if (stepBuilder_ == null) {
          return step_.get(index);  } else {
          return stepBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
           getStepOrBuilderList() {
        if (stepBuilder_ != null) {
          return stepBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(step_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder addStepBuilder() {
        return getStepFieldBuilder().addBuilder(
            inference.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder addStepBuilder(
          int index) {
        return getStepFieldBuilder().addBuilder(
            index, inference.ModelConfigOuterClass.ModelEnsembling.Step.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: Step step (repeated)
       *&#64;&#64;
       *&#64;&#64;     The models and the input / output mappings used within the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelEnsembling.Step step = 1;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder> 
           getStepBuilderList() {
        return getStepFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelEnsembling.Step, inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder, inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder> 
          getStepFieldBuilder() {
        if (stepBuilder_ == null) {
          stepBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelEnsembling.Step, inference.ModelConfigOuterClass.ModelEnsembling.Step.Builder, inference.ModelConfigOuterClass.ModelEnsembling.StepOrBuilder>(
                  step_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          step_ = null;
        }
        return stepBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelEnsembling)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelEnsembling)
    private static final inference.ModelConfigOuterClass.ModelEnsembling DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelEnsembling();
    }

    public static inference.ModelConfigOuterClass.ModelEnsembling getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelEnsembling>
        PARSER = new com.google.protobuf.AbstractParser<ModelEnsembling>() {
      @java.lang.Override
      public ModelEnsembling parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelEnsembling(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelEnsembling> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelEnsembling> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelEnsembling getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelParameterOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelParameter)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @return The stringValue.
     */
    java.lang.String getStringValue();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @return The bytes for stringValue.
     */
    com.google.protobuf.ByteString
        getStringValueBytes();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelParameter
   *&#64;&#64;
   *&#64;&#64;   A model parameter.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelParameter}
   */
  public static final class ModelParameter extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelParameter)
      ModelParameterOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelParameter.newBuilder() to construct.
    private ModelParameter(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelParameter() {
      stringValue_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelParameter();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelParameter(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              stringValue_ = s;
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelParameter_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelParameter_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelParameter.class, inference.ModelConfigOuterClass.ModelParameter.Builder.class);
    }

    public static final int STRING_VALUE_FIELD_NUMBER = 1;
    private volatile java.lang.Object stringValue_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @return The stringValue.
     */
    @java.lang.Override
    public java.lang.String getStringValue() {
      java.lang.Object ref = stringValue_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        stringValue_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string string_value
     *&#64;&#64;
     *&#64;&#64;     The string value of the parameter.
     *&#64;&#64;
     * </pre>
     *
     * <code>string string_value = 1;</code>
     * @return The bytes for stringValue.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getStringValueBytes() {
      java.lang.Object ref = stringValue_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        stringValue_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getStringValueBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, stringValue_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getStringValueBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, stringValue_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelParameter)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelParameter other = (inference.ModelConfigOuterClass.ModelParameter) obj;

      if (!getStringValue()
          .equals(other.getStringValue())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + STRING_VALUE_FIELD_NUMBER;
      hash = (53 * hash) + getStringValue().hashCode();
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelParameter parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelParameter prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelParameter
     *&#64;&#64;
     *&#64;&#64;   A model parameter.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelParameter}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelParameter)
        inference.ModelConfigOuterClass.ModelParameterOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelParameter_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelParameter_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelParameter.class, inference.ModelConfigOuterClass.ModelParameter.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelParameter.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        stringValue_ = "";

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelParameter_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelParameter getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelParameter.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelParameter build() {
        inference.ModelConfigOuterClass.ModelParameter result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelParameter buildPartial() {
        inference.ModelConfigOuterClass.ModelParameter result = new inference.ModelConfigOuterClass.ModelParameter(this);
        result.stringValue_ = stringValue_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelParameter) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelParameter)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelParameter other) {
        if (other == inference.ModelConfigOuterClass.ModelParameter.getDefaultInstance()) return this;
        if (!other.getStringValue().isEmpty()) {
          stringValue_ = other.stringValue_;
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelParameter parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelParameter) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private java.lang.Object stringValue_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @return The stringValue.
       */
      public java.lang.String getStringValue() {
        java.lang.Object ref = stringValue_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          stringValue_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @return The bytes for stringValue.
       */
      public com.google.protobuf.ByteString
          getStringValueBytes() {
        java.lang.Object ref = stringValue_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          stringValue_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @param value The stringValue to set.
       * @return This builder for chaining.
       */
      public Builder setStringValue(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        stringValue_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearStringValue() {
        
        stringValue_ = getDefaultInstance().getStringValue();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string string_value
       *&#64;&#64;
       *&#64;&#64;     The string value of the parameter.
       *&#64;&#64;
       * </pre>
       *
       * <code>string string_value = 1;</code>
       * @param value The bytes for stringValue to set.
       * @return This builder for chaining.
       */
      public Builder setStringValueBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        stringValue_ = value;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelParameter)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelParameter)
    private static final inference.ModelConfigOuterClass.ModelParameter DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelParameter();
    }

    public static inference.ModelConfigOuterClass.ModelParameter getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelParameter>
        PARSER = new com.google.protobuf.AbstractParser<ModelParameter>() {
      @java.lang.Override
      public ModelParameter parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelParameter(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelParameter> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelParameter> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelParameter getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelWarmupOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelWarmup)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1. If
     *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
     *&#64;&#64;     match the batch size requested.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 2;</code>
     * @return The batchSize.
     */
    int getBatchSize();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    int getInputsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    boolean containsInputs(
        java.lang.String key);
    /**
     * Use {@link #getInputsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
    getInputs();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
    getInputsMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrDefault(
        java.lang.String key,
        inference.ModelConfigOuterClass.ModelWarmup.Input defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrThrow(
        java.lang.String key);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelWarmup
   *&#64;&#64;
   *&#64;&#64;   Settings used to construct the request sample for model warmup.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelWarmup}
   */
  public static final class ModelWarmup extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelWarmup)
      ModelWarmupOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelWarmup.newBuilder() to construct.
    private ModelWarmup(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelWarmup() {
      name_ = "";
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelWarmup();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelWarmup(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 16: {

              batchSize_ = input.readUInt32();
              break;
            }
            case 26: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                inputs_ = com.google.protobuf.MapField.newMapField(
                    InputsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000001;
              }
              com.google.protobuf.MapEntry<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
              inputs__ = input.readMessage(
                  InputsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              inputs_.getMutableMap().put(
                  inputs__.getKey(), inputs__.getValue());
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 3:
          return internalGetInputs();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelWarmup.class, inference.ModelConfigOuterClass.ModelWarmup.Builder.class);
    }

    public interface InputOrBuilder extends
        // @@protoc_insertion_point(interface_extends:inference.ModelWarmup.Input)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      int getDataTypeValue();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @return The dataType.
       */
      inference.ModelConfigOuterClass.DataType getDataType();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @return A list containing the dims.
       */
      java.util.List<java.lang.Long> getDimsList();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @return The count of dims.
       */
      int getDimsCount();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @param index The index of the element to return.
       * @return The dims at the given index.
       */
      long getDims(int index);

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       * @return The zeroData.
       */
      boolean getZeroData();

      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       * @return The randomData.
       */
      boolean getRandomData();

      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return The inputDataFile.
       */
      java.lang.String getInputDataFile();
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return The bytes for inputDataFile.
       */
      com.google.protobuf.ByteString
          getInputDataFileBytes();

      public inference.ModelConfigOuterClass.ModelWarmup.Input.InputDataTypeCase getInputDataTypeCase();
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;  .. cpp:var:: message Input
     *&#64;&#64;
     *&#64;&#64;     Meta data associated with an input.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelWarmup.Input}
     */
    public static final class Input extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:inference.ModelWarmup.Input)
        InputOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use Input.newBuilder() to construct.
      private Input(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private Input() {
        dataType_ = 0;
        dims_ = emptyLongList();
      }

      @java.lang.Override
      @SuppressWarnings({"unused"})
      protected java.lang.Object newInstance(
          UnusedPrivateParameter unused) {
        return new Input();
      }

      @java.lang.Override
      public final com.google.protobuf.UnknownFieldSet
      getUnknownFields() {
        return this.unknownFields;
      }
      private Input(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        this();
        if (extensionRegistry == null) {
          throw new java.lang.NullPointerException();
        }
        int mutable_bitField0_ = 0;
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder();
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                int rawValue = input.readEnum();

                dataType_ = rawValue;
                break;
              }
              case 16: {
                if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                  dims_ = newLongList();
                  mutable_bitField0_ |= 0x00000001;
                }
                dims_.addLong(input.readInt64());
                break;
              }
              case 18: {
                int length = input.readRawVarint32();
                int limit = input.pushLimit(length);
                if (!((mutable_bitField0_ & 0x00000001) != 0) && input.getBytesUntilLimit() > 0) {
                  dims_ = newLongList();
                  mutable_bitField0_ |= 0x00000001;
                }
                while (input.getBytesUntilLimit() > 0) {
                  dims_.addLong(input.readInt64());
                }
                input.popLimit(limit);
                break;
              }
              case 24: {
                inputDataTypeCase_ = 3;
                inputDataType_ = input.readBool();
                break;
              }
              case 32: {
                inputDataTypeCase_ = 4;
                inputDataType_ = input.readBool();
                break;
              }
              case 42: {
                java.lang.String s = input.readStringRequireUtf8();
                inputDataTypeCase_ = 5;
                inputDataType_ = s;
                break;
              }
              default: {
                if (!parseUnknownField(
                    input, unknownFields, extensionRegistry, tag)) {
                  done = true;
                }
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(this);
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(
              e).setUnfinishedMessage(this);
        } finally {
          if (((mutable_bitField0_ & 0x00000001) != 0)) {
            dims_.makeImmutable(); // C
          }
          this.unknownFields = unknownFields.build();
          makeExtensionsImmutable();
        }
      }
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_Input_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_Input_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelWarmup.Input.class, inference.ModelConfigOuterClass.ModelWarmup.Input.Builder.class);
      }

      private int inputDataTypeCase_ = 0;
      private java.lang.Object inputDataType_;
      public enum InputDataTypeCase
          implements com.google.protobuf.Internal.EnumLite,
              com.google.protobuf.AbstractMessage.InternalOneOfEnum {
        ZERO_DATA(3),
        RANDOM_DATA(4),
        INPUT_DATA_FILE(5),
        INPUTDATATYPE_NOT_SET(0);
        private final int value;
        private InputDataTypeCase(int value) {
          this.value = value;
        }
        /**
         * @param value The number of the enum to look for.
         * @return The enum associated with the given number.
         * @deprecated Use {@link #forNumber(int)} instead.
         */
        @java.lang.Deprecated
        public static InputDataTypeCase valueOf(int value) {
          return forNumber(value);
        }

        public static InputDataTypeCase forNumber(int value) {
          switch (value) {
            case 3: return ZERO_DATA;
            case 4: return RANDOM_DATA;
            case 5: return INPUT_DATA_FILE;
            case 0: return INPUTDATATYPE_NOT_SET;
            default: return null;
          }
        }
        public int getNumber() {
          return this.value;
        }
      };

      public InputDataTypeCase
      getInputDataTypeCase() {
        return InputDataTypeCase.forNumber(
            inputDataTypeCase_);
      }

      public static final int DATA_TYPE_FIELD_NUMBER = 1;
      private int dataType_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @return The enum numeric value on the wire for dataType.
       */
      @java.lang.Override public int getDataTypeValue() {
        return dataType_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: DataType data_type
       *&#64;&#64;
       *&#64;&#64;       The data-type of the input.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.DataType data_type = 1;</code>
       * @return The dataType.
       */
      @java.lang.Override public inference.ModelConfigOuterClass.DataType getDataType() {
        @SuppressWarnings("deprecation")
        inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
        return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
      }

      public static final int DIMS_FIELD_NUMBER = 2;
      private com.google.protobuf.Internal.LongList dims_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @return A list containing the dims.
       */
      @java.lang.Override
      public java.util.List<java.lang.Long>
          getDimsList() {
        return dims_;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @return The count of dims.
       */
      public int getDimsCount() {
        return dims_.size();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
       *&#64;&#64;
       *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated int64 dims = 2;</code>
       * @param index The index of the element to return.
       * @return The dims at the given index.
       */
      public long getDims(int index) {
        return dims_.getLong(index);
      }
      private int dimsMemoizedSerializedSize = -1;

      public static final int ZERO_DATA_FIELD_NUMBER = 3;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool zero_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using zeros as input data. Note that the
       *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
       *&#64;&#64;       will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool zero_data = 3;</code>
       * @return The zeroData.
       */
      @java.lang.Override
      public boolean getZeroData() {
        if (inputDataTypeCase_ == 3) {
          return (java.lang.Boolean) inputDataType_;
        }
        return false;
      }

      public static final int RANDOM_DATA_FIELD_NUMBER = 4;
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;    .. cpp:var:: bool random_data
       *&#64;&#64;
       *&#64;&#64;       The identifier for using random data as input data. Note that
       *&#64;&#64;       the value of 'random_data' will not be checked, instead,
       *&#64;&#64;       random data will be used as long as the field is set.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool random_data = 4;</code>
       * @return The randomData.
       */
      @java.lang.Override
      public boolean getRandomData() {
        if (inputDataTypeCase_ == 4) {
          return (java.lang.Boolean) inputDataType_;
        }
        return false;
      }

      public static final int INPUT_DATA_FILE_FIELD_NUMBER = 5;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return The inputDataFile.
       */
      public java.lang.String getInputDataFile() {
        java.lang.Object ref = "";
        if (inputDataTypeCase_ == 5) {
          ref = inputDataType_;
        }
        if (ref instanceof java.lang.String) {
          return (java.lang.String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          if (inputDataTypeCase_ == 5) {
            inputDataType_ = s;
          }
          return s;
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: string input_data_file
       *&#64;&#64;
       *&#64;&#64;       The file whose content will be used as raw input data in
       *&#64;&#64;       row-major order. The file must be provided in a sub-directory
       *&#64;&#64;       'warmup' under the model directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>string input_data_file = 5;</code>
       * @return The bytes for inputDataFile.
       */
      public com.google.protobuf.ByteString
          getInputDataFileBytes() {
        java.lang.Object ref = "";
        if (inputDataTypeCase_ == 5) {
          ref = inputDataType_;
        }
        if (ref instanceof java.lang.String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          if (inputDataTypeCase_ == 5) {
            inputDataType_ = b;
          }
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      private byte memoizedIsInitialized = -1;
      @java.lang.Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @java.lang.Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
          output.writeEnum(1, dataType_);
        }
        if (getDimsList().size() > 0) {
          output.writeUInt32NoTag(18);
          output.writeUInt32NoTag(dimsMemoizedSerializedSize);
        }
        for (int i = 0; i < dims_.size(); i++) {
          output.writeInt64NoTag(dims_.getLong(i));
        }
        if (inputDataTypeCase_ == 3) {
          output.writeBool(
              3, (boolean)((java.lang.Boolean) inputDataType_));
        }
        if (inputDataTypeCase_ == 4) {
          output.writeBool(
              4, (boolean)((java.lang.Boolean) inputDataType_));
        }
        if (inputDataTypeCase_ == 5) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 5, inputDataType_);
        }
        unknownFields.writeTo(output);
      }

      @java.lang.Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (dataType_ != inference.ModelConfigOuterClass.DataType.TYPE_INVALID.getNumber()) {
          size += com.google.protobuf.CodedOutputStream
            .computeEnumSize(1, dataType_);
        }
        {
          int dataSize = 0;
          for (int i = 0; i < dims_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeInt64SizeNoTag(dims_.getLong(i));
          }
          size += dataSize;
          if (!getDimsList().isEmpty()) {
            size += 1;
            size += com.google.protobuf.CodedOutputStream
                .computeInt32SizeNoTag(dataSize);
          }
          dimsMemoizedSerializedSize = dataSize;
        }
        if (inputDataTypeCase_ == 3) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(
                3, (boolean)((java.lang.Boolean) inputDataType_));
        }
        if (inputDataTypeCase_ == 4) {
          size += com.google.protobuf.CodedOutputStream
            .computeBoolSize(
                4, (boolean)((java.lang.Boolean) inputDataType_));
        }
        if (inputDataTypeCase_ == 5) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(5, inputDataType_);
        }
        size += unknownFields.getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @java.lang.Override
      public boolean equals(final java.lang.Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof inference.ModelConfigOuterClass.ModelWarmup.Input)) {
          return super.equals(obj);
        }
        inference.ModelConfigOuterClass.ModelWarmup.Input other = (inference.ModelConfigOuterClass.ModelWarmup.Input) obj;

        if (dataType_ != other.dataType_) return false;
        if (!getDimsList()
            .equals(other.getDimsList())) return false;
        if (!getInputDataTypeCase().equals(other.getInputDataTypeCase())) return false;
        switch (inputDataTypeCase_) {
          case 3:
            if (getZeroData()
                != other.getZeroData()) return false;
            break;
          case 4:
            if (getRandomData()
                != other.getRandomData()) return false;
            break;
          case 5:
            if (!getInputDataFile()
                .equals(other.getInputDataFile())) return false;
            break;
          case 0:
          default:
        }
        if (!unknownFields.equals(other.unknownFields)) return false;
        return true;
      }

      @java.lang.Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + DATA_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + dataType_;
        if (getDimsCount() > 0) {
          hash = (37 * hash) + DIMS_FIELD_NUMBER;
          hash = (53 * hash) + getDimsList().hashCode();
        }
        switch (inputDataTypeCase_) {
          case 3:
            hash = (37 * hash) + ZERO_DATA_FIELD_NUMBER;
            hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
                getZeroData());
            break;
          case 4:
            hash = (37 * hash) + RANDOM_DATA_FIELD_NUMBER;
            hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
                getRandomData());
            break;
          case 5:
            hash = (37 * hash) + INPUT_DATA_FILE_FIELD_NUMBER;
            hash = (53 * hash) + getInputDataFile().hashCode();
            break;
          case 0:
          default:
        }
        hash = (29 * hash) + unknownFields.hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static inference.ModelConfigOuterClass.ModelWarmup.Input parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @java.lang.Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(inference.ModelConfigOuterClass.ModelWarmup.Input prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @java.lang.Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * <pre>
       *&#64;&#64;
       *&#64;&#64;  .. cpp:var:: message Input
       *&#64;&#64;
       *&#64;&#64;     Meta data associated with an input.
       *&#64;&#64;
       * </pre>
       *
       * Protobuf type {@code inference.ModelWarmup.Input}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:inference.ModelWarmup.Input)
          inference.ModelConfigOuterClass.ModelWarmup.InputOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_Input_descriptor;
        }

        @java.lang.Override
        protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_Input_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  inference.ModelConfigOuterClass.ModelWarmup.Input.class, inference.ModelConfigOuterClass.ModelWarmup.Input.Builder.class);
        }

        // Construct using inference.ModelConfigOuterClass.ModelWarmup.Input.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }

        private Builder(
            com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessageV3
                  .alwaysUseFieldBuilders) {
          }
        }
        @java.lang.Override
        public Builder clear() {
          super.clear();
          dataType_ = 0;

          dims_ = emptyLongList();
          bitField0_ = (bitField0_ & ~0x00000001);
          inputDataTypeCase_ = 0;
          inputDataType_ = null;
          return this;
        }

        @java.lang.Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_Input_descriptor;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelWarmup.Input getDefaultInstanceForType() {
          return inference.ModelConfigOuterClass.ModelWarmup.Input.getDefaultInstance();
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelWarmup.Input build() {
          inference.ModelConfigOuterClass.ModelWarmup.Input result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @java.lang.Override
        public inference.ModelConfigOuterClass.ModelWarmup.Input buildPartial() {
          inference.ModelConfigOuterClass.ModelWarmup.Input result = new inference.ModelConfigOuterClass.ModelWarmup.Input(this);
          int from_bitField0_ = bitField0_;
          result.dataType_ = dataType_;
          if (((bitField0_ & 0x00000001) != 0)) {
            dims_.makeImmutable();
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.dims_ = dims_;
          if (inputDataTypeCase_ == 3) {
            result.inputDataType_ = inputDataType_;
          }
          if (inputDataTypeCase_ == 4) {
            result.inputDataType_ = inputDataType_;
          }
          if (inputDataTypeCase_ == 5) {
            result.inputDataType_ = inputDataType_;
          }
          result.inputDataTypeCase_ = inputDataTypeCase_;
          onBuilt();
          return result;
        }

        @java.lang.Override
        public Builder clone() {
          return super.clone();
        }
        @java.lang.Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.setField(field, value);
        }
        @java.lang.Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @java.lang.Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @java.lang.Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, java.lang.Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @java.lang.Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            java.lang.Object value) {
          return super.addRepeatedField(field, value);
        }
        @java.lang.Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof inference.ModelConfigOuterClass.ModelWarmup.Input) {
            return mergeFrom((inference.ModelConfigOuterClass.ModelWarmup.Input)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(inference.ModelConfigOuterClass.ModelWarmup.Input other) {
          if (other == inference.ModelConfigOuterClass.ModelWarmup.Input.getDefaultInstance()) return this;
          if (other.dataType_ != 0) {
            setDataTypeValue(other.getDataTypeValue());
          }
          if (!other.dims_.isEmpty()) {
            if (dims_.isEmpty()) {
              dims_ = other.dims_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureDimsIsMutable();
              dims_.addAll(other.dims_);
            }
            onChanged();
          }
          switch (other.getInputDataTypeCase()) {
            case ZERO_DATA: {
              setZeroData(other.getZeroData());
              break;
            }
            case RANDOM_DATA: {
              setRandomData(other.getRandomData());
              break;
            }
            case INPUT_DATA_FILE: {
              inputDataTypeCase_ = 5;
              inputDataType_ = other.inputDataType_;
              onChanged();
              break;
            }
            case INPUTDATATYPE_NOT_SET: {
              break;
            }
          }
          this.mergeUnknownFields(other.unknownFields);
          onChanged();
          return this;
        }

        @java.lang.Override
        public final boolean isInitialized() {
          return true;
        }

        @java.lang.Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          inference.ModelConfigOuterClass.ModelWarmup.Input parsedMessage = null;
          try {
            parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            parsedMessage = (inference.ModelConfigOuterClass.ModelWarmup.Input) e.getUnfinishedMessage();
            throw e.unwrapIOException();
          } finally {
            if (parsedMessage != null) {
              mergeFrom(parsedMessage);
            }
          }
          return this;
        }
        private int inputDataTypeCase_ = 0;
        private java.lang.Object inputDataType_;
        public InputDataTypeCase
            getInputDataTypeCase() {
          return InputDataTypeCase.forNumber(
              inputDataTypeCase_);
        }

        public Builder clearInputDataType() {
          inputDataTypeCase_ = 0;
          inputDataType_ = null;
          onChanged();
          return this;
        }

        private int bitField0_;

        private int dataType_ = 0;
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @return The enum numeric value on the wire for dataType.
         */
        @java.lang.Override public int getDataTypeValue() {
          return dataType_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @param value The enum numeric value on the wire for dataType to set.
         * @return This builder for chaining.
         */
        public Builder setDataTypeValue(int value) {
          
          dataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @return The dataType.
         */
        @java.lang.Override
        public inference.ModelConfigOuterClass.DataType getDataType() {
          @SuppressWarnings("deprecation")
          inference.ModelConfigOuterClass.DataType result = inference.ModelConfigOuterClass.DataType.valueOf(dataType_);
          return result == null ? inference.ModelConfigOuterClass.DataType.UNRECOGNIZED : result;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @param value The dataType to set.
         * @return This builder for chaining.
         */
        public Builder setDataType(inference.ModelConfigOuterClass.DataType value) {
          if (value == null) {
            throw new NullPointerException();
          }
          
          dataType_ = value.getNumber();
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: DataType data_type
         *&#64;&#64;
         *&#64;&#64;       The data-type of the input.
         *&#64;&#64;
         * </pre>
         *
         * <code>.inference.DataType data_type = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearDataType() {
          
          dataType_ = 0;
          onChanged();
          return this;
        }

        private com.google.protobuf.Internal.LongList dims_ = emptyLongList();
        private void ensureDimsIsMutable() {
          if (!((bitField0_ & 0x00000001) != 0)) {
            dims_ = mutableCopy(dims_);
            bitField0_ |= 0x00000001;
           }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @return A list containing the dims.
         */
        public java.util.List<java.lang.Long>
            getDimsList() {
          return ((bitField0_ & 0x00000001) != 0) ?
                   java.util.Collections.unmodifiableList(dims_) : dims_;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @return The count of dims.
         */
        public int getDimsCount() {
          return dims_.size();
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @param index The index of the element to return.
         * @return The dims at the given index.
         */
        public long getDims(int index) {
          return dims_.getLong(index);
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @param index The index to set the value at.
         * @param value The dims to set.
         * @return This builder for chaining.
         */
        public Builder setDims(
            int index, long value) {
          ensureDimsIsMutable();
          dims_.setLong(index, value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @param value The dims to add.
         * @return This builder for chaining.
         */
        public Builder addDims(long value) {
          ensureDimsIsMutable();
          dims_.addLong(value);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @param values The dims to add.
         * @return This builder for chaining.
         */
        public Builder addAllDims(
            java.lang.Iterable<? extends java.lang.Long> values) {
          ensureDimsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, dims_);
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: int64 dims (repeated)
         *&#64;&#64;
         *&#64;&#64;       The shape of the input tensor, not including the batch dimension.
         *&#64;&#64;
         * </pre>
         *
         * <code>repeated int64 dims = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearDims() {
          dims_ = emptyLongList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         * @return The zeroData.
         */
        public boolean getZeroData() {
          if (inputDataTypeCase_ == 3) {
            return (java.lang.Boolean) inputDataType_;
          }
          return false;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         * @param value The zeroData to set.
         * @return This builder for chaining.
         */
        public Builder setZeroData(boolean value) {
          inputDataTypeCase_ = 3;
          inputDataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool zero_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using zeros as input data. Note that the
         *&#64;&#64;       value of 'zero_data' will not be checked, instead, zero data
         *&#64;&#64;       will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool zero_data = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearZeroData() {
          if (inputDataTypeCase_ == 3) {
            inputDataTypeCase_ = 0;
            inputDataType_ = null;
            onChanged();
          }
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         * @return The randomData.
         */
        public boolean getRandomData() {
          if (inputDataTypeCase_ == 4) {
            return (java.lang.Boolean) inputDataType_;
          }
          return false;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         * @param value The randomData to set.
         * @return This builder for chaining.
         */
        public Builder setRandomData(boolean value) {
          inputDataTypeCase_ = 4;
          inputDataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;
         *&#64;&#64;    .. cpp:var:: bool random_data
         *&#64;&#64;
         *&#64;&#64;       The identifier for using random data as input data. Note that
         *&#64;&#64;       the value of 'random_data' will not be checked, instead,
         *&#64;&#64;       random data will be used as long as the field is set.
         *&#64;&#64;
         * </pre>
         *
         * <code>bool random_data = 4;</code>
         * @return This builder for chaining.
         */
        public Builder clearRandomData() {
          if (inputDataTypeCase_ == 4) {
            inputDataTypeCase_ = 0;
            inputDataType_ = null;
            onChanged();
          }
          return this;
        }

        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @return The inputDataFile.
         */
        @java.lang.Override
        public java.lang.String getInputDataFile() {
          java.lang.Object ref = "";
          if (inputDataTypeCase_ == 5) {
            ref = inputDataType_;
          }
          if (!(ref instanceof java.lang.String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            java.lang.String s = bs.toStringUtf8();
            if (inputDataTypeCase_ == 5) {
              inputDataType_ = s;
            }
            return s;
          } else {
            return (java.lang.String) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @return The bytes for inputDataFile.
         */
        @java.lang.Override
        public com.google.protobuf.ByteString
            getInputDataFileBytes() {
          java.lang.Object ref = "";
          if (inputDataTypeCase_ == 5) {
            ref = inputDataType_;
          }
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (java.lang.String) ref);
            if (inputDataTypeCase_ == 5) {
              inputDataType_ = b;
            }
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @param value The inputDataFile to set.
         * @return This builder for chaining.
         */
        public Builder setInputDataFile(
            java.lang.String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  inputDataTypeCase_ = 5;
          inputDataType_ = value;
          onChanged();
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @return This builder for chaining.
         */
        public Builder clearInputDataFile() {
          if (inputDataTypeCase_ == 5) {
            inputDataTypeCase_ = 0;
            inputDataType_ = null;
            onChanged();
          }
          return this;
        }
        /**
         * <pre>
         *&#64;&#64;    .. cpp:var:: string input_data_file
         *&#64;&#64;
         *&#64;&#64;       The file whose content will be used as raw input data in
         *&#64;&#64;       row-major order. The file must be provided in a sub-directory
         *&#64;&#64;       'warmup' under the model directory.
         *&#64;&#64;
         * </pre>
         *
         * <code>string input_data_file = 5;</code>
         * @param value The bytes for inputDataFile to set.
         * @return This builder for chaining.
         */
        public Builder setInputDataFileBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
          inputDataTypeCase_ = 5;
          inputDataType_ = value;
          onChanged();
          return this;
        }
        @java.lang.Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @java.lang.Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:inference.ModelWarmup.Input)
      }

      // @@protoc_insertion_point(class_scope:inference.ModelWarmup.Input)
      private static final inference.ModelConfigOuterClass.ModelWarmup.Input DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelWarmup.Input();
      }

      public static inference.ModelConfigOuterClass.ModelWarmup.Input getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<Input>
          PARSER = new com.google.protobuf.AbstractParser<Input>() {
        @java.lang.Override
        public Input parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          return new Input(input, extensionRegistry);
        }
      };

      public static com.google.protobuf.Parser<Input> parser() {
        return PARSER;
      }

      @java.lang.Override
      public com.google.protobuf.Parser<Input> getParserForType() {
        return PARSER;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelWarmup.Input getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the request sample.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BATCH_SIZE_FIELD_NUMBER = 2;
    private int batchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: uint32 batch_size
     *&#64;&#64;
     *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
     *&#64;&#64;     models that don't support batching, batch_size must be 1. If
     *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
     *&#64;&#64;     match the batch size requested.
     *&#64;&#64;
     * </pre>
     *
     * <code>uint32 batch_size = 2;</code>
     * @return The batchSize.
     */
    @java.lang.Override
    public int getBatchSize() {
      return batchSize_;
    }

    public static final int INPUTS_FIELD_NUMBER = 3;
    private static final class InputsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>newDefaultInstance(
                  inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_InputsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  inference.ModelConfigOuterClass.ModelWarmup.Input.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> inputs_;
    private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
    internalGetInputs() {
      if (inputs_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            InputsDefaultEntryHolder.defaultEntry);
      }
      return inputs_;
    }

    public int getInputsCount() {
      return internalGetInputs().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */

    @java.lang.Override
    public boolean containsInputs(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetInputs().getMap().containsKey(key);
    }
    /**
     * Use {@link #getInputsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> getInputs() {
      return getInputsMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> getInputsMap() {
      return internalGetInputs().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrDefault(
        java.lang.String key,
        inference.ModelConfigOuterClass.ModelWarmup.Input defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> map =
          internalGetInputs().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
     *&#64;&#64;
     *&#64;&#64;     The warmup meta data associated with every model input, including
     *&#64;&#64;     control tensors.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> map =
          internalGetInputs().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (batchSize_ != 0) {
        output.writeUInt32(2, batchSize_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetInputs(),
          InputsDefaultEntryHolder.defaultEntry,
          3);
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (batchSize_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, batchSize_);
      }
      for (java.util.Map.Entry<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> entry
           : internalGetInputs().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
        inputs__ = InputsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(3, inputs__);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelWarmup)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelWarmup other = (inference.ModelConfigOuterClass.ModelWarmup) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (getBatchSize()
          != other.getBatchSize()) return false;
      if (!internalGetInputs().equals(
          other.internalGetInputs())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + BATCH_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + getBatchSize();
      if (!internalGetInputs().getMap().isEmpty()) {
        hash = (37 * hash) + INPUTS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetInputs().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelWarmup parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelWarmup prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelWarmup
     *&#64;&#64;
     *&#64;&#64;   Settings used to construct the request sample for model warmup.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelWarmup}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelWarmup)
        inference.ModelConfigOuterClass.ModelWarmupOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetInputs();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 3:
            return internalGetMutableInputs();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelWarmup.class, inference.ModelConfigOuterClass.ModelWarmup.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelWarmup.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        batchSize_ = 0;

        internalGetMutableInputs().clear();
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelWarmup_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelWarmup getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelWarmup.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelWarmup build() {
        inference.ModelConfigOuterClass.ModelWarmup result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelWarmup buildPartial() {
        inference.ModelConfigOuterClass.ModelWarmup result = new inference.ModelConfigOuterClass.ModelWarmup(this);
        int from_bitField0_ = bitField0_;
        result.name_ = name_;
        result.batchSize_ = batchSize_;
        result.inputs_ = internalGetInputs();
        result.inputs_.makeImmutable();
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelWarmup) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelWarmup)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelWarmup other) {
        if (other == inference.ModelConfigOuterClass.ModelWarmup.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (other.getBatchSize() != 0) {
          setBatchSize(other.getBatchSize());
        }
        internalGetMutableInputs().mergeFrom(
            other.internalGetInputs());
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelWarmup parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelWarmup) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the request sample.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private int batchSize_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1. If
       *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
       *&#64;&#64;     match the batch size requested.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 2;</code>
       * @return The batchSize.
       */
      @java.lang.Override
      public int getBatchSize() {
        return batchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1. If
       *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
       *&#64;&#64;     match the batch size requested.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 2;</code>
       * @param value The batchSize to set.
       * @return This builder for chaining.
       */
      public Builder setBatchSize(int value) {
        
        batchSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: uint32 batch_size
       *&#64;&#64;
       *&#64;&#64;     The batch size of the inference request. This must be &gt;= 1. For
       *&#64;&#64;     models that don't support batching, batch_size must be 1. If
       *&#64;&#64;     batch_size &gt; 1, the 'inputs' specified below will be duplicated to
       *&#64;&#64;     match the batch size requested.
       *&#64;&#64;
       * </pre>
       *
       * <code>uint32 batch_size = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearBatchSize() {
        
        batchSize_ = 0;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> inputs_;
      private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
      internalGetInputs() {
        if (inputs_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              InputsDefaultEntryHolder.defaultEntry);
        }
        return inputs_;
      }
      private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
      internalGetMutableInputs() {
        onChanged();;
        if (inputs_ == null) {
          inputs_ = com.google.protobuf.MapField.newMapField(
              InputsDefaultEntryHolder.defaultEntry);
        }
        if (!inputs_.isMutable()) {
          inputs_ = inputs_.copy();
        }
        return inputs_;
      }

      public int getInputsCount() {
        return internalGetInputs().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      @java.lang.Override
      public boolean containsInputs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetInputs().getMap().containsKey(key);
      }
      /**
       * Use {@link #getInputsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> getInputs() {
        return getInputsMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> getInputsMap() {
        return internalGetInputs().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrDefault(
          java.lang.String key,
          inference.ModelConfigOuterClass.ModelWarmup.Input defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> map =
            internalGetInputs().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelWarmup.Input getInputsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> map =
            internalGetInputs().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearInputs() {
        internalGetMutableInputs().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      public Builder removeInputs(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableInputs().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input>
      getMutableInputs() {
        return internalGetMutableInputs().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */
      public Builder putInputs(
          java.lang.String key,
          inference.ModelConfigOuterClass.ModelWarmup.Input value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableInputs().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string, Input&gt; inputs
       *&#64;&#64;
       *&#64;&#64;     The warmup meta data associated with every model input, including
       *&#64;&#64;     control tensors.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelWarmup.Input&gt; inputs = 3;</code>
       */

      public Builder putAllInputs(
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelWarmup.Input> values) {
        internalGetMutableInputs().getMutableMap()
            .putAll(values);
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelWarmup)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelWarmup)
    private static final inference.ModelConfigOuterClass.ModelWarmup DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelWarmup();
    }

    public static inference.ModelConfigOuterClass.ModelWarmup getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelWarmup>
        PARSER = new com.google.protobuf.AbstractParser<ModelWarmup>() {
      @java.lang.Override
      public ModelWarmup parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelWarmup(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelWarmup> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelWarmup> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelWarmup getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelOperationsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelOperations)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @return A list containing the opLibraryFilename.
     */
    java.util.List<java.lang.String>
        getOpLibraryFilenameList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @return The count of opLibraryFilename.
     */
    int getOpLibraryFilenameCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param index The index of the element to return.
     * @return The opLibraryFilename at the given index.
     */
    java.lang.String getOpLibraryFilename(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param index The index of the value to return.
     * @return The bytes of the opLibraryFilename at the given index.
     */
    com.google.protobuf.ByteString
        getOpLibraryFilenameBytes(int index);
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64; .. cpp:var:: message ModelOperations
   *&#64;&#64;
   *&#64;&#64;    The metadata of libraries providing custom operations for this model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelOperations}
   */
  public static final class ModelOperations extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelOperations)
      ModelOperationsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelOperations.newBuilder() to construct.
    private ModelOperations(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelOperations() {
      opLibraryFilename_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelOperations();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelOperations(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                opLibraryFilename_ = new com.google.protobuf.LazyStringArrayList();
                mutable_bitField0_ |= 0x00000001;
              }
              opLibraryFilename_.add(s);
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          opLibraryFilename_ = opLibraryFilename_.getUnmodifiableView();
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelOperations_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelOperations_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelOperations.class, inference.ModelConfigOuterClass.ModelOperations.Builder.class);
    }

    public static final int OP_LIBRARY_FILENAME_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList opLibraryFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @return A list containing the opLibraryFilename.
     */
    public com.google.protobuf.ProtocolStringList
        getOpLibraryFilenameList() {
      return opLibraryFilename_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @return The count of opLibraryFilename.
     */
    public int getOpLibraryFilenameCount() {
      return opLibraryFilename_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param index The index of the element to return.
     * @return The opLibraryFilename at the given index.
     */
    public java.lang.String getOpLibraryFilename(int index) {
      return opLibraryFilename_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string op_library_filename
     *&#64;&#64;
     *&#64;&#64;     Optional paths of the libraries providing custom operations for
     *&#64;&#64;     this model. Valid only for ONNX models.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated string op_library_filename = 1;</code>
     * @param index The index of the value to return.
     * @return The bytes of the opLibraryFilename at the given index.
     */
    public com.google.protobuf.ByteString
        getOpLibraryFilenameBytes(int index) {
      return opLibraryFilename_.getByteString(index);
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < opLibraryFilename_.size(); i++) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, opLibraryFilename_.getRaw(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < opLibraryFilename_.size(); i++) {
          dataSize += computeStringSizeNoTag(opLibraryFilename_.getRaw(i));
        }
        size += dataSize;
        size += 1 * getOpLibraryFilenameList().size();
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelOperations)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelOperations other = (inference.ModelConfigOuterClass.ModelOperations) obj;

      if (!getOpLibraryFilenameList()
          .equals(other.getOpLibraryFilenameList())) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getOpLibraryFilenameCount() > 0) {
        hash = (37 * hash) + OP_LIBRARY_FILENAME_FIELD_NUMBER;
        hash = (53 * hash) + getOpLibraryFilenameList().hashCode();
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelOperations parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelOperations prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64; .. cpp:var:: message ModelOperations
     *&#64;&#64;
     *&#64;&#64;    The metadata of libraries providing custom operations for this model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelOperations}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelOperations)
        inference.ModelConfigOuterClass.ModelOperationsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOperations_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOperations_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelOperations.class, inference.ModelConfigOuterClass.ModelOperations.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelOperations.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        opLibraryFilename_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelOperations_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOperations getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelOperations.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOperations build() {
        inference.ModelConfigOuterClass.ModelOperations result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelOperations buildPartial() {
        inference.ModelConfigOuterClass.ModelOperations result = new inference.ModelConfigOuterClass.ModelOperations(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) != 0)) {
          opLibraryFilename_ = opLibraryFilename_.getUnmodifiableView();
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.opLibraryFilename_ = opLibraryFilename_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelOperations) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelOperations)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelOperations other) {
        if (other == inference.ModelConfigOuterClass.ModelOperations.getDefaultInstance()) return this;
        if (!other.opLibraryFilename_.isEmpty()) {
          if (opLibraryFilename_.isEmpty()) {
            opLibraryFilename_ = other.opLibraryFilename_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureOpLibraryFilenameIsMutable();
            opLibraryFilename_.addAll(other.opLibraryFilename_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelOperations parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelOperations) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int bitField0_;

      private com.google.protobuf.LazyStringList opLibraryFilename_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureOpLibraryFilenameIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          opLibraryFilename_ = new com.google.protobuf.LazyStringArrayList(opLibraryFilename_);
          bitField0_ |= 0x00000001;
         }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @return A list containing the opLibraryFilename.
       */
      public com.google.protobuf.ProtocolStringList
          getOpLibraryFilenameList() {
        return opLibraryFilename_.getUnmodifiableView();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @return The count of opLibraryFilename.
       */
      public int getOpLibraryFilenameCount() {
        return opLibraryFilename_.size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param index The index of the element to return.
       * @return The opLibraryFilename at the given index.
       */
      public java.lang.String getOpLibraryFilename(int index) {
        return opLibraryFilename_.get(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param index The index of the value to return.
       * @return The bytes of the opLibraryFilename at the given index.
       */
      public com.google.protobuf.ByteString
          getOpLibraryFilenameBytes(int index) {
        return opLibraryFilename_.getByteString(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param index The index to set the value at.
       * @param value The opLibraryFilename to set.
       * @return This builder for chaining.
       */
      public Builder setOpLibraryFilename(
          int index, java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureOpLibraryFilenameIsMutable();
        opLibraryFilename_.set(index, value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param value The opLibraryFilename to add.
       * @return This builder for chaining.
       */
      public Builder addOpLibraryFilename(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureOpLibraryFilenameIsMutable();
        opLibraryFilename_.add(value);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param values The opLibraryFilename to add.
       * @return This builder for chaining.
       */
      public Builder addAllOpLibraryFilename(
          java.lang.Iterable<java.lang.String> values) {
        ensureOpLibraryFilenameIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, opLibraryFilename_);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearOpLibraryFilename() {
        opLibraryFilename_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string op_library_filename
       *&#64;&#64;
       *&#64;&#64;     Optional paths of the libraries providing custom operations for
       *&#64;&#64;     this model. Valid only for ONNX models.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated string op_library_filename = 1;</code>
       * @param value The bytes of the opLibraryFilename to add.
       * @return This builder for chaining.
       */
      public Builder addOpLibraryFilenameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        ensureOpLibraryFilenameIsMutable();
        opLibraryFilename_.add(value);
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelOperations)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelOperations)
    private static final inference.ModelConfigOuterClass.ModelOperations DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelOperations();
    }

    public static inference.ModelConfigOuterClass.ModelOperations getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelOperations>
        PARSER = new com.google.protobuf.AbstractParser<ModelOperations>() {
      @java.lang.Override
      public ModelOperations parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelOperations(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelOperations> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelOperations> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOperations getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelTransactionPolicyOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelTransactionPolicy)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool decoupled
     *&#64;&#64;
     *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
     *&#64;&#64;     the requests issued to it, which means the number of responses
     *&#64;&#64;     generated by model may differ from number of requests issued, and
     *&#64;&#64;     that the responses may be out of order relative to the order of
     *&#64;&#64;     requests. The default is false, which means the model will generate
     *&#64;&#64;     exactly one response for each request.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool decoupled = 1;</code>
     * @return The decoupled.
     */
    boolean getDecoupled();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64; .. cpp:var:: message ModelTransactionPolicy
   *&#64;&#64;
   *&#64;&#64;    The specification that describes the nature of transactions
   *&#64;&#64;    to be expected from the model.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelTransactionPolicy}
   */
  public static final class ModelTransactionPolicy extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelTransactionPolicy)
      ModelTransactionPolicyOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelTransactionPolicy.newBuilder() to construct.
    private ModelTransactionPolicy(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelTransactionPolicy() {
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelTransactionPolicy();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelTransactionPolicy(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 8: {

              decoupled_ = input.readBool();
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelTransactionPolicy_descriptor;
    }

    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelTransactionPolicy_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelTransactionPolicy.class, inference.ModelConfigOuterClass.ModelTransactionPolicy.Builder.class);
    }

    public static final int DECOUPLED_FIELD_NUMBER = 1;
    private boolean decoupled_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: bool decoupled
     *&#64;&#64;
     *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
     *&#64;&#64;     the requests issued to it, which means the number of responses
     *&#64;&#64;     generated by model may differ from number of requests issued, and
     *&#64;&#64;     that the responses may be out of order relative to the order of
     *&#64;&#64;     requests. The default is false, which means the model will generate
     *&#64;&#64;     exactly one response for each request.
     *&#64;&#64;
     * </pre>
     *
     * <code>bool decoupled = 1;</code>
     * @return The decoupled.
     */
    @java.lang.Override
    public boolean getDecoupled() {
      return decoupled_;
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (decoupled_ != false) {
        output.writeBool(1, decoupled_);
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (decoupled_ != false) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, decoupled_);
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelTransactionPolicy)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelTransactionPolicy other = (inference.ModelConfigOuterClass.ModelTransactionPolicy) obj;

      if (getDecoupled()
          != other.getDecoupled()) return false;
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + DECOUPLED_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
          getDecoupled());
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelTransactionPolicy parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelTransactionPolicy prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64; .. cpp:var:: message ModelTransactionPolicy
     *&#64;&#64;
     *&#64;&#64;    The specification that describes the nature of transactions
     *&#64;&#64;    to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelTransactionPolicy}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelTransactionPolicy)
        inference.ModelConfigOuterClass.ModelTransactionPolicyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelTransactionPolicy_descriptor;
      }

      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelTransactionPolicy_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelTransactionPolicy.class, inference.ModelConfigOuterClass.ModelTransactionPolicy.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelTransactionPolicy.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        decoupled_ = false;

        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelTransactionPolicy_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelTransactionPolicy getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelTransactionPolicy.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelTransactionPolicy build() {
        inference.ModelConfigOuterClass.ModelTransactionPolicy result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelTransactionPolicy buildPartial() {
        inference.ModelConfigOuterClass.ModelTransactionPolicy result = new inference.ModelConfigOuterClass.ModelTransactionPolicy(this);
        result.decoupled_ = decoupled_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelTransactionPolicy) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelTransactionPolicy)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelTransactionPolicy other) {
        if (other == inference.ModelConfigOuterClass.ModelTransactionPolicy.getDefaultInstance()) return this;
        if (other.getDecoupled() != false) {
          setDecoupled(other.getDecoupled());
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelTransactionPolicy parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelTransactionPolicy) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }

      private boolean decoupled_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool decoupled
       *&#64;&#64;
       *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
       *&#64;&#64;     the requests issued to it, which means the number of responses
       *&#64;&#64;     generated by model may differ from number of requests issued, and
       *&#64;&#64;     that the responses may be out of order relative to the order of
       *&#64;&#64;     requests. The default is false, which means the model will generate
       *&#64;&#64;     exactly one response for each request.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool decoupled = 1;</code>
       * @return The decoupled.
       */
      @java.lang.Override
      public boolean getDecoupled() {
        return decoupled_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool decoupled
       *&#64;&#64;
       *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
       *&#64;&#64;     the requests issued to it, which means the number of responses
       *&#64;&#64;     generated by model may differ from number of requests issued, and
       *&#64;&#64;     that the responses may be out of order relative to the order of
       *&#64;&#64;     requests. The default is false, which means the model will generate
       *&#64;&#64;     exactly one response for each request.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool decoupled = 1;</code>
       * @param value The decoupled to set.
       * @return This builder for chaining.
       */
      public Builder setDecoupled(boolean value) {
        
        decoupled_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: bool decoupled
       *&#64;&#64;
       *&#64;&#64;     Indicates whether responses generated by the model are decoupled with
       *&#64;&#64;     the requests issued to it, which means the number of responses
       *&#64;&#64;     generated by model may differ from number of requests issued, and
       *&#64;&#64;     that the responses may be out of order relative to the order of
       *&#64;&#64;     requests. The default is false, which means the model will generate
       *&#64;&#64;     exactly one response for each request.
       *&#64;&#64;
       * </pre>
       *
       * <code>bool decoupled = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearDecoupled() {
        
        decoupled_ = false;
        onChanged();
        return this;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelTransactionPolicy)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelTransactionPolicy)
    private static final inference.ModelConfigOuterClass.ModelTransactionPolicy DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelTransactionPolicy();
    }

    public static inference.ModelConfigOuterClass.ModelTransactionPolicy getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelTransactionPolicy>
        PARSER = new com.google.protobuf.AbstractParser<ModelTransactionPolicy>() {
      @java.lang.Override
      public ModelTransactionPolicy parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelTransactionPolicy(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelTransactionPolicy> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelTransactionPolicy> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTransactionPolicy getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ModelConfigOrBuilder extends
      // @@protoc_insertion_point(interface_extends:inference.ModelConfig)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    java.lang.String getName();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @return The platform.
     */
    java.lang.String getPlatform();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @return The bytes for platform.
     */
    com.google.protobuf.ByteString
        getPlatformBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @return The backend.
     */
    java.lang.String getBackend();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @return The bytes for backend.
     */
    com.google.protobuf.ByteString
        getBackendBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     * @return Whether the versionPolicy field is set.
     */
    boolean hasVersionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     * @return The versionPolicy.
     */
    inference.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     */
    inference.ModelConfigOuterClass.ModelVersionPolicyOrBuilder getVersionPolicyOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     * @return The maxBatchSize.
     */
    int getMaxBatchSize();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelInput> 
        getInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    inference.ModelConfigOuterClass.ModelInput getInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    int getInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    java.util.List<? extends inference.ModelConfigOuterClass.ModelInputOrBuilder> 
        getInputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    inference.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelOutput> 
        getOutputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    inference.ModelConfigOuterClass.ModelOutput getOutput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    int getOutputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    java.util.List<? extends inference.ModelConfigOuterClass.ModelOutputOrBuilder> 
        getOutputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    inference.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.BatchInput> 
        getBatchInputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    inference.ModelConfigOuterClass.BatchInput getBatchInput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    int getBatchInputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    java.util.List<? extends inference.ModelConfigOuterClass.BatchInputOrBuilder> 
        getBatchInputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    inference.ModelConfigOuterClass.BatchInputOrBuilder getBatchInputOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.BatchOutput> 
        getBatchOutputList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    inference.ModelConfigOuterClass.BatchOutput getBatchOutput(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    int getBatchOutputCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    java.util.List<? extends inference.ModelConfigOuterClass.BatchOutputOrBuilder> 
        getBatchOutputOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    inference.ModelConfigOuterClass.BatchOutputOrBuilder getBatchOutputOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     * @return Whether the optimization field is set.
     */
    boolean hasOptimization();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     * @return The optimization.
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     */
    inference.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder getOptimizationOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     * @return Whether the dynamicBatching field is set.
     */
    boolean hasDynamicBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     * @return The dynamicBatching.
     */
    inference.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    inference.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder getDynamicBatchingOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     * @return Whether the sequenceBatching field is set.
     */
    boolean hasSequenceBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     * @return The sequenceBatching.
     */
    inference.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     */
    inference.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder getSequenceBatchingOrBuilder();

    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     * @return Whether the ensembleScheduling field is set.
     */
    boolean hasEnsembleScheduling();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     * @return The ensembleScheduling.
     */
    inference.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling();
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    inference.ModelConfigOuterClass.ModelEnsemblingOrBuilder getEnsembleSchedulingOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelInstanceGroup> 
        getInstanceGroupList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    inference.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    int getInstanceGroupCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    java.util.List<? extends inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
        getInstanceGroupOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @return The defaultModelFilename.
     */
    java.lang.String getDefaultModelFilename();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @return The bytes for defaultModelFilename.
     */
    com.google.protobuf.ByteString
        getDefaultModelFilenameBytes();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    int getCcModelFilenamesCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    boolean containsCcModelFilenames(
        java.lang.String key);
    /**
     * Use {@link #getCcModelFilenamesMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getCcModelFilenames();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getCcModelFilenamesMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    java.lang.String getCcModelFilenamesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    java.lang.String getCcModelFilenamesOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    int getMetricTagsCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    boolean containsMetricTags(
        java.lang.String key);
    /**
     * Use {@link #getMetricTagsMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, java.lang.String>
    getMetricTags();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    java.util.Map<java.lang.String, java.lang.String>
    getMetricTagsMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    java.lang.String getMetricTagsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    java.lang.String getMetricTagsOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    int getParametersCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    boolean containsParameters(
        java.lang.String key);
    /**
     * Use {@link #getParametersMap()} instead.
     */
    @java.lang.Deprecated
    java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
    getParameters();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
    getParametersMap();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */

    inference.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
        java.lang.String key,
        inference.ModelConfigOuterClass.ModelParameter defaultValue);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */

    inference.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
        java.lang.String key);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    java.util.List<inference.ModelConfigOuterClass.ModelWarmup> 
        getModelWarmupList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    inference.ModelConfigOuterClass.ModelWarmup getModelWarmup(int index);
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    int getModelWarmupCount();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    java.util.List<? extends inference.ModelConfigOuterClass.ModelWarmupOrBuilder> 
        getModelWarmupOrBuilderList();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    inference.ModelConfigOuterClass.ModelWarmupOrBuilder getModelWarmupOrBuilder(
        int index);

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     * @return Whether the modelOperations field is set.
     */
    boolean hasModelOperations();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     * @return The modelOperations.
     */
    inference.ModelConfigOuterClass.ModelOperations getModelOperations();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     */
    inference.ModelConfigOuterClass.ModelOperationsOrBuilder getModelOperationsOrBuilder();

    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     * @return Whether the modelTransactionPolicy field is set.
     */
    boolean hasModelTransactionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     * @return The modelTransactionPolicy.
     */
    inference.ModelConfigOuterClass.ModelTransactionPolicy getModelTransactionPolicy();
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     */
    inference.ModelConfigOuterClass.ModelTransactionPolicyOrBuilder getModelTransactionPolicyOrBuilder();

    public inference.ModelConfigOuterClass.ModelConfig.SchedulingChoiceCase getSchedulingChoiceCase();
  }
  /**
   * <pre>
   *&#64;&#64;
   *&#64;&#64;.. cpp:var:: message ModelConfig
   *&#64;&#64;
   *&#64;&#64;   A model configuration.
   *&#64;&#64;
   * </pre>
   *
   * Protobuf type {@code inference.ModelConfig}
   */
  public static final class ModelConfig extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:inference.ModelConfig)
      ModelConfigOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ModelConfig.newBuilder() to construct.
    private ModelConfig(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ModelConfig() {
      name_ = "";
      platform_ = "";
      backend_ = "";
      input_ = java.util.Collections.emptyList();
      output_ = java.util.Collections.emptyList();
      batchInput_ = java.util.Collections.emptyList();
      batchOutput_ = java.util.Collections.emptyList();
      instanceGroup_ = java.util.Collections.emptyList();
      defaultModelFilename_ = "";
      modelWarmup_ = java.util.Collections.emptyList();
    }

    @java.lang.Override
    @SuppressWarnings({"unused"})
    protected java.lang.Object newInstance(
        UnusedPrivateParameter unused) {
      return new ModelConfig();
    }

    @java.lang.Override
    public final com.google.protobuf.UnknownFieldSet
    getUnknownFields() {
      return this.unknownFields;
    }
    private ModelConfig(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      this();
      if (extensionRegistry == null) {
        throw new java.lang.NullPointerException();
      }
      int mutable_bitField0_ = 0;
      com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder();
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              java.lang.String s = input.readStringRequireUtf8();

              name_ = s;
              break;
            }
            case 18: {
              java.lang.String s = input.readStringRequireUtf8();

              platform_ = s;
              break;
            }
            case 26: {
              inference.ModelConfigOuterClass.ModelVersionPolicy.Builder subBuilder = null;
              if (versionPolicy_ != null) {
                subBuilder = versionPolicy_.toBuilder();
              }
              versionPolicy_ = input.readMessage(inference.ModelConfigOuterClass.ModelVersionPolicy.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(versionPolicy_);
                versionPolicy_ = subBuilder.buildPartial();
              }

              break;
            }
            case 32: {

              maxBatchSize_ = input.readInt32();
              break;
            }
            case 42: {
              if (!((mutable_bitField0_ & 0x00000001) != 0)) {
                input_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelInput>();
                mutable_bitField0_ |= 0x00000001;
              }
              input_.add(
                  input.readMessage(inference.ModelConfigOuterClass.ModelInput.parser(), extensionRegistry));
              break;
            }
            case 50: {
              if (!((mutable_bitField0_ & 0x00000002) != 0)) {
                output_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelOutput>();
                mutable_bitField0_ |= 0x00000002;
              }
              output_.add(
                  input.readMessage(inference.ModelConfigOuterClass.ModelOutput.parser(), extensionRegistry));
              break;
            }
            case 58: {
              if (!((mutable_bitField0_ & 0x00000010) != 0)) {
                instanceGroup_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelInstanceGroup>();
                mutable_bitField0_ |= 0x00000010;
              }
              instanceGroup_.add(
                  input.readMessage(inference.ModelConfigOuterClass.ModelInstanceGroup.parser(), extensionRegistry));
              break;
            }
            case 66: {
              java.lang.String s = input.readStringRequireUtf8();

              defaultModelFilename_ = s;
              break;
            }
            case 74: {
              if (!((mutable_bitField0_ & 0x00000020) != 0)) {
                ccModelFilenames_ = com.google.protobuf.MapField.newMapField(
                    CcModelFilenamesDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000020;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              ccModelFilenames__ = input.readMessage(
                  CcModelFilenamesDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              ccModelFilenames_.getMutableMap().put(
                  ccModelFilenames__.getKey(), ccModelFilenames__.getValue());
              break;
            }
            case 82: {
              if (!((mutable_bitField0_ & 0x00000040) != 0)) {
                metricTags_ = com.google.protobuf.MapField.newMapField(
                    MetricTagsDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000040;
              }
              com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
              metricTags__ = input.readMessage(
                  MetricTagsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              metricTags_.getMutableMap().put(
                  metricTags__.getKey(), metricTags__.getValue());
              break;
            }
            case 90: {
              inference.ModelConfigOuterClass.ModelDynamicBatching.Builder subBuilder = null;
              if (schedulingChoiceCase_ == 11) {
                subBuilder = ((inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_).toBuilder();
              }
              schedulingChoice_ =
                  input.readMessage(inference.ModelConfigOuterClass.ModelDynamicBatching.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_);
                schedulingChoice_ = subBuilder.buildPartial();
              }
              schedulingChoiceCase_ = 11;
              break;
            }
            case 98: {
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.Builder subBuilder = null;
              if (optimization_ != null) {
                subBuilder = optimization_.toBuilder();
              }
              optimization_ = input.readMessage(inference.ModelConfigOuterClass.ModelOptimizationPolicy.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(optimization_);
                optimization_ = subBuilder.buildPartial();
              }

              break;
            }
            case 106: {
              inference.ModelConfigOuterClass.ModelSequenceBatching.Builder subBuilder = null;
              if (schedulingChoiceCase_ == 13) {
                subBuilder = ((inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_).toBuilder();
              }
              schedulingChoice_ =
                  input.readMessage(inference.ModelConfigOuterClass.ModelSequenceBatching.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_);
                schedulingChoice_ = subBuilder.buildPartial();
              }
              schedulingChoiceCase_ = 13;
              break;
            }
            case 114: {
              if (!((mutable_bitField0_ & 0x00000080) != 0)) {
                parameters_ = com.google.protobuf.MapField.newMapField(
                    ParametersDefaultEntryHolder.defaultEntry);
                mutable_bitField0_ |= 0x00000080;
              }
              com.google.protobuf.MapEntry<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
              parameters__ = input.readMessage(
                  ParametersDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
              parameters_.getMutableMap().put(
                  parameters__.getKey(), parameters__.getValue());
              break;
            }
            case 122: {
              inference.ModelConfigOuterClass.ModelEnsembling.Builder subBuilder = null;
              if (schedulingChoiceCase_ == 15) {
                subBuilder = ((inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_).toBuilder();
              }
              schedulingChoice_ =
                  input.readMessage(inference.ModelConfigOuterClass.ModelEnsembling.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom((inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_);
                schedulingChoice_ = subBuilder.buildPartial();
              }
              schedulingChoiceCase_ = 15;
              break;
            }
            case 130: {
              if (!((mutable_bitField0_ & 0x00000100) != 0)) {
                modelWarmup_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelWarmup>();
                mutable_bitField0_ |= 0x00000100;
              }
              modelWarmup_.add(
                  input.readMessage(inference.ModelConfigOuterClass.ModelWarmup.parser(), extensionRegistry));
              break;
            }
            case 138: {
              java.lang.String s = input.readStringRequireUtf8();

              backend_ = s;
              break;
            }
            case 146: {
              inference.ModelConfigOuterClass.ModelOperations.Builder subBuilder = null;
              if (modelOperations_ != null) {
                subBuilder = modelOperations_.toBuilder();
              }
              modelOperations_ = input.readMessage(inference.ModelConfigOuterClass.ModelOperations.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(modelOperations_);
                modelOperations_ = subBuilder.buildPartial();
              }

              break;
            }
            case 154: {
              inference.ModelConfigOuterClass.ModelTransactionPolicy.Builder subBuilder = null;
              if (modelTransactionPolicy_ != null) {
                subBuilder = modelTransactionPolicy_.toBuilder();
              }
              modelTransactionPolicy_ = input.readMessage(inference.ModelConfigOuterClass.ModelTransactionPolicy.parser(), extensionRegistry);
              if (subBuilder != null) {
                subBuilder.mergeFrom(modelTransactionPolicy_);
                modelTransactionPolicy_ = subBuilder.buildPartial();
              }

              break;
            }
            case 162: {
              if (!((mutable_bitField0_ & 0x00000004) != 0)) {
                batchInput_ = new java.util.ArrayList<inference.ModelConfigOuterClass.BatchInput>();
                mutable_bitField0_ |= 0x00000004;
              }
              batchInput_.add(
                  input.readMessage(inference.ModelConfigOuterClass.BatchInput.parser(), extensionRegistry));
              break;
            }
            case 170: {
              if (!((mutable_bitField0_ & 0x00000008) != 0)) {
                batchOutput_ = new java.util.ArrayList<inference.ModelConfigOuterClass.BatchOutput>();
                mutable_bitField0_ |= 0x00000008;
              }
              batchOutput_.add(
                  input.readMessage(inference.ModelConfigOuterClass.BatchOutput.parser(), extensionRegistry));
              break;
            }
            default: {
              if (!parseUnknownField(
                  input, unknownFields, extensionRegistry, tag)) {
                done = true;
              }
              break;
            }
          }
        }
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(this);
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(
            e).setUnfinishedMessage(this);
      } finally {
        if (((mutable_bitField0_ & 0x00000001) != 0)) {
          input_ = java.util.Collections.unmodifiableList(input_);
        }
        if (((mutable_bitField0_ & 0x00000002) != 0)) {
          output_ = java.util.Collections.unmodifiableList(output_);
        }
        if (((mutable_bitField0_ & 0x00000010) != 0)) {
          instanceGroup_ = java.util.Collections.unmodifiableList(instanceGroup_);
        }
        if (((mutable_bitField0_ & 0x00000100) != 0)) {
          modelWarmup_ = java.util.Collections.unmodifiableList(modelWarmup_);
        }
        if (((mutable_bitField0_ & 0x00000004) != 0)) {
          batchInput_ = java.util.Collections.unmodifiableList(batchInput_);
        }
        if (((mutable_bitField0_ & 0x00000008) != 0)) {
          batchOutput_ = java.util.Collections.unmodifiableList(batchOutput_);
        }
        this.unknownFields = unknownFields.build();
        makeExtensionsImmutable();
      }
    }
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelConfig_descriptor;
    }

    @SuppressWarnings({"rawtypes"})
    @java.lang.Override
    protected com.google.protobuf.MapField internalGetMapField(
        int number) {
      switch (number) {
        case 9:
          return internalGetCcModelFilenames();
        case 10:
          return internalGetMetricTags();
        case 14:
          return internalGetParameters();
        default:
          throw new RuntimeException(
              "Invalid map field number: " + number);
      }
    }
    @java.lang.Override
    protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return inference.ModelConfigOuterClass.internal_static_inference_ModelConfig_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              inference.ModelConfigOuterClass.ModelConfig.class, inference.ModelConfigOuterClass.ModelConfig.Builder.class);
    }

    private int schedulingChoiceCase_ = 0;
    private java.lang.Object schedulingChoice_;
    public enum SchedulingChoiceCase
        implements com.google.protobuf.Internal.EnumLite,
            com.google.protobuf.AbstractMessage.InternalOneOfEnum {
      DYNAMIC_BATCHING(11),
      SEQUENCE_BATCHING(13),
      ENSEMBLE_SCHEDULING(15),
      SCHEDULINGCHOICE_NOT_SET(0);
      private final int value;
      private SchedulingChoiceCase(int value) {
        this.value = value;
      }
      /**
       * @param value The number of the enum to look for.
       * @return The enum associated with the given number.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @java.lang.Deprecated
      public static SchedulingChoiceCase valueOf(int value) {
        return forNumber(value);
      }

      public static SchedulingChoiceCase forNumber(int value) {
        switch (value) {
          case 11: return DYNAMIC_BATCHING;
          case 13: return SEQUENCE_BATCHING;
          case 15: return ENSEMBLE_SCHEDULING;
          case 0: return SCHEDULINGCHOICE_NOT_SET;
          default: return null;
        }
      }
      public int getNumber() {
        return this.value;
      }
    };

    public SchedulingChoiceCase
    getSchedulingChoiceCase() {
      return SchedulingChoiceCase.forNumber(
          schedulingChoiceCase_);
    }

    public static final int NAME_FIELD_NUMBER = 1;
    private volatile java.lang.Object name_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The name.
     */
    @java.lang.Override
    public java.lang.String getName() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string name
     *&#64;&#64;
     *&#64;&#64;     The name of the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      java.lang.Object ref = name_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int PLATFORM_FIELD_NUMBER = 2;
    private volatile java.lang.Object platform_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @return The platform.
     */
    @java.lang.Override
    public java.lang.String getPlatform() {
      java.lang.Object ref = platform_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        platform_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string platform
     *&#64;&#64;
     *&#64;&#64;     The framework for the model. Possible values are
     *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
     *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
     *&#64;&#64;     "pytorch_libtorch" and "custom".
     *&#64;&#64;
     * </pre>
     *
     * <code>string platform = 2;</code>
     * @return The bytes for platform.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getPlatformBytes() {
      java.lang.Object ref = platform_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        platform_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int BACKEND_FIELD_NUMBER = 17;
    private volatile java.lang.Object backend_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @return The backend.
     */
    @java.lang.Override
    public java.lang.String getBackend() {
      java.lang.Object ref = backend_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        backend_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string backend
     *&#64;&#64;
     *&#64;&#64;     The backend used by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>string backend = 17;</code>
     * @return The bytes for backend.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getBackendBytes() {
      java.lang.Object ref = backend_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        backend_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VERSION_POLICY_FIELD_NUMBER = 3;
    private inference.ModelConfigOuterClass.ModelVersionPolicy versionPolicy_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     * @return Whether the versionPolicy field is set.
     */
    @java.lang.Override
    public boolean hasVersionPolicy() {
      return versionPolicy_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     * @return The versionPolicy.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy() {
      return versionPolicy_ == null ? inference.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
     *&#64;&#64;
     *&#64;&#64;     Policy indicating which version(s) of the model will be served.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelVersionPolicyOrBuilder getVersionPolicyOrBuilder() {
      return getVersionPolicy();
    }

    public static final int MAX_BATCH_SIZE_FIELD_NUMBER = 4;
    private int maxBatchSize_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: int32 max_batch_size
     *&#64;&#64;
     *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
     *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
     *&#64;&#64;     indicates that batching is not allowed for the model and the
     *&#64;&#64;     dimension/shape of the input and output tensors must exactly
     *&#64;&#64;     match what is specified in the input and output configuration. A
     *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
     *&#64;&#64;     so the model expects the input tensors to have an additional
     *&#64;&#64;     initial dimension for the batching that is not specified in the
     *&#64;&#64;     input (for example, if the model supports batched inputs of
     *&#64;&#64;     2-dimensional tensors then the model configuration will specify
     *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
     *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
     *&#64;&#64;     returned outputs will also have an additional initial dimension
     *&#64;&#64;     for the batch.
     *&#64;&#64;
     * </pre>
     *
     * <code>int32 max_batch_size = 4;</code>
     * @return The maxBatchSize.
     */
    @java.lang.Override
    public int getMaxBatchSize() {
      return maxBatchSize_;
    }

    public static final int INPUT_FIELD_NUMBER = 5;
    private java.util.List<inference.ModelConfigOuterClass.ModelInput> input_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelInput> getInputList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    @java.lang.Override
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelInputOrBuilder> 
        getInputOrBuilderList() {
      return input_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    @java.lang.Override
    public int getInputCount() {
      return input_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInput getInput(int index) {
      return input_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The inputs request by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInput input = 5;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
        int index) {
      return input_.get(index);
    }

    public static final int OUTPUT_FIELD_NUMBER = 6;
    private java.util.List<inference.ModelConfigOuterClass.ModelOutput> output_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelOutput> getOutputList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    @java.lang.Override
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelOutputOrBuilder> 
        getOutputOrBuilderList() {
      return output_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    @java.lang.Override
    public int getOutputCount() {
      return output_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOutput getOutput(int index) {
      return output_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelOutput output = 6;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
        int index) {
      return output_.get(index);
    }

    public static final int BATCH_INPUT_FIELD_NUMBER = 20;
    private java.util.List<inference.ModelConfigOuterClass.BatchInput> batchInput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.BatchInput> getBatchInputList() {
      return batchInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    @java.lang.Override
    public java.util.List<? extends inference.ModelConfigOuterClass.BatchInputOrBuilder> 
        getBatchInputOrBuilderList() {
      return batchInput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    @java.lang.Override
    public int getBatchInputCount() {
      return batchInput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchInput getBatchInput(int index) {
      return batchInput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
     *&#64;&#64;
     *&#64;&#64;     The model input(s) that the server should use to communicate
     *&#64;&#64;     batch related values to the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchInput batch_input = 20;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchInputOrBuilder getBatchInputOrBuilder(
        int index) {
      return batchInput_.get(index);
    }

    public static final int BATCH_OUTPUT_FIELD_NUMBER = 21;
    private java.util.List<inference.ModelConfigOuterClass.BatchOutput> batchOutput_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.BatchOutput> getBatchOutputList() {
      return batchOutput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    @java.lang.Override
    public java.util.List<? extends inference.ModelConfigOuterClass.BatchOutputOrBuilder> 
        getBatchOutputOrBuilderList() {
      return batchOutput_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    @java.lang.Override
    public int getBatchOutputCount() {
      return batchOutput_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchOutput getBatchOutput(int index) {
      return batchOutput_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
     *&#64;&#64;
     *&#64;&#64;     The outputs produced by the model that requires special handling
     *&#64;&#64;     by the model backend.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.BatchOutput batch_output = 21;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.BatchOutputOrBuilder getBatchOutputOrBuilder(
        int index) {
      return batchOutput_.get(index);
    }

    public static final int OPTIMIZATION_FIELD_NUMBER = 12;
    private inference.ModelConfigOuterClass.ModelOptimizationPolicy optimization_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     * @return Whether the optimization field is set.
     */
    @java.lang.Override
    public boolean hasOptimization() {
      return optimization_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     * @return The optimization.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization() {
      return optimization_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
     *&#64;&#64;
     *&#64;&#64;     Optimization configuration for the model. If not specified
     *&#64;&#64;     then default optimization policy is used.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder getOptimizationOrBuilder() {
      return getOptimization();
    }

    public static final int DYNAMIC_BATCHING_FIELD_NUMBER = 11;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     * @return Whether the dynamicBatching field is set.
     */
    @java.lang.Override
    public boolean hasDynamicBatching() {
      return schedulingChoiceCase_ == 11;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     * @return The dynamicBatching.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching() {
      if (schedulingChoiceCase_ == 11) {
         return (inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
      }
      return inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the dynamic-batching scheduling
     *&#64;&#64;       policy. With dynamic-batching the scheduler may group
     *&#64;&#64;       together independent requests into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder getDynamicBatchingOrBuilder() {
      if (schedulingChoiceCase_ == 11) {
         return (inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
      }
      return inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
    }

    public static final int SEQUENCE_BATCHING_FIELD_NUMBER = 13;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     * @return Whether the sequenceBatching field is set.
     */
    @java.lang.Override
    public boolean hasSequenceBatching() {
      return schedulingChoiceCase_ == 13;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     * @return The sequenceBatching.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching() {
      if (schedulingChoiceCase_ == 13) {
         return (inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
      }
      return inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the sequence-batching scheduling
     *&#64;&#64;       policy. With sequence-batching, inference requests
     *&#64;&#64;       with the same correlation ID are routed to the same
     *&#64;&#64;       model instance. Multiple sequences of inference requests
     *&#64;&#64;       may be batched together into a single batch to
     *&#64;&#64;       improve inference throughput.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder getSequenceBatchingOrBuilder() {
      if (schedulingChoiceCase_ == 13) {
         return (inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
      }
      return inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
    }

    public static final int ENSEMBLE_SCHEDULING_FIELD_NUMBER = 15;
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     * @return Whether the ensembleScheduling field is set.
     */
    @java.lang.Override
    public boolean hasEnsembleScheduling() {
      return schedulingChoiceCase_ == 15;
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     * @return The ensembleScheduling.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling() {
      if (schedulingChoiceCase_ == 15) {
         return (inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
      }
      return inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
    }
    /**
     * <pre>
     *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
     *&#64;&#64;
     *&#64;&#64;       If specified, enables the model-ensembling scheduling
     *&#64;&#64;       policy. With model-ensembling, inference requests
     *&#64;&#64;       will be processed according to the specification, such as an
     *&#64;&#64;       execution sequence of models. The input specified in this model
     *&#64;&#64;       config will be the input for the ensemble, and the output
     *&#64;&#64;       specified will be the output of the ensemble.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelEnsemblingOrBuilder getEnsembleSchedulingOrBuilder() {
      if (schedulingChoiceCase_ == 15) {
         return (inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
      }
      return inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
    }

    public static final int INSTANCE_GROUP_FIELD_NUMBER = 7;
    private java.util.List<inference.ModelConfigOuterClass.ModelInstanceGroup> instanceGroup_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelInstanceGroup> getInstanceGroupList() {
      return instanceGroup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    @java.lang.Override
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
        getInstanceGroupOrBuilderList() {
      return instanceGroup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    @java.lang.Override
    public int getInstanceGroupCount() {
      return instanceGroup_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index) {
      return instanceGroup_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
     *&#64;&#64;
     *&#64;&#64;     Instances of this model. If not specified, one instance
     *&#64;&#64;     of the model will be instantiated on each available GPU.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
        int index) {
      return instanceGroup_.get(index);
    }

    public static final int DEFAULT_MODEL_FILENAME_FIELD_NUMBER = 8;
    private volatile java.lang.Object defaultModelFilename_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @return The defaultModelFilename.
     */
    @java.lang.Override
    public java.lang.String getDefaultModelFilename() {
      java.lang.Object ref = defaultModelFilename_;
      if (ref instanceof java.lang.String) {
        return (java.lang.String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        java.lang.String s = bs.toStringUtf8();
        defaultModelFilename_ = s;
        return s;
      }
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: string default_model_filename
     *&#64;&#64;
     *&#64;&#64;     Optional filename of the model file to use if a
     *&#64;&#64;     compute-capability specific model is not specified in
     *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
     *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
     *&#64;&#64;     'model.pt' depending on the model type.
     *&#64;&#64;
     * </pre>
     *
     * <code>string default_model_filename = 8;</code>
     * @return The bytes for defaultModelFilename.
     */
    @java.lang.Override
    public com.google.protobuf.ByteString
        getDefaultModelFilenameBytes() {
      java.lang.Object ref = defaultModelFilename_;
      if (ref instanceof java.lang.String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (java.lang.String) ref);
        defaultModelFilename_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int CC_MODEL_FILENAMES_FIELD_NUMBER = 9;
    private static final class CcModelFilenamesDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  inference.ModelConfigOuterClass.internal_static_inference_ModelConfig_CcModelFilenamesEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> ccModelFilenames_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetCcModelFilenames() {
      if (ccModelFilenames_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            CcModelFilenamesDefaultEntryHolder.defaultEntry);
      }
      return ccModelFilenames_;
    }

    public int getCcModelFilenamesCount() {
      return internalGetCcModelFilenames().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */

    @java.lang.Override
    public boolean containsCcModelFilenames(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetCcModelFilenames().getMap().containsKey(key);
    }
    /**
     * Use {@link #getCcModelFilenamesMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenames() {
      return getCcModelFilenamesMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenamesMap() {
      return internalGetCcModelFilenames().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    @java.lang.Override

    public java.lang.String getCcModelFilenamesOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetCcModelFilenames().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
     *&#64;&#64;
     *&#64;&#64;     Optional map from CUDA compute capability to the filename of
     *&#64;&#64;     the model that supports that compute capability. The filename
     *&#64;&#64;     refers to a file within the model version directory.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
     */
    @java.lang.Override

    public java.lang.String getCcModelFilenamesOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetCcModelFilenames().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int METRIC_TAGS_FIELD_NUMBER = 10;
    private static final class MetricTagsDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, java.lang.String> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, java.lang.String>newDefaultInstance(
                  inference.ModelConfigOuterClass.internal_static_inference_ModelConfig_MetricTagsEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "");
    }
    private com.google.protobuf.MapField<
        java.lang.String, java.lang.String> metricTags_;
    private com.google.protobuf.MapField<java.lang.String, java.lang.String>
    internalGetMetricTags() {
      if (metricTags_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            MetricTagsDefaultEntryHolder.defaultEntry);
      }
      return metricTags_;
    }

    public int getMetricTagsCount() {
      return internalGetMetricTags().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */

    @java.lang.Override
    public boolean containsMetricTags(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetMetricTags().getMap().containsKey(key);
    }
    /**
     * Use {@link #getMetricTagsMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, java.lang.String> getMetricTags() {
      return getMetricTagsMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, java.lang.String> getMetricTagsMap() {
      return internalGetMetricTags().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricTagsOrDefault(
        java.lang.String key,
        java.lang.String defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetricTags().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
     *&#64;&#64;
     *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
     *&#64;&#64;     reported for this model. These tags are applied to the metrics
     *&#64;&#64;     reported on the HTTP metrics port.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, string&gt; metric_tags = 10;</code>
     */
    @java.lang.Override

    public java.lang.String getMetricTagsOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, java.lang.String> map =
          internalGetMetricTags().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int PARAMETERS_FIELD_NUMBER = 14;
    private static final class ParametersDefaultEntryHolder {
      static final com.google.protobuf.MapEntry<
          java.lang.String, inference.ModelConfigOuterClass.ModelParameter> defaultEntry =
              com.google.protobuf.MapEntry
              .<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>newDefaultInstance(
                  inference.ModelConfigOuterClass.internal_static_inference_ModelConfig_ParametersEntry_descriptor, 
                  com.google.protobuf.WireFormat.FieldType.STRING,
                  "",
                  com.google.protobuf.WireFormat.FieldType.MESSAGE,
                  inference.ModelConfigOuterClass.ModelParameter.getDefaultInstance());
    }
    private com.google.protobuf.MapField<
        java.lang.String, inference.ModelConfigOuterClass.ModelParameter> parameters_;
    private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
    internalGetParameters() {
      if (parameters_ == null) {
        return com.google.protobuf.MapField.emptyMapField(
            ParametersDefaultEntryHolder.defaultEntry);
      }
      return parameters_;
    }

    public int getParametersCount() {
      return internalGetParameters().getMap().size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */

    @java.lang.Override
    public boolean containsParameters(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      return internalGetParameters().getMap().containsKey(key);
    }
    /**
     * Use {@link #getParametersMap()} instead.
     */
    @java.lang.Override
    @java.lang.Deprecated
    public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> getParameters() {
      return getParametersMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    @java.lang.Override

    public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> getParametersMap() {
      return internalGetParameters().getMap();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
        java.lang.String key,
        inference.ModelConfigOuterClass.ModelParameter defaultValue) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> map =
          internalGetParameters().getMap();
      return map.containsKey(key) ? map.get(key) : defaultValue;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
     *&#64;&#64;
     *&#64;&#64;     Optional model parameters. User-specified parameter values that
     *&#64;&#64;     are made available to custom backends.
     *&#64;&#64;
     * </pre>
     *
     * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
     */
    @java.lang.Override

    public inference.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
        java.lang.String key) {
      if (key == null) { throw new java.lang.NullPointerException(); }
      java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> map =
          internalGetParameters().getMap();
      if (!map.containsKey(key)) {
        throw new java.lang.IllegalArgumentException();
      }
      return map.get(key);
    }

    public static final int MODEL_WARMUP_FIELD_NUMBER = 16;
    private java.util.List<inference.ModelConfigOuterClass.ModelWarmup> modelWarmup_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    @java.lang.Override
    public java.util.List<inference.ModelConfigOuterClass.ModelWarmup> getModelWarmupList() {
      return modelWarmup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    @java.lang.Override
    public java.util.List<? extends inference.ModelConfigOuterClass.ModelWarmupOrBuilder> 
        getModelWarmupOrBuilderList() {
      return modelWarmup_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    @java.lang.Override
    public int getModelWarmupCount() {
      return modelWarmup_.size();
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelWarmup getModelWarmup(int index) {
      return modelWarmup_.get(index);
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
     *&#64;&#64;
     *&#64;&#64;     Warmup setting of this model. If specified, all instances
     *&#64;&#64;     will be run with the request samples in sequence before
     *&#64;&#64;     serving the model.
     *&#64;&#64;     This field can only be specified if the model is not an ensemble
     *&#64;&#64;     model.
     *&#64;&#64;
     * </pre>
     *
     * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelWarmupOrBuilder getModelWarmupOrBuilder(
        int index) {
      return modelWarmup_.get(index);
    }

    public static final int MODEL_OPERATIONS_FIELD_NUMBER = 18;
    private inference.ModelConfigOuterClass.ModelOperations modelOperations_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     * @return Whether the modelOperations field is set.
     */
    @java.lang.Override
    public boolean hasModelOperations() {
      return modelOperations_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     * @return The modelOperations.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOperations getModelOperations() {
      return modelOperations_ == null ? inference.ModelConfigOuterClass.ModelOperations.getDefaultInstance() : modelOperations_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
     *&#64;&#64;
     *&#64;&#64;     Optional metadata of the libraries providing custom operations for
     *&#64;&#64;     this model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelOperations model_operations = 18;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelOperationsOrBuilder getModelOperationsOrBuilder() {
      return getModelOperations();
    }

    public static final int MODEL_TRANSACTION_POLICY_FIELD_NUMBER = 19;
    private inference.ModelConfigOuterClass.ModelTransactionPolicy modelTransactionPolicy_;
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     * @return Whether the modelTransactionPolicy field is set.
     */
    @java.lang.Override
    public boolean hasModelTransactionPolicy() {
      return modelTransactionPolicy_ != null;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     * @return The modelTransactionPolicy.
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTransactionPolicy getModelTransactionPolicy() {
      return modelTransactionPolicy_ == null ? inference.ModelConfigOuterClass.ModelTransactionPolicy.getDefaultInstance() : modelTransactionPolicy_;
    }
    /**
     * <pre>
     *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
     *&#64;&#64;
     *&#64;&#64;     Optional specification that describes the nature of transactions
     *&#64;&#64;     to be expected from the model.
     *&#64;&#64;
     * </pre>
     *
     * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
     */
    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelTransactionPolicyOrBuilder getModelTransactionPolicyOrBuilder() {
      return getModelTransactionPolicy();
    }

    private byte memoizedIsInitialized = -1;
    @java.lang.Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @java.lang.Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!getNameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      if (!getPlatformBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 2, platform_);
      }
      if (versionPolicy_ != null) {
        output.writeMessage(3, getVersionPolicy());
      }
      if (maxBatchSize_ != 0) {
        output.writeInt32(4, maxBatchSize_);
      }
      for (int i = 0; i < input_.size(); i++) {
        output.writeMessage(5, input_.get(i));
      }
      for (int i = 0; i < output_.size(); i++) {
        output.writeMessage(6, output_.get(i));
      }
      for (int i = 0; i < instanceGroup_.size(); i++) {
        output.writeMessage(7, instanceGroup_.get(i));
      }
      if (!getDefaultModelFilenameBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 8, defaultModelFilename_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetCcModelFilenames(),
          CcModelFilenamesDefaultEntryHolder.defaultEntry,
          9);
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetMetricTags(),
          MetricTagsDefaultEntryHolder.defaultEntry,
          10);
      if (schedulingChoiceCase_ == 11) {
        output.writeMessage(11, (inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_);
      }
      if (optimization_ != null) {
        output.writeMessage(12, getOptimization());
      }
      if (schedulingChoiceCase_ == 13) {
        output.writeMessage(13, (inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_);
      }
      com.google.protobuf.GeneratedMessageV3
        .serializeStringMapTo(
          output,
          internalGetParameters(),
          ParametersDefaultEntryHolder.defaultEntry,
          14);
      if (schedulingChoiceCase_ == 15) {
        output.writeMessage(15, (inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_);
      }
      for (int i = 0; i < modelWarmup_.size(); i++) {
        output.writeMessage(16, modelWarmup_.get(i));
      }
      if (!getBackendBytes().isEmpty()) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 17, backend_);
      }
      if (modelOperations_ != null) {
        output.writeMessage(18, getModelOperations());
      }
      if (modelTransactionPolicy_ != null) {
        output.writeMessage(19, getModelTransactionPolicy());
      }
      for (int i = 0; i < batchInput_.size(); i++) {
        output.writeMessage(20, batchInput_.get(i));
      }
      for (int i = 0; i < batchOutput_.size(); i++) {
        output.writeMessage(21, batchOutput_.get(i));
      }
      unknownFields.writeTo(output);
    }

    @java.lang.Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!getNameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      if (!getPlatformBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(2, platform_);
      }
      if (versionPolicy_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, getVersionPolicy());
      }
      if (maxBatchSize_ != 0) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(4, maxBatchSize_);
      }
      for (int i = 0; i < input_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, input_.get(i));
      }
      for (int i = 0; i < output_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, output_.get(i));
      }
      for (int i = 0; i < instanceGroup_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(7, instanceGroup_.get(i));
      }
      if (!getDefaultModelFilenameBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(8, defaultModelFilename_);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetCcModelFilenames().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        ccModelFilenames__ = CcModelFilenamesDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(9, ccModelFilenames__);
      }
      for (java.util.Map.Entry<java.lang.String, java.lang.String> entry
           : internalGetMetricTags().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, java.lang.String>
        metricTags__ = MetricTagsDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(10, metricTags__);
      }
      if (schedulingChoiceCase_ == 11) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(11, (inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_);
      }
      if (optimization_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(12, getOptimization());
      }
      if (schedulingChoiceCase_ == 13) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(13, (inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_);
      }
      for (java.util.Map.Entry<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> entry
           : internalGetParameters().getMap().entrySet()) {
        com.google.protobuf.MapEntry<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
        parameters__ = ParametersDefaultEntryHolder.defaultEntry.newBuilderForType()
            .setKey(entry.getKey())
            .setValue(entry.getValue())
            .build();
        size += com.google.protobuf.CodedOutputStream
            .computeMessageSize(14, parameters__);
      }
      if (schedulingChoiceCase_ == 15) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(15, (inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_);
      }
      for (int i = 0; i < modelWarmup_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(16, modelWarmup_.get(i));
      }
      if (!getBackendBytes().isEmpty()) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(17, backend_);
      }
      if (modelOperations_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(18, getModelOperations());
      }
      if (modelTransactionPolicy_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(19, getModelTransactionPolicy());
      }
      for (int i = 0; i < batchInput_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(20, batchInput_.get(i));
      }
      for (int i = 0; i < batchOutput_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(21, batchOutput_.get(i));
      }
      size += unknownFields.getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @java.lang.Override
    public boolean equals(final java.lang.Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof inference.ModelConfigOuterClass.ModelConfig)) {
        return super.equals(obj);
      }
      inference.ModelConfigOuterClass.ModelConfig other = (inference.ModelConfigOuterClass.ModelConfig) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (!getPlatform()
          .equals(other.getPlatform())) return false;
      if (!getBackend()
          .equals(other.getBackend())) return false;
      if (hasVersionPolicy() != other.hasVersionPolicy()) return false;
      if (hasVersionPolicy()) {
        if (!getVersionPolicy()
            .equals(other.getVersionPolicy())) return false;
      }
      if (getMaxBatchSize()
          != other.getMaxBatchSize()) return false;
      if (!getInputList()
          .equals(other.getInputList())) return false;
      if (!getOutputList()
          .equals(other.getOutputList())) return false;
      if (!getBatchInputList()
          .equals(other.getBatchInputList())) return false;
      if (!getBatchOutputList()
          .equals(other.getBatchOutputList())) return false;
      if (hasOptimization() != other.hasOptimization()) return false;
      if (hasOptimization()) {
        if (!getOptimization()
            .equals(other.getOptimization())) return false;
      }
      if (!getInstanceGroupList()
          .equals(other.getInstanceGroupList())) return false;
      if (!getDefaultModelFilename()
          .equals(other.getDefaultModelFilename())) return false;
      if (!internalGetCcModelFilenames().equals(
          other.internalGetCcModelFilenames())) return false;
      if (!internalGetMetricTags().equals(
          other.internalGetMetricTags())) return false;
      if (!internalGetParameters().equals(
          other.internalGetParameters())) return false;
      if (!getModelWarmupList()
          .equals(other.getModelWarmupList())) return false;
      if (hasModelOperations() != other.hasModelOperations()) return false;
      if (hasModelOperations()) {
        if (!getModelOperations()
            .equals(other.getModelOperations())) return false;
      }
      if (hasModelTransactionPolicy() != other.hasModelTransactionPolicy()) return false;
      if (hasModelTransactionPolicy()) {
        if (!getModelTransactionPolicy()
            .equals(other.getModelTransactionPolicy())) return false;
      }
      if (!getSchedulingChoiceCase().equals(other.getSchedulingChoiceCase())) return false;
      switch (schedulingChoiceCase_) {
        case 11:
          if (!getDynamicBatching()
              .equals(other.getDynamicBatching())) return false;
          break;
        case 13:
          if (!getSequenceBatching()
              .equals(other.getSequenceBatching())) return false;
          break;
        case 15:
          if (!getEnsembleScheduling()
              .equals(other.getEnsembleScheduling())) return false;
          break;
        case 0:
        default:
      }
      if (!unknownFields.equals(other.unknownFields)) return false;
      return true;
    }

    @java.lang.Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      hash = (37 * hash) + PLATFORM_FIELD_NUMBER;
      hash = (53 * hash) + getPlatform().hashCode();
      hash = (37 * hash) + BACKEND_FIELD_NUMBER;
      hash = (53 * hash) + getBackend().hashCode();
      if (hasVersionPolicy()) {
        hash = (37 * hash) + VERSION_POLICY_FIELD_NUMBER;
        hash = (53 * hash) + getVersionPolicy().hashCode();
      }
      hash = (37 * hash) + MAX_BATCH_SIZE_FIELD_NUMBER;
      hash = (53 * hash) + getMaxBatchSize();
      if (getInputCount() > 0) {
        hash = (37 * hash) + INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getInputList().hashCode();
      }
      if (getOutputCount() > 0) {
        hash = (37 * hash) + OUTPUT_FIELD_NUMBER;
        hash = (53 * hash) + getOutputList().hashCode();
      }
      if (getBatchInputCount() > 0) {
        hash = (37 * hash) + BATCH_INPUT_FIELD_NUMBER;
        hash = (53 * hash) + getBatchInputList().hashCode();
      }
      if (getBatchOutputCount() > 0) {
        hash = (37 * hash) + BATCH_OUTPUT_FIELD_NUMBER;
        hash = (53 * hash) + getBatchOutputList().hashCode();
      }
      if (hasOptimization()) {
        hash = (37 * hash) + OPTIMIZATION_FIELD_NUMBER;
        hash = (53 * hash) + getOptimization().hashCode();
      }
      if (getInstanceGroupCount() > 0) {
        hash = (37 * hash) + INSTANCE_GROUP_FIELD_NUMBER;
        hash = (53 * hash) + getInstanceGroupList().hashCode();
      }
      hash = (37 * hash) + DEFAULT_MODEL_FILENAME_FIELD_NUMBER;
      hash = (53 * hash) + getDefaultModelFilename().hashCode();
      if (!internalGetCcModelFilenames().getMap().isEmpty()) {
        hash = (37 * hash) + CC_MODEL_FILENAMES_FIELD_NUMBER;
        hash = (53 * hash) + internalGetCcModelFilenames().hashCode();
      }
      if (!internalGetMetricTags().getMap().isEmpty()) {
        hash = (37 * hash) + METRIC_TAGS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetMetricTags().hashCode();
      }
      if (!internalGetParameters().getMap().isEmpty()) {
        hash = (37 * hash) + PARAMETERS_FIELD_NUMBER;
        hash = (53 * hash) + internalGetParameters().hashCode();
      }
      if (getModelWarmupCount() > 0) {
        hash = (37 * hash) + MODEL_WARMUP_FIELD_NUMBER;
        hash = (53 * hash) + getModelWarmupList().hashCode();
      }
      if (hasModelOperations()) {
        hash = (37 * hash) + MODEL_OPERATIONS_FIELD_NUMBER;
        hash = (53 * hash) + getModelOperations().hashCode();
      }
      if (hasModelTransactionPolicy()) {
        hash = (37 * hash) + MODEL_TRANSACTION_POLICY_FIELD_NUMBER;
        hash = (53 * hash) + getModelTransactionPolicy().hashCode();
      }
      switch (schedulingChoiceCase_) {
        case 11:
          hash = (37 * hash) + DYNAMIC_BATCHING_FIELD_NUMBER;
          hash = (53 * hash) + getDynamicBatching().hashCode();
          break;
        case 13:
          hash = (37 * hash) + SEQUENCE_BATCHING_FIELD_NUMBER;
          hash = (53 * hash) + getSequenceBatching().hashCode();
          break;
        case 15:
          hash = (37 * hash) + ENSEMBLE_SCHEDULING_FIELD_NUMBER;
          hash = (53 * hash) + getEnsembleScheduling().hashCode();
          break;
        case 0:
        default:
      }
      hash = (29 * hash) + unknownFields.hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static inference.ModelConfigOuterClass.ModelConfig parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @java.lang.Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(inference.ModelConfigOuterClass.ModelConfig prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @java.lang.Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     *&#64;&#64;
     *&#64;&#64;.. cpp:var:: message ModelConfig
     *&#64;&#64;
     *&#64;&#64;   A model configuration.
     *&#64;&#64;
     * </pre>
     *
     * Protobuf type {@code inference.ModelConfig}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:inference.ModelConfig)
        inference.ModelConfigOuterClass.ModelConfigOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelConfig_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 9:
            return internalGetCcModelFilenames();
          case 10:
            return internalGetMetricTags();
          case 14:
            return internalGetParameters();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @SuppressWarnings({"rawtypes"})
      protected com.google.protobuf.MapField internalGetMutableMapField(
          int number) {
        switch (number) {
          case 9:
            return internalGetMutableCcModelFilenames();
          case 10:
            return internalGetMutableMetricTags();
          case 14:
            return internalGetMutableParameters();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @java.lang.Override
      protected com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelConfig_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                inference.ModelConfigOuterClass.ModelConfig.class, inference.ModelConfigOuterClass.ModelConfig.Builder.class);
      }

      // Construct using inference.ModelConfigOuterClass.ModelConfig.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          com.google.protobuf.GeneratedMessageV3.BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getInputFieldBuilder();
          getOutputFieldBuilder();
          getBatchInputFieldBuilder();
          getBatchOutputFieldBuilder();
          getInstanceGroupFieldBuilder();
          getModelWarmupFieldBuilder();
        }
      }
      @java.lang.Override
      public Builder clear() {
        super.clear();
        name_ = "";

        platform_ = "";

        backend_ = "";

        if (versionPolicyBuilder_ == null) {
          versionPolicy_ = null;
        } else {
          versionPolicy_ = null;
          versionPolicyBuilder_ = null;
        }
        maxBatchSize_ = 0;

        if (inputBuilder_ == null) {
          input_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          inputBuilder_.clear();
        }
        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          outputBuilder_.clear();
        }
        if (batchInputBuilder_ == null) {
          batchInput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
        } else {
          batchInputBuilder_.clear();
        }
        if (batchOutputBuilder_ == null) {
          batchOutput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
        } else {
          batchOutputBuilder_.clear();
        }
        if (optimizationBuilder_ == null) {
          optimization_ = null;
        } else {
          optimization_ = null;
          optimizationBuilder_ = null;
        }
        if (instanceGroupBuilder_ == null) {
          instanceGroup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
        } else {
          instanceGroupBuilder_.clear();
        }
        defaultModelFilename_ = "";

        internalGetMutableCcModelFilenames().clear();
        internalGetMutableMetricTags().clear();
        internalGetMutableParameters().clear();
        if (modelWarmupBuilder_ == null) {
          modelWarmup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
        } else {
          modelWarmupBuilder_.clear();
        }
        if (modelOperationsBuilder_ == null) {
          modelOperations_ = null;
        } else {
          modelOperations_ = null;
          modelOperationsBuilder_ = null;
        }
        if (modelTransactionPolicyBuilder_ == null) {
          modelTransactionPolicy_ = null;
        } else {
          modelTransactionPolicy_ = null;
          modelTransactionPolicyBuilder_ = null;
        }
        schedulingChoiceCase_ = 0;
        schedulingChoice_ = null;
        return this;
      }

      @java.lang.Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return inference.ModelConfigOuterClass.internal_static_inference_ModelConfig_descriptor;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelConfig getDefaultInstanceForType() {
        return inference.ModelConfigOuterClass.ModelConfig.getDefaultInstance();
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelConfig build() {
        inference.ModelConfigOuterClass.ModelConfig result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelConfig buildPartial() {
        inference.ModelConfigOuterClass.ModelConfig result = new inference.ModelConfigOuterClass.ModelConfig(this);
        int from_bitField0_ = bitField0_;
        result.name_ = name_;
        result.platform_ = platform_;
        result.backend_ = backend_;
        if (versionPolicyBuilder_ == null) {
          result.versionPolicy_ = versionPolicy_;
        } else {
          result.versionPolicy_ = versionPolicyBuilder_.build();
        }
        result.maxBatchSize_ = maxBatchSize_;
        if (inputBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            input_ = java.util.Collections.unmodifiableList(input_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.input_ = input_;
        } else {
          result.input_ = inputBuilder_.build();
        }
        if (outputBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            output_ = java.util.Collections.unmodifiableList(output_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.output_ = output_;
        } else {
          result.output_ = outputBuilder_.build();
        }
        if (batchInputBuilder_ == null) {
          if (((bitField0_ & 0x00000004) != 0)) {
            batchInput_ = java.util.Collections.unmodifiableList(batchInput_);
            bitField0_ = (bitField0_ & ~0x00000004);
          }
          result.batchInput_ = batchInput_;
        } else {
          result.batchInput_ = batchInputBuilder_.build();
        }
        if (batchOutputBuilder_ == null) {
          if (((bitField0_ & 0x00000008) != 0)) {
            batchOutput_ = java.util.Collections.unmodifiableList(batchOutput_);
            bitField0_ = (bitField0_ & ~0x00000008);
          }
          result.batchOutput_ = batchOutput_;
        } else {
          result.batchOutput_ = batchOutputBuilder_.build();
        }
        if (optimizationBuilder_ == null) {
          result.optimization_ = optimization_;
        } else {
          result.optimization_ = optimizationBuilder_.build();
        }
        if (schedulingChoiceCase_ == 11) {
          if (dynamicBatchingBuilder_ == null) {
            result.schedulingChoice_ = schedulingChoice_;
          } else {
            result.schedulingChoice_ = dynamicBatchingBuilder_.build();
          }
        }
        if (schedulingChoiceCase_ == 13) {
          if (sequenceBatchingBuilder_ == null) {
            result.schedulingChoice_ = schedulingChoice_;
          } else {
            result.schedulingChoice_ = sequenceBatchingBuilder_.build();
          }
        }
        if (schedulingChoiceCase_ == 15) {
          if (ensembleSchedulingBuilder_ == null) {
            result.schedulingChoice_ = schedulingChoice_;
          } else {
            result.schedulingChoice_ = ensembleSchedulingBuilder_.build();
          }
        }
        if (instanceGroupBuilder_ == null) {
          if (((bitField0_ & 0x00000010) != 0)) {
            instanceGroup_ = java.util.Collections.unmodifiableList(instanceGroup_);
            bitField0_ = (bitField0_ & ~0x00000010);
          }
          result.instanceGroup_ = instanceGroup_;
        } else {
          result.instanceGroup_ = instanceGroupBuilder_.build();
        }
        result.defaultModelFilename_ = defaultModelFilename_;
        result.ccModelFilenames_ = internalGetCcModelFilenames();
        result.ccModelFilenames_.makeImmutable();
        result.metricTags_ = internalGetMetricTags();
        result.metricTags_.makeImmutable();
        result.parameters_ = internalGetParameters();
        result.parameters_.makeImmutable();
        if (modelWarmupBuilder_ == null) {
          if (((bitField0_ & 0x00000100) != 0)) {
            modelWarmup_ = java.util.Collections.unmodifiableList(modelWarmup_);
            bitField0_ = (bitField0_ & ~0x00000100);
          }
          result.modelWarmup_ = modelWarmup_;
        } else {
          result.modelWarmup_ = modelWarmupBuilder_.build();
        }
        if (modelOperationsBuilder_ == null) {
          result.modelOperations_ = modelOperations_;
        } else {
          result.modelOperations_ = modelOperationsBuilder_.build();
        }
        if (modelTransactionPolicyBuilder_ == null) {
          result.modelTransactionPolicy_ = modelTransactionPolicy_;
        } else {
          result.modelTransactionPolicy_ = modelTransactionPolicyBuilder_.build();
        }
        result.schedulingChoiceCase_ = schedulingChoiceCase_;
        onBuilt();
        return result;
      }

      @java.lang.Override
      public Builder clone() {
        return super.clone();
      }
      @java.lang.Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.setField(field, value);
      }
      @java.lang.Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @java.lang.Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @java.lang.Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, java.lang.Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @java.lang.Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          java.lang.Object value) {
        return super.addRepeatedField(field, value);
      }
      @java.lang.Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof inference.ModelConfigOuterClass.ModelConfig) {
          return mergeFrom((inference.ModelConfigOuterClass.ModelConfig)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(inference.ModelConfigOuterClass.ModelConfig other) {
        if (other == inference.ModelConfigOuterClass.ModelConfig.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          onChanged();
        }
        if (!other.getPlatform().isEmpty()) {
          platform_ = other.platform_;
          onChanged();
        }
        if (!other.getBackend().isEmpty()) {
          backend_ = other.backend_;
          onChanged();
        }
        if (other.hasVersionPolicy()) {
          mergeVersionPolicy(other.getVersionPolicy());
        }
        if (other.getMaxBatchSize() != 0) {
          setMaxBatchSize(other.getMaxBatchSize());
        }
        if (inputBuilder_ == null) {
          if (!other.input_.isEmpty()) {
            if (input_.isEmpty()) {
              input_ = other.input_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureInputIsMutable();
              input_.addAll(other.input_);
            }
            onChanged();
          }
        } else {
          if (!other.input_.isEmpty()) {
            if (inputBuilder_.isEmpty()) {
              inputBuilder_.dispose();
              inputBuilder_ = null;
              input_ = other.input_;
              bitField0_ = (bitField0_ & ~0x00000001);
              inputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getInputFieldBuilder() : null;
            } else {
              inputBuilder_.addAllMessages(other.input_);
            }
          }
        }
        if (outputBuilder_ == null) {
          if (!other.output_.isEmpty()) {
            if (output_.isEmpty()) {
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureOutputIsMutable();
              output_.addAll(other.output_);
            }
            onChanged();
          }
        } else {
          if (!other.output_.isEmpty()) {
            if (outputBuilder_.isEmpty()) {
              outputBuilder_.dispose();
              outputBuilder_ = null;
              output_ = other.output_;
              bitField0_ = (bitField0_ & ~0x00000002);
              outputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getOutputFieldBuilder() : null;
            } else {
              outputBuilder_.addAllMessages(other.output_);
            }
          }
        }
        if (batchInputBuilder_ == null) {
          if (!other.batchInput_.isEmpty()) {
            if (batchInput_.isEmpty()) {
              batchInput_ = other.batchInput_;
              bitField0_ = (bitField0_ & ~0x00000004);
            } else {
              ensureBatchInputIsMutable();
              batchInput_.addAll(other.batchInput_);
            }
            onChanged();
          }
        } else {
          if (!other.batchInput_.isEmpty()) {
            if (batchInputBuilder_.isEmpty()) {
              batchInputBuilder_.dispose();
              batchInputBuilder_ = null;
              batchInput_ = other.batchInput_;
              bitField0_ = (bitField0_ & ~0x00000004);
              batchInputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getBatchInputFieldBuilder() : null;
            } else {
              batchInputBuilder_.addAllMessages(other.batchInput_);
            }
          }
        }
        if (batchOutputBuilder_ == null) {
          if (!other.batchOutput_.isEmpty()) {
            if (batchOutput_.isEmpty()) {
              batchOutput_ = other.batchOutput_;
              bitField0_ = (bitField0_ & ~0x00000008);
            } else {
              ensureBatchOutputIsMutable();
              batchOutput_.addAll(other.batchOutput_);
            }
            onChanged();
          }
        } else {
          if (!other.batchOutput_.isEmpty()) {
            if (batchOutputBuilder_.isEmpty()) {
              batchOutputBuilder_.dispose();
              batchOutputBuilder_ = null;
              batchOutput_ = other.batchOutput_;
              bitField0_ = (bitField0_ & ~0x00000008);
              batchOutputBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getBatchOutputFieldBuilder() : null;
            } else {
              batchOutputBuilder_.addAllMessages(other.batchOutput_);
            }
          }
        }
        if (other.hasOptimization()) {
          mergeOptimization(other.getOptimization());
        }
        if (instanceGroupBuilder_ == null) {
          if (!other.instanceGroup_.isEmpty()) {
            if (instanceGroup_.isEmpty()) {
              instanceGroup_ = other.instanceGroup_;
              bitField0_ = (bitField0_ & ~0x00000010);
            } else {
              ensureInstanceGroupIsMutable();
              instanceGroup_.addAll(other.instanceGroup_);
            }
            onChanged();
          }
        } else {
          if (!other.instanceGroup_.isEmpty()) {
            if (instanceGroupBuilder_.isEmpty()) {
              instanceGroupBuilder_.dispose();
              instanceGroupBuilder_ = null;
              instanceGroup_ = other.instanceGroup_;
              bitField0_ = (bitField0_ & ~0x00000010);
              instanceGroupBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getInstanceGroupFieldBuilder() : null;
            } else {
              instanceGroupBuilder_.addAllMessages(other.instanceGroup_);
            }
          }
        }
        if (!other.getDefaultModelFilename().isEmpty()) {
          defaultModelFilename_ = other.defaultModelFilename_;
          onChanged();
        }
        internalGetMutableCcModelFilenames().mergeFrom(
            other.internalGetCcModelFilenames());
        internalGetMutableMetricTags().mergeFrom(
            other.internalGetMetricTags());
        internalGetMutableParameters().mergeFrom(
            other.internalGetParameters());
        if (modelWarmupBuilder_ == null) {
          if (!other.modelWarmup_.isEmpty()) {
            if (modelWarmup_.isEmpty()) {
              modelWarmup_ = other.modelWarmup_;
              bitField0_ = (bitField0_ & ~0x00000100);
            } else {
              ensureModelWarmupIsMutable();
              modelWarmup_.addAll(other.modelWarmup_);
            }
            onChanged();
          }
        } else {
          if (!other.modelWarmup_.isEmpty()) {
            if (modelWarmupBuilder_.isEmpty()) {
              modelWarmupBuilder_.dispose();
              modelWarmupBuilder_ = null;
              modelWarmup_ = other.modelWarmup_;
              bitField0_ = (bitField0_ & ~0x00000100);
              modelWarmupBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getModelWarmupFieldBuilder() : null;
            } else {
              modelWarmupBuilder_.addAllMessages(other.modelWarmup_);
            }
          }
        }
        if (other.hasModelOperations()) {
          mergeModelOperations(other.getModelOperations());
        }
        if (other.hasModelTransactionPolicy()) {
          mergeModelTransactionPolicy(other.getModelTransactionPolicy());
        }
        switch (other.getSchedulingChoiceCase()) {
          case DYNAMIC_BATCHING: {
            mergeDynamicBatching(other.getDynamicBatching());
            break;
          }
          case SEQUENCE_BATCHING: {
            mergeSequenceBatching(other.getSequenceBatching());
            break;
          }
          case ENSEMBLE_SCHEDULING: {
            mergeEnsembleScheduling(other.getEnsembleScheduling());
            break;
          }
          case SCHEDULINGCHOICE_NOT_SET: {
            break;
          }
        }
        this.mergeUnknownFields(other.unknownFields);
        onChanged();
        return this;
      }

      @java.lang.Override
      public final boolean isInitialized() {
        return true;
      }

      @java.lang.Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        inference.ModelConfigOuterClass.ModelConfig parsedMessage = null;
        try {
          parsedMessage = PARSER.parsePartialFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          parsedMessage = (inference.ModelConfigOuterClass.ModelConfig) e.getUnfinishedMessage();
          throw e.unwrapIOException();
        } finally {
          if (parsedMessage != null) {
            mergeFrom(parsedMessage);
          }
        }
        return this;
      }
      private int schedulingChoiceCase_ = 0;
      private java.lang.Object schedulingChoice_;
      public SchedulingChoiceCase
          getSchedulingChoiceCase() {
        return SchedulingChoiceCase.forNumber(
            schedulingChoiceCase_);
      }

      public Builder clearSchedulingChoice() {
        schedulingChoiceCase_ = 0;
        schedulingChoice_ = null;
        onChanged();
        return this;
      }

      private int bitField0_;

      private java.lang.Object name_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The name.
       */
      public java.lang.String getName() {
        java.lang.Object ref = name_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        java.lang.Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        name_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        
        name_ = getDefaultInstance().getName();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string name
       *&#64;&#64;
       *&#64;&#64;     The name of the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        name_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object platform_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @return The platform.
       */
      public java.lang.String getPlatform() {
        java.lang.Object ref = platform_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          platform_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @return The bytes for platform.
       */
      public com.google.protobuf.ByteString
          getPlatformBytes() {
        java.lang.Object ref = platform_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          platform_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @param value The platform to set.
       * @return This builder for chaining.
       */
      public Builder setPlatform(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        platform_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearPlatform() {
        
        platform_ = getDefaultInstance().getPlatform();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string platform
       *&#64;&#64;
       *&#64;&#64;     The framework for the model. Possible values are
       *&#64;&#64;     "tensorrt_plan", "tensorflow_graphdef",
       *&#64;&#64;     "tensorflow_savedmodel", "onnxruntime_onnx",
       *&#64;&#64;     "pytorch_libtorch" and "custom".
       *&#64;&#64;
       * </pre>
       *
       * <code>string platform = 2;</code>
       * @param value The bytes for platform to set.
       * @return This builder for chaining.
       */
      public Builder setPlatformBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        platform_ = value;
        onChanged();
        return this;
      }

      private java.lang.Object backend_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @return The backend.
       */
      public java.lang.String getBackend() {
        java.lang.Object ref = backend_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          backend_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @return The bytes for backend.
       */
      public com.google.protobuf.ByteString
          getBackendBytes() {
        java.lang.Object ref = backend_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          backend_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @param value The backend to set.
       * @return This builder for chaining.
       */
      public Builder setBackend(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        backend_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @return This builder for chaining.
       */
      public Builder clearBackend() {
        
        backend_ = getDefaultInstance().getBackend();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string backend
       *&#64;&#64;
       *&#64;&#64;     The backend used by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>string backend = 17;</code>
       * @param value The bytes for backend to set.
       * @return This builder for chaining.
       */
      public Builder setBackendBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        backend_ = value;
        onChanged();
        return this;
      }

      private inference.ModelConfigOuterClass.ModelVersionPolicy versionPolicy_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelVersionPolicy, inference.ModelConfigOuterClass.ModelVersionPolicy.Builder, inference.ModelConfigOuterClass.ModelVersionPolicyOrBuilder> versionPolicyBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       * @return Whether the versionPolicy field is set.
       */
      public boolean hasVersionPolicy() {
        return versionPolicyBuilder_ != null || versionPolicy_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       * @return The versionPolicy.
       */
      public inference.ModelConfigOuterClass.ModelVersionPolicy getVersionPolicy() {
        if (versionPolicyBuilder_ == null) {
          return versionPolicy_ == null ? inference.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
        } else {
          return versionPolicyBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder setVersionPolicy(inference.ModelConfigOuterClass.ModelVersionPolicy value) {
        if (versionPolicyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          versionPolicy_ = value;
          onChanged();
        } else {
          versionPolicyBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder setVersionPolicy(
          inference.ModelConfigOuterClass.ModelVersionPolicy.Builder builderForValue) {
        if (versionPolicyBuilder_ == null) {
          versionPolicy_ = builderForValue.build();
          onChanged();
        } else {
          versionPolicyBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder mergeVersionPolicy(inference.ModelConfigOuterClass.ModelVersionPolicy value) {
        if (versionPolicyBuilder_ == null) {
          if (versionPolicy_ != null) {
            versionPolicy_ =
              inference.ModelConfigOuterClass.ModelVersionPolicy.newBuilder(versionPolicy_).mergeFrom(value).buildPartial();
          } else {
            versionPolicy_ = value;
          }
          onChanged();
        } else {
          versionPolicyBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public Builder clearVersionPolicy() {
        if (versionPolicyBuilder_ == null) {
          versionPolicy_ = null;
          onChanged();
        } else {
          versionPolicy_ = null;
          versionPolicyBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public inference.ModelConfigOuterClass.ModelVersionPolicy.Builder getVersionPolicyBuilder() {
        
        onChanged();
        return getVersionPolicyFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      public inference.ModelConfigOuterClass.ModelVersionPolicyOrBuilder getVersionPolicyOrBuilder() {
        if (versionPolicyBuilder_ != null) {
          return versionPolicyBuilder_.getMessageOrBuilder();
        } else {
          return versionPolicy_ == null ?
              inference.ModelConfigOuterClass.ModelVersionPolicy.getDefaultInstance() : versionPolicy_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelVersionPolicy version_policy
       *&#64;&#64;
       *&#64;&#64;     Policy indicating which version(s) of the model will be served.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelVersionPolicy version_policy = 3;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelVersionPolicy, inference.ModelConfigOuterClass.ModelVersionPolicy.Builder, inference.ModelConfigOuterClass.ModelVersionPolicyOrBuilder> 
          getVersionPolicyFieldBuilder() {
        if (versionPolicyBuilder_ == null) {
          versionPolicyBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelVersionPolicy, inference.ModelConfigOuterClass.ModelVersionPolicy.Builder, inference.ModelConfigOuterClass.ModelVersionPolicyOrBuilder>(
                  getVersionPolicy(),
                  getParentForChildren(),
                  isClean());
          versionPolicy_ = null;
        }
        return versionPolicyBuilder_;
      }

      private int maxBatchSize_ ;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       * @return The maxBatchSize.
       */
      @java.lang.Override
      public int getMaxBatchSize() {
        return maxBatchSize_;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       * @param value The maxBatchSize to set.
       * @return This builder for chaining.
       */
      public Builder setMaxBatchSize(int value) {
        
        maxBatchSize_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: int32 max_batch_size
       *&#64;&#64;
       *&#64;&#64;     Maximum batch size allowed for inference. This can only decrease
       *&#64;&#64;     what is allowed by the model itself. A max_batch_size value of 0
       *&#64;&#64;     indicates that batching is not allowed for the model and the
       *&#64;&#64;     dimension/shape of the input and output tensors must exactly
       *&#64;&#64;     match what is specified in the input and output configuration. A
       *&#64;&#64;     max_batch_size value &gt; 0 indicates that batching is allowed and
       *&#64;&#64;     so the model expects the input tensors to have an additional
       *&#64;&#64;     initial dimension for the batching that is not specified in the
       *&#64;&#64;     input (for example, if the model supports batched inputs of
       *&#64;&#64;     2-dimensional tensors then the model configuration will specify
       *&#64;&#64;     the input shape as [ X, Y ] but the model will expect the actual
       *&#64;&#64;     input tensors to have shape [ N, X, Y ]). For max_batch_size &gt; 0
       *&#64;&#64;     returned outputs will also have an additional initial dimension
       *&#64;&#64;     for the batch.
       *&#64;&#64;
       * </pre>
       *
       * <code>int32 max_batch_size = 4;</code>
       * @return This builder for chaining.
       */
      public Builder clearMaxBatchSize() {
        
        maxBatchSize_ = 0;
        onChanged();
        return this;
      }

      private java.util.List<inference.ModelConfigOuterClass.ModelInput> input_ =
        java.util.Collections.emptyList();
      private void ensureInputIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          input_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelInput>(input_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelInput, inference.ModelConfigOuterClass.ModelInput.Builder, inference.ModelConfigOuterClass.ModelInputOrBuilder> inputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelInput> getInputList() {
        if (inputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(input_);
        } else {
          return inputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public int getInputCount() {
        if (inputBuilder_ == null) {
          return input_.size();
        } else {
          return inputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelInput getInput(int index) {
        if (inputBuilder_ == null) {
          return input_.get(index);
        } else {
          return inputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder setInput(
          int index, inference.ModelConfigOuterClass.ModelInput value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.set(index, value);
          onChanged();
        } else {
          inputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder setInput(
          int index, inference.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.set(index, builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addInput(inference.ModelConfigOuterClass.ModelInput value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.add(value);
          onChanged();
        } else {
          inputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addInput(
          int index, inference.ModelConfigOuterClass.ModelInput value) {
        if (inputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInputIsMutable();
          input_.add(index, value);
          onChanged();
        } else {
          inputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addInput(
          inference.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.add(builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addInput(
          int index, inference.ModelConfigOuterClass.ModelInput.Builder builderForValue) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.add(index, builderForValue.build());
          onChanged();
        } else {
          inputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder addAllInput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelInput> values) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, input_);
          onChanged();
        } else {
          inputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder clearInput() {
        if (inputBuilder_ == null) {
          input_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          inputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public Builder removeInput(int index) {
        if (inputBuilder_ == null) {
          ensureInputIsMutable();
          input_.remove(index);
          onChanged();
        } else {
          inputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelInput.Builder getInputBuilder(
          int index) {
        return getInputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelInputOrBuilder getInputOrBuilder(
          int index) {
        if (inputBuilder_ == null) {
          return input_.get(index);  } else {
          return inputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelInputOrBuilder> 
           getInputOrBuilderList() {
        if (inputBuilder_ != null) {
          return inputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(input_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelInput.Builder addInputBuilder() {
        return getInputFieldBuilder().addBuilder(
            inference.ModelConfigOuterClass.ModelInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public inference.ModelConfigOuterClass.ModelInput.Builder addInputBuilder(
          int index) {
        return getInputFieldBuilder().addBuilder(
            index, inference.ModelConfigOuterClass.ModelInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInput input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The inputs request by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInput input = 5;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelInput.Builder> 
           getInputBuilderList() {
        return getInputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelInput, inference.ModelConfigOuterClass.ModelInput.Builder, inference.ModelConfigOuterClass.ModelInputOrBuilder> 
          getInputFieldBuilder() {
        if (inputBuilder_ == null) {
          inputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelInput, inference.ModelConfigOuterClass.ModelInput.Builder, inference.ModelConfigOuterClass.ModelInputOrBuilder>(
                  input_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          input_ = null;
        }
        return inputBuilder_;
      }

      private java.util.List<inference.ModelConfigOuterClass.ModelOutput> output_ =
        java.util.Collections.emptyList();
      private void ensureOutputIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          output_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelOutput>(output_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOutput, inference.ModelConfigOuterClass.ModelOutput.Builder, inference.ModelConfigOuterClass.ModelOutputOrBuilder> outputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelOutput> getOutputList() {
        if (outputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(output_);
        } else {
          return outputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public int getOutputCount() {
        if (outputBuilder_ == null) {
          return output_.size();
        } else {
          return outputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelOutput getOutput(int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);
        } else {
          return outputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder setOutput(
          int index, inference.ModelConfigOuterClass.ModelOutput value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.set(index, value);
          onChanged();
        } else {
          outputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder setOutput(
          int index, inference.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.set(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addOutput(inference.ModelConfigOuterClass.ModelOutput value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(value);
          onChanged();
        } else {
          outputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          int index, inference.ModelConfigOuterClass.ModelOutput value) {
        if (outputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureOutputIsMutable();
          output_.add(index, value);
          onChanged();
        } else {
          outputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          inference.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addOutput(
          int index, inference.ModelConfigOuterClass.ModelOutput.Builder builderForValue) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.add(index, builderForValue.build());
          onChanged();
        } else {
          outputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder addAllOutput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelOutput> values) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, output_);
          onChanged();
        } else {
          outputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder clearOutput() {
        if (outputBuilder_ == null) {
          output_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          outputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public Builder removeOutput(int index) {
        if (outputBuilder_ == null) {
          ensureOutputIsMutable();
          output_.remove(index);
          onChanged();
        } else {
          outputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelOutput.Builder getOutputBuilder(
          int index) {
        return getOutputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelOutputOrBuilder getOutputOrBuilder(
          int index) {
        if (outputBuilder_ == null) {
          return output_.get(index);  } else {
          return outputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelOutputOrBuilder> 
           getOutputOrBuilderList() {
        if (outputBuilder_ != null) {
          return outputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(output_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelOutput.Builder addOutputBuilder() {
        return getOutputFieldBuilder().addBuilder(
            inference.ModelConfigOuterClass.ModelOutput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public inference.ModelConfigOuterClass.ModelOutput.Builder addOutputBuilder(
          int index) {
        return getOutputFieldBuilder().addBuilder(
            index, inference.ModelConfigOuterClass.ModelOutput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOutput output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelOutput output = 6;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelOutput.Builder> 
           getOutputBuilderList() {
        return getOutputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOutput, inference.ModelConfigOuterClass.ModelOutput.Builder, inference.ModelConfigOuterClass.ModelOutputOrBuilder> 
          getOutputFieldBuilder() {
        if (outputBuilder_ == null) {
          outputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOutput, inference.ModelConfigOuterClass.ModelOutput.Builder, inference.ModelConfigOuterClass.ModelOutputOrBuilder>(
                  output_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          output_ = null;
        }
        return outputBuilder_;
      }

      private java.util.List<inference.ModelConfigOuterClass.BatchInput> batchInput_ =
        java.util.Collections.emptyList();
      private void ensureBatchInputIsMutable() {
        if (!((bitField0_ & 0x00000004) != 0)) {
          batchInput_ = new java.util.ArrayList<inference.ModelConfigOuterClass.BatchInput>(batchInput_);
          bitField0_ |= 0x00000004;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.BatchInput, inference.ModelConfigOuterClass.BatchInput.Builder, inference.ModelConfigOuterClass.BatchInputOrBuilder> batchInputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.BatchInput> getBatchInputList() {
        if (batchInputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(batchInput_);
        } else {
          return batchInputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public int getBatchInputCount() {
        if (batchInputBuilder_ == null) {
          return batchInput_.size();
        } else {
          return batchInputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public inference.ModelConfigOuterClass.BatchInput getBatchInput(int index) {
        if (batchInputBuilder_ == null) {
          return batchInput_.get(index);
        } else {
          return batchInputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder setBatchInput(
          int index, inference.ModelConfigOuterClass.BatchInput value) {
        if (batchInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBatchInputIsMutable();
          batchInput_.set(index, value);
          onChanged();
        } else {
          batchInputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder setBatchInput(
          int index, inference.ModelConfigOuterClass.BatchInput.Builder builderForValue) {
        if (batchInputBuilder_ == null) {
          ensureBatchInputIsMutable();
          batchInput_.set(index, builderForValue.build());
          onChanged();
        } else {
          batchInputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addBatchInput(inference.ModelConfigOuterClass.BatchInput value) {
        if (batchInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBatchInputIsMutable();
          batchInput_.add(value);
          onChanged();
        } else {
          batchInputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addBatchInput(
          int index, inference.ModelConfigOuterClass.BatchInput value) {
        if (batchInputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBatchInputIsMutable();
          batchInput_.add(index, value);
          onChanged();
        } else {
          batchInputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addBatchInput(
          inference.ModelConfigOuterClass.BatchInput.Builder builderForValue) {
        if (batchInputBuilder_ == null) {
          ensureBatchInputIsMutable();
          batchInput_.add(builderForValue.build());
          onChanged();
        } else {
          batchInputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addBatchInput(
          int index, inference.ModelConfigOuterClass.BatchInput.Builder builderForValue) {
        if (batchInputBuilder_ == null) {
          ensureBatchInputIsMutable();
          batchInput_.add(index, builderForValue.build());
          onChanged();
        } else {
          batchInputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder addAllBatchInput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.BatchInput> values) {
        if (batchInputBuilder_ == null) {
          ensureBatchInputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, batchInput_);
          onChanged();
        } else {
          batchInputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder clearBatchInput() {
        if (batchInputBuilder_ == null) {
          batchInput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
        } else {
          batchInputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public Builder removeBatchInput(int index) {
        if (batchInputBuilder_ == null) {
          ensureBatchInputIsMutable();
          batchInput_.remove(index);
          onChanged();
        } else {
          batchInputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public inference.ModelConfigOuterClass.BatchInput.Builder getBatchInputBuilder(
          int index) {
        return getBatchInputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public inference.ModelConfigOuterClass.BatchInputOrBuilder getBatchInputOrBuilder(
          int index) {
        if (batchInputBuilder_ == null) {
          return batchInput_.get(index);  } else {
          return batchInputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.BatchInputOrBuilder> 
           getBatchInputOrBuilderList() {
        if (batchInputBuilder_ != null) {
          return batchInputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(batchInput_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public inference.ModelConfigOuterClass.BatchInput.Builder addBatchInputBuilder() {
        return getBatchInputFieldBuilder().addBuilder(
            inference.ModelConfigOuterClass.BatchInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public inference.ModelConfigOuterClass.BatchInput.Builder addBatchInputBuilder(
          int index) {
        return getBatchInputFieldBuilder().addBuilder(
            index, inference.ModelConfigOuterClass.BatchInput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchInput batch_input (repeated)
       *&#64;&#64;
       *&#64;&#64;     The model input(s) that the server should use to communicate
       *&#64;&#64;     batch related values to the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchInput batch_input = 20;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.BatchInput.Builder> 
           getBatchInputBuilderList() {
        return getBatchInputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.BatchInput, inference.ModelConfigOuterClass.BatchInput.Builder, inference.ModelConfigOuterClass.BatchInputOrBuilder> 
          getBatchInputFieldBuilder() {
        if (batchInputBuilder_ == null) {
          batchInputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              inference.ModelConfigOuterClass.BatchInput, inference.ModelConfigOuterClass.BatchInput.Builder, inference.ModelConfigOuterClass.BatchInputOrBuilder>(
                  batchInput_,
                  ((bitField0_ & 0x00000004) != 0),
                  getParentForChildren(),
                  isClean());
          batchInput_ = null;
        }
        return batchInputBuilder_;
      }

      private java.util.List<inference.ModelConfigOuterClass.BatchOutput> batchOutput_ =
        java.util.Collections.emptyList();
      private void ensureBatchOutputIsMutable() {
        if (!((bitField0_ & 0x00000008) != 0)) {
          batchOutput_ = new java.util.ArrayList<inference.ModelConfigOuterClass.BatchOutput>(batchOutput_);
          bitField0_ |= 0x00000008;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.BatchOutput, inference.ModelConfigOuterClass.BatchOutput.Builder, inference.ModelConfigOuterClass.BatchOutputOrBuilder> batchOutputBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.BatchOutput> getBatchOutputList() {
        if (batchOutputBuilder_ == null) {
          return java.util.Collections.unmodifiableList(batchOutput_);
        } else {
          return batchOutputBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public int getBatchOutputCount() {
        if (batchOutputBuilder_ == null) {
          return batchOutput_.size();
        } else {
          return batchOutputBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public inference.ModelConfigOuterClass.BatchOutput getBatchOutput(int index) {
        if (batchOutputBuilder_ == null) {
          return batchOutput_.get(index);
        } else {
          return batchOutputBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder setBatchOutput(
          int index, inference.ModelConfigOuterClass.BatchOutput value) {
        if (batchOutputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBatchOutputIsMutable();
          batchOutput_.set(index, value);
          onChanged();
        } else {
          batchOutputBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder setBatchOutput(
          int index, inference.ModelConfigOuterClass.BatchOutput.Builder builderForValue) {
        if (batchOutputBuilder_ == null) {
          ensureBatchOutputIsMutable();
          batchOutput_.set(index, builderForValue.build());
          onChanged();
        } else {
          batchOutputBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addBatchOutput(inference.ModelConfigOuterClass.BatchOutput value) {
        if (batchOutputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBatchOutputIsMutable();
          batchOutput_.add(value);
          onChanged();
        } else {
          batchOutputBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addBatchOutput(
          int index, inference.ModelConfigOuterClass.BatchOutput value) {
        if (batchOutputBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureBatchOutputIsMutable();
          batchOutput_.add(index, value);
          onChanged();
        } else {
          batchOutputBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addBatchOutput(
          inference.ModelConfigOuterClass.BatchOutput.Builder builderForValue) {
        if (batchOutputBuilder_ == null) {
          ensureBatchOutputIsMutable();
          batchOutput_.add(builderForValue.build());
          onChanged();
        } else {
          batchOutputBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addBatchOutput(
          int index, inference.ModelConfigOuterClass.BatchOutput.Builder builderForValue) {
        if (batchOutputBuilder_ == null) {
          ensureBatchOutputIsMutable();
          batchOutput_.add(index, builderForValue.build());
          onChanged();
        } else {
          batchOutputBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder addAllBatchOutput(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.BatchOutput> values) {
        if (batchOutputBuilder_ == null) {
          ensureBatchOutputIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, batchOutput_);
          onChanged();
        } else {
          batchOutputBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder clearBatchOutput() {
        if (batchOutputBuilder_ == null) {
          batchOutput_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000008);
          onChanged();
        } else {
          batchOutputBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public Builder removeBatchOutput(int index) {
        if (batchOutputBuilder_ == null) {
          ensureBatchOutputIsMutable();
          batchOutput_.remove(index);
          onChanged();
        } else {
          batchOutputBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public inference.ModelConfigOuterClass.BatchOutput.Builder getBatchOutputBuilder(
          int index) {
        return getBatchOutputFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public inference.ModelConfigOuterClass.BatchOutputOrBuilder getBatchOutputOrBuilder(
          int index) {
        if (batchOutputBuilder_ == null) {
          return batchOutput_.get(index);  } else {
          return batchOutputBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.BatchOutputOrBuilder> 
           getBatchOutputOrBuilderList() {
        if (batchOutputBuilder_ != null) {
          return batchOutputBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(batchOutput_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public inference.ModelConfigOuterClass.BatchOutput.Builder addBatchOutputBuilder() {
        return getBatchOutputFieldBuilder().addBuilder(
            inference.ModelConfigOuterClass.BatchOutput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public inference.ModelConfigOuterClass.BatchOutput.Builder addBatchOutputBuilder(
          int index) {
        return getBatchOutputFieldBuilder().addBuilder(
            index, inference.ModelConfigOuterClass.BatchOutput.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: BatchOutput batch_output (repeated)
       *&#64;&#64;
       *&#64;&#64;     The outputs produced by the model that requires special handling
       *&#64;&#64;     by the model backend.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.BatchOutput batch_output = 21;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.BatchOutput.Builder> 
           getBatchOutputBuilderList() {
        return getBatchOutputFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.BatchOutput, inference.ModelConfigOuterClass.BatchOutput.Builder, inference.ModelConfigOuterClass.BatchOutputOrBuilder> 
          getBatchOutputFieldBuilder() {
        if (batchOutputBuilder_ == null) {
          batchOutputBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              inference.ModelConfigOuterClass.BatchOutput, inference.ModelConfigOuterClass.BatchOutput.Builder, inference.ModelConfigOuterClass.BatchOutputOrBuilder>(
                  batchOutput_,
                  ((bitField0_ & 0x00000008) != 0),
                  getParentForChildren(),
                  isClean());
          batchOutput_ = null;
        }
        return batchOutputBuilder_;
      }

      private inference.ModelConfigOuterClass.ModelOptimizationPolicy optimization_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder> optimizationBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       * @return Whether the optimization field is set.
       */
      public boolean hasOptimization() {
        return optimizationBuilder_ != null || optimization_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       * @return The optimization.
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy getOptimization() {
        if (optimizationBuilder_ == null) {
          return optimization_ == null ? inference.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
        } else {
          return optimizationBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder setOptimization(inference.ModelConfigOuterClass.ModelOptimizationPolicy value) {
        if (optimizationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          optimization_ = value;
          onChanged();
        } else {
          optimizationBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder setOptimization(
          inference.ModelConfigOuterClass.ModelOptimizationPolicy.Builder builderForValue) {
        if (optimizationBuilder_ == null) {
          optimization_ = builderForValue.build();
          onChanged();
        } else {
          optimizationBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder mergeOptimization(inference.ModelConfigOuterClass.ModelOptimizationPolicy value) {
        if (optimizationBuilder_ == null) {
          if (optimization_ != null) {
            optimization_ =
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.newBuilder(optimization_).mergeFrom(value).buildPartial();
          } else {
            optimization_ = value;
          }
          onChanged();
        } else {
          optimizationBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public Builder clearOptimization() {
        if (optimizationBuilder_ == null) {
          optimization_ = null;
          onChanged();
        } else {
          optimization_ = null;
          optimizationBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicy.Builder getOptimizationBuilder() {
        
        onChanged();
        return getOptimizationFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      public inference.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder getOptimizationOrBuilder() {
        if (optimizationBuilder_ != null) {
          return optimizationBuilder_.getMessageOrBuilder();
        } else {
          return optimization_ == null ?
              inference.ModelConfigOuterClass.ModelOptimizationPolicy.getDefaultInstance() : optimization_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOptimizationPolicy optimization
       *&#64;&#64;
       *&#64;&#64;     Optimization configuration for the model. If not specified
       *&#64;&#64;     then default optimization policy is used.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOptimizationPolicy optimization = 12;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOptimizationPolicy, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder> 
          getOptimizationFieldBuilder() {
        if (optimizationBuilder_ == null) {
          optimizationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOptimizationPolicy, inference.ModelConfigOuterClass.ModelOptimizationPolicy.Builder, inference.ModelConfigOuterClass.ModelOptimizationPolicyOrBuilder>(
                  getOptimization(),
                  getParentForChildren(),
                  isClean());
          optimization_ = null;
        }
        return optimizationBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelDynamicBatching, inference.ModelConfigOuterClass.ModelDynamicBatching.Builder, inference.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder> dynamicBatchingBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       * @return Whether the dynamicBatching field is set.
       */
      @java.lang.Override
      public boolean hasDynamicBatching() {
        return schedulingChoiceCase_ == 11;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       * @return The dynamicBatching.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelDynamicBatching getDynamicBatching() {
        if (dynamicBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 11) {
            return (inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
          }
          return inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
        } else {
          if (schedulingChoiceCase_ == 11) {
            return dynamicBatchingBuilder_.getMessage();
          }
          return inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder setDynamicBatching(inference.ModelConfigOuterClass.ModelDynamicBatching value) {
        if (dynamicBatchingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          schedulingChoice_ = value;
          onChanged();
        } else {
          dynamicBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 11;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder setDynamicBatching(
          inference.ModelConfigOuterClass.ModelDynamicBatching.Builder builderForValue) {
        if (dynamicBatchingBuilder_ == null) {
          schedulingChoice_ = builderForValue.build();
          onChanged();
        } else {
          dynamicBatchingBuilder_.setMessage(builderForValue.build());
        }
        schedulingChoiceCase_ = 11;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder mergeDynamicBatching(inference.ModelConfigOuterClass.ModelDynamicBatching value) {
        if (dynamicBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 11 &&
              schedulingChoice_ != inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance()) {
            schedulingChoice_ = inference.ModelConfigOuterClass.ModelDynamicBatching.newBuilder((inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            schedulingChoice_ = value;
          }
          onChanged();
        } else {
          if (schedulingChoiceCase_ == 11) {
            dynamicBatchingBuilder_.mergeFrom(value);
          }
          dynamicBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 11;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public Builder clearDynamicBatching() {
        if (dynamicBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 11) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
            onChanged();
          }
        } else {
          if (schedulingChoiceCase_ == 11) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
          }
          dynamicBatchingBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      public inference.ModelConfigOuterClass.ModelDynamicBatching.Builder getDynamicBatchingBuilder() {
        return getDynamicBatchingFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder getDynamicBatchingOrBuilder() {
        if ((schedulingChoiceCase_ == 11) && (dynamicBatchingBuilder_ != null)) {
          return dynamicBatchingBuilder_.getMessageOrBuilder();
        } else {
          if (schedulingChoiceCase_ == 11) {
            return (inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_;
          }
          return inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelDynamicBatching dynamic_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the dynamic-batching scheduling
       *&#64;&#64;       policy. With dynamic-batching the scheduler may group
       *&#64;&#64;       together independent requests into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelDynamicBatching dynamic_batching = 11;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelDynamicBatching, inference.ModelConfigOuterClass.ModelDynamicBatching.Builder, inference.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder> 
          getDynamicBatchingFieldBuilder() {
        if (dynamicBatchingBuilder_ == null) {
          if (!(schedulingChoiceCase_ == 11)) {
            schedulingChoice_ = inference.ModelConfigOuterClass.ModelDynamicBatching.getDefaultInstance();
          }
          dynamicBatchingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelDynamicBatching, inference.ModelConfigOuterClass.ModelDynamicBatching.Builder, inference.ModelConfigOuterClass.ModelDynamicBatchingOrBuilder>(
                  (inference.ModelConfigOuterClass.ModelDynamicBatching) schedulingChoice_,
                  getParentForChildren(),
                  isClean());
          schedulingChoice_ = null;
        }
        schedulingChoiceCase_ = 11;
        onChanged();;
        return dynamicBatchingBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelSequenceBatching, inference.ModelConfigOuterClass.ModelSequenceBatching.Builder, inference.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder> sequenceBatchingBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       * @return Whether the sequenceBatching field is set.
       */
      @java.lang.Override
      public boolean hasSequenceBatching() {
        return schedulingChoiceCase_ == 13;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       * @return The sequenceBatching.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatching getSequenceBatching() {
        if (sequenceBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 13) {
            return (inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
          }
          return inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
        } else {
          if (schedulingChoiceCase_ == 13) {
            return sequenceBatchingBuilder_.getMessage();
          }
          return inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder setSequenceBatching(inference.ModelConfigOuterClass.ModelSequenceBatching value) {
        if (sequenceBatchingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          schedulingChoice_ = value;
          onChanged();
        } else {
          sequenceBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 13;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder setSequenceBatching(
          inference.ModelConfigOuterClass.ModelSequenceBatching.Builder builderForValue) {
        if (sequenceBatchingBuilder_ == null) {
          schedulingChoice_ = builderForValue.build();
          onChanged();
        } else {
          sequenceBatchingBuilder_.setMessage(builderForValue.build());
        }
        schedulingChoiceCase_ = 13;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder mergeSequenceBatching(inference.ModelConfigOuterClass.ModelSequenceBatching value) {
        if (sequenceBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 13 &&
              schedulingChoice_ != inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance()) {
            schedulingChoice_ = inference.ModelConfigOuterClass.ModelSequenceBatching.newBuilder((inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            schedulingChoice_ = value;
          }
          onChanged();
        } else {
          if (schedulingChoiceCase_ == 13) {
            sequenceBatchingBuilder_.mergeFrom(value);
          }
          sequenceBatchingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 13;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public Builder clearSequenceBatching() {
        if (sequenceBatchingBuilder_ == null) {
          if (schedulingChoiceCase_ == 13) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
            onChanged();
          }
        } else {
          if (schedulingChoiceCase_ == 13) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
          }
          sequenceBatchingBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      public inference.ModelConfigOuterClass.ModelSequenceBatching.Builder getSequenceBatchingBuilder() {
        return getSequenceBatchingFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder getSequenceBatchingOrBuilder() {
        if ((schedulingChoiceCase_ == 13) && (sequenceBatchingBuilder_ != null)) {
          return sequenceBatchingBuilder_.getMessageOrBuilder();
        } else {
          if (schedulingChoiceCase_ == 13) {
            return (inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_;
          }
          return inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelSequenceBatching sequence_batching
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the sequence-batching scheduling
       *&#64;&#64;       policy. With sequence-batching, inference requests
       *&#64;&#64;       with the same correlation ID are routed to the same
       *&#64;&#64;       model instance. Multiple sequences of inference requests
       *&#64;&#64;       may be batched together into a single batch to
       *&#64;&#64;       improve inference throughput.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelSequenceBatching sequence_batching = 13;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelSequenceBatching, inference.ModelConfigOuterClass.ModelSequenceBatching.Builder, inference.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder> 
          getSequenceBatchingFieldBuilder() {
        if (sequenceBatchingBuilder_ == null) {
          if (!(schedulingChoiceCase_ == 13)) {
            schedulingChoice_ = inference.ModelConfigOuterClass.ModelSequenceBatching.getDefaultInstance();
          }
          sequenceBatchingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelSequenceBatching, inference.ModelConfigOuterClass.ModelSequenceBatching.Builder, inference.ModelConfigOuterClass.ModelSequenceBatchingOrBuilder>(
                  (inference.ModelConfigOuterClass.ModelSequenceBatching) schedulingChoice_,
                  getParentForChildren(),
                  isClean());
          schedulingChoice_ = null;
        }
        schedulingChoiceCase_ = 13;
        onChanged();;
        return sequenceBatchingBuilder_;
      }

      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelEnsembling, inference.ModelConfigOuterClass.ModelEnsembling.Builder, inference.ModelConfigOuterClass.ModelEnsemblingOrBuilder> ensembleSchedulingBuilder_;
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       * @return Whether the ensembleScheduling field is set.
       */
      @java.lang.Override
      public boolean hasEnsembleScheduling() {
        return schedulingChoiceCase_ == 15;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       * @return The ensembleScheduling.
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelEnsembling getEnsembleScheduling() {
        if (ensembleSchedulingBuilder_ == null) {
          if (schedulingChoiceCase_ == 15) {
            return (inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
          }
          return inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
        } else {
          if (schedulingChoiceCase_ == 15) {
            return ensembleSchedulingBuilder_.getMessage();
          }
          return inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder setEnsembleScheduling(inference.ModelConfigOuterClass.ModelEnsembling value) {
        if (ensembleSchedulingBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          schedulingChoice_ = value;
          onChanged();
        } else {
          ensembleSchedulingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 15;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder setEnsembleScheduling(
          inference.ModelConfigOuterClass.ModelEnsembling.Builder builderForValue) {
        if (ensembleSchedulingBuilder_ == null) {
          schedulingChoice_ = builderForValue.build();
          onChanged();
        } else {
          ensembleSchedulingBuilder_.setMessage(builderForValue.build());
        }
        schedulingChoiceCase_ = 15;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder mergeEnsembleScheduling(inference.ModelConfigOuterClass.ModelEnsembling value) {
        if (ensembleSchedulingBuilder_ == null) {
          if (schedulingChoiceCase_ == 15 &&
              schedulingChoice_ != inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance()) {
            schedulingChoice_ = inference.ModelConfigOuterClass.ModelEnsembling.newBuilder((inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_)
                .mergeFrom(value).buildPartial();
          } else {
            schedulingChoice_ = value;
          }
          onChanged();
        } else {
          if (schedulingChoiceCase_ == 15) {
            ensembleSchedulingBuilder_.mergeFrom(value);
          }
          ensembleSchedulingBuilder_.setMessage(value);
        }
        schedulingChoiceCase_ = 15;
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public Builder clearEnsembleScheduling() {
        if (ensembleSchedulingBuilder_ == null) {
          if (schedulingChoiceCase_ == 15) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
            onChanged();
          }
        } else {
          if (schedulingChoiceCase_ == 15) {
            schedulingChoiceCase_ = 0;
            schedulingChoice_ = null;
          }
          ensembleSchedulingBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      public inference.ModelConfigOuterClass.ModelEnsembling.Builder getEnsembleSchedulingBuilder() {
        return getEnsembleSchedulingFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      @java.lang.Override
      public inference.ModelConfigOuterClass.ModelEnsemblingOrBuilder getEnsembleSchedulingOrBuilder() {
        if ((schedulingChoiceCase_ == 15) && (ensembleSchedulingBuilder_ != null)) {
          return ensembleSchedulingBuilder_.getMessageOrBuilder();
        } else {
          if (schedulingChoiceCase_ == 15) {
            return (inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_;
          }
          return inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
        }
      }
      /**
       * <pre>
       *&#64;&#64;    .. cpp:var:: ModelEnsembling ensemble_scheduling
       *&#64;&#64;
       *&#64;&#64;       If specified, enables the model-ensembling scheduling
       *&#64;&#64;       policy. With model-ensembling, inference requests
       *&#64;&#64;       will be processed according to the specification, such as an
       *&#64;&#64;       execution sequence of models. The input specified in this model
       *&#64;&#64;       config will be the input for the ensemble, and the output
       *&#64;&#64;       specified will be the output of the ensemble.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelEnsembling ensemble_scheduling = 15;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelEnsembling, inference.ModelConfigOuterClass.ModelEnsembling.Builder, inference.ModelConfigOuterClass.ModelEnsemblingOrBuilder> 
          getEnsembleSchedulingFieldBuilder() {
        if (ensembleSchedulingBuilder_ == null) {
          if (!(schedulingChoiceCase_ == 15)) {
            schedulingChoice_ = inference.ModelConfigOuterClass.ModelEnsembling.getDefaultInstance();
          }
          ensembleSchedulingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelEnsembling, inference.ModelConfigOuterClass.ModelEnsembling.Builder, inference.ModelConfigOuterClass.ModelEnsemblingOrBuilder>(
                  (inference.ModelConfigOuterClass.ModelEnsembling) schedulingChoice_,
                  getParentForChildren(),
                  isClean());
          schedulingChoice_ = null;
        }
        schedulingChoiceCase_ = 15;
        onChanged();;
        return ensembleSchedulingBuilder_;
      }

      private java.util.List<inference.ModelConfigOuterClass.ModelInstanceGroup> instanceGroup_ =
        java.util.Collections.emptyList();
      private void ensureInstanceGroupIsMutable() {
        if (!((bitField0_ & 0x00000010) != 0)) {
          instanceGroup_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelInstanceGroup>(instanceGroup_);
          bitField0_ |= 0x00000010;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelInstanceGroup, inference.ModelConfigOuterClass.ModelInstanceGroup.Builder, inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> instanceGroupBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelInstanceGroup> getInstanceGroupList() {
        if (instanceGroupBuilder_ == null) {
          return java.util.Collections.unmodifiableList(instanceGroup_);
        } else {
          return instanceGroupBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public int getInstanceGroupCount() {
        if (instanceGroupBuilder_ == null) {
          return instanceGroup_.size();
        } else {
          return instanceGroupBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public inference.ModelConfigOuterClass.ModelInstanceGroup getInstanceGroup(int index) {
        if (instanceGroupBuilder_ == null) {
          return instanceGroup_.get(index);
        } else {
          return instanceGroupBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder setInstanceGroup(
          int index, inference.ModelConfigOuterClass.ModelInstanceGroup value) {
        if (instanceGroupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInstanceGroupIsMutable();
          instanceGroup_.set(index, value);
          onChanged();
        } else {
          instanceGroupBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder setInstanceGroup(
          int index, inference.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.set(index, builderForValue.build());
          onChanged();
        } else {
          instanceGroupBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(inference.ModelConfigOuterClass.ModelInstanceGroup value) {
        if (instanceGroupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(value);
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          int index, inference.ModelConfigOuterClass.ModelInstanceGroup value) {
        if (instanceGroupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(index, value);
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          inference.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(builderForValue.build());
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addInstanceGroup(
          int index, inference.ModelConfigOuterClass.ModelInstanceGroup.Builder builderForValue) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.add(index, builderForValue.build());
          onChanged();
        } else {
          instanceGroupBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder addAllInstanceGroup(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelInstanceGroup> values) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, instanceGroup_);
          onChanged();
        } else {
          instanceGroupBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder clearInstanceGroup() {
        if (instanceGroupBuilder_ == null) {
          instanceGroup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000010);
          onChanged();
        } else {
          instanceGroupBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public Builder removeInstanceGroup(int index) {
        if (instanceGroupBuilder_ == null) {
          ensureInstanceGroupIsMutable();
          instanceGroup_.remove(index);
          onChanged();
        } else {
          instanceGroupBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public inference.ModelConfigOuterClass.ModelInstanceGroup.Builder getInstanceGroupBuilder(
          int index) {
        return getInstanceGroupFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder getInstanceGroupOrBuilder(
          int index) {
        if (instanceGroupBuilder_ == null) {
          return instanceGroup_.get(index);  } else {
          return instanceGroupBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
           getInstanceGroupOrBuilderList() {
        if (instanceGroupBuilder_ != null) {
          return instanceGroupBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(instanceGroup_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public inference.ModelConfigOuterClass.ModelInstanceGroup.Builder addInstanceGroupBuilder() {
        return getInstanceGroupFieldBuilder().addBuilder(
            inference.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public inference.ModelConfigOuterClass.ModelInstanceGroup.Builder addInstanceGroupBuilder(
          int index) {
        return getInstanceGroupFieldBuilder().addBuilder(
            index, inference.ModelConfigOuterClass.ModelInstanceGroup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelInstanceGroup instance_group (repeated)
       *&#64;&#64;
       *&#64;&#64;     Instances of this model. If not specified, one instance
       *&#64;&#64;     of the model will be instantiated on each available GPU.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelInstanceGroup instance_group = 7;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelInstanceGroup.Builder> 
           getInstanceGroupBuilderList() {
        return getInstanceGroupFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelInstanceGroup, inference.ModelConfigOuterClass.ModelInstanceGroup.Builder, inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder> 
          getInstanceGroupFieldBuilder() {
        if (instanceGroupBuilder_ == null) {
          instanceGroupBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelInstanceGroup, inference.ModelConfigOuterClass.ModelInstanceGroup.Builder, inference.ModelConfigOuterClass.ModelInstanceGroupOrBuilder>(
                  instanceGroup_,
                  ((bitField0_ & 0x00000010) != 0),
                  getParentForChildren(),
                  isClean());
          instanceGroup_ = null;
        }
        return instanceGroupBuilder_;
      }

      private java.lang.Object defaultModelFilename_ = "";
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @return The defaultModelFilename.
       */
      public java.lang.String getDefaultModelFilename() {
        java.lang.Object ref = defaultModelFilename_;
        if (!(ref instanceof java.lang.String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          java.lang.String s = bs.toStringUtf8();
          defaultModelFilename_ = s;
          return s;
        } else {
          return (java.lang.String) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @return The bytes for defaultModelFilename.
       */
      public com.google.protobuf.ByteString
          getDefaultModelFilenameBytes() {
        java.lang.Object ref = defaultModelFilename_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (java.lang.String) ref);
          defaultModelFilename_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @param value The defaultModelFilename to set.
       * @return This builder for chaining.
       */
      public Builder setDefaultModelFilename(
          java.lang.String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  
        defaultModelFilename_ = value;
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @return This builder for chaining.
       */
      public Builder clearDefaultModelFilename() {
        
        defaultModelFilename_ = getDefaultInstance().getDefaultModelFilename();
        onChanged();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: string default_model_filename
       *&#64;&#64;
       *&#64;&#64;     Optional filename of the model file to use if a
       *&#64;&#64;     compute-capability specific model is not specified in
       *&#64;&#64;     :cpp:var:`cc_model_filenames`. If not specified the default name
       *&#64;&#64;     is 'model.graphdef', 'model.savedmodel', 'model.plan' or
       *&#64;&#64;     'model.pt' depending on the model type.
       *&#64;&#64;
       * </pre>
       *
       * <code>string default_model_filename = 8;</code>
       * @param value The bytes for defaultModelFilename to set.
       * @return This builder for chaining.
       */
      public Builder setDefaultModelFilenameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  checkByteStringIsUtf8(value);
        
        defaultModelFilename_ = value;
        onChanged();
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> ccModelFilenames_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetCcModelFilenames() {
        if (ccModelFilenames_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              CcModelFilenamesDefaultEntryHolder.defaultEntry);
        }
        return ccModelFilenames_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableCcModelFilenames() {
        onChanged();;
        if (ccModelFilenames_ == null) {
          ccModelFilenames_ = com.google.protobuf.MapField.newMapField(
              CcModelFilenamesDefaultEntryHolder.defaultEntry);
        }
        if (!ccModelFilenames_.isMutable()) {
          ccModelFilenames_ = ccModelFilenames_.copy();
        }
        return ccModelFilenames_;
      }

      public int getCcModelFilenamesCount() {
        return internalGetCcModelFilenames().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      @java.lang.Override
      public boolean containsCcModelFilenames(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetCcModelFilenames().getMap().containsKey(key);
      }
      /**
       * Use {@link #getCcModelFilenamesMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenames() {
        return getCcModelFilenamesMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getCcModelFilenamesMap() {
        return internalGetCcModelFilenames().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      @java.lang.Override

      public java.lang.String getCcModelFilenamesOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetCcModelFilenames().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      @java.lang.Override

      public java.lang.String getCcModelFilenamesOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetCcModelFilenames().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearCcModelFilenames() {
        internalGetMutableCcModelFilenames().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public Builder removeCcModelFilenames(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableCcModelFilenames().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableCcModelFilenames() {
        return internalGetMutableCcModelFilenames().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */
      public Builder putCcModelFilenames(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableCcModelFilenames().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; cc_model_filenames
       *&#64;&#64;
       *&#64;&#64;     Optional map from CUDA compute capability to the filename of
       *&#64;&#64;     the model that supports that compute capability. The filename
       *&#64;&#64;     refers to a file within the model version directory.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; cc_model_filenames = 9;</code>
       */

      public Builder putAllCcModelFilenames(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableCcModelFilenames().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, java.lang.String> metricTags_;
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMetricTags() {
        if (metricTags_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              MetricTagsDefaultEntryHolder.defaultEntry);
        }
        return metricTags_;
      }
      private com.google.protobuf.MapField<java.lang.String, java.lang.String>
      internalGetMutableMetricTags() {
        onChanged();;
        if (metricTags_ == null) {
          metricTags_ = com.google.protobuf.MapField.newMapField(
              MetricTagsDefaultEntryHolder.defaultEntry);
        }
        if (!metricTags_.isMutable()) {
          metricTags_ = metricTags_.copy();
        }
        return metricTags_;
      }

      public int getMetricTagsCount() {
        return internalGetMetricTags().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      @java.lang.Override
      public boolean containsMetricTags(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetMetricTags().getMap().containsKey(key);
      }
      /**
       * Use {@link #getMetricTagsMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String> getMetricTags() {
        return getMetricTagsMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, java.lang.String> getMetricTagsMap() {
        return internalGetMetricTags().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricTagsOrDefault(
          java.lang.String key,
          java.lang.String defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetricTags().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      @java.lang.Override

      public java.lang.String getMetricTagsOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, java.lang.String> map =
            internalGetMetricTags().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearMetricTags() {
        internalGetMutableMetricTags().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public Builder removeMetricTags(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetricTags().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, java.lang.String>
      getMutableMetricTags() {
        return internalGetMutableMetricTags().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */
      public Builder putMetricTags(
          java.lang.String key,
          java.lang.String value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableMetricTags().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,string&gt; metric_tags
       *&#64;&#64;
       *&#64;&#64;     Optional metric tags. User-specific key-value pairs for metrics
       *&#64;&#64;     reported for this model. These tags are applied to the metrics
       *&#64;&#64;     reported on the HTTP metrics port.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, string&gt; metric_tags = 10;</code>
       */

      public Builder putAllMetricTags(
          java.util.Map<java.lang.String, java.lang.String> values) {
        internalGetMutableMetricTags().getMutableMap()
            .putAll(values);
        return this;
      }

      private com.google.protobuf.MapField<
          java.lang.String, inference.ModelConfigOuterClass.ModelParameter> parameters_;
      private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
      internalGetParameters() {
        if (parameters_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ParametersDefaultEntryHolder.defaultEntry);
        }
        return parameters_;
      }
      private com.google.protobuf.MapField<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
      internalGetMutableParameters() {
        onChanged();;
        if (parameters_ == null) {
          parameters_ = com.google.protobuf.MapField.newMapField(
              ParametersDefaultEntryHolder.defaultEntry);
        }
        if (!parameters_.isMutable()) {
          parameters_ = parameters_.copy();
        }
        return parameters_;
      }

      public int getParametersCount() {
        return internalGetParameters().getMap().size();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */

      @java.lang.Override
      public boolean containsParameters(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        return internalGetParameters().getMap().containsKey(key);
      }
      /**
       * Use {@link #getParametersMap()} instead.
       */
      @java.lang.Override
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> getParameters() {
        return getParametersMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      @java.lang.Override

      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> getParametersMap() {
        return internalGetParameters().getMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelParameter getParametersOrDefault(
          java.lang.String key,
          inference.ModelConfigOuterClass.ModelParameter defaultValue) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> map =
            internalGetParameters().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      @java.lang.Override

      public inference.ModelConfigOuterClass.ModelParameter getParametersOrThrow(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> map =
            internalGetParameters().getMap();
        if (!map.containsKey(key)) {
          throw new java.lang.IllegalArgumentException();
        }
        return map.get(key);
      }

      public Builder clearParameters() {
        internalGetMutableParameters().getMutableMap()
            .clear();
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */

      public Builder removeParameters(
          java.lang.String key) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableParameters().getMutableMap()
            .remove(key);
        return this;
      }
      /**
       * Use alternate mutation accessors instead.
       */
      @java.lang.Deprecated
      public java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter>
      getMutableParameters() {
        return internalGetMutableParameters().getMutableMap();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */
      public Builder putParameters(
          java.lang.String key,
          inference.ModelConfigOuterClass.ModelParameter value) {
        if (key == null) { throw new java.lang.NullPointerException(); }
        if (value == null) { throw new java.lang.NullPointerException(); }
        internalGetMutableParameters().getMutableMap()
            .put(key, value);
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: map&lt;string,ModelParameter&gt; parameters
       *&#64;&#64;
       *&#64;&#64;     Optional model parameters. User-specified parameter values that
       *&#64;&#64;     are made available to custom backends.
       *&#64;&#64;
       * </pre>
       *
       * <code>map&lt;string, .inference.ModelParameter&gt; parameters = 14;</code>
       */

      public Builder putAllParameters(
          java.util.Map<java.lang.String, inference.ModelConfigOuterClass.ModelParameter> values) {
        internalGetMutableParameters().getMutableMap()
            .putAll(values);
        return this;
      }

      private java.util.List<inference.ModelConfigOuterClass.ModelWarmup> modelWarmup_ =
        java.util.Collections.emptyList();
      private void ensureModelWarmupIsMutable() {
        if (!((bitField0_ & 0x00000100) != 0)) {
          modelWarmup_ = new java.util.ArrayList<inference.ModelConfigOuterClass.ModelWarmup>(modelWarmup_);
          bitField0_ |= 0x00000100;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelWarmup, inference.ModelConfigOuterClass.ModelWarmup.Builder, inference.ModelConfigOuterClass.ModelWarmupOrBuilder> modelWarmupBuilder_;

      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelWarmup> getModelWarmupList() {
        if (modelWarmupBuilder_ == null) {
          return java.util.Collections.unmodifiableList(modelWarmup_);
        } else {
          return modelWarmupBuilder_.getMessageList();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public int getModelWarmupCount() {
        if (modelWarmupBuilder_ == null) {
          return modelWarmup_.size();
        } else {
          return modelWarmupBuilder_.getCount();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public inference.ModelConfigOuterClass.ModelWarmup getModelWarmup(int index) {
        if (modelWarmupBuilder_ == null) {
          return modelWarmup_.get(index);
        } else {
          return modelWarmupBuilder_.getMessage(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder setModelWarmup(
          int index, inference.ModelConfigOuterClass.ModelWarmup value) {
        if (modelWarmupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureModelWarmupIsMutable();
          modelWarmup_.set(index, value);
          onChanged();
        } else {
          modelWarmupBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder setModelWarmup(
          int index, inference.ModelConfigOuterClass.ModelWarmup.Builder builderForValue) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          modelWarmup_.set(index, builderForValue.build());
          onChanged();
        } else {
          modelWarmupBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(inference.ModelConfigOuterClass.ModelWarmup value) {
        if (modelWarmupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureModelWarmupIsMutable();
          modelWarmup_.add(value);
          onChanged();
        } else {
          modelWarmupBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(
          int index, inference.ModelConfigOuterClass.ModelWarmup value) {
        if (modelWarmupBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureModelWarmupIsMutable();
          modelWarmup_.add(index, value);
          onChanged();
        } else {
          modelWarmupBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(
          inference.ModelConfigOuterClass.ModelWarmup.Builder builderForValue) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          modelWarmup_.add(builderForValue.build());
          onChanged();
        } else {
          modelWarmupBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addModelWarmup(
          int index, inference.ModelConfigOuterClass.ModelWarmup.Builder builderForValue) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          modelWarmup_.add(index, builderForValue.build());
          onChanged();
        } else {
          modelWarmupBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder addAllModelWarmup(
          java.lang.Iterable<? extends inference.ModelConfigOuterClass.ModelWarmup> values) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, modelWarmup_);
          onChanged();
        } else {
          modelWarmupBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder clearModelWarmup() {
        if (modelWarmupBuilder_ == null) {
          modelWarmup_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000100);
          onChanged();
        } else {
          modelWarmupBuilder_.clear();
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public Builder removeModelWarmup(int index) {
        if (modelWarmupBuilder_ == null) {
          ensureModelWarmupIsMutable();
          modelWarmup_.remove(index);
          onChanged();
        } else {
          modelWarmupBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public inference.ModelConfigOuterClass.ModelWarmup.Builder getModelWarmupBuilder(
          int index) {
        return getModelWarmupFieldBuilder().getBuilder(index);
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public inference.ModelConfigOuterClass.ModelWarmupOrBuilder getModelWarmupOrBuilder(
          int index) {
        if (modelWarmupBuilder_ == null) {
          return modelWarmup_.get(index);  } else {
          return modelWarmupBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public java.util.List<? extends inference.ModelConfigOuterClass.ModelWarmupOrBuilder> 
           getModelWarmupOrBuilderList() {
        if (modelWarmupBuilder_ != null) {
          return modelWarmupBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(modelWarmup_);
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public inference.ModelConfigOuterClass.ModelWarmup.Builder addModelWarmupBuilder() {
        return getModelWarmupFieldBuilder().addBuilder(
            inference.ModelConfigOuterClass.ModelWarmup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public inference.ModelConfigOuterClass.ModelWarmup.Builder addModelWarmupBuilder(
          int index) {
        return getModelWarmupFieldBuilder().addBuilder(
            index, inference.ModelConfigOuterClass.ModelWarmup.getDefaultInstance());
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelWarmup model_warmup (repeated)
       *&#64;&#64;
       *&#64;&#64;     Warmup setting of this model. If specified, all instances
       *&#64;&#64;     will be run with the request samples in sequence before
       *&#64;&#64;     serving the model.
       *&#64;&#64;     This field can only be specified if the model is not an ensemble
       *&#64;&#64;     model.
       *&#64;&#64;
       * </pre>
       *
       * <code>repeated .inference.ModelWarmup model_warmup = 16;</code>
       */
      public java.util.List<inference.ModelConfigOuterClass.ModelWarmup.Builder> 
           getModelWarmupBuilderList() {
        return getModelWarmupFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelWarmup, inference.ModelConfigOuterClass.ModelWarmup.Builder, inference.ModelConfigOuterClass.ModelWarmupOrBuilder> 
          getModelWarmupFieldBuilder() {
        if (modelWarmupBuilder_ == null) {
          modelWarmupBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelWarmup, inference.ModelConfigOuterClass.ModelWarmup.Builder, inference.ModelConfigOuterClass.ModelWarmupOrBuilder>(
                  modelWarmup_,
                  ((bitField0_ & 0x00000100) != 0),
                  getParentForChildren(),
                  isClean());
          modelWarmup_ = null;
        }
        return modelWarmupBuilder_;
      }

      private inference.ModelConfigOuterClass.ModelOperations modelOperations_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOperations, inference.ModelConfigOuterClass.ModelOperations.Builder, inference.ModelConfigOuterClass.ModelOperationsOrBuilder> modelOperationsBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       * @return Whether the modelOperations field is set.
       */
      public boolean hasModelOperations() {
        return modelOperationsBuilder_ != null || modelOperations_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       * @return The modelOperations.
       */
      public inference.ModelConfigOuterClass.ModelOperations getModelOperations() {
        if (modelOperationsBuilder_ == null) {
          return modelOperations_ == null ? inference.ModelConfigOuterClass.ModelOperations.getDefaultInstance() : modelOperations_;
        } else {
          return modelOperationsBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public Builder setModelOperations(inference.ModelConfigOuterClass.ModelOperations value) {
        if (modelOperationsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          modelOperations_ = value;
          onChanged();
        } else {
          modelOperationsBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public Builder setModelOperations(
          inference.ModelConfigOuterClass.ModelOperations.Builder builderForValue) {
        if (modelOperationsBuilder_ == null) {
          modelOperations_ = builderForValue.build();
          onChanged();
        } else {
          modelOperationsBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public Builder mergeModelOperations(inference.ModelConfigOuterClass.ModelOperations value) {
        if (modelOperationsBuilder_ == null) {
          if (modelOperations_ != null) {
            modelOperations_ =
              inference.ModelConfigOuterClass.ModelOperations.newBuilder(modelOperations_).mergeFrom(value).buildPartial();
          } else {
            modelOperations_ = value;
          }
          onChanged();
        } else {
          modelOperationsBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public Builder clearModelOperations() {
        if (modelOperationsBuilder_ == null) {
          modelOperations_ = null;
          onChanged();
        } else {
          modelOperations_ = null;
          modelOperationsBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public inference.ModelConfigOuterClass.ModelOperations.Builder getModelOperationsBuilder() {
        
        onChanged();
        return getModelOperationsFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      public inference.ModelConfigOuterClass.ModelOperationsOrBuilder getModelOperationsOrBuilder() {
        if (modelOperationsBuilder_ != null) {
          return modelOperationsBuilder_.getMessageOrBuilder();
        } else {
          return modelOperations_ == null ?
              inference.ModelConfigOuterClass.ModelOperations.getDefaultInstance() : modelOperations_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelOperations model_operations
       *&#64;&#64;
       *&#64;&#64;     Optional metadata of the libraries providing custom operations for
       *&#64;&#64;     this model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelOperations model_operations = 18;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelOperations, inference.ModelConfigOuterClass.ModelOperations.Builder, inference.ModelConfigOuterClass.ModelOperationsOrBuilder> 
          getModelOperationsFieldBuilder() {
        if (modelOperationsBuilder_ == null) {
          modelOperationsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelOperations, inference.ModelConfigOuterClass.ModelOperations.Builder, inference.ModelConfigOuterClass.ModelOperationsOrBuilder>(
                  getModelOperations(),
                  getParentForChildren(),
                  isClean());
          modelOperations_ = null;
        }
        return modelOperationsBuilder_;
      }

      private inference.ModelConfigOuterClass.ModelTransactionPolicy modelTransactionPolicy_;
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelTransactionPolicy, inference.ModelConfigOuterClass.ModelTransactionPolicy.Builder, inference.ModelConfigOuterClass.ModelTransactionPolicyOrBuilder> modelTransactionPolicyBuilder_;
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       * @return Whether the modelTransactionPolicy field is set.
       */
      public boolean hasModelTransactionPolicy() {
        return modelTransactionPolicyBuilder_ != null || modelTransactionPolicy_ != null;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       * @return The modelTransactionPolicy.
       */
      public inference.ModelConfigOuterClass.ModelTransactionPolicy getModelTransactionPolicy() {
        if (modelTransactionPolicyBuilder_ == null) {
          return modelTransactionPolicy_ == null ? inference.ModelConfigOuterClass.ModelTransactionPolicy.getDefaultInstance() : modelTransactionPolicy_;
        } else {
          return modelTransactionPolicyBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public Builder setModelTransactionPolicy(inference.ModelConfigOuterClass.ModelTransactionPolicy value) {
        if (modelTransactionPolicyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          modelTransactionPolicy_ = value;
          onChanged();
        } else {
          modelTransactionPolicyBuilder_.setMessage(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public Builder setModelTransactionPolicy(
          inference.ModelConfigOuterClass.ModelTransactionPolicy.Builder builderForValue) {
        if (modelTransactionPolicyBuilder_ == null) {
          modelTransactionPolicy_ = builderForValue.build();
          onChanged();
        } else {
          modelTransactionPolicyBuilder_.setMessage(builderForValue.build());
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public Builder mergeModelTransactionPolicy(inference.ModelConfigOuterClass.ModelTransactionPolicy value) {
        if (modelTransactionPolicyBuilder_ == null) {
          if (modelTransactionPolicy_ != null) {
            modelTransactionPolicy_ =
              inference.ModelConfigOuterClass.ModelTransactionPolicy.newBuilder(modelTransactionPolicy_).mergeFrom(value).buildPartial();
          } else {
            modelTransactionPolicy_ = value;
          }
          onChanged();
        } else {
          modelTransactionPolicyBuilder_.mergeFrom(value);
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public Builder clearModelTransactionPolicy() {
        if (modelTransactionPolicyBuilder_ == null) {
          modelTransactionPolicy_ = null;
          onChanged();
        } else {
          modelTransactionPolicy_ = null;
          modelTransactionPolicyBuilder_ = null;
        }

        return this;
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public inference.ModelConfigOuterClass.ModelTransactionPolicy.Builder getModelTransactionPolicyBuilder() {
        
        onChanged();
        return getModelTransactionPolicyFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      public inference.ModelConfigOuterClass.ModelTransactionPolicyOrBuilder getModelTransactionPolicyOrBuilder() {
        if (modelTransactionPolicyBuilder_ != null) {
          return modelTransactionPolicyBuilder_.getMessageOrBuilder();
        } else {
          return modelTransactionPolicy_ == null ?
              inference.ModelConfigOuterClass.ModelTransactionPolicy.getDefaultInstance() : modelTransactionPolicy_;
        }
      }
      /**
       * <pre>
       *&#64;&#64;  .. cpp:var:: ModelTransactionPolicy model_transaction_policy
       *&#64;&#64;
       *&#64;&#64;     Optional specification that describes the nature of transactions
       *&#64;&#64;     to be expected from the model.
       *&#64;&#64;
       * </pre>
       *
       * <code>.inference.ModelTransactionPolicy model_transaction_policy = 19;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          inference.ModelConfigOuterClass.ModelTransactionPolicy, inference.ModelConfigOuterClass.ModelTransactionPolicy.Builder, inference.ModelConfigOuterClass.ModelTransactionPolicyOrBuilder> 
          getModelTransactionPolicyFieldBuilder() {
        if (modelTransactionPolicyBuilder_ == null) {
          modelTransactionPolicyBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              inference.ModelConfigOuterClass.ModelTransactionPolicy, inference.ModelConfigOuterClass.ModelTransactionPolicy.Builder, inference.ModelConfigOuterClass.ModelTransactionPolicyOrBuilder>(
                  getModelTransactionPolicy(),
                  getParentForChildren(),
                  isClean());
          modelTransactionPolicy_ = null;
        }
        return modelTransactionPolicyBuilder_;
      }
      @java.lang.Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @java.lang.Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:inference.ModelConfig)
    }

    // @@protoc_insertion_point(class_scope:inference.ModelConfig)
    private static final inference.ModelConfigOuterClass.ModelConfig DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new inference.ModelConfigOuterClass.ModelConfig();
    }

    public static inference.ModelConfigOuterClass.ModelConfig getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ModelConfig>
        PARSER = new com.google.protobuf.AbstractParser<ModelConfig>() {
      @java.lang.Override
      public ModelConfig parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return new ModelConfig(input, extensionRegistry);
      }
    };

    public static com.google.protobuf.Parser<ModelConfig> parser() {
      return PARSER;
    }

    @java.lang.Override
    public com.google.protobuf.Parser<ModelConfig> getParserForType() {
      return PARSER;
    }

    @java.lang.Override
    public inference.ModelConfigOuterClass.ModelConfig getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelRateLimiter_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelRateLimiter_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelRateLimiter_Resource_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelRateLimiter_Resource_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelInstanceGroup_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelInstanceGroup_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelTensorReshape_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelTensorReshape_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelInput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelInput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOutput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOutput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_BatchInput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_BatchInput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_BatchOutput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_BatchOutput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelVersionPolicy_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelVersionPolicy_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelVersionPolicy_Latest_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelVersionPolicy_Latest_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelVersionPolicy_All_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelVersionPolicy_All_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelVersionPolicy_Specific_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelVersionPolicy_Specific_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_Graph_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_Graph_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_Cuda_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_Cuda_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_InputEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_InputEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_InputEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_InputEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelQueuePolicy_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelQueuePolicy_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelDynamicBatching_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelDynamicBatching_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelDynamicBatching_PriorityQueuePolicyEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelDynamicBatching_PriorityQueuePolicyEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelSequenceBatching_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelSequenceBatching_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelSequenceBatching_Control_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelSequenceBatching_Control_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelSequenceBatching_ControlInput_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelSequenceBatching_ControlInput_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelSequenceBatching_StrategyDirect_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelSequenceBatching_StrategyDirect_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelSequenceBatching_StrategyOldest_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelSequenceBatching_StrategyOldest_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelEnsembling_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelEnsembling_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelEnsembling_Step_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelEnsembling_Step_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelEnsembling_Step_InputMapEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelEnsembling_Step_InputMapEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelEnsembling_Step_OutputMapEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelEnsembling_Step_OutputMapEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelParameter_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelParameter_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelWarmup_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelWarmup_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelWarmup_Input_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelWarmup_Input_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelWarmup_InputsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelWarmup_InputsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelOperations_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelOperations_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelTransactionPolicy_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelTransactionPolicy_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelConfig_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelConfig_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelConfig_CcModelFilenamesEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelConfig_CcModelFilenamesEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelConfig_MetricTagsEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelConfig_MetricTagsEntry_fieldAccessorTable;
  private static final com.google.protobuf.Descriptors.Descriptor
    internal_static_inference_ModelConfig_ParametersEntry_descriptor;
  private static final 
    com.google.protobuf.GeneratedMessageV3.FieldAccessorTable
      internal_static_inference_ModelConfig_ParametersEntry_fieldAccessorTable;

  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static  com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n\022model_config.proto\022\tinference\"\226\001\n\020Mode" +
      "lRateLimiter\0227\n\tresources\030\001 \003(\0132$.infere" +
      "nce.ModelRateLimiter.Resource\022\020\n\010priorit" +
      "y\030\002 \001(\r\0327\n\010Resource\022\014\n\004name\030\001 \001(\t\022\016\n\006glo" +
      "bal\030\002 \001(\010\022\r\n\005count\030\003 \001(\r\"\370\001\n\022ModelInstan" +
      "ceGroup\022\014\n\004name\030\001 \001(\t\0220\n\004kind\030\004 \001(\0162\".in" +
      "ference.ModelInstanceGroup.Kind\022\r\n\005count" +
      "\030\002 \001(\005\0221\n\014rate_limiter\030\006 \001(\0132\033.inference" +
      ".ModelRateLimiter\022\014\n\004gpus\030\003 \003(\005\022\017\n\007profi" +
      "le\030\005 \003(\t\"A\n\004Kind\022\r\n\tKIND_AUTO\020\000\022\014\n\010KIND_" +
      "GPU\020\001\022\014\n\010KIND_CPU\020\002\022\016\n\nKIND_MODEL\020\003\"#\n\022M" +
      "odelTensorReshape\022\r\n\005shape\030\001 \003(\003\"\240\002\n\nMod" +
      "elInput\022\014\n\004name\030\001 \001(\t\022&\n\tdata_type\030\002 \001(\016" +
      "2\023.inference.DataType\022,\n\006format\030\003 \001(\0162\034." +
      "inference.ModelInput.Format\022\014\n\004dims\030\004 \003(" +
      "\003\022.\n\007reshape\030\005 \001(\0132\035.inference.ModelTens" +
      "orReshape\022\027\n\017is_shape_tensor\030\006 \001(\010\022\032\n\022al" +
      "low_ragged_batch\030\007 \001(\010\";\n\006Format\022\017\n\013FORM" +
      "AT_NONE\020\000\022\017\n\013FORMAT_NHWC\020\001\022\017\n\013FORMAT_NCH" +
      "W\020\002\"\262\001\n\013ModelOutput\022\014\n\004name\030\001 \001(\t\022&\n\tdat" +
      "a_type\030\002 \001(\0162\023.inference.DataType\022\014\n\004dim" +
      "s\030\003 \003(\003\022.\n\007reshape\030\005 \001(\0132\035.inference.Mod" +
      "elTensorReshape\022\026\n\016label_filename\030\004 \001(\t\022" +
      "\027\n\017is_shape_tensor\030\006 \001(\010\"\245\002\n\nBatchInput\022" +
      "(\n\004kind\030\001 \001(\0162\032.inference.BatchInput.Kin" +
      "d\022\023\n\013target_name\030\002 \003(\t\022&\n\tdata_type\030\003 \001(" +
      "\0162\023.inference.DataType\022\024\n\014source_input\030\004" +
      " \003(\t\"\231\001\n\004Kind\022\027\n\023BATCH_ELEMENT_COUNT\020\000\022#" +
      "\n\037BATCH_ACCUMULATED_ELEMENT_COUNT\020\001\022-\n)B" +
      "ATCH_ACCUMULATED_ELEMENT_COUNT_WITH_ZERO" +
      "\020\002\022$\n BATCH_MAX_ELEMENT_COUNT_AS_SHAPE\020\003" +
      "\"\217\001\n\013BatchOutput\022\023\n\013target_name\030\001 \003(\t\022)\n" +
      "\004kind\030\002 \001(\0162\033.inference.BatchOutput.Kind" +
      "\022\024\n\014source_input\030\003 \003(\t\"*\n\004Kind\022\"\n\036BATCH_" +
      "SCATTER_WITH_INPUT_SHAPE\020\000\"\220\002\n\022ModelVers" +
      "ionPolicy\0226\n\006latest\030\001 \001(\0132$.inference.Mo" +
      "delVersionPolicy.LatestH\000\0220\n\003all\030\002 \001(\0132!" +
      ".inference.ModelVersionPolicy.AllH\000\022:\n\010s" +
      "pecific\030\003 \001(\0132&.inference.ModelVersionPo" +
      "licy.SpecificH\000\032\036\n\006Latest\022\024\n\014num_version" +
      "s\030\001 \001(\r\032\005\n\003All\032\034\n\010Specific\022\020\n\010versions\030\001" +
      " \003(\003B\017\n\rpolicy_choice\"\241\r\n\027ModelOptimizat" +
      "ionPolicy\0227\n\005graph\030\001 \001(\0132(.inference.Mod" +
      "elOptimizationPolicy.Graph\022B\n\010priority\030\002" +
      " \001(\01620.inference.ModelOptimizationPolicy" +
      ".ModelPriority\0225\n\004cuda\030\003 \001(\0132\'.inference" +
      ".ModelOptimizationPolicy.Cuda\022X\n\026executi" +
      "on_accelerators\030\004 \001(\01328.inference.ModelO" +
      "ptimizationPolicy.ExecutionAccelerators\022" +
      "R\n\023input_pinned_memory\030\005 \001(\01325.inference" +
      ".ModelOptimizationPolicy.PinnedMemoryBuf" +
      "fer\022S\n\024output_pinned_memory\030\006 \001(\01325.infe" +
      "rence.ModelOptimizationPolicy.PinnedMemo" +
      "ryBuffer\032\026\n\005Graph\022\r\n\005level\030\001 \001(\005\032\236\005\n\004Cud" +
      "a\022\016\n\006graphs\030\001 \001(\010\022\030\n\020busy_wait_events\030\002 " +
      "\001(\010\022E\n\ngraph_spec\030\003 \003(\01321.inference.Mode" +
      "lOptimizationPolicy.Cuda.GraphSpec\032\244\004\n\tG" +
      "raphSpec\022\022\n\nbatch_size\030\001 \001(\005\022K\n\005input\030\002 " +
      "\003(\0132<.inference.ModelOptimizationPolicy." +
      "Cuda.GraphSpec.InputEntry\022W\n\021graph_lower" +
      "_bound\030\003 \001(\0132<.inference.ModelOptimizati" +
      "onPolicy.Cuda.GraphSpec.LowerBound\032\024\n\005Sh" +
      "ape\022\013\n\003dim\030\001 \003(\003\032\337\001\n\nLowerBound\022\022\n\nbatch" +
      "_size\030\001 \001(\005\022V\n\005input\030\002 \003(\0132G.inference.M" +
      "odelOptimizationPolicy.Cuda.GraphSpec.Lo" +
      "werBound.InputEntry\032e\n\nInputEntry\022\013\n\003key" +
      "\030\001 \001(\t\022F\n\005value\030\002 \001(\01327.inference.ModelO" +
      "ptimizationPolicy.Cuda.GraphSpec.Shape:\002" +
      "8\001\032e\n\nInputEntry\022\013\n\003key\030\001 \001(\t\022F\n\005value\030\002" +
      " \001(\01327.inference.ModelOptimizationPolicy" +
      ".Cuda.GraphSpec.Shape:\0028\001\032\244\003\n\025ExecutionA" +
      "ccelerators\022g\n\031gpu_execution_accelerator" +
      "\030\001 \003(\0132D.inference.ModelOptimizationPoli" +
      "cy.ExecutionAccelerators.Accelerator\022g\n\031" +
      "cpu_execution_accelerator\030\002 \003(\0132D.infere" +
      "nce.ModelOptimizationPolicy.ExecutionAcc" +
      "elerators.Accelerator\032\270\001\n\013Accelerator\022\014\n" +
      "\004name\030\001 \001(\t\022h\n\nparameters\030\002 \003(\0132T.infere" +
      "nce.ModelOptimizationPolicy.ExecutionAcc" +
      "elerators.Accelerator.ParametersEntry\0321\n" +
      "\017ParametersEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002" +
      " \001(\t:\0028\001\032$\n\022PinnedMemoryBuffer\022\016\n\006enable" +
      "\030\001 \001(\010\"I\n\rModelPriority\022\024\n\020PRIORITY_DEFA" +
      "ULT\020\000\022\020\n\014PRIORITY_MAX\020\001\022\020\n\014PRIORITY_MIN\020" +
      "\002\"\333\001\n\020ModelQueuePolicy\022A\n\016timeout_action" +
      "\030\001 \001(\0162).inference.ModelQueuePolicy.Time" +
      "outAction\022$\n\034default_timeout_microsecond" +
      "s\030\002 \001(\004\022\036\n\026allow_timeout_override\030\003 \001(\010\022" +
      "\026\n\016max_queue_size\030\004 \001(\r\"&\n\rTimeoutAction" +
      "\022\n\n\006REJECT\020\000\022\t\n\005DELAY\020\001\"\233\003\n\024ModelDynamic" +
      "Batching\022\034\n\024preferred_batch_size\030\001 \003(\005\022$" +
      "\n\034max_queue_delay_microseconds\030\002 \001(\004\022\031\n\021" +
      "preserve_ordering\030\003 \001(\010\022\027\n\017priority_leve" +
      "ls\030\004 \001(\r\022\036\n\026default_priority_level\030\005 \001(\r" +
      "\0229\n\024default_queue_policy\030\006 \001(\0132\033.inferen" +
      "ce.ModelQueuePolicy\022W\n\025priority_queue_po" +
      "licy\030\007 \003(\01328.inference.ModelDynamicBatch" +
      "ing.PriorityQueuePolicyEntry\032W\n\030Priority" +
      "QueuePolicyEntry\022\013\n\003key\030\001 \001(\r\022*\n\005value\030\002" +
      " \001(\0132\033.inference.ModelQueuePolicy:\0028\001\"\343\006" +
      "\n\025ModelSequenceBatching\022A\n\006direct\030\003 \001(\0132" +
      "/.inference.ModelSequenceBatching.Strate" +
      "gyDirectH\000\022A\n\006oldest\030\004 \001(\0132/.inference.M" +
      "odelSequenceBatching.StrategyOldestH\000\022&\n" +
      "\036max_sequence_idle_microseconds\030\001 \001(\004\022D\n" +
      "\rcontrol_input\030\002 \003(\0132-.inference.ModelSe" +
      "quenceBatching.ControlInput\032\230\002\n\007Control\022" +
      ";\n\004kind\030\001 \001(\0162-.inference.ModelSequenceB" +
      "atching.Control.Kind\022\030\n\020int32_false_true" +
      "\030\002 \003(\005\022\027\n\017fp32_false_true\030\003 \003(\002\022&\n\tdata_" +
      "type\030\004 \001(\0162\023.inference.DataType\"u\n\004Kind\022" +
      "\032\n\026CONTROL_SEQUENCE_START\020\000\022\032\n\026CONTROL_S" +
      "EQUENCE_READY\020\001\022\030\n\024CONTROL_SEQUENCE_END\020" +
      "\002\022\033\n\027CONTROL_SEQUENCE_CORRID\020\003\032W\n\014Contro" +
      "lInput\022\014\n\004name\030\001 \001(\t\0229\n\007control\030\002 \003(\0132(." +
      "inference.ModelSequenceBatching.Control\032" +
      "X\n\016StrategyDirect\022$\n\034max_queue_delay_mic" +
      "roseconds\030\001 \001(\004\022 \n\030minimum_slot_utilizat" +
      "ion\030\002 \001(\002\032u\n\016StrategyOldest\022\037\n\027max_candi" +
      "date_sequences\030\001 \001(\005\022\034\n\024preferred_batch_" +
      "size\030\002 \003(\005\022$\n\034max_queue_delay_microsecon" +
      "ds\030\003 \001(\004B\021\n\017strategy_choice\"\335\002\n\017ModelEns" +
      "embling\022-\n\004step\030\001 \003(\0132\037.inference.ModelE" +
      "nsembling.Step\032\232\002\n\004Step\022\022\n\nmodel_name\030\001 " +
      "\001(\t\022\025\n\rmodel_version\030\002 \001(\003\022@\n\tinput_map\030" +
      "\003 \003(\0132-.inference.ModelEnsembling.Step.I" +
      "nputMapEntry\022B\n\noutput_map\030\004 \003(\0132..infer" +
      "ence.ModelEnsembling.Step.OutputMapEntry" +
      "\032/\n\rInputMapEntry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030" +
      "\002 \001(\t:\0028\001\0320\n\016OutputMapEntry\022\013\n\003key\030\001 \001(\t" +
      "\022\r\n\005value\030\002 \001(\t:\0028\001\"&\n\016ModelParameter\022\024\n" +
      "\014string_value\030\001 \001(\t\"\312\002\n\013ModelWarmup\022\014\n\004n" +
      "ame\030\001 \001(\t\022\022\n\nbatch_size\030\002 \001(\r\0222\n\006inputs\030" +
      "\003 \003(\0132\".inference.ModelWarmup.InputsEntr" +
      "y\032\227\001\n\005Input\022&\n\tdata_type\030\001 \001(\0162\023.inferen" +
      "ce.DataType\022\014\n\004dims\030\002 \003(\003\022\023\n\tzero_data\030\003" +
      " \001(\010H\000\022\025\n\013random_data\030\004 \001(\010H\000\022\031\n\017input_d" +
      "ata_file\030\005 \001(\tH\000B\021\n\017input_data_type\032K\n\013I" +
      "nputsEntry\022\013\n\003key\030\001 \001(\t\022+\n\005value\030\002 \001(\0132\034" +
      ".inference.ModelWarmup.Input:\0028\001\".\n\017Mode" +
      "lOperations\022\033\n\023op_library_filename\030\001 \003(\t" +
      "\"+\n\026ModelTransactionPolicy\022\021\n\tdecoupled\030" +
      "\001 \001(\010\"\270\t\n\013ModelConfig\022\014\n\004name\030\001 \001(\t\022\020\n\010p" +
      "latform\030\002 \001(\t\022\017\n\007backend\030\021 \001(\t\0225\n\016versio" +
      "n_policy\030\003 \001(\0132\035.inference.ModelVersionP" +
      "olicy\022\026\n\016max_batch_size\030\004 \001(\005\022$\n\005input\030\005" +
      " \003(\0132\025.inference.ModelInput\022&\n\006output\030\006 " +
      "\003(\0132\026.inference.ModelOutput\022*\n\013batch_inp" +
      "ut\030\024 \003(\0132\025.inference.BatchInput\022,\n\014batch" +
      "_output\030\025 \003(\0132\026.inference.BatchOutput\0228\n" +
      "\014optimization\030\014 \001(\0132\".inference.ModelOpt" +
      "imizationPolicy\022;\n\020dynamic_batching\030\013 \001(" +
      "\0132\037.inference.ModelDynamicBatchingH\000\022=\n\021" +
      "sequence_batching\030\r \001(\0132 .inference.Mode" +
      "lSequenceBatchingH\000\0229\n\023ensemble_scheduli" +
      "ng\030\017 \001(\0132\032.inference.ModelEnsemblingH\000\0225" +
      "\n\016instance_group\030\007 \003(\0132\035.inference.Model" +
      "InstanceGroup\022\036\n\026default_model_filename\030" +
      "\010 \001(\t\022H\n\022cc_model_filenames\030\t \003(\0132,.infe" +
      "rence.ModelConfig.CcModelFilenamesEntry\022" +
      ";\n\013metric_tags\030\n \003(\0132&.inference.ModelCo" +
      "nfig.MetricTagsEntry\022:\n\nparameters\030\016 \003(\013" +
      "2&.inference.ModelConfig.ParametersEntry" +
      "\022,\n\014model_warmup\030\020 \003(\0132\026.inference.Model" +
      "Warmup\0224\n\020model_operations\030\022 \001(\0132\032.infer" +
      "ence.ModelOperations\022C\n\030model_transactio" +
      "n_policy\030\023 \001(\0132!.inference.ModelTransact" +
      "ionPolicy\0327\n\025CcModelFilenamesEntry\022\013\n\003ke" +
      "y\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001\0321\n\017MetricTags" +
      "Entry\022\013\n\003key\030\001 \001(\t\022\r\n\005value\030\002 \001(\t:\0028\001\032L\n" +
      "\017ParametersEntry\022\013\n\003key\030\001 \001(\t\022(\n\005value\030\002" +
      " \001(\0132\031.inference.ModelParameter:\0028\001B\023\n\021s" +
      "cheduling_choice*\353\001\n\010DataType\022\020\n\014TYPE_IN" +
      "VALID\020\000\022\r\n\tTYPE_BOOL\020\001\022\016\n\nTYPE_UINT8\020\002\022\017" +
      "\n\013TYPE_UINT16\020\003\022\017\n\013TYPE_UINT32\020\004\022\017\n\013TYPE" +
      "_UINT64\020\005\022\r\n\tTYPE_INT8\020\006\022\016\n\nTYPE_INT16\020\007" +
      "\022\016\n\nTYPE_INT32\020\010\022\016\n\nTYPE_INT64\020\t\022\r\n\tTYPE" +
      "_FP16\020\n\022\r\n\tTYPE_FP32\020\013\022\r\n\tTYPE_FP64\020\014\022\017\n" +
      "\013TYPE_STRING\020\rb\006proto3"
    };
    descriptor = com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        });
    internal_static_inference_ModelRateLimiter_descriptor =
      getDescriptor().getMessageTypes().get(0);
    internal_static_inference_ModelRateLimiter_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelRateLimiter_descriptor,
        new java.lang.String[] { "Resources", "Priority", });
    internal_static_inference_ModelRateLimiter_Resource_descriptor =
      internal_static_inference_ModelRateLimiter_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelRateLimiter_Resource_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelRateLimiter_Resource_descriptor,
        new java.lang.String[] { "Name", "Global", "Count", });
    internal_static_inference_ModelInstanceGroup_descriptor =
      getDescriptor().getMessageTypes().get(1);
    internal_static_inference_ModelInstanceGroup_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelInstanceGroup_descriptor,
        new java.lang.String[] { "Name", "Kind", "Count", "RateLimiter", "Gpus", "Profile", });
    internal_static_inference_ModelTensorReshape_descriptor =
      getDescriptor().getMessageTypes().get(2);
    internal_static_inference_ModelTensorReshape_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelTensorReshape_descriptor,
        new java.lang.String[] { "Shape", });
    internal_static_inference_ModelInput_descriptor =
      getDescriptor().getMessageTypes().get(3);
    internal_static_inference_ModelInput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelInput_descriptor,
        new java.lang.String[] { "Name", "DataType", "Format", "Dims", "Reshape", "IsShapeTensor", "AllowRaggedBatch", });
    internal_static_inference_ModelOutput_descriptor =
      getDescriptor().getMessageTypes().get(4);
    internal_static_inference_ModelOutput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOutput_descriptor,
        new java.lang.String[] { "Name", "DataType", "Dims", "Reshape", "LabelFilename", "IsShapeTensor", });
    internal_static_inference_BatchInput_descriptor =
      getDescriptor().getMessageTypes().get(5);
    internal_static_inference_BatchInput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_BatchInput_descriptor,
        new java.lang.String[] { "Kind", "TargetName", "DataType", "SourceInput", });
    internal_static_inference_BatchOutput_descriptor =
      getDescriptor().getMessageTypes().get(6);
    internal_static_inference_BatchOutput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_BatchOutput_descriptor,
        new java.lang.String[] { "TargetName", "Kind", "SourceInput", });
    internal_static_inference_ModelVersionPolicy_descriptor =
      getDescriptor().getMessageTypes().get(7);
    internal_static_inference_ModelVersionPolicy_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelVersionPolicy_descriptor,
        new java.lang.String[] { "Latest", "All", "Specific", "PolicyChoice", });
    internal_static_inference_ModelVersionPolicy_Latest_descriptor =
      internal_static_inference_ModelVersionPolicy_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelVersionPolicy_Latest_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelVersionPolicy_Latest_descriptor,
        new java.lang.String[] { "NumVersions", });
    internal_static_inference_ModelVersionPolicy_All_descriptor =
      internal_static_inference_ModelVersionPolicy_descriptor.getNestedTypes().get(1);
    internal_static_inference_ModelVersionPolicy_All_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelVersionPolicy_All_descriptor,
        new java.lang.String[] { });
    internal_static_inference_ModelVersionPolicy_Specific_descriptor =
      internal_static_inference_ModelVersionPolicy_descriptor.getNestedTypes().get(2);
    internal_static_inference_ModelVersionPolicy_Specific_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelVersionPolicy_Specific_descriptor,
        new java.lang.String[] { "Versions", });
    internal_static_inference_ModelOptimizationPolicy_descriptor =
      getDescriptor().getMessageTypes().get(8);
    internal_static_inference_ModelOptimizationPolicy_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_descriptor,
        new java.lang.String[] { "Graph", "Priority", "Cuda", "ExecutionAccelerators", "InputPinnedMemory", "OutputPinnedMemory", });
    internal_static_inference_ModelOptimizationPolicy_Graph_descriptor =
      internal_static_inference_ModelOptimizationPolicy_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelOptimizationPolicy_Graph_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_Graph_descriptor,
        new java.lang.String[] { "Level", });
    internal_static_inference_ModelOptimizationPolicy_Cuda_descriptor =
      internal_static_inference_ModelOptimizationPolicy_descriptor.getNestedTypes().get(1);
    internal_static_inference_ModelOptimizationPolicy_Cuda_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_Cuda_descriptor,
        new java.lang.String[] { "Graphs", "BusyWaitEvents", "GraphSpec", });
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_descriptor =
      internal_static_inference_ModelOptimizationPolicy_Cuda_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_descriptor,
        new java.lang.String[] { "BatchSize", "Input", "GraphLowerBound", });
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_descriptor =
      internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_Shape_descriptor,
        new java.lang.String[] { "Dim", });
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_descriptor =
      internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_descriptor.getNestedTypes().get(1);
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_descriptor,
        new java.lang.String[] { "BatchSize", "Input", });
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_InputEntry_descriptor =
      internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_InputEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_LowerBound_InputEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_InputEntry_descriptor =
      internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_descriptor.getNestedTypes().get(2);
    internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_InputEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_Cuda_GraphSpec_InputEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_descriptor =
      internal_static_inference_ModelOptimizationPolicy_descriptor.getNestedTypes().get(2);
    internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_descriptor,
        new java.lang.String[] { "GpuExecutionAccelerator", "CpuExecutionAccelerator", });
    internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor =
      internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor,
        new java.lang.String[] { "Name", "Parameters", });
    internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_descriptor =
      internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_ExecutionAccelerators_Accelerator_ParametersEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_descriptor =
      internal_static_inference_ModelOptimizationPolicy_descriptor.getNestedTypes().get(3);
    internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOptimizationPolicy_PinnedMemoryBuffer_descriptor,
        new java.lang.String[] { "Enable", });
    internal_static_inference_ModelQueuePolicy_descriptor =
      getDescriptor().getMessageTypes().get(9);
    internal_static_inference_ModelQueuePolicy_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelQueuePolicy_descriptor,
        new java.lang.String[] { "TimeoutAction", "DefaultTimeoutMicroseconds", "AllowTimeoutOverride", "MaxQueueSize", });
    internal_static_inference_ModelDynamicBatching_descriptor =
      getDescriptor().getMessageTypes().get(10);
    internal_static_inference_ModelDynamicBatching_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelDynamicBatching_descriptor,
        new java.lang.String[] { "PreferredBatchSize", "MaxQueueDelayMicroseconds", "PreserveOrdering", "PriorityLevels", "DefaultPriorityLevel", "DefaultQueuePolicy", "PriorityQueuePolicy", });
    internal_static_inference_ModelDynamicBatching_PriorityQueuePolicyEntry_descriptor =
      internal_static_inference_ModelDynamicBatching_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelDynamicBatching_PriorityQueuePolicyEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelDynamicBatching_PriorityQueuePolicyEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_inference_ModelSequenceBatching_descriptor =
      getDescriptor().getMessageTypes().get(11);
    internal_static_inference_ModelSequenceBatching_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelSequenceBatching_descriptor,
        new java.lang.String[] { "Direct", "Oldest", "MaxSequenceIdleMicroseconds", "ControlInput", "StrategyChoice", });
    internal_static_inference_ModelSequenceBatching_Control_descriptor =
      internal_static_inference_ModelSequenceBatching_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelSequenceBatching_Control_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelSequenceBatching_Control_descriptor,
        new java.lang.String[] { "Kind", "Int32FalseTrue", "Fp32FalseTrue", "DataType", });
    internal_static_inference_ModelSequenceBatching_ControlInput_descriptor =
      internal_static_inference_ModelSequenceBatching_descriptor.getNestedTypes().get(1);
    internal_static_inference_ModelSequenceBatching_ControlInput_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelSequenceBatching_ControlInput_descriptor,
        new java.lang.String[] { "Name", "Control", });
    internal_static_inference_ModelSequenceBatching_StrategyDirect_descriptor =
      internal_static_inference_ModelSequenceBatching_descriptor.getNestedTypes().get(2);
    internal_static_inference_ModelSequenceBatching_StrategyDirect_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelSequenceBatching_StrategyDirect_descriptor,
        new java.lang.String[] { "MaxQueueDelayMicroseconds", "MinimumSlotUtilization", });
    internal_static_inference_ModelSequenceBatching_StrategyOldest_descriptor =
      internal_static_inference_ModelSequenceBatching_descriptor.getNestedTypes().get(3);
    internal_static_inference_ModelSequenceBatching_StrategyOldest_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelSequenceBatching_StrategyOldest_descriptor,
        new java.lang.String[] { "MaxCandidateSequences", "PreferredBatchSize", "MaxQueueDelayMicroseconds", });
    internal_static_inference_ModelEnsembling_descriptor =
      getDescriptor().getMessageTypes().get(12);
    internal_static_inference_ModelEnsembling_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelEnsembling_descriptor,
        new java.lang.String[] { "Step", });
    internal_static_inference_ModelEnsembling_Step_descriptor =
      internal_static_inference_ModelEnsembling_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelEnsembling_Step_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelEnsembling_Step_descriptor,
        new java.lang.String[] { "ModelName", "ModelVersion", "InputMap", "OutputMap", });
    internal_static_inference_ModelEnsembling_Step_InputMapEntry_descriptor =
      internal_static_inference_ModelEnsembling_Step_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelEnsembling_Step_InputMapEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelEnsembling_Step_InputMapEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_inference_ModelEnsembling_Step_OutputMapEntry_descriptor =
      internal_static_inference_ModelEnsembling_Step_descriptor.getNestedTypes().get(1);
    internal_static_inference_ModelEnsembling_Step_OutputMapEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelEnsembling_Step_OutputMapEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_inference_ModelParameter_descriptor =
      getDescriptor().getMessageTypes().get(13);
    internal_static_inference_ModelParameter_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelParameter_descriptor,
        new java.lang.String[] { "StringValue", });
    internal_static_inference_ModelWarmup_descriptor =
      getDescriptor().getMessageTypes().get(14);
    internal_static_inference_ModelWarmup_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelWarmup_descriptor,
        new java.lang.String[] { "Name", "BatchSize", "Inputs", });
    internal_static_inference_ModelWarmup_Input_descriptor =
      internal_static_inference_ModelWarmup_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelWarmup_Input_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelWarmup_Input_descriptor,
        new java.lang.String[] { "DataType", "Dims", "ZeroData", "RandomData", "InputDataFile", "InputDataType", });
    internal_static_inference_ModelWarmup_InputsEntry_descriptor =
      internal_static_inference_ModelWarmup_descriptor.getNestedTypes().get(1);
    internal_static_inference_ModelWarmup_InputsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelWarmup_InputsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_inference_ModelOperations_descriptor =
      getDescriptor().getMessageTypes().get(15);
    internal_static_inference_ModelOperations_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelOperations_descriptor,
        new java.lang.String[] { "OpLibraryFilename", });
    internal_static_inference_ModelTransactionPolicy_descriptor =
      getDescriptor().getMessageTypes().get(16);
    internal_static_inference_ModelTransactionPolicy_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelTransactionPolicy_descriptor,
        new java.lang.String[] { "Decoupled", });
    internal_static_inference_ModelConfig_descriptor =
      getDescriptor().getMessageTypes().get(17);
    internal_static_inference_ModelConfig_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelConfig_descriptor,
        new java.lang.String[] { "Name", "Platform", "Backend", "VersionPolicy", "MaxBatchSize", "Input", "Output", "BatchInput", "BatchOutput", "Optimization", "DynamicBatching", "SequenceBatching", "EnsembleScheduling", "InstanceGroup", "DefaultModelFilename", "CcModelFilenames", "MetricTags", "Parameters", "ModelWarmup", "ModelOperations", "ModelTransactionPolicy", "SchedulingChoice", });
    internal_static_inference_ModelConfig_CcModelFilenamesEntry_descriptor =
      internal_static_inference_ModelConfig_descriptor.getNestedTypes().get(0);
    internal_static_inference_ModelConfig_CcModelFilenamesEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelConfig_CcModelFilenamesEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_inference_ModelConfig_MetricTagsEntry_descriptor =
      internal_static_inference_ModelConfig_descriptor.getNestedTypes().get(1);
    internal_static_inference_ModelConfig_MetricTagsEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelConfig_MetricTagsEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
    internal_static_inference_ModelConfig_ParametersEntry_descriptor =
      internal_static_inference_ModelConfig_descriptor.getNestedTypes().get(2);
    internal_static_inference_ModelConfig_ParametersEntry_fieldAccessorTable = new
      com.google.protobuf.GeneratedMessageV3.FieldAccessorTable(
        internal_static_inference_ModelConfig_ParametersEntry_descriptor,
        new java.lang.String[] { "Key", "Value", });
  }

  // @@protoc_insertion_point(outer_class_scope)
}
