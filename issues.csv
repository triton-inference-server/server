6044,OPEN,Is there a triton endpoint to check if there is requests in the model or not?,,2023-07-11 21:27:12 +0000 UTC
6042,OPEN,transformer model output mismatch,,2023-07-11 09:14:56 +0000 UTC
6038,OPEN,Memory Leak When Using Python/TensorRT Backend,bug,2023-07-10 20:55:34 +0000 UTC
6036,OPEN,How to convert an image into the input required by the model in Java,question,2023-07-07 14:21:43 +0000 UTC
6035,OPEN,How can I get OpenVINO model configure example?,question,2023-07-07 12:38:49 +0000 UTC
6034,OPEN,triton aiohttp client report "Timeout context manager should be used inside a task" error,question,2023-07-07 12:33:59 +0000 UTC
6030,CLOSED,error running inference using grpc client for java,invalid, question,2023-07-07 14:16:14 +0000 UTC
6029,CLOSED,[QST] when/where to exactly load the pytorch mode?,question,2023-07-06 04:17:57 +0000 UTC
6028,OPEN,model lock for python backend Model Loading API,enhancement, question,2023-07-06 17:51:03 +0000 UTC
6027,OPEN,Create a torch::multipy (torch::deploy) backend for eager mode serving,enhancement,2023-07-07 20:20:54 +0000 UTC
6025,OPEN,About the deployment of the chat-glm model,question,2023-07-06 02:32:11 +0000 UTC
6024,CLOSED,How to set up the config.pbtxt for 2 completely different shaped inputs?,,2023-07-05 03:24:43 +0000 UTC
6023,CLOSED,There is a bug when I set the initial_state,invalid,2023-07-10 20:18:17 +0000 UTC
6021,OPEN,model repository to use semantic versioning,enhancement,2023-07-05 14:15:38 +0000 UTC
6020,OPEN,Support client to build with CMAKE_MSVC_RUNTIME_LIBRARY,enhancement,2023-07-05 22:55:51 +0000 UTC
6015,OPEN,Build and run client in mac, ios and android,enhancement,2023-07-04 05:58:49 +0000 UTC
6014,OPEN,Newer versions of triton server have a consirable slowdown in start time,bug, investigating,2023-07-07 18:31:56 +0000 UTC
6013,CLOSED,Question About Batching Process in Triton,question,2023-07-06 15:31:08 +0000 UTC
6012,CLOSED,Compiling error,question,2023-07-05 20:59:41 +0000 UTC
6011,CLOSED,Prometheus metric nv_inference_request_summary_us missing in 23.06 release,investigating,2023-07-07 06:14:28 +0000 UTC
6009,OPEN,memory_usage is [] on A6000s,,2023-07-03 12:17:06 +0000 UTC
6008,OPEN,How can i identify the model instances in the instance group ?,question,2023-07-06 22:46:34 +0000 UTC
6007,OPEN,How can we update models in all triton servers using mlflow triton plugin,,2023-06-29 22:03:11 +0000 UTC
6004,CLOSED,Default Models not working,,2023-06-29 17:58:35 +0000 UTC
6003,CLOSED,async_stream_infer has no keyword argument 'enable_empty_final_response',,2023-06-30 01:28:08 +0000 UTC
6002,CLOSED,Dynamic batching does not work in decoupled model,,2023-07-03 01:54:37 +0000 UTC
5999,CLOSED,json data,,2023-06-28 11:36:51 +0000 UTC
5998,CLOSED,how to return json data,question,2023-06-29 02:08:54 +0000 UTC
5997,OPEN,UNAVAILABLE: Internal: failed to load model 'yolov5': PytorchStreamReader failed locating file constants.pkl: file not found,question,2023-06-30 19:10:11 +0000 UTC
5996,CLOSED,how to set the ensemble_scheduling when do inference of difference outputs,question,2023-07-05 06:03:13 +0000 UTC
5989,OPEN,PeftModel work well on python backend?,question, investigating,2023-06-29 01:20:02 +0000 UTC
5988,CLOSED,【PythonBackend】TRITONBACKEND_ResponseSend hangs,,2023-06-30 05:55:39 +0000 UTC
5987,OPEN,OpenTelemetry Tracing allow to configure service name,enhancement,2023-06-26 19:19:47 +0000 UTC
5983,OPEN,How to restart a model instance,enhancement,2023-06-28 00:09:02 +0000 UTC
5982,OPEN,allow model parameters to be specified in ensemble config,enhancement,2023-06-23 19:50:14 +0000 UTC
5978,CLOSED,[Question] What is the recommended way to run Triton ?,question,2023-06-23 01:50:28 +0000 UTC
5977,CLOSED,how can i use triton_python_backend_utils?,question,2023-06-23 19:30:50 +0000 UTC
5975,CLOSED,Incompatible TensorRT versions between nvcr.io/nvidia/tritonserver:23.05-py3 and nvcr.io/nvidia/tensorrt:23.05-py3 images,,2023-07-07 22:34:20 +0000 UTC
5973,OPEN,Raise exception when falling back to pinned memory,enhancement,2023-06-21 18:47:52 +0000 UTC
5972,OPEN,Error when using models with zero-length inputs,bug,2023-06-22 01:29:58 +0000 UTC
5971,OPEN,CORS Issue,enhancement,2023-06-23 00:12:12 +0000 UTC
5970,CLOSED,[Question] How does Triton run ensemble models ?,,2023-06-22 14:07:24 +0000 UTC
5969,CLOSED,multi triton_python_backend cause image preprocess (torchvision) very slow.,question,2023-07-07 22:34:36 +0000 UTC
5968,OPEN,LoRA support,enhancement,2023-07-10 23:45:47 +0000 UTC
5964,OPEN,Could not load model using mlflow triton plugin with S3/minio as model repository,bug,2023-06-23 10:18:02 +0000 UTC
5961,OPEN,Allow introspection and static analysis of `pb_utils` (Python backend),enhancement,2023-06-20 19:38:38 +0000 UTC
5960,CLOSED,Error with float16 datatype,question,2023-07-07 22:35:34 +0000 UTC
5959,OPEN,BF16 support for integrated TensorRT precision mode,enhancement,2023-06-20 19:31:18 +0000 UTC
5958,CLOSED,Am I the only one who can't pull nvidia image from `nvcr.io/nvidia` ?,question,2023-06-21 18:17:31 +0000 UTC
5957,OPEN,The performance of the model is greatly affected by the request distribution,,2023-07-11 02:37:40 +0000 UTC
5956,CLOSED,Invalid argument - input 'INPUT' already exists in request,,2023-07-07 22:37:23 +0000 UTC
5953,OPEN,Performance analyzer with real data byte size mismatch,question,2023-06-23 00:39:26 +0000 UTC
5952,CLOSED,python backend error: c_python_backend_utils.TritonModelException: Tensor is stored in GPU and cannot be converted to NumPy,,2023-06-16 23:34:58 +0000 UTC
5951,OPEN,Decoupled Models Hang Unexpectedly with No Clear Error Message,,2023-06-22 21:41:38 +0000 UTC
5949,CLOSED,boost::interprocess::lock_exception,investigating,2023-07-06 16:58:43 +0000 UTC
5948,CLOSED,socket timed out when an inference request is made using a client.py script,,2023-07-07 22:37:59 +0000 UTC
5947,CLOSED,need InferenceRequest and InferenceResponse definitions to better test user codes,question,2023-07-07 22:37:50 +0000 UTC
5944,OPEN,InferenceRequestNew cost time will slowly increase with the number of model reloads,,2023-06-28 12:52:07 +0000 UTC
5943,CLOSED,Poll failed for model directory 'ensemble': output 'OUTPUT_0' for ensemble 'ensemble' is not written,,2023-07-06 15:35:35 +0000 UTC
5942,OPEN,Why does a process still occupy the CPU after the server is started?,,2023-06-14 01:46:05 +0000 UTC
5940,CLOSED,Questions about loading models,question,2023-07-07 22:38:10 +0000 UTC
5938,CLOSED,TensorRT model inference speed varies dramatically depending on if an ONNX model is also loaded,,2023-06-13 20:06:27 +0000 UTC
5934,OPEN,The Docker build process has failed.,,2023-06-13 00:54:27 +0000 UTC
5933,CLOSED,version `GLIBCXX_3.4.30' not found (required by /opt/tritonserver/backends/python/triton_python_backend_stub,,2023-06-28 21:44:45 +0000 UTC
5932,OPEN,How to request the deployed server through postman？,question,2023-06-13 02:04:28 +0000 UTC
5931,OPEN,ERROR: The NVIDIA Driver is present, but CUDA failed to initialize.,,2023-07-07 10:18:27 +0000 UTC
5929,CLOSED,How the aio stream client gets the response,,2023-06-29 09:39:54 +0000 UTC
5927,OPEN,Metrics: number of requests in queue,enhancement,2023-06-13 04:52:38 +0000 UTC
5926,CLOSED,[Python Backend] Dynamic batching does not work in decoupled model,,2023-06-12 19:20:32 +0000 UTC
5920,CLOSED,Triton server returns Request Timeout error after a few hours of continuous inference,,2023-06-20 05:07:00 +0000 UTC
5918,CLOSED,gRPC version mismatch between the Triton client and DeepStream,,2023-06-11 11:36:27 +0000 UTC
5913,CLOSED,Yielding Tokens During LM Inference,,2023-07-01 11:41:46 +0000 UTC
5912,CLOSED,Triton will initialize logging twice when starting,,2023-06-27 23:33:52 +0000 UTC
5907,OPEN,onnxruntime_backend doesn't support loading models concurrently,,2023-06-07 16:39:18 +0000 UTC
5906,CLOSED,Provide server configuration through file,,2023-06-06 10:57:43 +0000 UTC
5905,OPEN,the c++ API of backends get output from tensrort and onnx is different,,2023-06-21 22:05:06 +0000 UTC
5904,CLOSED,the c++ API of backends get output from tensrort and onnx,,2023-06-06 19:27:17 +0000 UTC
5903,CLOSED,How to view server logs,question,2023-06-07 07:51:36 +0000 UTC
5902,OPEN,how to serve thousands of models in python backend?,,2023-06-26 14:17:12 +0000 UTC
5901,CLOSED,Python backend custom metrics: Unsupported TRITONSERVER_MetricKind,,2023-06-12 22:59:07 +0000 UTC
5899,CLOSED,perf_analyzer removes arguments with empty contents,,2023-07-03 15:26:03 +0000 UTC
5895,CLOSED,Python version used in backend updated to 3.10?,,2023-06-02 20:13:27 +0000 UTC
5893,CLOSED,about triton client c++ api send float,,2023-07-07 22:53:53 +0000 UTC
5891,CLOSED,Triton server produces inconsistent results,question,2023-06-07 05:08:22 +0000 UTC
5890,CLOSED,Ensemble model versioning,question,2023-06-05 18:27:53 +0000 UTC
5889,CLOSED,How to serve Python models on GPU,question,2023-06-23 20:06:35 +0000 UTC
5887,CLOSED,Issues regarding the performance of cudashm.set_shared_memory_region(shm_input_handle, [img]),,2023-07-07 22:36:52 +0000 UTC
5886,CLOSED,How to interpret perf_analyzer results,question,2023-07-07 22:58:28 +0000 UTC
5879,CLOSED,[Question] Can not infer model because of batch in client request smaller than batch config in server,question,2023-06-02 02:02:51 +0000 UTC
5874,CLOSED,how to config BLS model to instantiate multi BLS model,question,2023-07-07 22:59:14 +0000 UTC
5872,CLOSED,How to convert pytorch model to onnx for use ragged batching?,,2023-05-30 12:44:38 +0000 UTC
5870,CLOSED,multi-gpu error,,2023-05-30 22:14:19 +0000 UTC
5869,CLOSED,Shared memory error occured when using Triton Inference Server in SageMaker Batch Transform.,question,2023-06-14 19:48:21 +0000 UTC
5868,CLOSED,No space left on device on when deployed using python backend,,2023-05-30 05:29:58 +0000 UTC
5867,CLOSED,Triton client InferenceServerException: Failed to process request due to TypeError,,2023-05-30 13:33:47 +0000 UTC
5866,CLOSED,Model unable to support batching on triton inference server,question,2023-05-31 18:56:50 +0000 UTC
5865,CLOSED,Unknown performance bottleneck when using ensemble model,,2023-06-23 19:57:04 +0000 UTC
5864,CLOSED,Incorrect outputs in TRT model,question,2023-06-14 19:48:45 +0000 UTC
5863,CLOSED,convert tensorrt sucessfully same version with triton tensorrt, but can not load,,2023-05-31 09:39:10 +0000 UTC
5862,CLOSED,custom repo agent for model repo on s3,,2023-05-30 12:47:45 +0000 UTC
5859,CLOSED,Why get huge matrix, cannot get detection results, when using C API to implement " densenet_onnx" model inference like simple.cc,,bug,2023-05-30 02:01:30 +0000 UTC
5858,OPEN,tritonserver stopped at the following step, it seems model loaded fail,question,2023-07-10 16:14:04 +0000 UTC
5853,OPEN,How to pass traceparent in the OpenTelemetry tracing feature,enhancement,2023-06-26 21:47:08 +0000 UTC
5850,CLOSED,Model on DEVICE_CPU become 4x slower when it is part of an ensemble,,2023-05-24 15:59:45 +0000 UTC
5848,CLOSED,how to use perf_analyzer without cuda ?,question,2023-06-12 21:56:10 +0000 UTC
5847,OPEN,Triton issue,,2023-05-24 09:48:09 +0000 UTC
5846,CLOSED,Error "unable to create stream: the provided PTX was compiled with an unsupported toolchain" once installed other version of CUDA,,2023-06-12 21:55:59 +0000 UTC
5844,CLOSED,Onnxruntime backend `gpu_mem_limit` is not respected,,2023-05-24 14:18:28 +0000 UTC
5843,CLOSED,Run onnixruntime backend occured fault ！,,2023-07-07 23:44:36 +0000 UTC
5841,OPEN,GPU memory leak when loading/unloading models,bug, investigating,2023-06-20 17:15:32 +0000 UTC
5840,OPEN,Triton crash when query is > Aproximatively 4 MB,bug, investigating,2023-05-25 17:45:09 +0000 UTC
5839,CLOSED,Triton Inference Server installation failure,question,2023-06-14 19:49:18 +0000 UTC
5838,CLOSED,[Question] Inference with torchscript model,question,2023-06-12 21:57:19 +0000 UTC
5837,OPEN,[Question] How does triton inference server limit endpoint access for HTTP protocol ?,enhancement,2023-07-07 23:43:38 +0000 UTC
5835,CLOSED,[Question] How does triton server use s3 as model-respoitory?,question,2023-05-23 15:39:37 +0000 UTC
5834,CLOSED,Model Configuration is wrongly restrictive (2),question,2023-05-24 08:53:11 +0000 UTC
5833,CLOSED,How to terminate a grpc streaming request immediately during tritonserver inference with a FasterTransformer backend?,question,2023-06-01 02:48:26 +0000 UTC
5830,CLOSED,[python-backend] How to load script model with _extra files in model.pt?,,2023-07-06 15:35:12 +0000 UTC
5829,OPEN,The speed of pre- and post- processing based on python backend is very slow!,,2023-07-07 23:24:45 +0000 UTC
5828,CLOSED,Triton server crashes with a signal 11 when attempting to call nonexistent model inference,bug, investigating,2023-05-23 17:12:11 +0000 UTC
5827,CLOSED,Cant build server with debug symbols,question,2023-06-12 21:57:49 +0000 UTC
5823,CLOSED,Can I use triton with deepspeed inference?,question,2023-05-24 15:29:24 +0000 UTC
5822,CLOSED,model_configuration -> model_transaction_policy -> decoupled mode -> requests[0].get_response_sender(),,2023-06-12 21:58:04 +0000 UTC
5821,OPEN,error: private field 'event_' is not used [-Werror,-Wunused-private-field],question,2023-06-08 04:25:50 +0000 UTC
5820,CLOSED,python_backend dlpack: Bool type is not supported,question,2023-05-29 16:07:50 +0000 UTC
5819,OPEN,python_backend tries to chmod the triton_python_backend_stub even after running `chmod 777 triton_python_backend_stub`,,2023-06-26 20:30:54 +0000 UTC
5818,CLOSED,Triton server does not load models onto all MiG instances,question,2023-05-22 23:00:53 +0000 UTC
5817,OPEN,different performance between 23.03 and 23.04,investigating,2023-07-07 21:58:16 +0000 UTC
5816,CLOSED,DLPack tensor is not contiguous. Only contiguous DLPack tensors that are stored in C-Order are supported,,2023-07-06 15:34:29 +0000 UTC
5815,CLOSED,Calling TRT backend from BLS and bypassing TritonPythonModel.execute() call - Question,question,2023-06-12 21:59:07 +0000 UTC
5814,OPEN,int8_mode=2 loading model fails,,2023-05-18 08:39:55 +0000 UTC
5813,OPEN,Install Python Backend via pip locally,enhancement,2023-06-26 14:12:51 +0000 UTC
5812,CLOSED,triton server (23.04) doesn't support specific version of tensorrt,question,2023-06-12 21:58:24 +0000 UTC
5811,OPEN,GRPC prediction calls with BYTES input errors out in Big Endian Machines,investigating,2023-05-26 06:13:04 +0000 UTC
5809,CLOSED,Tritonserver:23.04 crashes, but it wasn't with 23.03.,question,2023-05-22 19:54:10 +0000 UTC
5804,CLOSED,ubuntu 20.04 GDB 10.2 debug child process occurred error；,,2023-06-12 22:01:01 +0000 UTC
5803,CLOSED,Pytorch Backend Undefined Symbol [commit 4a8a870],,2023-05-17 09:24:36 +0000 UTC
5802,OPEN,dynamic batching log created batch size,,2023-05-17 13:45:00 +0000 UTC
5798,CLOSED,Failed to register CUDA shared mem: failed to open CUDA IPC handle: invalid resource handle,,2023-05-22 08:19:13 +0000 UTC
5794,CLOSED,Cannot enable OpenTelemetry trace option; "unrecognized option",,2023-05-16 00:33:38 +0000 UTC
5793,CLOSED,tritonserver start error: basic_string::_S_construct null not valid,,2023-05-26 19:12:17 +0000 UTC
5792,CLOSED,Using Triton Python backend to serve as an RTSP camera grabber (Question).,question,2023-06-12 21:59:15 +0000 UTC
5791,CLOSED,Loading ensemble reloads already loaded models,,2023-05-30 18:23:02 +0000 UTC
5790,CLOSED,Unable to load models from "Private" s3 location,,2023-06-08 21:59:05 +0000 UTC
5789,OPEN,tensorrt backend coredump at using cudaGraph with output_copy_stream True,,2023-05-18 00:45:42 +0000 UTC
5788,OPEN,Can Triton achieve a return in typewriter form?,,2023-05-15 08:46:53 +0000 UTC
5787,CLOSED,Custom auto scaling,question,2023-05-19 16:01:06 +0000 UTC
5786,CLOSED,Triton Inference Server(23.03 container) fails to load all models.,,2023-07-07 22:52:43 +0000 UTC
5785,CLOSED,Reiterating the suggestion for an enhanced extension of CUDA tensors to cudashm.,,2023-06-01 07:46:09 +0000 UTC
5784,CLOSED,How does the function ModelInstanceState::ProcessRequests do inference for each request?,,2023-07-07 23:44:23 +0000 UTC
5783,OPEN,Inaccurate request handling when configuring queue policy,,2023-05-13 15:01:10 +0000 UTC
5781,CLOSED,what should I do if i want to write a plugin for pulling model repository from other file system,,2023-06-21 21:41:15 +0000 UTC
5779,OPEN,python backend: cuDNN error: CUDNN_STATUS_MAPPING_ERROR and following CUDA error: an illegal memory access was encountered,bug, investigating,2023-07-03 20:06:25 +0000 UTC
5777,OPEN,Custom Backend Tracing,enhancement,2023-05-12 19:47:21 +0000 UTC
5774,OPEN,How can I install TF2 in the triton python_backend ?,,2023-05-12 18:21:06 +0000 UTC
5773,OPEN,Order of tensors in Response does not correspond to config.pbtxt,bug,2023-05-12 21:54:50 +0000 UTC
5772,OPEN,Sanity check before updating Sagemaker endpoint,,2023-05-12 21:16:15 +0000 UTC
5768,CLOSED,Founded batching opportunity,question,2023-05-23 15:13:24 +0000 UTC
5765,OPEN,Server should exit on unrecoverable errors in underlying runtime that cannot be resolved by model reloading,bug,2023-06-27 05:14:11 +0000 UTC
5759,CLOSED,Reshape property not functional for python models.,,2023-05-22 18:39:04 +0000 UTC
5758,CLOSED,some errors build from source,,2023-05-22 18:40:02 +0000 UTC
5757,OPEN,downtime on the server when reloading a model in explicit mode (when using aws s3 repository),bug,2023-05-12 00:20:24 +0000 UTC
5754,CLOSED,is there any custom C++ post-process backend and pre-process backend examples？,question,2023-05-12 06:30:02 +0000 UTC
5752,CLOSED,Dynamic Input Shape Handling in triton,,2023-06-07 18:24:45 +0000 UTC
5751,CLOSED,How about "Support for Reporting Metric Data with Timestamp Labels in Prometheus Metric Reporting Functionality"?,enhancement,2023-05-22 18:40:24 +0000 UTC
5749,CLOSED,An error occurred while making inference requests for tritonserver,,2023-06-20 07:21:18 +0000 UTC
5748,CLOSED,Does triton support pulling model repo from OSS,,2023-05-10 08:54:03 +0000 UTC
5747,CLOSED,1,,2023-05-05 11:23:56 +0000 UTC
5746,CLOSED,Perf_analyzer input JSON data type,,2023-05-22 18:40:39 +0000 UTC
5745,OPEN,Occasionally meet Request for unknown model: 'xxx' has no available versions for existing models,bug,2023-07-07 23:59:22 +0000 UTC
5744,CLOSED,Help on serving OpenPose to Triton backend,,2023-05-10 07:23:16 +0000 UTC
5743,CLOSED,Question about "max_sequence_idle_microseconds" config value,question,2023-05-22 18:40:31 +0000 UTC
5742,CLOSED,Issue creating ensembling config, error msg not helpful,,2023-06-02 22:18:53 +0000 UTC
5741,CLOSED,Where can I find the information about the values for various platforms such as onnx and tensorflow,,2023-05-08 23:44:15 +0000 UTC
5740,CLOSED,Is it possible to deploy a stable-diffusion-like unet model with a loop using ONNX or TRT?,,2023-05-22 18:41:22 +0000 UTC
5736,CLOSED,ONNX instances on same device are not running concurrently,,2023-05-26 06:48:37 +0000 UTC
5735,CLOSED,input and torch.tensor or tensor?,enhancement,2023-05-08 15:41:21 +0000 UTC
5733,CLOSED,Concurrent inferences get mixed up when preprocessing python model runs torch operations on GPU,,2023-05-20 09:35:36 +0000 UTC
5732,OPEN,Update the documentation about the triton throughput while using MIG,,2023-05-03 13:19:50 +0000 UTC
5730,CLOSED,Release notes saying ONNXRuntime for 23.04 is 1.13.1,investigating,2023-05-05 00:01:22 +0000 UTC
5726,CLOSED,How to load list of strings into input_tensors,,2023-05-03 17:50:47 +0000 UTC
5725,CLOSED,Not able to fully utilize A16 GPUs with Triton.,,2023-06-23 20:07:54 +0000 UTC
5722,CLOSED,Better log message for the case when error happens inside of TRITONBACKEND_ModelBatcherInitialize(),bug, investigating,2023-05-04 16:32:27 +0000 UTC
5721,CLOSED,Dynamic batching does not work on server side,question,2023-06-05 15:56:21 +0000 UTC
5717,CLOSED,Issue with 23.04-tf2-python-py3 docker image on arm64,investigating,2023-05-19 22:26:17 +0000 UTC
5715,OPEN,_cshm_get_shared_memory_handle_info allocates memory on different GPU,investigating,2023-05-03 14:49:45 +0000 UTC
5712,CLOSED,Tesla K80 GPU is not supported by triton-inference-server container 21.10,,2023-04-29 00:42:50 +0000 UTC
5711,CLOSED,How to upgrade the model configuration file to poll mode for tritonserver,question,2023-05-06 05:33:55 +0000 UTC
5709,OPEN,Is concurrent model instance loading supported?,enhancement,2023-05-01 17:36:41 +0000 UTC
5703,CLOSED,Accessing model_config of ensemble config.pbtxt,question,2023-05-01 17:55:06 +0000 UTC
5702,CLOSED,Triton server crashed when requesting deployed Python and ONNX model services,bug,2023-05-24 16:20:29 +0000 UTC
5701,CLOSED,Intermittent Error with Python BLS Backend Model,,2023-05-12 21:20:57 +0000 UTC
5698,CLOSED,Deploy yolov5 Model with Triton Inference Server issue and error : expecting model output to be a vector,question,2023-05-12 21:22:24 +0000 UTC
5694,CLOSED,Support instance group of type 'MODEL' in pytorch backend,enhancement,2023-06-13 05:42:55 +0000 UTC
5693,CLOSED,protobuf version used,question,2023-05-12 21:23:42 +0000 UTC
5690,CLOSED,Output response Parsing Error,question,2023-05-12 21:24:40 +0000 UTC
5689,OPEN,Docs for Multi model serving with over-commit,enhancement,2023-04-24 18:14:49 +0000 UTC
5688,CLOSED,Docs for Parameters Support by Triton Backends,investigating,2023-07-08 00:03:23 +0000 UTC
5687,CLOSED,Question: scheduling on multiple GPUs when using cuda shared memory,question,2023-04-27 18:43:57 +0000 UTC
5685,OPEN,Inference exception in torchscript backend,bug,2023-04-28 02:04:08 +0000 UTC
5683,CLOSED,With ensemble mode submodel can run in different gpus, like pipeline parallelism,question,2023-04-24 11:05:54 +0000 UTC
5682,OPEN,How to implement a CPU-only Rust backend,enhancement,2023-04-21 19:58:14 +0000 UTC
5681,CLOSED,Alternatives to increasing shared memory,question,2023-04-22 05:04:18 +0000 UTC
5680,CLOSED,Possible issue with cuda shared memories during batch inference,,2023-04-25 07:05:07 +0000 UTC
5679,CLOSED,how can i get a C++backends,question,2023-05-12 21:26:10 +0000 UTC
5678,CLOSED,Vulnerabilities are reported while using the Triton for inference.,,2023-05-22 18:41:48 +0000 UTC
5677,OPEN,Segmentation fault while triton python BLS model execute infer request,,2023-05-18 19:36:06 +0000 UTC
5676,CLOSED,cmake failed！,,2023-05-12 21:28:10 +0000 UTC
5675,CLOSED,Higher inference times than without using Triton inference server,,2023-05-12 21:29:35 +0000 UTC
5668,OPEN,How long does it take to compile triton? Every time you execute cmake, you will download the dependent library. Can you set a global cache to speed up the build? Is there a more time-saving and labor-saving way?,enhancement,2023-04-21 03:23:01 +0000 UTC
5666,CLOSED,Install mlflow-triton plugin in mlflow container (ghcr.io/mlflow/mlflow:latest) results error in processing aiohttp,,2023-04-21 22:28:35 +0000 UTC
5665,CLOSED,Error reported while running BLS instance,,2023-06-08 23:32:48 +0000 UTC
5664,CLOSED,vector length overflow,bug, investigating,2023-05-16 14:52:12 +0000 UTC
5661,CLOSED,Unable to run Onnx model converted databricks/dolly-v1-6b (GPT-J),question,2023-05-12 21:31:31 +0000 UTC
5654,OPEN,ONNX TensorRT FP16 Inference results incorrect,question,2023-04-30 10:26:48 +0000 UTC
5649,CLOSED,Error: InferenceServerException: inference header size should be in range (0, -1012208453), got: 187,bug,2023-07-08 00:02:25 +0000 UTC
5648,CLOSED,Best practice for an input pair <I_1, I,2>^0, ..., <I_1, I,2>^N,question,2023-05-12 21:32:20 +0000 UTC
5647,CLOSED,How does windows11 compile triton locally?,,2023-05-12 21:33:21 +0000 UTC
5645,CLOSED,CMake Error at /usr/share/cmake-3.25/Modules/ExternalProject.cmake:3115 (message): No download info given for 'triton-server' and its source directory:,,2023-05-12 21:34:29 +0000 UTC
5644,CLOSED,Unused private field if TRITON_ENABLE_GPU is false,bug,2023-05-17 07:46:50 +0000 UTC
5643,OPEN,Compile error with ubuntu 22.04 because of -Werror,enhancement,2023-04-20 22:57:49 +0000 UTC
5641,CLOSED,docker can‘t run --gpus in orin,,2023-05-12 21:35:08 +0000 UTC
5640,CLOSED,Inference results differ for diffrent batch size,question,2023-04-18 07:43:46 +0000 UTC
5639,OPEN,Parse error for models that might return empty output,,2023-04-17 10:51:00 +0000 UTC
5638,CLOSED,Any community group in slack or telegram for Triton?,question,2023-05-22 18:44:19 +0000 UTC
5636,CLOSED,It seems that the model was loaded successfully, but why was it unloaded immediately?,,2023-04-13 06:17:42 +0000 UTC
5633,CLOSED,capping resources assigned to each model in multi model serving,question,2023-05-22 18:43:08 +0000 UTC
5627,OPEN,Does triton-inference-server only support slurm for multi-node deployment?,question,2023-04-13 21:48:10 +0000 UTC
5622,OPEN,triton inference client pinned to geventhttpclient==2.0.2, cabundle doesn't support letsencrypt,,2023-04-12 18:00:12 +0000 UTC
5617,CLOSED,HTTPS support,question,2023-06-28 21:34:04 +0000 UTC
5614,CLOSED,Integrate triton with vector database in Python backend,question,2023-05-22 18:43:47 +0000 UTC
5610,OPEN,TYPE_STRING datatype is throwing error while using tritonserver/Python backend/HTTP endpoint for Linux-s390x,question,2023-04-12 05:30:28 +0000 UTC
5609,CLOSED,Implicit State Management in Pytorch,,2023-05-22 18:42:55 +0000 UTC
5608,CLOSED,How to use golang's ModelStreamInfer interface,question,2023-04-20 03:38:02 +0000 UTC
5606,CLOSED,Any examples for writing the input.json for perf_analyzer on fastertransformer models,,2023-05-22 18:41:02 +0000 UTC
5605,CLOSED,Memory illegal access when using perf_analyzer,,2023-04-07 05:34:38 +0000 UTC
5603,CLOSED,Question about how dynamic batching works,question,2023-04-11 06:59:22 +0000 UTC
5602,CLOSED,Trouble starting Triton with 3 ONNX models loaded,,2023-04-13 01:17:22 +0000 UTC
5598,OPEN,r23.03 onnxruntime backend consumes more GPU memory. might need cudnn_conv_use_max_workspace exposed,investigating,2023-04-25 09:44:30 +0000 UTC
5594,OPEN,Build from source triggers errors "../libtritonserver.so: undefined reference to `absl::lts_20211102::variant_internal::ThrowBadVariantAccess()'",,2023-04-12 20:15:27 +0000 UTC
5593,OPEN,Slow Inference using Triton Java Bindings,bug, investigating,2023-07-06 15:33:52 +0000 UTC
5590,CLOSED,Question about the dynamic batcher and multi-instance model,question,2023-04-25 18:32:20 +0000 UTC
5587,CLOSED,throughput of perf analyzer,question,2023-04-05 02:22:09 +0000 UTC
5586,CLOSED,Batch between model in ensemble model,question,2023-07-08 00:01:12 +0000 UTC
5585,CLOSED,Cannot build QA environment,,2023-04-14 07:17:47 +0000 UTC
5584,OPEN,Triton Server Client libraries linker errors with the Response classes,question,2023-04-07 12:59:50 +0000 UTC
5583,CLOSED,Serve tf-trt converted model return error: NodeDef mentions attr 'max_batch_size' not in Op: name=TRTEngineOp,question,2023-04-04 07:56:16 +0000 UTC
5579,OPEN,Questions about model instances and dynamic batch when setting model concurrency,question,2023-04-06 14:15:36 +0000 UTC
5578,OPEN,GPU tensor support for python backend on Jetson,enhancement,2023-06-23 20:11:10 +0000 UTC
5577,CLOSED,Triton multi node - multi GPU inference,question,2023-04-10 20:26:36 +0000 UTC
5576,OPEN,Error deploying TensorRT engine on Triton, possible nonzero op issue with data-dependent output shape.,bug, investigating,2023-04-20 23:17:17 +0000 UTC
5574,CLOSED,When will ubuntu22.04 be supported?,question,2023-03-30 17:56:12 +0000 UTC
5573,CLOSED,Error when run multiple triton server process in one machine,question,2023-06-28 21:37:41 +0000 UTC
5572,CLOSED,AutoCompleteConfig() help,question,2023-04-10 20:26:51 +0000 UTC
5567,CLOSED,google cloud model repository not working,,2023-03-29 14:17:09 +0000 UTC
5566,CLOSED,more example for ssl for grpc,question,2023-04-11 14:35:14 +0000 UTC
5565,CLOSED,Can I add model instances without restarting the triton server?,question,2023-04-04 01:28:14 +0000 UTC
5564,OPEN,Route request to model instance running on specified GPU device id,enhancement,2023-06-23 19:55:43 +0000 UTC
5563,CLOSED,Can connect Triton directly to Grafana?,question,2023-04-10 20:27:05 +0000 UTC
5561,CLOSED,custom backend [undefined symbol],,2023-03-29 02:55:54 +0000 UTC
5559,CLOSED,the c++ custom backend works with infer but crashes with async_stream_infer,question,2023-03-28 16:23:38 +0000 UTC
5558,OPEN,Authorisation for endpoints,enhancement,2023-03-31 20:07:43 +0000 UTC
5555,CLOSED,HTTP 400: Bad Request on all HTTP endpoints,,2023-03-28 04:58:10 +0000 UTC
5552,CLOSED,Serving python libraries using Triton Inference Server,,2023-03-27 15:36:51 +0000 UTC
5551,OPEN,Precision Setting for Performance Analyzer Output Validation,,2023-07-08 00:06:37 +0000 UTC
5548,CLOSED,Logging doesn't output useful information when exception happens,,2023-04-10 20:28:20 +0000 UTC
5547,OPEN,Error Unrecognized attribute: mask_filter_value for operator Attention,,2023-03-30 07:51:46 +0000 UTC
5546,OPEN,Linker problems with Error class in Client libraries,,2023-03-24 00:36:12 +0000 UTC
5545,OPEN,Triton server not combined requests to batch in python_backend,,2023-03-24 01:47:45 +0000 UTC
5543,CLOSED,error using grpc,,2023-04-17 14:25:25 +0000 UTC
5537,CLOSED,Is there a way to start triton server without using service account json in GCP ?,,2023-04-10 20:28:30 +0000 UTC
5536,CLOSED,How to use java client api transfer images and texts to service backend by grcp?,,2023-04-10 20:28:57 +0000 UTC
5534,OPEN,Multiple GPUs do not scale at the expected rate,,2023-04-05 23:17:19 +0000 UTC
5533,CLOSED,Unable to Load Models from Azure Storage,,2023-06-06 21:51:35 +0000 UTC
5530,CLOSED,Question about ragged batching,question,2023-04-10 20:25:42 +0000 UTC
5527,CLOSED,Error when building grpc gcc11/ubuntu22.04?,,2023-03-21 19:25:02 +0000 UTC
5525,CLOSED,Runing both normal onnx model and stateful model,,2023-04-10 20:29:14 +0000 UTC
5524,CLOSED,TRITONBACKEND_ModelInstanceInitialize: xxx (CPU device 0),,2023-04-10 20:29:24 +0000 UTC
5523,CLOSED,Slower inference times on using triton onnx backend instead of python backend,,2023-04-10 20:25:06 +0000 UTC
5520,CLOSED,Triton not working with Inferentia in AWS,bug,2023-04-27 19:55:56 +0000 UTC
5518,CLOSED,Citation of Triton inference server,,2023-03-23 22:45:25 +0000 UTC
5517,CLOSED,can the custom backend as flexible as the python backend?,question,2023-03-18 02:53:12 +0000 UTC
5516,CLOSED,Triton support for Red Hat Enterprise Linux,question,2023-03-21 08:08:52 +0000 UTC
5515,CLOSED,Connection refused,,2023-03-27 18:37:16 +0000 UTC
5513,OPEN,High GPU consumption when deploying into Kubernetes,,2023-03-20 17:54:32 +0000 UTC
5512,CLOSED,How to get headers of request?,,2023-03-16 15:10:19 +0000 UTC
5508,CLOSED,c++ grpc client send raw image to triton ensemble, can not use PIL.Image open,,2023-03-16 07:49:35 +0000 UTC
5507,CLOSED,When triton loaded the python backend model preload libtensorflow_framework.so, a segment error,,2023-03-20 15:54:58 +0000 UTC
5506,CLOSED,Some error about building custom Pytorch backend,,2023-03-27 18:37:48 +0000 UTC
5503,CLOSED,Request ID not in Response when accessed by In-Process Triton Server API,bug, investigating,2023-03-21 01:57:58 +0000 UTC
5501,OPEN,Minio model repository stuck on downloading files with <=2.31.0,investigating,2023-04-25 06:47:21 +0000 UTC
5496,CLOSED,How to access inputs in TRITONSERVER_InferenceRequest,,2023-03-13 23:07:24 +0000 UTC
5495,OPEN,README.md for client repo doesn't give complete instructions for Windows build,,2023-03-16 15:31:25 +0000 UTC
5494,CLOSED,[Question / Bug?] DLPack tensor is not contiguous, even though I use tensor.contiguous in torch,question,2023-04-03 08:30:55 +0000 UTC
5493,CLOSED,400 Bad request - http client,,2023-03-22 08:30:42 +0000 UTC
5491,CLOSED,from /tmp/tritonbuild/tritonserver/build/_deps/repo-third-party-src/libevhtp/libevhtp/triton_timestamp.cc:29: /usr/include/c++/4.8.2/bits/c++0x_warning.h:32:2: error: #error This file requires compiler and library support for the ISO C++ 2011 standard. This support is currently experimental, and must be enabled with the -std=c++11 or -std=gnu++11 compiler options. #error This file requires compiler and library support for the \,question,2023-03-27 18:42:47 +0000 UTC
5489,CLOSED,Onnx model with initializers as extra input,enhancement,2023-04-10 20:16:06 +0000 UTC
5487,OPEN,Use boost::interprocess::shared_memory_object i.s.o POSIX shared memory functions,enhancement,2023-03-10 20:53:35 +0000 UTC
5485,CLOSED,I build a centos image by compiling the source code on mac m1 aarch 64, but the cmake compilation fails,,2023-03-10 21:00:36 +0000 UTC
5484,CLOSED,Server is returning BYTES when output datatype is FP32,,2023-03-10 12:42:56 +0000 UTC
5483,CLOSED,Failed to open the cudaIpcHandle when I call an ONNX / TRT backend from Python backend,bug,2023-04-20 19:58:38 +0000 UTC
5479,CLOSED,TensorRT: batching is unavailable,,2023-03-11 00:37:32 +0000 UTC
5478,CLOSED,How to build a tritonserver image based on centos7, which is now based on ubuntu, and the current build.py does not support it;,,2023-03-11 14:49:19 +0000 UTC
5477,CLOSED,The difference between triton bls and python banckend,,2023-03-10 21:36:04 +0000 UTC
5476,CLOSED,> Hello,,,2023-03-09 09:49:26 +0000 UTC
5475,CLOSED,The difference between triton bls and python banckend,,2023-03-09 09:49:05 +0000 UTC
5474,CLOSED,Invalid argument: ensemble 'face_pose' depends on 'face_pose_trt' whose required version 0 is not loaded,question,2023-03-27 18:43:02 +0000 UTC
5473,CLOSED,Questions about the "--disable-auto-complete-config" flag,,2023-03-10 21:40:21 +0000 UTC
5472,CLOSED,Confusion about passing/converting NumPy/PyTorch tensors from and to pb_utils.Tensor for inference request,question,2023-03-24 15:59:42 +0000 UTC
5471,CLOSED,Inference result of single batch ONNX model contains all zeros and also emits "Failed to open the cudaIpcHandle." error in additional inference calls,bug,2023-04-25 15:24:58 +0000 UTC
5467,OPEN,Configurable rate-limiting / queue policy for sequence batcher,enhancement,2023-03-10 23:03:30 +0000 UTC
5466,CLOSED,How to deploy a model whose size exceeds the memory of a single gpu,,2023-03-09 02:11:45 +0000 UTC
5465,CLOSED,ensemble model transfer a context,,2023-03-15 03:38:54 +0000 UTC
5464,CLOSED,Is dynamic batch use in ensemble model ? can i create a data pineline in triton API?,,2023-03-09 02:19:36 +0000 UTC
5461,OPEN,23.02-pyt-python-py3 ModuleNotFoundError: No module named 'torch',enhancement,2023-03-31 21:05:30 +0000 UTC
5460,CLOSED,unable to load shared library: `/lib/x86_64-linux-gnu/libstdc++.so.6`,question,2023-03-27 18:37:26 +0000 UTC
5459,CLOSED,triton inference latency increases rapidly if the interval of inference request is more than 1 second.,,2023-03-10 05:28:53 +0000 UTC
5454,CLOSED,Feature Request: More flexible model interactions,,2023-03-07 23:28:59 +0000 UTC
5453,OPEN,Feature Request: Server side callbacks to evict state,enhancement,2023-03-22 09:29:06 +0000 UTC
5452,CLOSED,Python BLS script fails to load additional models on sagemaker,enhancement,2023-03-10 21:35:09 +0000 UTC
5451,CLOSED,Build for Windows 10 fails with hcsshim::PrepareLayer Win32: Incorrect function,question,2023-03-27 18:42:55 +0000 UTC
5450,CLOSED,Segmentation fault on perf_client with STRING input and custom input_data,,2023-03-07 10:56:26 +0000 UTC
5448,CLOSED,Error when parsing ONNX model from file in tao-toolkit-triton-apps docker container.,,2023-03-08 14:59:32 +0000 UTC
5447,CLOSED,Lack of nvcr.io/nvidia/tritonserver:23.02-py3,,2023-03-27 18:43:39 +0000 UTC
5446,CLOSED,Multiple implicit state,,2023-03-02 10:17:11 +0000 UTC
5445,OPEN,Best Practices for a secured Triton installation,,2023-03-02 02:29:31 +0000 UTC
5440,OPEN,Better Error reporting when using device or host memory (C-API),enhancement,2023-03-01 19:26:07 +0000 UTC
5439,CLOSED,triton server logging time is not right after change timezone in container,enhancement, question,2023-07-06 18:21:19 +0000 UTC
5438,CLOSED,Why I still perform inference while triton server is not ready?,,2023-03-01 19:01:16 +0000 UTC
5434,CLOSED,Installation guide for OpenShift,,2023-02-28 17:07:59 +0000 UTC
5433,OPEN,Connecting to Triton server takes about 2 minutes,question,2023-03-14 16:07:52 +0000 UTC
5432,CLOSED,Redis Cache Repository does not exist,,2023-02-28 18:33:08 +0000 UTC
5431,OPEN,Document memory exchanges in an Ensemble and in a BLS,enhancement,2023-03-01 19:34:04 +0000 UTC
5422,CLOSED,TRITON_ENABLE_METRICS_CPU not found,,2023-02-27 17:10:10 +0000 UTC
5421,CLOSED,Long queue time when preprocessing using the python backend,question,2023-03-08 02:59:43 +0000 UTC
5420,CLOSED,Classification extension wrong for multiple images inference with Triton server 22.12-py3 or 23.01-py3 with MacBook M1,,2023-03-16 18:20:48 +0000 UTC
5419,CLOSED,The deployed model loses a lot of accuracy!,,2023-03-27 18:44:53 +0000 UTC
5417,CLOSED,Is this to add support for jetson in the model gen scripts?,,2023-02-27 18:09:40 +0000 UTC
5416,CLOSED,Ensemble model with shared memory,,2023-02-25 01:07:23 +0000 UTC
5412,CLOSED,Use perf_client on models with string as input and variable shape,,2023-02-24 18:06:11 +0000 UTC
5410,CLOSED,[Question] Setting the default input and output tensor names in the config.pbtxt,,2023-02-24 18:12:33 +0000 UTC
5408,CLOSED,Unable to Connect to Triton Server at localhost:8000,,2023-02-24 18:00:39 +0000 UTC
5396,CLOSED,FasterTransformer: Start to forward terminate called after throwing an instance of 'std::out_of_range',,2023-02-23 19:12:20 +0000 UTC
5395,CLOSED,unexpected explicit tensor data for input tensor 'attention_mask' for model 'pipeline-poc-inference__isvc-211152d1e7' of type 'INT32', expected datatype 'INT64',,2023-02-23 21:43:32 +0000 UTC
5393,OPEN,add --version info option to tritonserver,,2023-02-25 12:31:57 +0000 UTC
5392,OPEN,Triton Server costs too much memory,,2023-02-24 09:12:21 +0000 UTC
5391,OPEN,Pass a python dict to triton server python backend,enhancement,2023-02-22 20:56:40 +0000 UTC
5390,CLOSED,TritonServer output tensor diff from original onnx runtime inference,,2023-05-12 02:46:05 +0000 UTC
5389,CLOSED,ONNX model inferencing :: "vmodel not found",,2023-02-21 14:22:12 +0000 UTC
5388,CLOSED,Triton failed to load a large (19GB) ONNX model,,2023-03-06 18:56:27 +0000 UTC
5387,CLOSED,Failed to build on Windows when upgrade from r22.02 to r23.01,,2023-03-10 02:15:08 +0000 UTC
5386,CLOSED,Set the input data takes so many time,invalid,2023-02-28 00:35:37 +0000 UTC
5385,CLOSED,Use Encrypted ONNX model in Triton Server,,2023-02-23 00:38:16 +0000 UTC
5384,CLOSED,Response Cache Metrics Usage,,2023-03-22 00:08:30 +0000 UTC
5383,CLOSED,Reduce image size and increase build frequency to cut high/medium vulnerabilities in tritonserver docker images,,2023-02-23 00:37:04 +0000 UTC
5382,OPEN,All deployed model inference outputs are 0 and -1,,2023-02-24 07:38:05 +0000 UTC
5381,CLOSED,[python backend] Triton hangs when a python backend model process is killed by kernel OOM killer,,2023-03-30 23:54:10 +0000 UTC
5379,OPEN,Unify building experience among windows and posix,,2023-02-17 23:44:59 +0000 UTC
5378,CLOSED,Enabling triton client to build with minial set to rid needless dependencies such as zlib and re2,enhancement,2023-05-08 17:08:06 +0000 UTC
5377,OPEN,Enablging triton client to build and run on Mac,,2023-02-17 23:41:15 +0000 UTC
5376,OPEN,Documentation for creating small versions of Triton Docker images is not sufficient,,2023-06-21 19:54:05 +0000 UTC
5374,CLOSED,Need help in understanding triton client "get_inference_statistics" output.,,2023-03-27 18:43:14 +0000 UTC
5373,CLOSED,How to use a fixed TensorRT version in triton server?,question,2023-02-23 00:35:22 +0000 UTC
5372,OPEN,Metrics from Metric port being mixed when both Triton Model Analyzer and Triton Inference Server being started,,2023-02-27 18:56:38 +0000 UTC
5371,CLOSED,Unable to create S3 filesystem client. Check account credentials.,,2023-03-15 09:39:35 +0000 UTC
5370,CLOSED,torchscript can not serving on multi GPUs,investigating,2023-05-08 19:06:34 +0000 UTC
5369,CLOSED,Dumping Logs of Triton Inference Server for tools like Grafana Loki,,2023-03-27 18:41:11 +0000 UTC
5366,OPEN,Custom Header And GRPC MetaData IN TritonServer,question,2023-02-16 05:00:04 +0000 UTC
5365,CLOSED,Can multiple models share the same python-backend stub?,,2023-03-27 18:45:07 +0000 UTC
5364,CLOSED,Customize-triton-container using compose.py failed,,2023-02-20 01:42:22 +0000 UTC
5359,CLOSED,Missing default tag (`latest`) in `nvcr.io/nvidia/tritonserver` container,,2023-03-16 18:23:14 +0000 UTC
5358,CLOSED,Suggestions for k8s_onprem helm chart,,2023-02-23 00:40:48 +0000 UTC
5357,CLOSED,How can I use dynamic batch?,question,2023-02-23 00:36:20 +0000 UTC
5356,CLOSED,How to get the dependencies between backends using BLS,question,2023-02-16 07:02:51 +0000 UTC
5354,OPEN,Triton limitations when deploying gpt-like generative models,,2023-02-16 16:29:26 +0000 UTC
5353,CLOSED,Unable to load models from s3 location,question,2023-04-10 20:28:17 +0000 UTC
5352,CLOSED,slow when I use triton torchscript model to infer,,2023-03-16 18:21:31 +0000 UTC
5351,CLOSED,How to handle "output config" when "empty tensor" is the output of the detection model ?,question,2023-02-14 18:14:49 +0000 UTC
5350,CLOSED,Unable to run my own model!,,2023-02-23 00:41:57 +0000 UTC
5349,OPEN,Question about server/docs/examples/stable_diffusion/,question,2023-05-01 11:02:19 +0000 UTC
5348,CLOSED,Request timeout expired While The Grpc Deadline Not Exceeded,,2023-02-23 06:59:58 +0000 UTC
5346,CLOSED,Unable to connect to the triton service!,,2023-02-13 16:12:15 +0000 UTC
5345,CLOSED,Dynamic model loading/unloading depending on requests?,,2023-02-13 17:50:09 +0000 UTC
5343,CLOSED,OpenVINO Backend 2022.3 support,investigating,2023-07-07 17:15:06 +0000 UTC
5342,CLOSED,Python backend Cannot Concurrent Load Model,question,2023-04-28 00:22:14 +0000 UTC
5341,OPEN,Deploying Triton on Kubernetes results in Crashloopbackoff,question,2023-02-10 22:57:01 +0000 UTC
5338,CLOSED,Need for help! Is that my configuration wrong?,question,2023-02-19 13:22:54 +0000 UTC
5337,CLOSED,pytorch(torchscript) custom classes,,2023-02-10 06:39:46 +0000 UTC
5335,CLOSED,Perf_client reporting different requests per second depending on network,performance,2023-02-09 20:26:50 +0000 UTC
5334,CLOSED,dynamic batch not work,question,2023-02-09 12:25:06 +0000 UTC
5333,OPEN,Allow to pass custom logging format via options,enhancement,2023-02-09 10:54:25 +0000 UTC
5332,CLOSED,Memory segmentation fault in `libtriton_fil.so` for an xgboost model,,2023-02-14 20:51:38 +0000 UTC
5331,OPEN,Timeout was reached,enhancement,2023-02-10 20:35:24 +0000 UTC
5328,CLOSED,Signal 11 received while stress testing,bug,2023-02-27 16:10:04 +0000 UTC
5327,CLOSED,Build without container has protobuf and curl issues,question,2023-06-28 21:40:37 +0000 UTC
5326,CLOSED,Model Warmup Invalid!,performance,2023-02-09 02:08:12 +0000 UTC
5325,CLOSED,/v2/repository/index api is very slowly,investigating,2023-02-28 00:16:05 +0000 UTC
5324,OPEN,Triton cannot retrieve GPU metrics with MIG-enabled GPU devices (A100 and A30),enhancement,2023-02-07 01:54:25 +0000 UTC
5323,OPEN,run triton docker,print () of model.py in python_backend cannot be printed,How to solve it?,,2023-02-08 08:09:08 +0000 UTC
5322,CLOSED,How do i set labels for my Triton Model?,,2023-04-24 17:12:39 +0000 UTC
5321,CLOSED,Custom Build Python Backend Locale Error,investigating,2023-06-26 15:54:14 +0000 UTC
5320,CLOSED,Shape mismatch attempting to re-use buffer. {1,16,48,48} != {4,16,48,48},,2023-02-08 07:47:11 +0000 UTC
5319,CLOSED,Document how expected model inputs tensor gets an extra dimension (of -1 items) and how to deal with it,,2023-02-08 18:14:08 +0000 UTC
5318,CLOSED,Enable cache in TritonServer,,2023-02-21 17:50:59 +0000 UTC
5317,CLOSED,Running preprocessing on TritonPythonModel is bottlenecking,performance,2023-02-10 05:37:15 +0000 UTC
5316,CLOSED,TCP port was full and the triton server could not accept any request.,,2023-02-21 17:50:54 +0000 UTC
5315,CLOSED,Server CPU usage decreases after a new model request arrives,,2023-02-09 10:12:25 +0000 UTC
5313,OPEN,Unable to run triton outside container with python backend,,2023-02-04 19:24:33 +0000 UTC
5309,CLOSED,Onnx runtime build error for 23.01,,2023-02-27 20:20:47 +0000 UTC
5308,CLOSED,Proper input in json file, for python model accepting dictionary with string values as input,,2023-02-15 14:15:03 +0000 UTC
5307,CLOSED,When loading a pretrained model,question,2023-02-21 17:51:47 +0000 UTC
5305,CLOSED,Does `tritonserver` Support IPv6 `--http-address` bind?,,2023-04-10 22:19:44 +0000 UTC
5304,CLOSED,[TRT] | Complete error not propagated to the http server,enhancement,2023-04-19 20:02:33 +0000 UTC
5301,OPEN,[Feature request]Can you provide a golang grpc image infer client?,enhancement,2023-02-13 16:14:30 +0000 UTC
5300,CLOSED,Triton onnxruntime backend serving having different result compared to the native python inference on a resnet18 model.,,2023-02-06 01:48:54 +0000 UTC
5299,CLOSED,[Question] Inference image ensemble model with gRPC request got error,question,2023-02-23 22:21:44 +0000 UTC
5296,CLOSED,Wrong prometheus metrics reports while using horizontally placed nodes in an ensemble model DAG,bug, investigating,2023-03-22 23:37:42 +0000 UTC
5295,CLOSED,Scaling is not happening for slow models,question,2023-02-14 21:21:39 +0000 UTC
5294,OPEN,How to serve n identical models (except for their weights) without using n times the GPU memory ?,enhancement,2023-03-09 17:49:02 +0000 UTC
5293,CLOSED,[Question] can different models served within the same container share the input names ?,,2023-01-31 22:19:37 +0000 UTC
5292,CLOSED,Include Python protobuf files in install folder,,2023-02-02 14:24:24 +0000 UTC
5291,CLOSED,how to build "xx.yy-py3-min" for base image "ubuntu:20.04"?,question,2023-01-31 22:42:48 +0000 UTC
5286,CLOSED,[Question] Is there a way to send raw string as input to triton server?,question,2023-01-31 18:37:59 +0000 UTC
5285,CLOSED,[Feature Request] Add load_model api to triton_python_backend_utils,question,2023-02-02 03:03:10 +0000 UTC
5284,CLOSED,[Question] Inference diffusion model with gRPC request got error,,2023-01-31 07:07:35 +0000 UTC
5283,CLOSED,[Question]About Data collection and Data send back,,2023-01-31 22:20:44 +0000 UTC
5279,CLOSED,[Python Backend] When using print() and multiple models, the print logs show out-of-order.,question,2023-04-10 22:12:42 +0000 UTC
5278,CLOSED,Incorrect output for a movinet model on a tensorflow backend - Triton 22.04,bug,2023-03-01 21:04:24 +0000 UTC
5277,CLOSED,Backend configs ignoring/not receiving some config flags in latest release,bug, investigating,2023-03-16 21:56:46 +0000 UTC
5276,CLOSED,Every other sequence_id is set to zero using pytorch backend with stateful TS model.,bug,2023-02-08 23:18:06 +0000 UTC
5274,CLOSED,Observing gradually increase in response time of pytorch model,,2023-02-13 19:09:25 +0000 UTC
5273,CLOSED,Build CMake target missing .so definition,,2023-02-21 21:56:29 +0000 UTC
5272,CLOSED,[Question] How image data should be serialized for warmup,,2023-02-13 19:18:20 +0000 UTC
5271,CLOSED,Guidance for model instaces and gpu count,,2023-02-06 15:04:44 +0000 UTC
5269,CLOSED,Starting triton docker container on kube cluster,,2023-02-22 08:51:50 +0000 UTC
5268,CLOSED,E0120 09:32:45.604616 1854 model_repository_manager.cc:1002] Poll failed for model directory 'ensemble_model': output 'decoded_sequence' for ensemble 'ensemble_model' is not written,,2023-03-03 12:18:53 +0000 UTC
5266,CLOSED,libtritonserver.so and onnxruntime files are missing from cpu build,,2023-01-20 15:43:04 +0000 UTC
5259,OPEN,Suggestion to reduce RAM consumption,investigating,2023-02-27 03:44:20 +0000 UTC
5255,CLOSED,E0116 09:33:42.589212 1 model_repository_manager.cc:1002] Poll failed for model directory 'pytorch_classifier': Invalid model name: Could not determine backend for model 'pytorch_classifier' with no backend in model configuration. Expected model name of the form 'model.<backend_name>'.,question,2023-01-19 22:59:47 +0000 UTC
5254,OPEN,Keeps on getting "Invalid private key" when using tritonclient.grpc with SSL,bug,2023-04-11 06:53:39 +0000 UTC
5253,CLOSED,Setup Triton server with build.py Error at setting up libonnxruntime.so,,2023-01-30 18:52:25 +0000 UTC
5252,CLOSED,Get per requests timing data,question,2023-01-20 01:19:45 +0000 UTC
5248,CLOSED,Issue with system shared memory and OpenCL (failed reading shared memory buffer with clEnqueueWriteBuffer ),bug,2023-02-01 20:15:09 +0000 UTC
5242,CLOSED,Traces. OpenTelemetry,enhancement,2023-04-26 20:05:25 +0000 UTC
5240,OPEN,Using PyTorch 2.0 for the PyTorch Backend,enhancement,2023-07-04 15:10:36 +0000 UTC
5239,CLOSED,Triton per model resource distribution monitoring,question,2023-02-13 19:17:22 +0000 UTC
5238,OPEN,ehancement(client): Python type-hints,enhancement,2023-01-31 23:33:27 +0000 UTC
5237,OPEN,Does ensemble model release CUDA cache?,,2023-01-18 05:23:53 +0000 UTC
5236,OPEN,[RFC] Provide an option to start any backend out-of-proc to help with memory management on UNLOAD,enhancement,2023-01-20 18:51:55 +0000 UTC
5234,CLOSED,[Bug] Triton Server crashes and becomes unavailable for a few seconds before restarting,,2023-04-06 16:06:06 +0000 UTC
5232,CLOSED,Unable to load shared library: libc10.so when building Pytorch backend using Docker image,,2023-01-11 13:29:32 +0000 UTC
5231,CLOSED,Triton server restarts after polling a new model from GCS,,2023-02-10 20:00:46 +0000 UTC
5230,CLOSED,Setup Triton Inference Server on a Windows 2019 server with Tesla GPU + inference using python,,2023-02-13 19:19:19 +0000 UTC
5229,CLOSED,Triton inference is 2 times slower than non triton inference for me,,2023-03-16 18:27:09 +0000 UTC
5227,CLOSED,Custom metrics for Python backend?,enhancement,2023-05-04 00:38:35 +0000 UTC
5225,CLOSED,[Question] Tips on sending gRPC requests from .NET,question,2023-02-01 14:41:52 +0000 UTC
5224,CLOSED,cpu build is failing with --- Target "caffe2plan" links to target "CUDA::cudart" but the target was not found,question,2023-01-30 18:55:31 +0000 UTC
5223,CLOSED,Server crash running torchvision.io.decode_image on GPU,bug, investigating,2023-04-14 00:04:28 +0000 UTC
5222,CLOSED,can not set the log file path,,2023-01-06 08:30:56 +0000 UTC
5221,CLOSED,/v2/repository/index api is very slowly,question,2023-02-01 09:48:18 +0000 UTC
5220,CLOSED,Python client grpcio==1.42.0 requirement too strict,,2023-01-17 07:59:44 +0000 UTC
5217,CLOSED,UNAVAILABLE: Internal: unable to create stream: the provided PTX was compiled with an unsupported toolchain.,,2023-01-04 20:18:02 +0000 UTC
5216,CLOSED,AttributeError: 'InferenceServerClient' object has no attribute '_pool',,2023-01-04 22:26:27 +0000 UTC
5215,CLOSED,Is it necessary to specify max_batch_size when using dynamic batch?,question,2023-01-30 18:55:53 +0000 UTC
5214,CLOSED,[Question 🙋] Is this fps performance normal?,question,2023-01-30 18:55:46 +0000 UTC
5212,CLOSED,Client memory leak on unreachable,,2023-01-03 03:07:05 +0000 UTC
5211,CLOSED,Triton server stuck during initialization/reload of python models,bug, investigating,2023-01-12 18:27:00 +0000 UTC
5210,CLOSED,python backend IPC makes triton-model QPS drop rapidly,bug,2023-01-30 18:56:05 +0000 UTC
5209,OPEN,Is there a way to call other methods rather "forward"?,enhancement,2023-02-03 00:45:31 +0000 UTC
5208,CLOSED,Input_memories in libtorch backend,,2022-12-30 21:56:32 +0000 UTC
5207,OPEN,How to start triton server after building the Windows 10 "Min" Image?,question,2023-01-10 06:52:27 +0000 UTC
5206,CLOSED,Encounter error Invalid argument: unable to find 'libtriton_tensorrt_plan.so' when starting triton server,,2023-01-28 00:00:01 +0000 UTC
5205,CLOSED,[question] How to make sure that dynamic batching works making concurrent requests?,question,2023-05-17 11:46:56 +0000 UTC
5201,CLOSED,[Question] Is it possible to load a separate TRT model from the initialize function of a BLS model ?,question,2022-12-29 09:37:06 +0000 UTC
5200,CLOSED,Has anyone implemented a backend for Goldwasser GPU inference?,question,2022-12-27 18:32:12 +0000 UTC
5199,CLOSED,can fully use gpu-utils with Python client ( grpc, http ) ?,question,2023-01-30 18:56:46 +0000 UTC
5198,CLOSED,GRPC client and decoupled mode - detecting last response of an infer request,duplicate,2022-12-27 22:49:37 +0000 UTC
5197,CLOSED,Serve a Model in 3 Easy Steps is erro,question,2022-12-29 02:27:13 +0000 UTC
5196,CLOSED,Python backend (BLS) abnormally consumes large GPU memory,,2023-05-22 18:43:02 +0000 UTC
5195,CLOSED,Internal: An input of type 'Tensor?' was detected in the model. Only a single input of type Dict(str, Tensor) or input(s) of type Tensor are surpported,,2023-01-09 19:51:58 +0000 UTC
5194,CLOSED,Question about Triton's Setup for a Speech-To-Text model,question,2023-01-09 19:51:27 +0000 UTC
5192,CLOSED,jetson compilation,question,2022-12-27 17:35:15 +0000 UTC
5191,CLOSED,Question about basic sequence stream example,question,2023-01-04 09:47:53 +0000 UTC
5189,CLOSED,Core dump when load model with config which containning repoagent in explicit mode,question,2023-01-30 18:57:28 +0000 UTC
5188,CLOSED,Can't use tensorrt with stateful model,question,2023-01-25 22:01:34 +0000 UTC
5187,CLOSED,Release the tar file contains the Triton server executable and shared libraries,,2023-01-28 00:00:35 +0000 UTC
5185,CLOSED,HTTP response from server does not include "content-encoding" header even if the response body is encoded,bug,2023-04-19 20:38:57 +0000 UTC
5183,OPEN,Can no longer use GCS for model store with latest release,enhancement, investigating,2022-12-16 21:04:55 +0000 UTC
5182,CLOSED,how to load weights files for python execution environment in the cloud?,question,2023-01-09 19:52:42 +0000 UTC
5180,CLOSED,Can I free the memory of model instance?,question,2023-01-09 19:52:16 +0000 UTC
5175,CLOSED,Assigning GPU fraction to a model,,2022-12-15 22:51:46 +0000 UTC
5174,CLOSED,what's the behavior about python_backend.InferenceRequest.exec()?,,2023-07-07 07:48:28 +0000 UTC
5171,CLOSED,question: What is the easiest way to check my custom backend library for memory errors?,question,2023-01-09 19:54:07 +0000 UTC
5170,CLOSED,Supports of group instance count,question,2023-07-08 00:35:17 +0000 UTC
5168,CLOSED,Python client getting significantly less performance than perf_client tool,question,2023-01-09 16:15:37 +0000 UTC
5167,CLOSED,Fatal error - No such file or directory when building custom PyTorch backend,,2023-01-11 13:29:22 +0000 UTC
5165,CLOSED,[Question] Is it possible to add custom information in a model config ?,,2022-12-14 14:17:58 +0000 UTC
5164,CLOSED,[Question] Is there a way to specify a different filename depending on the GPU ?,,2022-12-14 14:17:46 +0000 UTC
5163,CLOSED,problem about daemonts svclb-tritoninferenceserve,,2023-02-21 21:57:06 +0000 UTC
5162,CLOSED,How can I make this scenario with image_client?,,2022-12-16 09:25:56 +0000 UTC
5160,OPEN,Question: Determine if all responses are processed for a given request.,enhancement,2022-12-14 21:14:04 +0000 UTC
5159,CLOSED,Question: Order of responses for the same request for decoupled models.,question,2023-01-09 19:53:08 +0000 UTC
5158,CLOSED,triton is slower than direct access the trt engine file,,2022-12-12 13:56:14 +0000 UTC
5157,CLOSED,[Question]Why triton inference server uses different gpu devices when inference?,question,2022-12-13 07:27:14 +0000 UTC
5156,CLOSED,archive_write_data_block() failed with error code = -20, error message is Write failed for model.py (ensemble model deployment on triton),,2022-12-25 16:25:25 +0000 UTC
5155,CLOSED,tritonserver: unrecognized option '--cloud-credentials=path/to/creds/.json',,2023-02-16 09:32:10 +0000 UTC
5154,CLOSED,Detecting abrupt stream closing in C++ client library,question,2022-12-14 04:24:55 +0000 UTC
5152,CLOSED,Torchscript model inference time too slow, how to know if model is running on GPU,performance,2023-07-11 15:36:02 +0000 UTC
5150,CLOSED,Change model namespacing to allow Triton to re-use model names across different model repos,bug,2023-03-29 19:10:42 +0000 UTC
5147,CLOSED,[Question] about NVFUSER with pytorch backend,,2022-12-09 08:10:52 +0000 UTC
5146,CLOSED,how to keep model output on gpu in python-backend?,,2022-12-12 08:59:40 +0000 UTC
5145,CLOSED,how to add lib and header into different c++ project,,2022-12-07 19:13:08 +0000 UTC
5144,OPEN,Stub process is unhealthy and it will be restarted.,bug,2023-07-11 02:23:56 +0000 UTC
5134,CLOSED,[Question] Is it possible to disable model concurrency ?,,2022-12-06 18:51:29 +0000 UTC
5133,CLOSED,Can not create Customized Python Backends with Python 3.10 - 3.11 due to a Conda Pack issue,,2022-12-13 16:22:08 +0000 UTC
5132,CLOSED,Calling metrics endpoint stops the triton server,,2023-01-28 00:06:16 +0000 UTC
5131,CLOSED,All models load successfully but unload themselves.,,2022-12-05 15:28:00 +0000 UTC
5130,CLOSED,How to convert to dictionary in Python, Getting TypeError,,2022-12-04 18:32:55 +0000 UTC
5129,CLOSED,model_warmup is ignored,,2022-12-29 13:28:28 +0000 UTC
5128,CLOSED,NvFuser is disabled,,2022-12-07 15:44:00 +0000 UTC
5127,CLOSED,Mutex lock in statistics report,,2022-12-29 13:28:40 +0000 UTC
5124,CLOSED,Does triton support kv_cache?,question,2022-12-05 02:53:40 +0000 UTC
5123,CLOSED,Torchscript model input requires tensor on gpu,,2022-12-05 10:40:34 +0000 UTC
5121,CLOSED,CPU consumtion much higher when using Triton server + aws inferentia vs aws inferentia alone,question,2022-12-12 14:49:25 +0000 UTC
5120,CLOSED,Parameters CPU_THREADS_NUM in openvino_backend and instance_group in python_backend don't work well,,2023-01-28 00:00:26 +0000 UTC
5116,CLOSED,Error while nginx proxy,bug, investigating,2023-01-28 16:01:36 +0000 UTC
5115,CLOSED,Torchscript backend **MUCH** slower only with FP16 on 1650,,2022-11-28 19:22:30 +0000 UTC
5114,CLOSED,Bump up curl version for compatibility for Linux,enhancement, investigating,2022-12-05 06:08:38 +0000 UTC
5113,CLOSED,Ensemble output waiting,question,2022-12-19 20:22:01 +0000 UTC
5110,CLOSED,Can not create a customized Python backend when using Python 3.11 for the Conda environment,bug,2022-12-05 15:39:57 +0000 UTC
5109,CLOSED,GRPC error when return the client results,question,2022-12-20 20:33:17 +0000 UTC
5108,CLOSED,Multi instances's performance is slightly low. [pytorch_backend],question,2022-12-03 05:38:05 +0000 UTC
5107,CLOSED,Can not use container with tensorflow in python backend,,2022-12-19 20:23:03 +0000 UTC
5106,CLOSED,The end-to-end request duration (_nv_inference_request_duration_us) is 10x the actual inference duration of the model (_nv_inference_compute_infer_duration_us_),question,2022-12-19 20:23:32 +0000 UTC
5105,CLOSED,How to determine which instance the request is on？,question,2022-12-19 20:19:53 +0000 UTC
5103,CLOSED,Build Error Windows without docker,question,2022-12-19 20:21:03 +0000 UTC
5102,CLOSED,Triton client failed to build on Mac - <rapidjson/document.h> file not found,,2023-01-28 00:04:23 +0000 UTC
5100,CLOSED,Want to install specific version of TensorRT in Triton server,question,2022-12-08 17:48:57 +0000 UTC
5098,OPEN,Question about 22.09,bug,2022-11-29 14:37:03 +0000 UTC
5097,CLOSED,Is there any way to check if the model is now used by another client ? Then my client will not unload this model,question,2022-12-01 17:48:13 +0000 UTC
5095,CLOSED,What are the things to do to make inference faster or more responsive?,,2023-01-28 00:03:49 +0000 UTC
5094,CLOSED,Cannot server custom cuda kernel GPU pytorch models via the python backend with conda pack,,2022-11-29 01:19:28 +0000 UTC
5092,CLOSED,Tensorrt output is wrong in GPU-A40 but GPU-P40 is right,,2022-11-21 18:57:35 +0000 UTC
5091,CLOSED,Error case handling,question,2022-12-19 20:23:58 +0000 UTC
5090,CLOSED,need help about the deployment of large model,,2022-12-19 20:24:49 +0000 UTC
5089,CLOSED,How to setup a local server for realtime streaming inferences?,,2023-01-28 00:03:56 +0000 UTC
5087,CLOSED,Tritonserver load recommended backend failed,,2022-11-18 03:54:23 +0000 UTC
5084,CLOSED,onnx model causes core dump in 22.08+, works with 22.06,bug,2023-01-03 20:04:34 +0000 UTC
5082,CLOSED,Preprocessing input on server causing increased latency,,2022-11-16 16:19:17 +0000 UTC
5081,CLOSED,Question about the "python_backend",question,2022-12-06 19:02:42 +0000 UTC
5080,CLOSED,Model Ensemble using only the latest configuration files,,2022-12-06 19:04:07 +0000 UTC
5079,CLOSED,getting incorrect output shape,,2022-12-06 19:04:13 +0000 UTC
5078,CLOSED,client fails to compile,,2022-12-06 19:03:58 +0000 UTC
5077,CLOSED,triton server can not start if docker run with specified cpusets using docker run -cpuset-cpus,,2023-03-07 02:49:07 +0000 UTC
5074,CLOSED,Provide an API to access configpb.txt programmatically to know which models are configured to run on Triton,enhancement,2022-12-01 00:36:59 +0000 UTC
5073,CLOSED,docs/examples/stable_diffusion -> `scale_model_input` function should be called before `step`,bug,2022-11-28 22:54:33 +0000 UTC
5072,CLOSED,class inference::RepositoryModelLoadRequest has no member named ‘mutable_parameters’,,2022-11-15 00:33:18 +0000 UTC
5071,CLOSED,Failed to serve model coverted by torch2trt,,2022-11-17 00:52:46 +0000 UTC
5069,CLOSED,mobilenet based model deploy success, but run fail,,2023-02-21 21:58:08 +0000 UTC
5067,OPEN,Thread control options in PyTorch backend,enhancement,2022-11-14 04:26:46 +0000 UTC
5065,CLOSED,Triton doesn't recognize any model,,2022-11-10 20:55:56 +0000 UTC
5064,CLOSED,When i use 3060 gpu,can not load model,,2022-11-14 05:41:29 +0000 UTC
5063,CLOSED,Problems with installation of python_backend from source,,2022-11-10 20:55:14 +0000 UTC
5062,CLOSED,Support Load/ Unload Model Type Of PyTorch Geometric With TorchScript,,2022-11-29 16:57:16 +0000 UTC
5055,CLOSED,Stable-diffusion Example Inference Error due to Triton Server side's triton version update,bug,2022-11-17 00:33:42 +0000 UTC
5053,CLOSED,Infer error - unknown request input name input__0,,2022-11-13 17:06:50 +0000 UTC
5052,CLOSED,Deploying multiple Triton containers on a single GPU and unable to servce by GRPC at the same time,,2023-01-27 16:44:55 +0000 UTC
5050,OPEN,Ragged batching support for PyTorch backend,enhancement,2022-11-14 04:51:35 +0000 UTC
5049,CLOSED,Our onnx models need onnxruntime version 1.10.0. I'm using triton server version 22.08 which has onnxruntime version 1.11.1 in onnx backend. How can i use the required version?,,2022-11-29 16:57:35 +0000 UTC
5046,CLOSED,Can't make more than one request,,2023-07-06 18:30:25 +0000 UTC
5045,CLOSED,Triton server container taking long time to launch. How to reduce the time when scaling to new instances,,2023-02-07 10:46:12 +0000 UTC
5042,CLOSED,Triton-server TF backend, delayed response during explicit model update,,2022-11-29 16:58:07 +0000 UTC
5041,CLOSED,Error on loading Pytorch Model using Triton-PytorchBackedn,,2022-11-29 16:58:15 +0000 UTC
5040,CLOSED,How to build TensorFlow Backend With Custom TensorFlow?,,2022-11-25 12:38:46 +0000 UTC
5039,CLOSED,Unable to access GCS bucket with workload identity mechanism in GKE,bug,2023-04-18 09:59:19 +0000 UTC
5037,OPEN,allow constant input tensors in (ensemble) models,enhancement,2022-11-04 23:17:28 +0000 UTC
5035,CLOSED,Inputs' shape with reshape for batch inference has wired behaviour,,2023-02-23 22:23:52 +0000 UTC
5034,CLOSED,async_infer GRPC result mapping,question,2022-11-06 12:57:29 +0000 UTC
5031,CLOSED,Question about deploying multiple docker models on a single GPU,question,2022-11-07 18:54:43 +0000 UTC
5027,CLOSED,InferenceServerClient request time is large than model-analyzer,question,2022-11-09 17:33:50 +0000 UTC
5026,CLOSED,AVX,,2023-02-06 10:42:03 +0000 UTC
5025,CLOSED,22.09 Logger still unavailable in python_backend,question,2022-11-02 18:08:27 +0000 UTC
5024,CLOSED,Inference Speed drops,question,2022-11-29 16:58:23 +0000 UTC
5023,OPEN,Socket Closed when running on K8S,bug,2022-11-03 21:33:35 +0000 UTC
5021,CLOSED,Pytorch Backend Compatibility on AGX,question,2022-11-29 17:02:35 +0000 UTC
5018,CLOSED,USE GRPC ON OPENSHIFT,,2022-10-28 22:58:20 +0000 UTC
5017,CLOSED,report error when model batchsize>1.,,2022-12-29 13:36:41 +0000 UTC
5014,CLOSED,How to decrease GPU memory in onnxruntime?,,2022-11-22 02:26:06 +0000 UTC
5013,CLOSED,Why does TensorRT engine only supports max-batch 1,,2022-11-19 14:21:09 +0000 UTC
5009,CLOSED,Possibility to recreate new connection to the triton server,,2022-11-29 17:02:51 +0000 UTC
5008,CLOSED,why jetpack5.0 not supported S3 storage?,,2023-02-10 14:36:24 +0000 UTC
5007,CLOSED,Triton inference results lower than the corresponding .tlt model,,2023-01-28 00:04:52 +0000 UTC
5001,CLOSED,Can I use Triton server for inference on GPU AWS graviton instances,question,2022-11-29 17:04:05 +0000 UTC
5000,CLOSED,Windows triton build cannot make https call because of a reduced default libcurl,bug, investigating,2022-11-18 23:01:28 +0000 UTC
4999,CLOSED,How do I know that Triton's return is over?,duplicate,2023-06-28 12:45:01 +0000 UTC
4998,CLOSED,Triton Inference Server Image Python Libraries,,2022-10-20 19:16:10 +0000 UTC
4997,CLOSED,Model config and data transmitted is type F32, response in BYTES,bug, investigating,2022-10-21 01:16:03 +0000 UTC
4996,CLOSED,UnicodeEncodeError when using python backend to encode utf-8,question,2022-10-20 04:47:26 +0000 UTC
4995,CLOSED,How to properly use dynamic input shape with Triton?,bug, investigating,2022-10-31 20:45:56 +0000 UTC
4994,CLOSED,ModelInfer RPC doesn't support models with decoupled transaction policy,question,2022-10-25 23:22:51 +0000 UTC
4992,CLOSED,NVIDIA A2 Volatile GPU-Util = 88% just max_batch_size=2,question,2022-12-29 13:36:59 +0000 UTC
4990,CLOSED,Triton server throwing error on inference request after upgrading to latest version - INVALID_ARGUMENT: Fail to proof the equality of two dimensions at compile time,,2022-10-17 19:57:01 +0000 UTC
4989,CLOSED,Logging question,question,2022-12-19 20:28:19 +0000 UTC
4988,OPEN,Triton image_client.py fails to run,enhancement, investigating,2022-10-17 20:55:29 +0000 UTC
4987,CLOSED,The Issue of Triton Server Load Same Model Many Times,,2022-11-22 03:12:45 +0000 UTC
4986,CLOSED,Model profiling without model analyzer,,2023-02-17 19:55:21 +0000 UTC
4985,CLOSED,tritonclient.utils.InferenceServerException: [StatusCode.UNAVAILABLE] Connection reset by peer,,2022-10-18 03:16:09 +0000 UTC
4984,OPEN,gRPC client generated python files are really old,enhancement, investigating,2023-02-08 11:09:07 +0000 UTC
4983,CLOSED,Ho to debug the model file while using Python backend,question,2022-12-19 20:32:18 +0000 UTC
4981,CLOSED,Docker build fail on windows,bug,2022-12-05 19:58:34 +0000 UTC
4980,CLOSED,Can not run the demo example : Request for unknown model: 'densenet_onnx' is not found,question,2022-10-14 01:14:01 +0000 UTC
4979,CLOSED,Build error on windows,bug,2022-11-28 18:36:29 +0000 UTC
4978,CLOSED,Question: Triton server scaling when too many models,question,2022-10-19 17:25:33 +0000 UTC
4977,CLOSED,Unable to Launch container ImagePull Back Off Issue,question,2022-10-18 00:25:37 +0000 UTC
4976,CLOSED,Load/Unload model,,2022-10-26 01:01:19 +0000 UTC
4975,CLOSED,Python backend: enable sharing memory between multiple instances of a model,,2023-01-04 22:36:23 +0000 UTC
4974,CLOSED,Loading parameters at model loading time,question,2022-12-19 20:28:56 +0000 UTC
4973,CLOSED,How to discover for Tensorrt backend if batch size for the model is restricted to exactly max_batch_size?,question,2022-12-20 20:31:49 +0000 UTC
4972,CLOSED,python backend {'error': 'GRPC Execute Failed, message: Received message larger than max (4915244 vs. 4194304)'},,2022-11-22 03:12:55 +0000 UTC
4971,CLOSED,The results returned by the interface can be customized,,2022-10-13 00:37:23 +0000 UTC
4970,CLOSED,Shared Memory Header and Libraries not included in the prebuilt libraries. Can they be included?,,2022-10-11 23:38:19 +0000 UTC
4967,CLOSED,terminate called after throwing an instance of 'std::out_of_range',,2022-11-22 03:12:50 +0000 UTC
4966,CLOSED,ONNX model unload does not free system memory,investigating,2023-01-30 18:57:56 +0000 UTC
4965,CLOSED,Error while loading shared libraries: libdcgm.so.2,bug,2023-05-22 22:37:22 +0000 UTC
4959,CLOSED,Triton server with python backend slow for YOLO inferencing,question,2022-10-18 11:48:52 +0000 UTC
4958,CLOSED,8000 port is not responding,,2022-11-22 03:13:00 +0000 UTC
4957,CLOSED,Unable to load custom YOLOX model with CPU only mode - failed to stat file,question,2022-10-11 05:55:59 +0000 UTC
4956,CLOSED,Triton Server crash when upload a wrong serializationVersion model,bug, investigating,2022-10-10 21:50:13 +0000 UTC
4955,CLOSED,Failed to create environment directory for '/tmp/python_env_QNQ3ug/5',investigating,2022-12-04 03:55:47 +0000 UTC
4954,CLOSED,how to get triton_python.dll?,question,2022-10-14 23:26:14 +0000 UTC
4952,CLOSED,libgomp-d22c30c5.so.1: cannot allocate memory in static TLS block,,2022-10-25 02:51:48 +0000 UTC
4947,CLOSED,Load python model GPU failed,,2022-11-22 02:57:02 +0000 UTC
4942,OPEN,Add example model for FIL backend,enhancement,2022-11-22 03:13:42 +0000 UTC
4941,CLOSED,Get request_id_ from failed request,enhancement,2023-01-19 18:30:16 +0000 UTC
4940,CLOSED,Simultaneous execution and dynamic batching,question,2022-10-06 17:36:21 +0000 UTC
4938,CLOSED,[INFO] How does the internal queue/batching work?,question,2022-10-06 16:40:16 +0000 UTC
4937,OPEN,Missing doc for `platform`,bug,2022-10-03 16:29:53 +0000 UTC
4936,CLOSED,Deploy triton model on multiple nodes,question,2022-10-06 16:42:14 +0000 UTC
4935,CLOSED,Load model to repository,question,2022-12-19 20:33:01 +0000 UTC
4934,CLOSED,RuntimeError: Error in dlopen: libtorch_cuda_linalg.so: cannot open shared object file: No such file or directory,bug,2023-02-21 21:01:11 +0000 UTC
4932,CLOSED,Python Backend Uses Lots of GPU Memory,question,2022-12-26 09:21:08 +0000 UTC
4931,CLOSED,Failed to process the request(s) for model '***', message: error: unpack_from requires a buffer of at least 578635587 bytes for unpacking 578486289 bytes at offset 149298 (actual buffer size is 242891) At: /opt/tritonserver/backends/python/triton_python_backend_utils.py(116): deserialize_bytes_tensor,bug,2023-05-19 22:31:16 +0000 UTC
4930,CLOSED,tritonclient with pycuda integration,enhancement,2023-06-21 01:33:39 +0000 UTC
4929,CLOSED,Error Resuming when not pause,bug,2022-12-20 20:22:03 +0000 UTC
4927,CLOSED,Seek help: Unrecognized data format,,2022-09-28 02:52:48 +0000 UTC
4925,CLOSED,Occasional segfault in dynamic batch scheduler using ONNX runtime,bug, investigating,2022-10-07 15:51:53 +0000 UTC
4924,CLOSED,[Tensorflow Backend] | [Segfault, intermittent] | Unloading model on GPU results in a segfault i.e. tritonserver crashes,bug,2022-10-28 23:52:04 +0000 UTC
4919,OPEN,Add support to "parameters" in Python tritonclient package,enhancement,2022-09-26 21:17:58 +0000 UTC
4918,CLOSED,Responses arrives in different order than has been sent,question,2023-02-23 23:12:41 +0000 UTC
4917,CLOSED,Grpc_serve unable to handle Java client UINT32 requests,,2022-11-10 16:33:32 +0000 UTC
4916,CLOSED,Bytes/String Datatype golang grpc client example,,2022-10-07 01:12:19 +0000 UTC
4915,CLOSED,Triton failed to open the cudaIpcHandle upon prediction request when launched in container under WSL2,bug,2022-10-19 02:45:41 +0000 UTC
4910,CLOSED,explicit mode load model failed,,2022-10-13 00:52:00 +0000 UTC
4909,CLOSED,Multiple Classification,,2022-10-18 00:02:10 +0000 UTC
4908,CLOSED,module 'triton_python_backend_utils' has no attribute 'Logger',,2022-09-25 17:57:47 +0000 UTC
4907,CLOSED,Yolo in the Cloud,,2022-11-22 03:13:56 +0000 UTC
4906,CLOSED,Server Hangs when loading python backend w/ pytorch (including the example),,2022-11-22 03:14:23 +0000 UTC
4904,CLOSED,Broken metrics link in the backend example,,2022-10-03 18:57:24 +0000 UTC
4903,CLOSED,Unexpected shape for input 'TEXT' for model 'ensemble_model'. Expected [-1,-1], got [2],question,2022-12-20 20:28:38 +0000 UTC
4902,CLOSED,wrong format of returned results,,2022-09-22 07:04:12 +0000 UTC
4899,CLOSED,Parameter CPU_THREADS_NUM setting bug in openvino_backend,,2022-09-22 13:57:36 +0000 UTC
4898,CLOSED,Triton Server return wrong results,,2022-09-21 07:08:13 +0000 UTC
4896,CLOSED,Supporting MIG with multi model instance groups,,2022-10-04 15:24:17 +0000 UTC
4894,CLOSED,Fatal error: EfficientNMS_TRT is not a registered function/op,question,2022-09-30 22:45:32 +0000 UTC
4893,CLOSED,Fatal error: EfficientNMS_TRT is not a registered function/op,duplicate,2022-09-19 18:50:43 +0000 UTC
4892,CLOSED,[QUESTION] What are Benefits of Using Backends?,question,2022-09-21 05:06:34 +0000 UTC
4890,CLOSED,How to invoke multiple models using a single BLS?,,2022-10-04 15:24:34 +0000 UTC
4889,CLOSED,[QUESTION] About Concurrent Model Execution Feature,,2022-09-28 06:53:20 +0000 UTC
4888,CLOSED,Splitting a batch of images into small batches for different instances.,,2022-10-04 15:25:40 +0000 UTC
4887,CLOSED,UNAVAILABLE: Internal: Unable to set NUMA memory policy: Operation not permitted,,2022-10-10 23:59:26 +0000 UTC
4886,CLOSED,Dynamic batching but require fixed input size,bug,2022-11-29 17:03:48 +0000 UTC
4885,CLOSED,Get shm memory status,,2022-11-22 03:15:02 +0000 UTC
4881,CLOSED,image_client.py does not get enough responses from server,question,2022-09-20 10:25:10 +0000 UTC
4880,CLOSED,too many open files,bug, investigating,2022-09-27 14:31:28 +0000 UTC
4875,CLOSED,Lower latency with high cpu usage, while higher latency with low cpu usage,question,2022-09-14 17:07:08 +0000 UTC
4874,CLOSED,build file after edit image_client.py code,question,2022-09-16 01:54:53 +0000 UTC
4872,CLOSED,Transfering results of Async Requests to Queue Best Practice,,2022-09-13 15:48:19 +0000 UTC
4870,OPEN,Python backend cannot import Tensor,enhancement,2023-01-29 10:59:57 +0000 UTC
4869,CLOSED,python backend: How to set the shm-size with triton helm chart?,,2022-09-12 11:37:55 +0000 UTC
4868,CLOSED,Memory leek problem in 22.07 of tensorrt backend,,2022-09-11 09:02:50 +0000 UTC
4867,CLOSED,Run constrained beam search T5 with TensorRT Triton,question,2022-09-30 22:45:29 +0000 UTC
4866,CLOSED,Lost file when deploy Triton from S3,question,2022-09-30 22:42:52 +0000 UTC
4865,CLOSED,Python backend 0-size GPU tensors causing "failed to get cuda pointer device attribute: invalid argument",bug, investigating,2022-09-29 00:42:27 +0000 UTC
4858,CLOSED,Tritonserver: symbol lookup error: _sentencepiece_tokenizer.so,question,2022-09-22 21:39:51 +0000 UTC
4857,CLOSED,python backend crash,,2022-09-30 22:42:45 +0000 UTC
4856,CLOSED,question about the priority in ratelimiter,question,2022-11-08 23:02:47 +0000 UTC
4855,CLOSED,strange coredump in libtritonserver.so,bug,2022-09-30 18:47:14 +0000 UTC
4854,CLOSED,how could I know the default value of model_config.proto,question,2022-09-09 01:58:23 +0000 UTC
4853,CLOSED,Impossible to load custom python backend,investigating,2023-03-15 10:06:26 +0000 UTC
4851,CLOSED,How to use triton for Multi Object Tracking,question,2022-09-30 22:47:28 +0000 UTC
4850,CLOSED,Question with client with cuda shared memory,question,2022-09-06 17:06:21 +0000 UTC
4849,CLOSED,`tritonclient[grpc]==2.24.0` Produces OOMs When Async gRPC Calls Are Performed,,2022-11-22 03:14:18 +0000 UTC
4848,CLOSED,use triton container 22.07 sdk load torchscript model failed,,2022-09-05 11:46:07 +0000 UTC
4847,CLOSED,Can't build the Dockerfile.win10.min,,2022-11-22 18:19:50 +0000 UTC
4846,CLOSED,data serializing error when the concurrency of requests is high,,2022-09-30 22:47:02 +0000 UTC
4845,CLOSED,Ensemble scheduler parallel,question,2022-09-09 01:59:11 +0000 UTC
4844,CLOSED,Errors when loading new models multiple times in quick succession,,2022-11-22 03:15:18 +0000 UTC
4843,OPEN,How stub methods are using?,question,2022-09-09 23:53:29 +0000 UTC
4842,CLOSED,BERT model is returning NaN logits values in output,,2022-09-30 22:44:01 +0000 UTC
4841,CLOSED,Under high load, stateful batcher queues small batches,bug,2022-11-18 10:15:24 +0000 UTC
4840,CLOSED,GRPC: unable to provide 'prob' in GPU, will use CPU,,2022-11-22 03:10:49 +0000 UTC
4839,CLOSED,building without docker: How to specify backend's CUDA / CuDNN version?,,2022-09-02 09:01:19 +0000 UTC
4838,CLOSED,Is there any sort of C++ equivalent of the Python BLS in Triton?,,2022-09-02 16:53:14 +0000 UTC
4837,CLOSED,Unable to create cluster with single t4 gpu , 2 core 12 gb ram in GKE,question,2022-10-13 00:52:59 +0000 UTC
4836,OPEN,Dose StreamInfer assure connecting with same model instance?,question,2022-09-02 17:01:24 +0000 UTC
4834,CLOSED,Using shm in bls,,2022-11-22 02:07:43 +0000 UTC
4833,CLOSED,when using model load api to load model in explicit mode by passing the model config, get attempt to access JSON non-string as string exception,,2022-08-31 11:58:10 +0000 UTC
4832,CLOSED,How to make different model version use different config?,question,2022-10-11 00:00:30 +0000 UTC
4831,CLOSED,example for Kubernetes configuration multi-node multi-gpu,,2023-02-13 19:19:42 +0000 UTC
4829,CLOSED,README.md steps not working for me,question, investigating,2023-07-08 00:31:18 +0000 UTC
4828,CLOSED,Wrong behaviour of ensemble unloading using HTTP API,,2022-08-31 11:27:13 +0000 UTC
4827,CLOSED,How to use nsight system in triton inference server,,2022-08-30 08:23:43 +0000 UTC
4826,CLOSED,max_batch_size configuration issue,question,2022-09-12 17:33:43 +0000 UTC
4825,CLOSED,Buiding without docker failed,,2022-11-22 03:10:12 +0000 UTC
4824,CLOSED,Load a new pytorch model but get an error: configuration expects 0 inputs, model provides 2,,2022-10-27 01:02:30 +0000 UTC
4821,CLOSED,Cannot use pb_utils.Logger,,2022-08-29 18:40:33 +0000 UTC
4820,CLOSED,Model config for empty dimensions,,2022-08-29 16:53:46 +0000 UTC
4819,OPEN,Core rebuild is extremely long,enhancement,2022-08-30 20:08:42 +0000 UTC
4818,CLOSED,Request Cancellation,question,2022-09-12 17:33:20 +0000 UTC
4817,CLOSED,Tensorflow model does not work on inference,question,2022-09-12 17:35:11 +0000 UTC
4815,CLOSED,Triton not loading custom ONNX model,question,2022-09-14 18:58:41 +0000 UTC
4814,CLOSED,Error when doing inference using tf-trt converted frozen model,,2022-10-11 00:00:00 +0000 UTC
4813,CLOSED,triton_python_backend_utils's api document?,,2022-09-09 07:17:57 +0000 UTC
4812,CLOSED,Incomprehensible overhead in Tritonserver inference,question, performance,2023-02-23 23:10:42 +0000 UTC
4811,CLOSED,onnx model dynamic axis not working,,2022-10-13 00:48:32 +0000 UTC
4810,CLOSED,[12th Gen Intel(R)] container was built for CPUs supporting at least the AVX instruction set,,2022-11-22 03:11:33 +0000 UTC
4809,CLOSED,No-copy Tensor transfer in python backend-based ensemble,question, performance,2022-11-29 17:03:24 +0000 UTC
4808,CLOSED,If I open the log verbose in python http client, will time cost increase?,,2022-08-26 08:46:36 +0000 UTC
4807,CLOSED,Support for Mac M1 chips?,,2022-08-24 07:02:28 +0000 UTC
4806,CLOSED,Check torchlib torch version,,2022-08-23 22:51:37 +0000 UTC
4804,CLOSED,Failed to set cuda graph shape when I set max_batch_size==0,bug,2022-09-23 18:27:21 +0000 UTC
4800,CLOSED,Shared memory failing in gunicorn following example,,2022-11-22 03:15:29 +0000 UTC
4799,CLOSED,Multiple instances on the same GPU,question,2022-08-23 00:24:44 +0000 UTC
4798,CLOSED,Error details: Error setting the binding dimension,,2022-08-24 10:19:16 +0000 UTC
4797,CLOSED,W0821 08:49:41.251296 104802 server.cc:208] failed to enable peer access for some device pairs,,2022-10-13 00:49:24 +0000 UTC
4796,CLOSED,What should I do when backend_input_collector return need_cuda_input_sync of True value?,question,2022-09-13 20:54:51 +0000 UTC
4795,CLOSED,Images Downloaded Location?,question,2022-08-24 18:27:45 +0000 UTC
4790,CLOSED,Low gpu utilization,,2022-11-22 03:13:23 +0000 UTC
4788,CLOSED,Question about inferring on multiple cards using cuda_shared_memory,question,2022-09-12 17:33:56 +0000 UTC
4787,CLOSED,How can I get the model list when I set --model-control-mode=poll by using "curl -X POST http://<ip>: <port>/v2/reporitory/models/....",question,2022-08-31 20:37:56 +0000 UTC
4786,OPEN,Show ensemble stage where error happens,enhancement,2022-08-19 15:57:22 +0000 UTC
4784,CLOSED,Changing Maximum Batch Size after model deployment,question,2022-08-19 15:41:38 +0000 UTC
4783,CLOSED,Python backend bug when using model load api to reload model in explicit mode,bug, investigating,2023-03-16 22:28:34 +0000 UTC
4782,CLOSED,How long support of current backend API you are guarantee.,question,2022-08-18 21:54:47 +0000 UTC
4781,OPEN,Container images for Jetson devices,enhancement,2022-08-17 23:20:00 +0000 UTC
4780,CLOSED,[BUG] Multi-input model with varying sizes gives error with gRPC client,,2022-08-18 13:32:53 +0000 UTC
4779,OPEN,Tests for backend examples,enhancement,2022-08-17 18:37:03 +0000 UTC
4778,CLOSED,triton pytorch backend malloc coredump,bug,2022-09-05 09:25:38 +0000 UTC
4776,CLOSED,Socket operation on non-socket when using multiprocessing,,2022-08-18 06:01:16 +0000 UTC
4775,CLOSED,Issues with implicit state management,,2022-08-17 18:40:25 +0000 UTC
4772,OPEN,Python Backend to support GPU instance,enhancement,2023-07-06 18:14:38 +0000 UTC
4769,CLOSED,Core dump when dynamic batch Infer using tensorflow backend,bug, investigating,2023-04-10 16:22:00 +0000 UTC
4767,CLOSED,Unexpected inference output 'model_output', allowed outputs are: logits,question,2022-09-21 12:49:20 +0000 UTC
4766,OPEN,CUDNN_STATUS_EXECUTION_FAILED when Triton server is running,bug,2022-09-01 00:34:30 +0000 UTC
4765,CLOSED,WebRTC Support ?,,2022-08-31 20:36:57 +0000 UTC
4764,CLOSED,Does model should instantly free memory after unload?,bug,2022-08-31 20:36:43 +0000 UTC
4763,CLOSED,Run server with public IP !,question,2022-08-15 16:49:01 +0000 UTC
4761,CLOSED,Triton pod not scheduled in all GPUs in a physical server.,question,2022-08-13 01:56:48 +0000 UTC
4756,CLOSED,TF_SIGNATURE_DEF is not used when selected on service startup,bug,2023-03-24 00:40:27 +0000 UTC
4754,CLOSED,Real time inference using Triton C APIs for multiple models,,2022-11-10 16:35:14 +0000 UTC
4753,CLOSED,Limit maximum nubmer of concurrent requests for triton server,question,2022-08-10 17:17:22 +0000 UTC
4752,CLOSED,Build a custom python backend environment for old fashion model. How to use specific CUDA version in conda environment?,,2022-11-22 03:03:02 +0000 UTC
4749,CLOSED,Have any roadmap to support http2?,,2022-09-06 23:43:31 +0000 UTC
4748,CLOSED,[question] Development workflow for custom C++ backends,,2022-09-02 10:26:06 +0000 UTC
4745,CLOSED,Is it possible to add logging to python backend?,,2022-08-05 19:29:11 +0000 UTC
4744,CLOSED,Triton server always crash during stress test.,,2022-09-22 11:13:16 +0000 UTC
4743,CLOSED,Python Backend complains "triton_python_backend_utils" has no attribute "InferenceRequest",bug,2022-09-30 22:43:27 +0000 UTC
4742,CLOSED,onnxruntime: `--no-container-build` not honored,,2022-09-09 20:38:15 +0000 UTC
4739,CLOSED,Couldn't get temp CUBIN file name - TensorFlow XLA,investigating,2022-08-17 22:05:44 +0000 UTC
4737,CLOSED,How to run triton on windows10?,investigating,2023-05-29 08:22:16 +0000 UTC
4734,CLOSED,Perf_analyzer json file error on unspecified optional tensors.,,2022-08-03 14:32:27 +0000 UTC
4733,OPEN,Python backend dynamic loading model uses configuration parameters, and loading fails,bug,2022-08-05 21:05:57 +0000 UTC
4731,OPEN,enh: Trace to capture the child models invoked from BLS,enhancement,2022-08-05 00:37:14 +0000 UTC
4730,CLOSED,build error: /workspace/src/grpc_server.cc:822:35: error: 'google::protobuf::stringpiece_internal' has not been declared,,2022-09-07 21:47:14 +0000 UTC
4728,CLOSED,Python backend shared codebase and code import,question,2022-08-03 05:56:51 +0000 UTC
4726,CLOSED,Question complex output with different shape for every sample - Token classification,,2022-08-28 12:24:51 +0000 UTC
4725,OPEN,Fail to fetch PR with `--repo-tag`,bug,2022-08-04 22:54:42 +0000 UTC
4724,CLOSED,failed to load model,,2022-09-07 21:47:37 +0000 UTC
4722,CLOSED,Customize response when using raw binary request.,enhancement,2022-08-12 16:17:41 +0000 UTC
4720,CLOSED,Trace summary script doesn't correctly handle splitting string values,,2022-10-21 23:42:35 +0000 UTC
4719,CLOSED,Large model output is copied when working from a BLS,,2023-02-01 20:46:26 +0000 UTC
4718,CLOSED,Add request id to trace output,enhancement,2023-03-07 19:36:32 +0000 UTC
4716,CLOSED,Parallel model inferencing flakey after upgrading triton,,2022-10-13 00:42:15 +0000 UTC
4715,CLOSED,Triton server periodically stops responding to `ServerLive`, `ServerReady` and `RepositoryIndex` requests,,2022-11-22 03:31:59 +0000 UTC
4714,CLOSED,oops,,2022-07-29 18:51:27 +0000 UTC
4713,CLOSED,Error message printed when install python packages in `nvcr.io/nvidia/tritonserver:22.06-py3` docker,,2022-08-01 22:23:01 +0000 UTC
4712,CLOSED,Tensorrt is very slow when batch size is small,,2022-09-07 21:48:51 +0000 UTC
4711,CLOSED,[Question] About perf_analyzer request rate,bug,2023-02-10 03:18:17 +0000 UTC
4708,CLOSED,can't print chinese in Python backend,,2022-09-09 20:40:55 +0000 UTC
4703,CLOSED,Python backend initialize 'model_repository' arg breaking change,,2022-07-29 18:45:34 +0000 UTC
4702,CLOSED,Error in accessing MINIO with self signed cert,,2022-09-07 21:49:19 +0000 UTC
4700,CLOSED,How to load plugin dynamically,enhancement,2023-07-06 18:47:59 +0000 UTC
4697,CLOSED,Failed to load tensorflow model: Not loaded: No model version was found,,2022-07-26 23:02:44 +0000 UTC
4696,CLOSED,Two issues when trying to do the homework of Triton server lesson from the NVIDIA DLI,,2022-07-27 02:11:34 +0000 UTC
4691,CLOSED,Compute output much larger than compute input,,2022-11-22 03:31:55 +0000 UTC
4690,CLOSED,openvinobackend inference only support synchronous mode? why not asynchronous mode,,2022-11-22 03:03:09 +0000 UTC
4689,CLOSED,Using S3 repository for Triton does not working properly / Cannot plug in to S3,,2022-07-26 17:35:12 +0000 UTC
4688,CLOSED,NGC container 22.07,question,2022-07-25 17:49:58 +0000 UTC
4687,CLOSED,Support TensorRT 8.4,,2022-07-23 13:16:32 +0000 UTC
4685,CLOSED,tritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] Unable to open shared memory region: '/tVHIApxKugk6TwsknLr1b',question,2023-02-23 23:05:02 +0000 UTC
4683,CLOSED,TensorRT Optimization in ConfigMap,question,2022-07-31 12:39:49 +0000 UTC
4679,CLOSED,Clients supporting sending multiple synchronous inferences at the same time,question,2022-10-11 00:01:56 +0000 UTC
4678,CLOSED,How can I run an uninterrupted thread in python backend?,question,2022-07-25 01:58:39 +0000 UTC
4675,CLOSED,Optional usage of shared memory for python backend,,2022-07-21 11:27:53 +0000 UTC
4674,CLOSED,perf_analyzer,,2022-07-21 17:32:40 +0000 UTC
4673,CLOSED,Ensemble model can't obtain data from pre-processor,question,2022-07-21 02:03:52 +0000 UTC
4672,CLOSED,What is the difference between `Plattform` and `Backend`,question,2022-10-11 00:03:01 +0000 UTC
4671,CLOSED,run testing triton fail,question,2022-09-07 21:50:27 +0000 UTC
4669,CLOSED,Questions for asynchronous Triton deployment,question,2022-07-20 08:21:27 +0000 UTC
4668,OPEN,One click deployment to GKE no longer works as Istio deprecated,enhancement, investigating,2023-02-16 18:11:46 +0000 UTC
4667,CLOSED,The average latency of the fp16 bert demo trt.engine with dynamic batch size is up to 2s,question,2022-07-21 21:16:14 +0000 UTC
4665,CLOSED,Server crashes on loading shared libraries,bug, investigating,2023-01-28 02:50:58 +0000 UTC
4662,CLOSED,Server died forever when overloaded,question,2022-07-19 21:49:57 +0000 UTC
4661,OPEN,Add source distribution to Python client package,enhancement,2022-07-19 21:41:10 +0000 UTC
4660,CLOSED,I am new to triton inference. I am looking for documents related to A/B testing but couldn't find them so far. If someone here is already used this feature, please let me the details.,invalid,2022-07-18 20:55:21 +0000 UTC
4659,CLOSED,exec /opt/nvidia/nvidia_entrypoint.sh: exec format error,,2022-07-18 20:49:17 +0000 UTC
4658,CLOSED,How to profile applications running on the triton-server?,question,2022-08-08 20:12:17 +0000 UTC
4657,CLOSED,The cc_model_filenames does not work,question,2022-08-10 01:31:55 +0000 UTC
4655,CLOSED,TRTIS 19.10 does notfind GPUs | failed call to cuInit: UNKNOWN ERROR (303),,2022-08-10 15:14:36 +0000 UTC
4651,CLOSED,Cannot load Custom Op file in the container LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.,,2022-07-19 16:54:55 +0000 UTC
4647,OPEN,[python backend] Add class_count argument for inference requests with BLS scripting,enhancement,2022-07-15 22:00:40 +0000 UTC
4646,CLOSED,How to get predict_proba as inference output of pytorch model?,question,2022-08-08 09:54:52 +0000 UTC
4644,CLOSED,jetson AGX xavier: Shared memory in docker container.,question,2022-07-14 15:30:31 +0000 UTC
4643,CLOSED,question about batch mechanism,question,2022-07-15 13:42:32 +0000 UTC
4639,CLOSED,Failing to get output tensor on GPU device,,2022-07-28 17:07:22 +0000 UTC
4638,CLOSED,enh: Extend model management to support load / unload at model version,duplicate,2022-07-13 15:42:30 +0000 UTC
4637,CLOSED,onnxruntime tensorrt is faster then triton server tensorrt,,2022-10-31 14:36:35 +0000 UTC
4636,CLOSED,bug: Trace output for `BYTES` has invalid JSON encoding,bug, investigating,2022-08-01 00:28:10 +0000 UTC
4630,OPEN,Dynamically loaded models don't work with ensemble,bug, investigating,2022-07-20 18:58:08 +0000 UTC
4629,CLOSED,[question] Adding timeout options for both client and server with a custom backend and stateful batching,question,2022-07-19 16:09:09 +0000 UTC
4628,CLOSED,Client gets into deadlock when max_sequence_idle_microseconds timeout occurs on triton server,,2022-07-19 08:38:42 +0000 UTC
4627,CLOSED,[question] Versioning for ensemble models,question,2022-07-13 08:36:50 +0000 UTC
4620,CLOSED,Why is C api recommended on Jetson ?,question,2022-07-14 14:22:05 +0000 UTC
4619,CLOSED,[question] performance comparison, ensemble vs. BLS,question,2022-07-26 19:22:08 +0000 UTC
4618,CLOSED,Can I stop execution?,question,2022-07-13 14:28:05 +0000 UTC
4617,CLOSED,Get unexpected deadlock with ensemble model,,2022-07-13 02:43:36 +0000 UTC
4616,CLOSED,How ot imporve throughput on tritonserver,,2022-09-07 21:51:09 +0000 UTC
4615,CLOSED,How to connect to remote server with GRPC?,,2022-07-09 15:03:25 +0000 UTC
4610,CLOSED,Ensemble model scheduler,question,2022-07-13 20:09:26 +0000 UTC
4609,CLOSED,Changing the gRPC protocol to implement standard gRPC Health Checking Protocol,enhancement,2023-01-23 21:03:08 +0000 UTC
4608,CLOSED,Triton server how to schedule GPU resource?,question,2022-07-11 20:37:20 +0000 UTC
4606,CLOSED,Automating Image and Payload Upload for ONNX Backend Inference - Image Shape and Data payload,,2022-09-06 23:46:43 +0000 UTC
4605,CLOSED,Cannot start Triton inference server with Python backend stub and ONNX models,,2023-03-17 15:45:19 +0000 UTC
4604,CLOSED,Fail loading TensorRT model: could not set binding dimension,,2022-07-07 13:29:49 +0000 UTC
4603,CLOSED,Accumulate inference time with an ensemble model is way slower than the slowest individual,question,2022-09-07 21:46:30 +0000 UTC
4600,CLOSED,MT-NLG - Are we ever getting access to the 530 B parameters trained model?,,2022-07-06 21:59:45 +0000 UTC
4598,CLOSED,[confused] Does Triton-server support to handle data from multiple video streams , maybe with model like detection + tracking(deep sort) + classification(stateful model),,2022-09-07 21:46:09 +0000 UTC
4597,CLOSED,Autoscale instances,,2022-07-08 17:45:39 +0000 UTC
4594,CLOSED,Add support for loading onnx files with the tensorRT backend,,2023-01-28 00:03:20 +0000 UTC
4593,CLOSED,Triton server doesn't detect GPUs,,2022-08-01 06:52:51 +0000 UTC
4590,CLOSED,Build latest triton custom image for Ubuntu 18.04,,2022-12-29 13:24:33 +0000 UTC
4587,OPEN,ONNXRuntime TensorRT cache gets regenerated every time a model is uploaded even with correct settings,investigating,2022-07-29 03:00:28 +0000 UTC
4585,CLOSED,Metrics,question,2022-07-05 21:42:05 +0000 UTC
4584,CLOSED,Will warmup been done when we start the server with --model-control-mode=explicit,,2022-07-05 21:25:12 +0000 UTC
4583,CLOSED,Help! triton_client.load_model return timeout error!,,2022-11-16 08:02:53 +0000 UTC
4582,CLOSED,How can I send variable-length tensors as a batch in one request using Python APIs?,,2022-07-05 21:33:01 +0000 UTC
4581,CLOSED,python backend doesn't support run each instance on multiple gpu,,2022-07-04 14:39:15 +0000 UTC
4580,CLOSED,failed to launch triton-server,,2022-07-04 14:41:53 +0000 UTC
4572,CLOSED,Use-cases and benefits of "Streaming" inference,,2022-07-22 23:57:37 +0000 UTC
4571,CLOSED,Auto-Generated Model Configuraton with label file,,2022-07-14 01:19:36 +0000 UTC
4570,CLOSED,request should include at least one InferRequestedOutput object,,2022-11-22 03:30:09 +0000 UTC
4566,CLOSED,Triton terminated with Signal (6),bug,2022-10-08 06:54:57 +0000 UTC
4563,CLOSED,tritonclient expects a different shape as defined in config.pbtxt,,2022-06-30 01:04:28 +0000 UTC
4562,CLOSED,UNAVAILABLE: Internal: archive_read_open_filename() failed.,,2022-11-23 02:18:34 +0000 UTC
4561,CLOSED,Release tags for tritonclient,,2022-06-29 17:32:04 +0000 UTC
4560,CLOSED,Poll failed for model directory 'full-pipeline': output 'OUT' for ensemble 'full-pipeline' is not written,,2022-06-29 14:00:25 +0000 UTC
4559,CLOSED,Assertion `batchSize > 0' failed, when deploy the tf-trt int8 optimization model,,2022-06-29 16:48:47 +0000 UTC
4558,CLOSED,failed to split the output tensor 'dets' in responses: expected batch size of atleast 2 in model output, got 1,,2022-07-15 18:28:02 +0000 UTC
4557,CLOSED,UNAVAILABLE: Internal: output 'labels' does not follow naming convention i.e. <name>__<index>.,,2022-06-29 19:09:36 +0000 UTC
4556,CLOSED,support Hyper-Q in triton-server,,2022-07-15 18:27:47 +0000 UTC
4555,CLOSED,Triton Server Docker Image with ONNXRuntime support,question,2022-07-14 23:14:01 +0000 UTC
4554,CLOSED,Failed when building for Windows 10,,2022-09-07 21:44:49 +0000 UTC
4550,CLOSED,[question] simple.cc, why std::vector<char>,question,2022-07-15 18:52:11 +0000 UTC
4549,CLOSED,Server not ready: Warmup using python BLS,,2022-11-22 03:31:02 +0000 UTC
4548,CLOSED,Questions regarding the Rate Limiter,question,2022-06-30 09:04:20 +0000 UTC
4547,OPEN,Splitting a batch to max_batch_size if the batch size is larger than max_batch_size,enhancement, investigating,2023-07-07 23:11:44 +0000 UTC
4545,CLOSED,When using perf_analyzer, throughput decreases sharply as concurrency increases.,,2022-07-11 14:18:23 +0000 UTC
4542,CLOSED,Request the feature to send metadata in BLS,enhancement,2022-10-06 18:21:17 +0000 UTC
4541,OPEN,python tritonclient stream_infer should send end signal to callback,enhancement,2022-08-26 17:17:25 +0000 UTC
4540,OPEN,[Question] Customize HTTP response status code for malformed GPU card,bug, investigating,2022-06-29 00:20:52 +0000 UTC
4538,OPEN,Support for the new CUDA virtual memory management functions for shared memory.,enhancement,2023-05-19 03:50:33 +0000 UTC
4537,CLOSED,Server start stuck when loading python model instantiating certain transformers model,bug, investigating,2023-02-01 12:15:31 +0000 UTC
4533,CLOSED,what is raw_mug_data?,question,2022-07-08 18:57:45 +0000 UTC
4531,CLOSED,Triton's resource consumption,,2022-06-27 08:51:33 +0000 UTC
4530,OPEN,support decoupled mode in perf_analyzer,enhancement,2022-06-20 15:04:19 +0000 UTC
4529,OPEN,Hardening guide for Triton Server,enhancement,2022-06-21 19:08:35 +0000 UTC
4528,CLOSED,Encounter memory leak issue when using http /load api to load new version's model,,2022-08-10 03:14:19 +0000 UTC
4527,CLOSED,Loosing a horrible amount of recall on triton server inference,,2022-07-14 05:19:31 +0000 UTC
4526,CLOSED,Option for adding or overriding model config attributes at server startup,enhancement,2023-07-08 00:36:26 +0000 UTC
4525,CLOSED,pytorch_backend[undefined symbol],question,2022-07-08 18:58:31 +0000 UTC
4524,CLOSED,tritonserver sometimes turn slow when gpu error,bug,2023-07-10 23:33:54 +0000 UTC
4523,CLOSED,GRPC Health check method,,2022-06-18 09:29:35 +0000 UTC
4520,CLOSED,Token classification,,2022-07-08 18:59:22 +0000 UTC
4519,CLOSED,Triton 2.10.0 Build without docker on Ubuntu 20.04 with ONNX Backend,,2022-11-22 03:23:19 +0000 UTC
4517,CLOSED,Tritonserver hanging on startup,,2022-06-17 21:40:10 +0000 UTC
4516,CLOSED,How to use java client api transfer images and texts to python backend?,question,2022-06-21 17:03:37 +0000 UTC
4515,CLOSED,Relative path for s3 storage,bug,2022-12-16 18:12:11 +0000 UTC
4513,CLOSED,add param for pytorch backend to specific get_method other than forward,,2022-06-17 06:04:48 +0000 UTC
4512,CLOSED,accuracy difference in local inference and triton inference,question, investigating,2022-06-15 10:11:57 +0000 UTC
4511,CLOSED,perf_analyzer and perf_client don't have 'x' permission after pip install tritonclient(2.22.3),bug,2022-06-20 16:03:20 +0000 UTC
4509,CLOSED,triton image classification example client http memory leak,bug,2022-07-08 19:33:19 +0000 UTC
4508,CLOSED,On custom config and accessing other models config on python backend,question,2022-07-13 16:19:43 +0000 UTC
4507,CLOSED,first run ok,seconde run error:xxx_model(version:1)inference error1:PyTorch execute failure: UNSUPPORTED DTYPE: Device,,2022-06-27 14:53:32 +0000 UTC
4506,CLOSED,AttributeError: 'python backend utils.Inference Request' object has no attribute 'as_numpy',,2022-06-13 15:03:58 +0000 UTC
4505,CLOSED,Status Message: CUDNN error executing cudnnFindConvolutionForwardAlgorithmEx,bug,2023-02-03 21:03:29 +0000 UTC
4504,CLOSED,Triton 22.03-py3 stuck at TRITONBACKEND_ModelInstanceInitialize on older Ubuntu 18.04,question,2023-02-23 17:12:58 +0000 UTC
4503,CLOSED,Warm up by sending multiple requests,question,2022-07-08 18:59:01 +0000 UTC
4502,CLOSED,Multiple models in Triton,,2022-06-15 18:19:35 +0000 UTC
4492,CLOSED,Throughput and concurrency values,question,2022-06-08 15:19:55 +0000 UTC
4491,CLOSED,Triton server stops while making Async request from multiple threads,bug,2022-06-16 08:54:25 +0000 UTC
4490,CLOSED,How to delete a backend?,question,2022-06-08 15:23:00 +0000 UTC
4489,CLOSED,[Solved] Bug: socket.timeout: timed out. Server failed to respond to requests,,2022-09-12 17:51:07 +0000 UTC
4488,CLOSED,Tensorflow and NVIDIA Triton Setup Issue,,2022-06-08 22:03:52 +0000 UTC
4487,CLOSED,Is there a good solution for video streaming? For example, RTSP and RTMP,,2022-06-09 02:14:36 +0000 UTC
4486,CLOSED,openvino backend 2022,question,2022-07-27 01:48:18 +0000 UTC
4485,OPEN,SHARK Backend integration,enhancement,2022-06-07 21:17:48 +0000 UTC
4483,CLOSED,Compute infer time increases linearly with batch size even with batching,,2022-06-14 10:10:56 +0000 UTC
4482,CLOSED,Python backend with additional OS libraries,,2022-06-27 14:54:35 +0000 UTC
4481,CLOSED,how to debug the model file when use python backend,question,2022-06-08 06:55:20 +0000 UTC
4480,CLOSED,run ci test fail,,2022-06-07 18:41:59 +0000 UTC
4479,CLOSED,Failed to register CUDA shared memory region 'fc6_1',,2023-06-28 10:30:12 +0000 UTC
4478,CLOSED,C-API onnx runtime error 2: not enough space: expected 3145728, got 786432,,2023-05-29 11:33:17 +0000 UTC
4477,CLOSED,Server stuck for a while when declaring gpu tensor of torch or cupy in python backend in first time of inference,,2022-06-10 07:35:54 +0000 UTC
4471,CLOSED,Run and query a finetuned T5 model in Triton Inference Server,,2022-06-06 20:07:32 +0000 UTC
4470,CLOSED,Can ragged input used together with stateful model?,,2022-11-22 03:23:03 +0000 UTC
4467,CLOSED,Build from source failed,,2022-07-08 19:01:40 +0000 UTC
4460,CLOSED,InferRequestedOutput throws memory error during object destruction,,2022-06-04 08:24:51 +0000 UTC
4457,CLOSED,Conversion of Pbutils Output Tensor to Numpy Array without Torch dl pack,,2022-06-21 19:17:42 +0000 UTC
4456,CLOSED,Cannot load model from GCS when LD_PRELOAD env var is set,bug,2023-07-06 18:44:41 +0000 UTC
4454,CLOSED,Unable to use S3 model storage,,2022-06-08 00:14:30 +0000 UTC
4453,CLOSED,"POST v2/repository/models/${MODEL_NAME}/load" failed on 22.05 but works fine on 21.08,,2022-07-12 20:54:31 +0000 UTC
4452,CLOSED,Question: support complex model out data struct (List[Dict[str, Tensor]])?,question,2022-05-31 19:00:45 +0000 UTC
4451,OPEN,Torchscript backend **MUCH** slower only with FP16 on 1650,investigating,2023-02-24 21:48:34 +0000 UTC
4450,CLOSED,Build simple.cc,,2022-05-31 15:43:11 +0000 UTC
4449,CLOSED,Torchscript backend error in multi-gpu environment,,2022-06-08 13:00:16 +0000 UTC
4448,CLOSED,Tritonclient cushm on multi-gpu server,,2022-06-05 12:41:57 +0000 UTC
4447,CLOSED,Issue with running .trt model,,2022-06-14 15:00:48 +0000 UTC
4445,CLOSED,model expected the shape of dimension 0 to be between 1 and 1 but received 32,,2022-06-16 00:01:48 +0000 UTC
4444,CLOSED,Download nvcr.io/nvidia/tritonserver:22.04-py3 file,,2022-05-30 10:12:49 +0000 UTC
4443,CLOSED,[Question] Reducing payload size,,2022-06-02 15:33:54 +0000 UTC
4436,CLOSED,WSL2 CUDA SHM support,,2022-12-28 18:30:58 +0000 UTC
4434,CLOSED,WSL2 CUDA SHM support,,2022-05-27 17:32:03 +0000 UTC
4433,CLOSED,run ci test fail,,2022-06-11 05:12:34 +0000 UTC
4432,OPEN,Internal: An input of type 'Tensor[]' was detected in the model. Only a single input of type Dict(str, Tensor) or input(s) of type Tensor are supported.,enhancement,2022-12-29 23:12:58 +0000 UTC
4430,CLOSED,Triton inference is slow then normal pytorch model on GPU, http_server.cc:1226] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU,,2022-05-26 05:30:56 +0000 UTC
4429,CLOSED,Triton inference is slow then normal pytorch model on GPU http_server.cc:1226] HTTP: unable to provide 'OUTPUT__0' in GPU, will use CPU,,2022-05-25 15:33:13 +0000 UTC
4425,CLOSED,Triton docker client image does not have cmake installed (/usr/bin/cmake),,2022-05-25 15:02:23 +0000 UTC
4422,CLOSED,Reduce load/unload model time,,2022-11-22 03:21:56 +0000 UTC
4421,CLOSED,config.pbtxt format in Python Protobuf text_format,,2022-05-25 00:36:31 +0000 UTC
4416,CLOSED,Allow loading/unloading of specific version for a given model,enhancement,2023-01-17 11:47:28 +0000 UTC
4415,CLOSED,how to deploy stateful model without implicit state management,,2022-06-01 20:32:35 +0000 UTC
4414,CLOSED,[mlflow-plugin] create a separate repository for mlflow-triton-plugin,,2022-11-22 03:21:42 +0000 UTC
4412,CLOSED,Cannot warmup with python backend with batch_size,,2022-07-08 19:02:04 +0000 UTC
4411,CLOSED,Segmentation fault, Core dumped during inference tensorrt model,,2022-05-23 11:18:04 +0000 UTC
4408,CLOSED,GPT NeoX 20B,question,2023-03-03 00:53:04 +0000 UTC
4407,CLOSED,Cannot build triton docker container due GPG keys rotation,,2022-07-08 19:02:48 +0000 UTC
4406,CLOSED,Parse error at offset 0: The document is empty.,,2022-07-08 19:00:24 +0000 UTC
4405,CLOSED,Shape does not match with data while using kserve http protocol BYTES data type,bug,2022-07-14 20:59:59 +0000 UTC
4404,CLOSED,Option to return non binary data from when sending raw binary request,,2022-06-08 21:45:23 +0000 UTC
4402,CLOSED,L0_https Client Example Hangs,,2022-05-31 23:06:23 +0000 UTC
4400,CLOSED,Dynamically passing model parameters in inference request,,2022-06-08 21:47:58 +0000 UTC
4399,CLOSED,pytorch_backend build fail,,2022-06-07 08:43:48 +0000 UTC
4398,CLOSED,Using custom Python backend stub causes no such file or directory error,,2022-05-19 12:47:46 +0000 UTC
4397,CLOSED,[question] about the GPU metrics,question,2022-07-11 22:41:43 +0000 UTC
4396,CLOSED,Run dynamic shapes engine in Triton Server,,2022-05-18 23:28:16 +0000 UTC
4394,CLOSED,Triton Crashes with onnxruntime error,,2022-07-08 19:37:48 +0000 UTC
4391,CLOSED,python backend example for BERT like architecture,,2022-07-08 19:38:52 +0000 UTC
4390,CLOSED,Internal: An input of type 'str' was detected in the model. Only a single input of type Dict(str, Tensor) or input(s) of type Tensor are supported.,,2023-07-06 13:51:28 +0000 UTC
4389,CLOSED,[Question] perf_analyzer, "Server Compute Input" meaning,question,2022-05-17 18:33:28 +0000 UTC
4388,CLOSED,What was the reason for designing Implicit State Management,question,2022-05-20 07:24:09 +0000 UTC
4387,CLOSED,Tritonbackend multiple instances of the same model run sequentially instead of in parallel on the same device with asyncrhonous requests.,,2023-01-03 01:44:37 +0000 UTC
4386,CLOSED,Why is an extra 1 added to the first element of the result of model simple_squence?,question,2022-05-20 07:25:53 +0000 UTC
4385,CLOSED,pytorch_backend build fail,question,2022-07-14 23:11:17 +0000 UTC
4384,CLOSED,Request specifies invalid shape for input 'images' for yolov5x6_tensorrt. Error details: model expected the shape of dimension 0 to be between 1 and 1 but received 32,,2022-07-08 19:47:05 +0000 UTC
4380,CLOSED,GPG Keys Expired For Apt Packages,,2022-05-19 12:52:09 +0000 UTC
4379,CLOSED,Response cache is not working well,bug,2022-08-03 21:44:38 +0000 UTC
4378,OPEN,perf_analyzer socket closed error,investigating,2022-11-01 00:18:09 +0000 UTC
4376,CLOSED,docker run not working,,2022-07-08 19:52:59 +0000 UTC
4375,CLOSED,[Question] Incorrect generation of model configuration for ONNX model,,2022-05-13 16:08:05 +0000 UTC
4374,CLOSED,Using custom Python backend stub causes Operation not permitted error,,2022-11-22 03:21:25 +0000 UTC
4373,CLOSED,[Question] Setting dynamic batching with warmup,,2022-06-02 15:33:14 +0000 UTC
4372,CLOSED,Possible GPU memory leak in Triton. Not draining.,,2022-11-30 08:31:03 +0000 UTC
4371,CLOSED,Triton Server build with Docker fails,,2022-05-17 08:48:47 +0000 UTC
4367,CLOSED,Parallel model loading on multiple GPUs on startup,enhancement,2023-06-26 20:34:29 +0000 UTC
4366,CLOSED,[Feature Request] Add load_model api to BLS triton_python_backend_utils,enhancement,2023-06-29 20:51:03 +0000 UTC
4365,CLOSED,[Question] Plugins from MMDeploy,question,2023-01-30 18:58:37 +0000 UTC
4363,OPEN,Triton Batch size feature requests,enhancement,2022-05-11 23:44:36 +0000 UTC
4362,OPEN,"inference failed: response output count mismatch" from gRPC client after enabling response cache,bug,2022-09-11 01:54:10 +0000 UTC
4361,CLOSED,Why triton serving shared memory failed with running multiple workers in uvicorn in order to send multiple request concurrently to the models?,,2022-09-30 22:45:17 +0000 UTC
4353,CLOSED,A few questions on C++ API,,2022-05-10 09:49:20 +0000 UTC
4352,CLOSED,Accept JSON files as configuration files,enhancement,2022-05-10 01:45:14 +0000 UTC
4351,OPEN,Multiple configuration files for the same model,enhancement,2023-06-02 01:35:16 +0000 UTC
4350,CLOSED,Poor Performance on Triton vs inferencing w/o triton,,2022-05-27 19:46:02 +0000 UTC
4346,CLOSED,Loading ONNX model fails because of insufficient CUDA driver version,bug,2022-05-11 23:50:24 +0000 UTC
4345,CLOSED,Loading ONNX model fails because of insufficient CUDA driver version,,2022-05-06 11:06:24 +0000 UTC
4344,CLOSED,Is there any scheduling strategy that drops old requests?,question,2022-05-27 19:49:06 +0000 UTC
4341,CLOSED,Can ensemble model support to handle multiple inference requests simultaneously?,,2022-10-10 16:20:15 +0000 UTC
4340,CLOSED,Python backend stub compilation fails because of TRITONSERVER_TYPE_BF16,,2022-05-06 17:35:22 +0000 UTC
4335,CLOSED,Docs link error,,2022-05-04 19:54:12 +0000 UTC
4333,CLOSED,Build failure when building PyTorch CPU tritonserver container: /usr/bin/ld: cannot find -ltorch,,2022-05-12 20:36:47 +0000 UTC
4332,CLOSED,Third party won't build with clang 13+,,2022-11-22 03:20:19 +0000 UTC
4331,CLOSED,Python backend stuck at TRITONBACKEND_ModelInstanceInitialize,bug,2023-01-27 16:42:04 +0000 UTC
4330,CLOSED,Model warmup fails, yet load is reported successful,bug,2022-05-19 18:19:21 +0000 UTC
4329,CLOSED,fp16 onnx model does not load on triton server,,2022-05-23 21:42:08 +0000 UTC
4328,CLOSED,Support for dynamic shapes with TFTRT model,,2022-11-22 03:21:08 +0000 UTC
4321,CLOSED,[Question] Semantic Segmentation : Add an argmax layer,question,2022-05-12 08:37:12 +0000 UTC
4319,CLOSED,how multiple instances on the same device can be concurrent,question,2023-06-21 21:38:02 +0000 UTC
4310,CLOSED,image_client.py example is broken for multiple classification outputs,bug,2022-05-04 17:55:59 +0000 UTC
4308,CLOSED,how can i use custom tensorrt backends,question,2022-05-27 19:46:30 +0000 UTC
4306,CLOSED,Triton Server build failed because DCGM 2.2.9 public key is not available,,2022-05-27 19:47:37 +0000 UTC
4303,OPEN,[Question] explicit model control mode - load models in parallel,enhancement, investigating,2023-07-08 00:28:43 +0000 UTC
4302,CLOSED,"no module named tensorflow" when running Triton server w/ python backend,,2022-04-28 19:59:45 +0000 UTC
4300,CLOSED,Torchscript model loading throws error UNAVAILABLE: INTERNAL: An Input of type 'Tensor?' was detected in the model,,2022-04-29 18:25:40 +0000 UTC
4299,CLOSED,Run tritonserver on Windows10 but it exited without any error,,2022-06-23 02:16:01 +0000 UTC
4298,CLOSED,Running inference with Pytorch backend on Jetson nano,,2022-04-28 10:45:11 +0000 UTC
4297,CLOSED,Questions about performance test results,question,2022-06-08 21:53:08 +0000 UTC
4293,CLOSED,ONNX with TensorRT Optimization (ORT-TRT) Warmup,,2022-09-06 23:45:03 +0000 UTC
4288,CLOSED,Triton inference server container freezes on startup,,2022-05-27 19:49:46 +0000 UTC
4287,CLOSED,Docker A2 GPU support,,2022-04-27 07:28:54 +0000 UTC
4286,CLOSED,How to connect Triton client library to my C++ client project?,,2022-06-05 03:34:09 +0000 UTC
4285,CLOSED,Use custom integer data for warmup,question,2022-06-08 21:54:09 +0000 UTC
4284,CLOSED,Custom Metrics per server,question,2022-05-23 21:40:45 +0000 UTC
4278,CLOSED,model inference of yolov5 torchscript format runs slow,,2022-05-12 00:57:16 +0000 UTC
4277,CLOSED,I want to load from a serialized model_config message with the Triton Server API.,,2022-05-23 21:47:53 +0000 UTC
4276,CLOSED,nvidia-triton flower demo performace,,2023-04-28 13:11:52 +0000 UTC
4275,CLOSED,Abnormal gpu memory usage,,2022-05-23 21:40:06 +0000 UTC
4274,CLOSED,Proper Densenet_onnx Classification Input File Format Error,,2022-05-24 14:15:57 +0000 UTC
4273,CLOSED,【question】cross compile for riscv64,,2022-05-18 08:07:56 +0000 UTC
4272,CLOSED,tritonserver: error while loading shared libraries: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: file too short,,2022-11-22 03:20:32 +0000 UTC
4270,CLOSED,Optimal Jetpack 4.x build,,2022-11-22 03:19:03 +0000 UTC
4269,CLOSED,PyTorch inference returns incorrect values,,2022-04-25 14:24:52 +0000 UTC
4266,CLOSED,Example in Python to create a pipeline with Gst-nvinferserver,,2022-04-24 12:22:08 +0000 UTC
4265,CLOSED,tritonclient.utils.InferenceServerException: [StatusCode.UNAVAILABLE] unavailable,,2022-11-22 03:18:44 +0000 UTC
4264,CLOSED,Load model parallelly,enhancement,2023-07-11 21:21:41 +0000 UTC
4263,CLOSED,compose.py failure: module 'build' has no attribute 'get_container_versions',bug,2022-04-22 21:28:04 +0000 UTC
4262,CLOSED,openvino 2022.01 support,enhancement,2022-06-09 00:35:59 +0000 UTC
4261,CLOSED,Cannot import tritonclient.grpc after using conda pack,,2022-05-13 23:01:12 +0000 UTC
4260,CLOSED,inference failed: ensemble unexpected deadlock,,2022-11-22 03:15:43 +0000 UTC
4255,CLOSED,How to run inference for T5 tensorrt model deployed on nvidia triton?,,2023-04-07 06:55:03 +0000 UTC
4254,CLOSED,How to test triton-inference-server with jmeter,,2022-05-13 22:58:51 +0000 UTC
4250,CLOSED,Can you tell me the meaning of overhead in perf_analyzer report,question,2022-04-26 01:01:38 +0000 UTC
4249,CLOSED,Can Triton support TensorFlow1.10.0?,,2022-04-19 16:20:04 +0000 UTC
4248,CLOSED,How to maintain confidentiality of models in local deployments,,2023-01-14 09:28:17 +0000 UTC
4247,CLOSED,torchscript model don't use Dynamic Batching,,2022-04-22 16:07:44 +0000 UTC
4245,CLOSED,Clean and Concise documentation,,2022-11-11 08:56:23 +0000 UTC
4244,CLOSED,400 Error during Inference,,2022-11-22 03:18:50 +0000 UTC
4243,CLOSED,Test L0_memory_growth failed to load all models for tritonserver image including only onnx and python backends,,2022-05-16 21:36:53 +0000 UTC
4242,CLOSED,Test L0_custom_ops failed to load all models for tritonserver image including only onnx and python backends,,2022-04-18 15:42:20 +0000 UTC
4241,CLOSED,Test L0_https failed to load all models for tritonserver image including only onnx and python backends,,2022-04-18 15:45:06 +0000 UTC
4240,CLOSED,Test L0_parallel_copy failed to load all models for tritonserver image including only onnx and python backends,,2022-04-18 15:46:21 +0000 UTC
4239,CLOSED,Test L0_large_payload failed to load all models for tritonserver image including only onnx and python backends,,2022-04-18 15:46:41 +0000 UTC
4238,CLOSED,Test L0_model_config failed to load all models for tritonserver image including only onnx and python backends,,2022-07-12 21:37:49 +0000 UTC
4237,CLOSED,Test L0_output_name failed to load all models for tritonserver image including only onnx and python backends,,2022-04-18 15:49:11 +0000 UTC
4236,CLOSED,Test L0_nullchar_string failed to load all models for tritonserver image including only onnx and python backends,,2022-04-18 15:49:29 +0000 UTC
4235,CLOSED,Test L0_multi_server failed for tritonserver image including only onnx and python backends,,2022-04-18 15:49:52 +0000 UTC
4234,CLOSED,triton python backend load time of pytorch model is 4x slower than an ONNX model load time.,,2022-11-22 03:21:34 +0000 UTC
4233,CLOSED,Test L0_grpc failed to load all models for tritonserver image including only onnx and python backends,,2022-04-18 16:06:04 +0000 UTC
4232,CLOSED,Test L0_http failed to load all models for tritonserver image including only onnx and python backends,,2022-04-18 16:05:52 +0000 UTC
4231,CLOSED,Test L0_backend_python is coupled with AWS S3,,2022-04-18 16:09:06 +0000 UTC
4225,CLOSED,[Question]The performance of triton-server itself,question,2022-05-31 16:47:27 +0000 UTC
4223,OPEN,Python InferenceServerClient (http) should not call close() from __del__,investigating,2023-03-29 13:54:47 +0000 UTC
4221,CLOSED,Serverside Postprocessing,question,2022-04-15 00:19:25 +0000 UTC
4220,CLOSED,[FEATURE REQUEST] Support types supported by TensorRT,,2022-04-15 12:51:16 +0000 UTC
4219,CLOSED,Async requests not increasing throughput with high network latency,,2022-07-12 17:52:52 +0000 UTC
4215,CLOSED,Python Model with BLS: failed to get cuda pointer device attribute,bug,2022-06-24 14:50:11 +0000 UTC
4214,CLOSED,[mlflow-triton-plugin] wrong "backend" value when deploying in onnx flavor,,2022-04-13 11:49:47 +0000 UTC
4213,CLOSED,Tritonserver Log: model-level grading,enhancement,2022-08-18 18:05:29 +0000 UTC
4212,CLOSED,Custom ops with LD_PRELOAD trick failed,,2022-05-13 21:42:22 +0000 UTC
4206,CLOSED,Can Triton improve QPS by increasing CPU utilization?,,2022-05-06 01:48:01 +0000 UTC
4205,CLOSED,Launch the TIS encountering the issue of "Unable to get power limit for GPU 0: Not Supported",,2022-04-15 05:35:38 +0000 UTC
4203,CLOSED,Batching by any axis,enhancement,2022-04-20 09:00:26 +0000 UTC
4202,CLOSED,feat: multiple triton server can bind on the same http/grpc port,enhancement,2022-08-09 15:41:29 +0000 UTC
4201,CLOSED,python backend: BLS triton_python_backend_utils support url arguments,question,2022-04-13 02:23:12 +0000 UTC
4200,CLOSED,Failed to load tensorflow savedmodel,,2022-04-12 07:38:30 +0000 UTC
4199,CLOSED,tritonserver ensemble SEGV in nvidia::inferenceserver::RateLimiter::EnqueuePayload,bug,2022-05-02 16:18:37 +0000 UTC
4198,CLOSED,how to ensemble models with detection and classsification,question,2023-02-23 17:06:56 +0000 UTC
4197,CLOSED,Triton dosen't start in kubernetes,,2022-04-10 17:11:02 +0000 UTC
4196,CLOSED,Failed to update context stat: Timer not set correctly. Send time from 1649545273035059093 to 0.,,2022-05-13 21:40:52 +0000 UTC
4194,CLOSED,UNAVAILABLE: Invalid argument: model 'keypoints_pose_0', tensor 'input.1': the model expects 4 dimensions (shape [1,3,224,224]) but the model configuration specifies 4 dimensions (shape [1,3,224,244]),,2022-04-09 02:36:41 +0000 UTC
4193,CLOSED,[request] Able to dynamically load ensembles without polling for model dependencies,,2022-04-10 04:01:40 +0000 UTC
4188,CLOSED,[request] automatic on-the-fly model load/unload based on init & process resources,,2022-05-23 22:12:42 +0000 UTC
4187,CLOSED,model config is unavaible to client if model is not loaded by the server,,2022-04-08 18:06:36 +0000 UTC
4184,CLOSED,Will you submit the min-image for history triton?,,2022-04-13 03:22:43 +0000 UTC
4183,CLOSED,Need minimum CI test set to validate customized tritonsever gpu image built using optional features,,2022-05-13 20:55:01 +0000 UTC
4179,CLOSED,Triton server container lockup on stop when pinned memory is too big,investigating,2022-06-06 12:50:07 +0000 UTC
4178,CLOSED,[Question] The way of working of sequence_end control signal in sequece batcher,question,2022-04-14 09:09:06 +0000 UTC
4172,CLOSED,Jetson PyTorch wheel broken link,bug,2022-04-07 17:13:14 +0000 UTC
4170,CLOSED,sometimes slower response time with smaller batch size,question,2022-04-10 05:09:24 +0000 UTC
4168,CLOSED,HTTP ERROR 400 when load or unload model,,2022-04-06 06:06:35 +0000 UTC
4165,CLOSED,Not able to use S3 bucket as a model storage,question,2022-04-19 10:16:42 +0000 UTC
4162,OPEN,Support for tfio in tensorflow backend,enhancement,2022-04-07 00:44:10 +0000 UTC
4160,CLOSED,Specific item names cause Triton placeholders to not work in a GUI,,2022-04-05 14:46:34 +0000 UTC
4154,OPEN,pinned_memory_manager Killed,bug,2022-04-15 17:43:26 +0000 UTC
4153,CLOSED,Previous success build of full tritonserver failed for most recent release branches (r21.12...r22.03),bug, investigating,2022-05-02 22:25:33 +0000 UTC
4152,CLOSED,ValueError: assignment destination is read-only,question,2022-04-18 16:13:38 +0000 UTC
4151,CLOSED,Cannot start python-backend with TYPE_GPU instance type,question,2022-04-12 07:39:17 +0000 UTC
4150,CLOSED,Optional input for python backend,bug,2022-09-29 22:47:55 +0000 UTC
4149,CLOSED,Can't find tritonclient.utils.shared_memory on WIN10,,2022-04-04 18:40:50 +0000 UTC
4145,CLOSED,python backend stuck in Starting Python backend stub when launching multiple servers simultaneously,,2022-04-04 23:08:52 +0000 UTC
4144,CLOSED,How should I request a model whose input is a dictionary？,,2022-10-27 03:45:25 +0000 UTC
4142,CLOSED,Triton inference time extremely slow at scale,,2022-06-27 14:54:22 +0000 UTC
4139,CLOSED,Request custom model metadata,,2022-04-22 15:58:06 +0000 UTC
4138,CLOSED,[Question] memory consumption of model loading for different instance_group count,,2022-03-31 18:00:51 +0000 UTC
4137,CLOSED,Q. Is BLS and model excution run with pipeline parallelism,,2022-09-10 02:40:52 +0000 UTC
4134,CLOSED,Triton hangs on tensorflow1 backend cpu-only build,,2022-03-31 17:53:04 +0000 UTC
4133,CLOSED,Dynamic Batching not creating batches correctly and incorrect inference results,bug,2022-06-24 21:44:43 +0000 UTC
4132,CLOSED,python backend always loading!,,2022-03-31 13:49:41 +0000 UTC
4131,CLOSED,Grpc compression increases the latency quite a lot?,,2022-04-04 18:48:12 +0000 UTC
4130,CLOSED,error creating a triton deployment mlflow plugin,investigating,2022-04-08 20:31:35 +0000 UTC
4129,CLOSED,Dynamic Batching is not creating batches during inference,,2022-03-30 16:04:52 +0000 UTC
4127,CLOSED,perf_analyzer failed with --shared-memory=cuda,,2022-03-30 00:58:11 +0000 UTC
4126,CLOSED,pytorch backend：backend_memory.cc:177] failed to free CUDA memory: an illegal memory access was encountered,,2022-03-31 22:56:58 +0000 UTC
4118,CLOSED,[Feature Request] Allow ensemble model's sub-models to be inside the model dir,,2022-05-13 19:22:43 +0000 UTC
4113,CLOSED,Endless wait when loading Python backend model on Jetson - v2.19.0,bug,2022-04-06 15:17:25 +0000 UTC
4112,CLOSED,inference failed: PyTorch execute failure: Global alloc not supported yet,,2022-04-04 13:50:21 +0000 UTC
4111,CLOSED,how should i run the fastertransformer(FT) custom op with TIS?,,2022-03-28 04:42:56 +0000 UTC
4105,CLOSED,ONNX TensorRT gives widely different result for fp16 quantized CLIP text embedding,question,2022-05-18 23:52:20 +0000 UTC
4104,CLOSED,Post Processing with Triton Ensemble,,2022-03-25 18:14:33 +0000 UTC
4103,CLOSED,No response after a long period,,2022-04-09 01:37:52 +0000 UTC
4102,CLOSED,Deploy Triton server with MinIO as Model Store,,2022-03-25 05:42:30 +0000 UTC
4101,CLOSED,Failed to load model - Unknown Builtin Op Torch Sparse,,2022-03-25 18:06:50 +0000 UTC
4096,CLOSED,Client Hung,,2022-03-28 20:48:37 +0000 UTC
4095,OPEN,Is it possible to make gRPC to use a unix socket instead of TCP in Triton Server?,enhancement,2022-09-01 17:05:35 +0000 UTC
4094,CLOSED,Some confusions about MessageQueue in python backend.,,2022-03-24 01:47:29 +0000 UTC
4089,CLOSED,Input to the script for publishing models to mlflow is overly particular with inputs,bug,2022-04-22 15:58:55 +0000 UTC
4088,CLOSED,Request to cherry-pick fixes in tensorrt-backend for 22.03,,2022-03-22 21:09:04 +0000 UTC
4087,CLOSED,Device memory is insufficient for Jetson example,,2022-03-23 09:17:40 +0000 UTC
4085,CLOSED,YoloV4 Inference with Triton produces different output than with TensorRT,,2022-04-07 06:51:17 +0000 UTC
4082,OPEN,feat: Add `TYPE_STRING` support to PyTorch backend,enhancement,2022-05-30 22:47:01 +0000 UTC
4081,CLOSED,object class 'GstNvInferServer' has no property named 'input-tensor-meta',question,2022-03-21 19:10:54 +0000 UTC
4079,CLOSED,Triton Inference Server taking adding 3 seconds to get YOLOv4 Inference,,2022-03-31 05:52:32 +0000 UTC
4078,CLOSED,POSTing Base64 image to Triton running YOLOV4/TensorRT model,,2022-08-09 02:21:56 +0000 UTC
4072,OPEN,Support connection strings for Azure-backed modelrepo,enhancement,2022-05-24 17:36:47 +0000 UTC
4068,CLOSED,Perf_Analyzer always throwing 'std::length_error',bug,2022-03-31 17:18:26 +0000 UTC
4067,CLOSED,build the server in a container,,2022-04-07 01:25:46 +0000 UTC
4066,CLOSED,Perf_Analyzer requires libcudart,,2022-03-29 23:31:34 +0000 UTC
4065,CLOSED,Model Configuration is wrongly restrictive,,2023-05-23 04:57:46 +0000 UTC
4064,CLOSED,MOT model deploy error,,2022-08-09 07:15:41 +0000 UTC
4062,CLOSED,Please add option to specify `grpc.default_authority` when creating python InferenceServerClient,enhancement,2022-05-10 04:01:55 +0000 UTC
4061,CLOSED,Error and server shutdown on http request.,,2023-01-31 09:05:36 +0000 UTC
4059,OPEN,Understanding Trace File,bug,2022-06-27 03:27:43 +0000 UTC
4083,CLOSED,RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`,,2022-04-07 01:35:20 +0000 UTC
4058,CLOSED,Server unloads models automatically after starting,,2022-03-15 05:08:13 +0000 UTC
4057,CLOSED,Build openvino backend fail in container,question,2022-03-15 16:40:26 +0000 UTC
4053,CLOSED,【Question】how to set the ensemble_scheduling when do inference of multiple models at the same time,,2022-03-29 23:50:49 +0000 UTC
4052,CLOSED,TorchScript model inference grid sampler error,question,2022-04-18 16:18:15 +0000 UTC
4051,CLOSED,[Question]Can TRITON POLL MODE hot-update the ensemble model?,bug, investigating,2022-05-31 17:54:09 +0000 UTC
4050,CLOSED,Failed with Jetson NX using tensorrt model and docker from nvcr.io/nvidia/tritonserver:22.02-py3,,2022-03-17 07:50:26 +0000 UTC
4049,OPEN,[feature request] classification by axis support in classification extension,enhancement,2022-10-02 07:17:23 +0000 UTC
4048,CLOSED,Onnx batchsize greater than 1,question,2022-03-15 16:40:49 +0000 UTC
4046,CLOSED,Triton server crashed unexpectedly during loading TensorRT models,,2022-03-15 08:33:41 +0000 UTC
4045,CLOSED,Auto-Setting upstream container version in build.py for no container build,bug,2022-04-13 15:33:29 +0000 UTC
4044,CLOSED,Question on Concurrent Execution on the same GPU device,question,2022-03-25 16:48:53 +0000 UTC
4043,CLOSED,Python Backend can not be loaded!!,,2022-03-17 09:08:55 +0000 UTC
4042,CLOSED,tensorflow_text support for Triton,,2022-08-20 00:05:28 +0000 UTC
4038,CLOSED,How to set dynamic batching for tensorrt model?,question,2022-03-11 01:24:26 +0000 UTC
4037,CLOSED,when i increase the number of instance_group,the latency is not decrease. i use tensorrt platform,,2022-05-17 18:06:11 +0000 UTC
4036,CLOSED,Expose gRPC channel options,enhancement,2022-05-20 19:11:30 +0000 UTC
4035,CLOSED,Possible to write dirty data in deprecated share memory in python backend?,,2022-03-10 15:12:43 +0000 UTC
4030,CLOSED,Permission denied message,question,2022-03-10 22:55:10 +0000 UTC
4028,CLOSED,Can we load TensorRT model within the self.initialize method of the TritonPythonModel?,question,2022-03-11 00:49:17 +0000 UTC
4027,CLOSED,How to build server source code in a container?,question,2022-03-11 16:32:56 +0000 UTC
4026,CLOSED,Ensemble model using BLS: stub unhealthy,bug,2023-03-17 15:40:30 +0000 UTC
4023,CLOSED,Graceful handing of oom errors,enhancement,2022-03-09 18:47:15 +0000 UTC
4022,CLOSED,Triton Python backend not able to use conda env built on different OS,wontfix,2022-03-09 02:33:49 +0000 UTC
4021,CLOSED,ONNX CPU slower performance with a series of classification requests vs single one,question,2022-05-17 18:06:51 +0000 UTC
4020,CLOSED,Dynamically load multiple instances for the same model,question,2022-03-10 08:05:53 +0000 UTC
4019,CLOSED,Inference time with triton server is more than the inference time without triton,performance,2022-03-25 16:45:31 +0000 UTC
4018,CLOSED,ONNX configuration example,,2022-03-08 09:11:48 +0000 UTC
4017,CLOSED,tritonserver exited with coredump when using cuda graph optimization,bug,2022-04-09 07:02:20 +0000 UTC
4016,CLOSED,Segmentation fault (core dumped),,2022-03-07 21:35:30 +0000 UTC
4015,CLOSED,How to use Triton Inference Server with Docker Swarm,question,2022-03-10 17:02:05 +0000 UTC
4014,CLOSED,[Question] - Is it possible to cache inferences for TTS,,2022-03-25 16:44:34 +0000 UTC
4013,CLOSED,run c++ example code on win10,,2022-03-08 00:59:59 +0000 UTC
4011,CLOSED,YOLOv5x ONNXRuntime with OpenVINO EP failed - need to upgrade OpenVINO EP,,2022-03-10 04:53:39 +0000 UTC
4010,CLOSED,triton server failed exited with coredump,,2022-03-25 16:50:27 +0000 UTC
4008,CLOSED,Signal 11 received and server down when inferring bert model,,2022-04-22 16:00:13 +0000 UTC
4007,CLOSED,quantized model inference slow with Triton server than inference directly in python code,,2022-06-08 23:19:16 +0000 UTC
4003,CLOSED,Revise build environment and release Debian package to improve integration with existing systems,,2022-04-11 16:41:45 +0000 UTC
4002,CLOSED,(triton-third-party) Azure-storage-cpplite dependency will not build with tag 0.3.0,bug, investigating,2022-03-15 14:51:29 +0000 UTC
4001,CLOSED,(Question) Is there existing a way to reschedule requests in concurrent model execution?,,2022-03-02 16:53:19 +0000 UTC
3998,CLOSED,Question: Is it possible to access request body 'parameters' in the python backend?,enhancement, question,2023-03-15 22:47:23 +0000 UTC
3997,CLOSED,Memory not released,bug, investigating,2023-02-09 04:03:56 +0000 UTC
3996,CLOSED,Tensorflow2 backend UNAVAILABLE: Not found: unable to load shared library: libnccl.so.2,,2022-03-02 18:23:55 +0000 UTC
3994,CLOSED,How to get the time each inference cost?,,2022-03-02 01:50:38 +0000 UTC
3992,CLOSED,Python backend BLS unable to handle response from GPU model,,2022-04-11 19:00:36 +0000 UTC
3990,CLOSED,Ask about the meaning of output.,,2022-03-01 01:37:22 +0000 UTC
3989,CLOSED,Deploying my own crnn model using triton, the output has the wrong shape,,2022-03-07 02:30:23 +0000 UTC
3987,CLOSED,Python client shm functions set_shared_memory_region and get_contents_as_numpy should support offset !=0,,2022-03-15 23:59:07 +0000 UTC
3986,CLOSED,Set_shared_memory of the class InferInput in python client doesn't support offset !=0,,2022-03-15 23:58:39 +0000 UTC
3985,CLOSED,How to download dependency in advance?,question,2022-03-02 06:40:39 +0000 UTC
3984,OPEN,Batching support by stacking input arrays in python backend,enhancement,2023-04-14 07:32:27 +0000 UTC
3980,CLOSED,CPU-only mode unable to load Models got CUDA error,,2022-11-16 02:41:02 +0000 UTC
3979,CLOSED,Support for OpenPPL Backend,question,2022-05-17 18:32:40 +0000 UTC
3978,CLOSED,Triton cannot inference `tf.math.l2_normalize` correctly from ngc 21.06 ~ ngc 22.03 ( triton 2.20.0),bug, investigating,2022-08-04 17:05:11 +0000 UTC
3976,CLOSED,Edge compute using Transform4rec models with ONNX runtime,,2022-04-12 01:03:54 +0000 UTC
3973,CLOSED,Update python preprocessor example to showcase batch processing,,2022-05-13 21:38:27 +0000 UTC
3972,OPEN,Way to capture headers in model.py when using python backend,enhancement,2023-03-07 23:32:45 +0000 UTC
3971,CLOSED,Auto generated model configuration for custom backend,question,2022-02-28 01:49:57 +0000 UTC
3970,CLOSED,Cannot get CUDA device count, GPU metrics will not be available on multi-gpus,bug,2022-05-19 00:51:01 +0000 UTC
3968,CLOSED,error when trying to allocate a region of cuda shared memory,,2022-02-22 19:11:35 +0000 UTC
3967,CLOSED,Error about layer datatype when load onnx model,,2022-02-24 22:58:17 +0000 UTC
3965,CLOSED,ONNX Backend Installation Error,,2023-04-27 05:23:18 +0000 UTC
3962,CLOSED,Build trtion server image with container failed,bug,2022-02-23 19:19:56 +0000 UTC
3961,OPEN,python backend: how does the conda environment support multiple versions,enhancement,2022-02-19 22:06:11 +0000 UTC
3960,OPEN,Request for non-gpu version docker image (to decrease the image size),enhancement,2023-02-03 06:31:38 +0000 UTC
3955,CLOSED,Triton server - required NVIDIA driver version vs CUDA minor version compatibility,question,2022-02-25 05:40:17 +0000 UTC
3953,CLOSED,inference queue time high,,2022-09-09 23:42:35 +0000 UTC
3952,CLOSED,Bug for building triton server with onnx backend with docker,,2022-03-10 04:57:02 +0000 UTC
3951,CLOSED,Questions about ragged batching of higher dimensional tensors,,2022-02-28 06:19:17 +0000 UTC
3948,CLOSED,[client c++] build script uses system libcurl instead of own third-party/curl,bug,2022-03-03 11:20:19 +0000 UTC
3944,CLOSED,python backend error: c_python_backend_utils.TritonModelException: Tensor is stored in GPU and cannot be converted to NumPy,,2023-03-22 14:47:41 +0000 UTC
3942,CLOSED,[Question] Details examples for how to use the rate limiter?,question,2022-03-01 13:54:58 +0000 UTC
3941,CLOSED,[Question] About perf_analyzer request & execution count,,2022-06-07 02:02:13 +0000 UTC
3940,CLOSED,Docs for building custom TF backend are obsolete,bug,2022-03-10 23:28:23 +0000 UTC
3937,CLOSED,torchscripted model fails to load on triton server,,2022-02-27 14:46:55 +0000 UTC
3936,CLOSED,triton for windows(r22.01) build error,,2022-05-13 21:38:59 +0000 UTC
3935,CLOSED,Best practices for loading a new model version across Triton instances,question,2022-03-10 23:20:54 +0000 UTC
3930,OPEN,VPU support for OpenVINO backend,enhancement,2022-02-14 21:22:15 +0000 UTC
3929,CLOSED,Same output for every batches when using shared memory,bug, investigating,2022-03-04 10:14:59 +0000 UTC
3928,CLOSED,The error means that the trt plan does not support the shape [16,128].,,2022-04-11 09:50:52 +0000 UTC
3927,CLOSED,Test “L0_trt_dla” failed in branch r21.12 due to missing model “resnet50_plan”,,2022-02-18 21:48:08 +0000 UTC
3926,OPEN,Does triton-inference-server run on Drive AGX?,enhancement,2022-02-18 21:52:36 +0000 UTC
3924,CLOSED,Build image "tritonserver_qa" failed due to “Dockerfile.QA” references to a non-existing folder,,2022-02-22 18:08:54 +0000 UTC
3923,CLOSED,Build tritonserver image failed due to hardcoded the”/tmp” folder in build files,enhancement,2022-03-14 20:11:06 +0000 UTC
3922,CLOSED,[client c++] Could not find a package configuration file provided by "RapidJSON",,2022-11-22 03:31:12 +0000 UTC
3920,CLOSED,NVIDIA Tesla T4 is not being used during inference,,2022-02-11 14:32:12 +0000 UTC
3919,CLOSED,UNAVAILABLE: Internal: trt failed to set binding dimension to [8,10,128] for input 'input_id' for paraRecognition,,2022-02-11 17:13:18 +0000 UTC
3918,CLOSED,Where to modify the apt sources in building such container?,,2022-02-17 19:47:53 +0000 UTC
3917,CLOSED,grpc node.js client unable to send uint32 inputs,,2022-11-22 03:18:17 +0000 UTC
3916,CLOSED,Triton Server Crash,,2022-03-01 19:13:14 +0000 UTC
3915,CLOSED,Use ensemble to start Python Backend and PyTorch Backend, prompting not supported for Pytorch Backend.,,2022-02-11 01:18:01 +0000 UTC
3914,CLOSED,Error: Failed to process the request(s). error: unpack_from requires a buffer of at least ...,,2022-02-13 13:23:35 +0000 UTC
3913,CLOSED,CPU-Only Image: Dockerfile to build them, or release to nvcr,,2022-02-28 16:34:59 +0000 UTC
3912,CLOSED,Model has kind KIND_GPU but no GPUs are available,question,2022-02-11 08:49:56 +0000 UTC
3909,CLOSED,TorchScript GELU error,pytorch ngc,2022-02-11 19:05:29 +0000 UTC
3908,CLOSED,Failed to build perf_analyzer on macOS,bug, enhancement, investigating,2022-04-05 19:44:29 +0000 UTC
3907,CLOSED,Question: support for older CPUs (no AVX),question,2022-02-09 16:52:16 +0000 UTC
3906,CLOSED,Triton tries to use the `tensorflow1` backend for `22.01-tf2-python-py3` image,question,2022-02-10 08:29:49 +0000 UTC
3905,CLOSED,Experiencing Bottlenecking at Scale - is it related to having a single gRPC connection?,question,2022-02-09 20:29:48 +0000 UTC
3904,CLOSED,Unclear TensorRT version match,question,2022-02-09 01:32:00 +0000 UTC
3903,CLOSED,Directing requests to correct triton deployment on kubernetes,question,2022-02-10 07:58:01 +0000 UTC
3902,CLOSED,Is there a way to load LD_PRELOAD plugins dynamically?,,2023-07-06 18:46:10 +0000 UTC
3901,OPEN,How to add reshape[] to states with implicit state management,enhancement,2022-02-08 16:26:34 +0000 UTC
3898,CLOSED,Client side ratio perf_ Analyzer requests are much slower,,2022-02-15 19:48:51 +0000 UTC
3897,CLOSED,DCGM_FI_DEV_GPU_UTIL for HPA is showing error "no metrics returned from custom metrics API",,2022-03-10 23:23:59 +0000 UTC
3896,CLOSED,Conditional model inference,,2022-02-07 14:20:06 +0000 UTC
3895,CLOSED,unexpected input format FORMAT_NONE, expecting FORMAT_NCHW or FORMAT_NHWC,,2022-03-10 23:24:15 +0000 UTC
3891,CLOSED,Pass outputs of one model to inputs of another in BLS,,2022-02-07 19:23:59 +0000 UTC
3885,CLOSED,Triton server not combined requests to batch in python backend,,2022-02-03 20:29:39 +0000 UTC
3884,CLOSED,triton server for jetpack not provided in release 2.18,,2022-02-28 16:37:09 +0000 UTC
3883,OPEN,tensorrt slower than onnx,bug,2022-08-18 11:30:47 +0000 UTC
3880,CLOSED,Batch Inference,,2022-02-02 23:19:43 +0000 UTC
3877,CLOSED,UNAVAILABLE: Internal: unable to create stream: the provided PTX was compiled with an unsupported toolchain,,2023-01-20 13:01:12 +0000 UTC
3869,CLOSED,Dynamic_batching not working correctly with tensorflow models,,2022-02-01 17:37:37 +0000 UTC
3866,CLOSED,Why the third party of grpc-new not appear to contain CMakeLists.txt,bug,2022-03-25 18:19:00 +0000 UTC
3865,CLOSED,[Question] how to update config.prototxt to support different model input shape,,2022-01-29 00:13:54 +0000 UTC
3864,CLOSED,Windows Dockerfile reference new cuDNN folder structure,,2022-01-31 21:26:48 +0000 UTC
3859,CLOSED,Incorrect order of outputs,,2022-01-27 18:55:07 +0000 UTC
3857,CLOSED,BLS script + FORCE_CPU_ONLY_INPUT_TENSORS -> output tensor from ORT is NEVER on GPU memory,,2022-02-01 21:53:11 +0000 UTC
3856,CLOSED,Single model scaling,,2022-07-06 16:10:14 +0000 UTC
3855,CLOSED,tensorrt slower than others,,2022-02-15 15:58:30 +0000 UTC
3854,OPEN,rate limiting based on number of requests,enhancement,2023-06-01 20:01:46 +0000 UTC
3852,CLOSED,modification of the dimension check in EvaluateTensorRTContext,bug, investigating,2022-03-16 00:53:07 +0000 UTC
3851,CLOSED,Why Triton's allow_ragged_batch feature doesn't works?,,2022-02-02 23:20:54 +0000 UTC
3847,CLOSED,Understanding Backends,,2022-03-15 10:45:17 +0000 UTC
3846,CLOSED,how to enable Dynamic batching for Ensembling models?,,2022-01-24 14:07:20 +0000 UTC
3845,CLOSED,E0124 07:16:50.138736 59 logging.cc:43] 1: [stdArchiveReader.cpp::StdArchiveReader::34] Error Code 1: Serialization (Serialization assertion safeVersionRead == safeSerializationVersion failed.Version tag does not match. Note: Current Version: 43, Serialized Engine Version: 0),,2022-02-01 17:39:30 +0000 UTC
3844,CLOSED,How do I create properly formatted input_data_file for warmup,investigating,2022-01-26 04:05:44 +0000 UTC
3842,CLOSED,Prometheus always shows one GPU in Triton on Kubernetes,,2022-01-21 17:39:03 +0000 UTC
3841,CLOSED,Pytorch backend forJetson Nano,,2022-01-21 18:16:07 +0000 UTC
3840,CLOSED,python model get stuck on instance initialization step,,2022-03-30 14:00:10 +0000 UTC
3839,CLOSED,allow_ragged_batch,,2022-01-28 18:05:48 +0000 UTC
3838,CLOSED,The problem of low cpu usage,,2022-02-09 20:42:11 +0000 UTC
3837,CLOSED,[Question] Is there data populating when request batch_size is less than tensorrt offline model batch_size,,2022-01-21 17:58:05 +0000 UTC
3834,CLOSED,[Question] Error when loading models with python backend,,2022-01-25 01:42:41 +0000 UTC
3833,CLOSED,Differing batch_size between input and output,,2022-01-21 15:56:24 +0000 UTC
3832,CLOSED,[question] model loading,,2022-01-20 23:34:12 +0000 UTC
3831,OPEN,Missing logs incase of incorrect model name,enhancement,2022-01-20 14:46:01 +0000 UTC
3830,CLOSED,[Question]: Fairseq model to Triton server,,2022-01-20 23:30:35 +0000 UTC
3825,CLOSED,Tensorflow Backend had an unexpected memory increase while updating models,question, investigating,2022-06-21 09:30:55 +0000 UTC
3824,CLOSED,server's NGC container has no source code and build tools like cmake.,question,2022-01-20 23:41:42 +0000 UTC
3822,CLOSED,[Question] Is ensemble model sequential in all cases?,,2022-01-20 23:31:11 +0000 UTC
3820,CLOSED,its easy to compilation failure in China,,2022-01-20 23:49:38 +0000 UTC
3818,CLOSED,How to understand queue_batch_size payload_batch_size pending_batch_size batch_size next_preferred_batch_size_ in dynamic_batch_scheduler.cc,,2022-01-21 00:07:25 +0000 UTC
3817,CLOSED,Possible Network Performance Bottleneck,,2022-02-28 16:35:41 +0000 UTC
3816,CLOSED,How to run tests after building tritonserver,question,2022-01-20 23:40:48 +0000 UTC
3813,CLOSED,Driver incompatibility for triton image on sagemaker instance,,2022-01-21 17:08:56 +0000 UTC
3808,CLOSED,Triton HTTP python client library call `get_result()` on `InferAsyncRequest` object in different thread results in `greenlet .error`,bug, question,2022-01-21 01:39:19 +0000 UTC
3807,CLOSED,Memory efficient BLS with input-dependent number of inference requests,,2022-05-02 16:27:11 +0000 UTC
3805,CLOSED,Failed to fetch anonymous token when trying to pull Triton Docker image in CI,bug,2023-05-02 19:32:22 +0000 UTC
3804,CLOSED,Question: Return label map along with predictions,question, investigating,2022-05-24 17:43:35 +0000 UTC
3802,CLOSED,Model load call is throwing error only on the first call `POST /v2/repository/models/{MODEL}/load`,bug, investigating,2022-03-10 18:33:47 +0000 UTC
3801,CLOSED,struct.error: unpack_from requires a buffer of at least 1150092984 bytes for unpacking 1150092980 bytes at offset 4 (actual buffer size is 97920),,2022-01-25 06:10:43 +0000 UTC
3800,CLOSED,`is_server_live()` python GRPC client got no response intermittently, while c++ is OK,bug, investigating,2023-03-13 22:39:17 +0000 UTC
3796,CLOSED,Nonlinear increase of throughput as the number of CPU instances increases,,2022-01-31 14:09:59 +0000 UTC
3795,CLOSED,Specifying model signatures in config.pbtxt serving TensorFlow Models,enhancement,2022-02-03 18:28:32 +0000 UTC
3794,CLOSED,Python backend ensemble model extra input params,question,2022-02-03 18:27:12 +0000 UTC
3787,CLOSED,Semantic segmentation model serving,bug, question, investigating,2022-01-21 18:07:23 +0000 UTC
3786,CLOSED,Allow tritonserver to stay up on model load failure?,question,2023-05-11 04:31:18 +0000 UTC
3784,CLOSED,Efficient Way to Send And Retrieve Image Inference Response,question,2022-10-08 07:52:35 +0000 UTC
3781,OPEN,Server goes down trying to predict on certain BERT based, TensorRT optimized model in Tenosrflow Savedmodel format,bug, investigating,2022-03-22 21:18:29 +0000 UTC
3779,CLOSED,python_backend consuming too much CPU without any incoming request,bug, investigating,2022-01-11 16:24:39 +0000 UTC
3777,CLOSED,free() invalid pointer,bug, investigating,2022-02-23 04:14:15 +0000 UTC
3774,CLOSED,why tensorrt is slow than onnx,,2022-01-21 23:36:21 +0000 UTC
3773,CLOSED,No CUDA-capable device is detected (CUDA_ERROR_NO_DEVICE) cuInit()=100,,2022-01-10 05:50:00 +0000 UTC
3770,CLOSED,Could not invoke stateful service in triton bls,bug,2022-01-24 03:51:54 +0000 UTC
3769,CLOSED,How to protect python model in triton server?,,2022-01-21 18:28:21 +0000 UTC
3765,OPEN,Standard Log format like JSON or XML,enhancement,2023-02-08 11:15:11 +0000 UTC
3764,CLOSED,when we can use `compose.py` to build the CPU-only containers?,,2022-02-09 15:49:44 +0000 UTC
3763,OPEN,21.12-py3 Server launching error when hosting a TRT model with custom plugin,enhancement,2022-01-31 21:22:50 +0000 UTC
3762,CLOSED,The set_data_from_numpy function of class InferInput returns a list of **Nonetypes**,,2022-01-21 01:30:40 +0000 UTC
3761,CLOSED,GPU memory never release.,,2022-12-06 17:38:26 +0000 UTC
3759,CLOSED,custom tritonserver build, but the protobuf version is too low, causing an error,,2022-01-05 01:30:50 +0000 UTC
3758,CLOSED,Memory not being released after triton inference - Python,bug, investigating,2022-01-11 00:03:28 +0000 UTC
3757,CLOSED,[Question] - Artifacts with Python backend,,2022-01-04 15:03:23 +0000 UTC
3756,CLOSED,Class labels are not returned when label_filename is provided,,2022-01-04 07:55:22 +0000 UTC
3755,CLOSED,[question][performance] Triton ensemble scheduling in parallel,question,2022-01-05 10:36:42 +0000 UTC
3754,CLOSED,/bin/bash: error while loading shared libraries: libnvinfer.so.8: cannot open shared object file: No such file or directory,,2022-01-03 16:44:11 +0000 UTC
3753,CLOSED,use docker pull triton on jetson,,2022-01-05 19:23:05 +0000 UTC
3752,CLOSED,[Question] How to serve sklearn preprocessing pipeline with triton,,2021-12-31 05:21:24 +0000 UTC
3750,CLOSED,pinned buffer: failed to perform CUDA copy: invalid argument,,2022-01-27 18:25:51 +0000 UTC
3749,CLOSED,Benchmark/Measure switching time between models,,2021-12-30 17:42:03 +0000 UTC
3748,CLOSED,What is the generation method of the first four bytes of bytestring?,,2021-12-30 23:21:29 +0000 UTC
3747,CLOSED,Sporadic streaming gRPC error "2 UNKNOWN: TRTIS response timeout",investigating,2022-01-05 13:32:14 +0000 UTC
3746,CLOSED,ERROR: infer_trtis_server.cpp:261 Triton: TritonServer response error received., triton_err_str:Internal, err_msg:PyTorch execute failure: Expected Tensor but got GenericDict,question,2022-01-03 21:28:32 +0000 UTC
3745,CLOSED,got error when output is zero rank in BLS,bug,2022-01-06 20:40:24 +0000 UTC
3744,CLOSED,Read-only file system error,,2021-12-30 10:47:55 +0000 UTC
3743,CLOSED,change the request input shape will make triton server hang up？,,2022-01-03 16:36:04 +0000 UTC
3742,CLOSED,Onnx with TensorRT in Windows 10 platform,,2021-12-31 10:19:59 +0000 UTC
3741,CLOSED,Question: ragged batch with ONNX backend,,2021-12-30 06:39:42 +0000 UTC
3740,CLOSED,'c++: fatal error: Killed signal terminated program cc1plus' while building the container,,2021-12-28 06:20:56 +0000 UTC
3739,CLOSED,tritonserver died after sometime,,2022-01-13 07:47:08 +0000 UTC
3738,CLOSED,Messages of `BUG: soft lockup` and freezing when stopping tritonserver container,bug,2022-03-10 23:27:28 +0000 UTC
3737,CLOSED,pytorch model error,,2021-12-30 06:50:47 +0000 UTC
3736,CLOSED,Why does TRITONSERVER_Server used like this?,,2021-12-29 17:48:24 +0000 UTC
3734,CLOSED,tensorrt plugin error？,question,2022-01-21 18:34:24 +0000 UTC
3733,CLOSED,S3 sync unloading all the models in triton,,2021-12-26 14:48:05 +0000 UTC
3732,CLOSED,how to send paramters between different models when use ensemble,,2021-12-23 07:42:07 +0000 UTC
3731,CLOSED,CPU only mode cannot load models, got CUDA error,,2022-02-24 09:59:09 +0000 UTC
3730,CLOSED,Failed to allocate memory for requested buffer of size 13565952,,2021-12-22 17:01:02 +0000 UTC
3729,CLOSED,How to implement custom pre/post processing in Triton inference server?,,2021-12-22 14:53:25 +0000 UTC
3728,CLOSED,[developing custom triton backend] Segmentation fault when return error in TRITONBACKEND_ModelInstanceInitialize,,2022-01-21 18:19:48 +0000 UTC
3727,CLOSED,Make grpc_client thread-safe,,2022-01-10 19:38:59 +0000 UTC
3725,CLOSED,Error configuring triton with s3 | Could not get MetaData for object at s3://,,2022-01-21 18:17:48 +0000 UTC
3719,CLOSED,[question] What is the server actions when got new requests immediately after the exiting,,2021-12-20 23:08:05 +0000 UTC
3718,CLOSED,Unable to load Openvino shufflenet model (Input 'axes' should be Constant.),,2021-12-23 07:51:20 +0000 UTC
3717,CLOSED,Segmentation fault when delete server through C API,bug,2022-03-17 06:09:44 +0000 UTC
3716,CLOSED,Starting a Triton Inference Server using Terraform on AWS throwing stub error,investigating,2021-12-23 07:07:45 +0000 UTC
3715,CLOSED,Windows Tensorflow and Pytorch support,,2021-12-29 17:32:24 +0000 UTC
3711,CLOSED,Server returns broken json requests when using TensorRT model config,bug,2022-02-01 19:38:00 +0000 UTC
3710,CLOSED,My tensorrt model can not be loaded by triton server,,2022-04-21 17:54:35 +0000 UTC
3705,CLOSED,How to understand the priority in rate_limiter?,,2021-12-16 03:46:38 +0000 UTC
3701,CLOSED,timeouts when using gpu_execution_accelerator 'tensorrt',,2021-12-15 22:29:05 +0000 UTC
3700,CLOSED,Segmentation fault,bug,2022-01-21 16:58:00 +0000 UTC
3697,CLOSED,Triton Server python backend doesn't provide permission to create directories [Errno 13] Permission denied,,2021-12-13 22:56:09 +0000 UTC
3696,CLOSED,question about warmup,question,2022-07-25 14:15:47 +0000 UTC
3695,CLOSED,question about tensorflow backend,,2021-12-13 19:22:01 +0000 UTC
3694,OPEN,[BUG] Triton Server with Kaldi Backend does not return final response to client.,bug,2021-12-20 21:49:32 +0000 UTC
3693,CLOSED,question about batch inference,question,2021-12-21 01:23:54 +0000 UTC
3688,CLOSED,Cannot start Triton servers following default instructions,,2021-12-10 19:13:12 +0000 UTC
3687,CLOSED,Fails to load the models from model_repository,,2021-12-10 12:04:46 +0000 UTC
3686,CLOSED,Onnx runtime error. grid_sampler is not a registered function/op,,2021-12-10 19:54:44 +0000 UTC
3685,CLOSED,docker: Command not found in build server with backend,,2022-01-21 18:49:21 +0000 UTC
3683,CLOSED,JetPack 4.6 do not support pytorch?,,2021-12-10 20:13:17 +0000 UTC
3681,CLOSED,About the Model Warmup part of the document,question,2021-12-10 20:31:05 +0000 UTC
3679,CLOSED,Triton hosted with kubernetes on Jetson Nano,question,2022-01-10 19:43:12 +0000 UTC
3678,CLOSED,Stub process is unhealthy and it will be restarted,bug,2023-03-17 17:07:51 +0000 UTC
3677,CLOSED,error: ‘TRITONSERVER_ResponseAllocatorQueryFn_t’ has not been declared,,2022-01-05 18:57:56 +0000 UTC
3671,CLOSED,Asynchronous web client sending request to triton server,question,2022-12-26 16:37:56 +0000 UTC
3669,CLOSED,yolov5 inference time increases in triton?how to get infer time details in triton?,question,2021-12-09 21:40:50 +0000 UTC
3668,CLOSED,[Question] Client code halted at ModelRepositoryIndex via gRPC?,question,2022-01-21 18:51:44 +0000 UTC
3664,OPEN,Triton's TF backend does not support ScaNN operations needed for tf recommenders models,enhancement, investigating,2023-03-21 22:53:39 +0000 UTC
3658,CLOSED,How to get requests headers in the backend,question,2021-12-07 02:03:34 +0000 UTC
3657,CLOSED,when use perf_analyzer, what's the compute input meaning?,question,2021-12-06 22:01:45 +0000 UTC
3656,CLOSED,Can't define a constant value in input_map within config.pbtxt,,2021-12-06 22:08:57 +0000 UTC
3655,CLOSED,Class InferResult not released causing memory leak,,2021-12-06 22:26:30 +0000 UTC
3652,CLOSED,[QUESTION] grpc_server.cpp ModelInferHandler::Process the same Request instance for different correlation_id,question,2022-03-25 22:49:21 +0000 UTC
3650,CLOSED,unexpected platform type python for <model_name>,,2021-12-03 17:20:57 +0000 UTC
3649,CLOSED,pytorch models not being loaded,,2021-12-05 22:44:32 +0000 UTC
3648,CLOSED,Pose estimation models,,2021-12-05 22:43:29 +0000 UTC
3647,CLOSED,error: failed to register input shared memory region: failed to register CUDA shared memory region 'input_data1',bug,2021-12-22 00:26:42 +0000 UTC
3646,CLOSED,system crash problem,,2022-01-19 09:30:53 +0000 UTC
3645,CLOSED,start failed with s3 model repo,,2021-12-02 20:40:46 +0000 UTC
3644,OPEN,[Feature Request] Support label look up for tensors of higher rank in classification protocol,enhancement, investigating,2021-12-06 23:44:51 +0000 UTC
3643,CLOSED,can I specify the log file with auto rolling with file size option when run triton?,enhancement,2023-07-10 22:52:31 +0000 UTC
3640,CLOSED,faster_rcnn_r50 pretrained converted to ONNX hosted in Triton model server,question,2023-01-30 07:45:59 +0000 UTC
3637,CLOSED,[Feature Request] Proper documentation on usage of "label_filename" and code example for server side label look up,,2021-12-22 18:46:14 +0000 UTC
3634,CLOSED,[QUESTION] TensorRT model with variable-sized input / output dimensions returns null,bug, investigating,2023-07-10 22:54:59 +0000 UTC
3633,CLOSED,About PyTorch execute failure: forward() is missing value for argument 'input'. error,,2021-12-01 18:48:46 +0000 UTC
3631,CLOSED,triton with onnx model error when load,,2021-12-13 22:21:13 +0000 UTC
3630,CLOSED,When I use Onnx In Triton, The CPU Only 70% utilization can be measured, When I add the concurrent, The time delay increase, but CPU usages can't increase any more.,,2021-12-10 22:11:13 +0000 UTC
3629,CLOSED,Will the triton server schedule requests in the queue to GPUs with low memory usage?,,2021-12-01 03:36:26 +0000 UTC
3628,CLOSED,Update model,,2022-10-31 11:40:25 +0000 UTC
3627,OPEN,health check should not say it's ready when cuda device-side assertion error is triggered,enhancement,2022-08-08 21:44:18 +0000 UTC
3626,CLOSED,When I run tritonserver.exe on windows, I encountered the following problems,,2023-01-11 08:32:15 +0000 UTC
3624,CLOSED,dynamic batching not working properly while requests waiting in queue,bug, investigating,2022-01-24 16:08:41 +0000 UTC
3623,CLOSED,failed to load model :at least one version must be available under the version policy of model,,2021-11-29 19:51:58 +0000 UTC
3622,CLOSED,backend development: cannot convert string to datatype,,2021-11-26 13:56:19 +0000 UTC
3621,CLOSED,release memory,enhancement,2021-11-30 09:50:13 +0000 UTC
3620,CLOSED,Failed to build triton server with docker at branch r20.12,,2021-12-01 08:36:34 +0000 UTC
3619,CLOSED,How to Get Model's FLOPS In TIS?,enhancement,2023-07-10 22:55:31 +0000 UTC
3617,CLOSED,Failed to build r21.05 in Docker container,,2021-11-29 10:04:36 +0000 UTC
3616,CLOSED,Getting (unsorted) value of all classes as output,,2021-11-26 13:46:37 +0000 UTC
3615,CLOSED,Error in using S3-Compatible Storage [Oracle Cloud Infrastructure (OCI) Object Storage],,2021-11-30 15:36:36 +0000 UTC
3614,CLOSED,floating point exception using self built python backend and triton, centos,,2021-12-01 01:46:21 +0000 UTC
3613,CLOSED,Segmentation fault in libtriton_pytorch.so with invalid inputs,,2022-11-16 09:05:43 +0000 UTC
3612,CLOSED,Allow different input types for different inputs in perf_analyzer,,2021-12-22 18:30:07 +0000 UTC
3610,OPEN,Triton Inference Server binary distribution for Ubuntu x64,enhancement,2022-02-01 03:44:49 +0000 UTC
3609,CLOSED,The model deployed with TensorRT could not be load,,2023-05-27 16:35:12 +0000 UTC
3608,CLOSED,python_backend always tries to chmod the triton_python_backend_stub,,2022-06-16 20:26:08 +0000 UTC
3607,CLOSED,undefined symbol: TRITONBACKEND_StateBuffer,,2021-11-29 05:52:33 +0000 UTC
3604,CLOSED,Op type not registered 'SentencepieceOp' for Universal Sentence Encoder,question,2021-12-02 04:24:28 +0000 UTC
3603,CLOSED,Pip install tritonclient[all] not working in ubuntu 20.04,,2023-03-29 03:41:52 +0000 UTC
3602,CLOSED,Ensemble models with multiple inputs or outputs,,2021-11-29 18:07:48 +0000 UTC
3601,CLOSED,protobuf version issue,,2023-04-10 16:19:00 +0000 UTC
3599,CLOSED,Considerably higher "compute infer" time when client geographically further from server,investigating,2022-01-21 18:54:28 +0000 UTC
3597,OPEN,Allow triton to read from multiple cloud model repositories,enhancement,2021-11-23 03:27:30 +0000 UTC
3596,CLOSED,Hi, I have some question about triton/onnxruntime/tensorrt,,2021-12-07 23:32:57 +0000 UTC
3595,CLOSED,no model loaded by triton,,2021-12-07 23:34:13 +0000 UTC
3594,OPEN,Support external shared memory stored in a Redis server,enhancement,2021-11-27 13:39:36 +0000 UTC
3593,CLOSED,Error when converting the automatic config json to config.pbtxt,bug,2022-03-29 16:38:38 +0000 UTC
3584,CLOSED,Generate config.pbtxt,,2021-11-24 10:18:04 +0000 UTC
3583,CLOSED,automatic model load / unload or a lockable store extension,enhancement,2022-01-20 23:51:47 +0000 UTC
3581,CLOSED,Is there any way to check if the client is disconnected? Or a way to force close the model?,question,2021-11-29 18:02:04 +0000 UTC
3580,CLOSED,Stateful model example for the python backend,,2021-11-21 09:53:54 +0000 UTC
3579,CLOSED,triton server supports multi-version TensorRT backend,,2021-11-24 02:55:48 +0000 UTC
3578,CLOSED,Unknown type name 'NoneType',question,2021-12-10 22:18:52 +0000 UTC
3577,CLOSED,Triton Server return sequence flags errors in each 6 batch processing,,2021-12-07 00:12:21 +0000 UTC
3576,CLOSED,Unable to find the implementation of the pure virtual function Run() function of the BackendContext structure,,2021-11-17 18:55:57 +0000 UTC
3575,OPEN,[Question] Poll mode with --load-model,enhancement,2022-10-31 14:49:59 +0000 UTC
3574,CLOSED,Slower ONNX inference on tritonserver than on jetson-voice,,2021-11-30 22:39:12 +0000 UTC
3573,CLOSED,Unexpected inference output 'detections' for model 'yolov4',,2021-11-17 06:58:24 +0000 UTC
3572,CLOSED,Triton Inference Server is 10X Slower than TensorFlow Serving!!!?,performance, investigating,2022-01-31 21:55:42 +0000 UTC
3571,CLOSED,How does the client call HTTP request to send an image file (such as a.jpg) to the server?,,2021-11-15 23:00:29 +0000 UTC
3570,CLOSED,CMake Error In Centos,,2022-08-23 06:23:02 +0000 UTC
3566,CLOSED,Can Give a Centos version Model_analyzer build.py. Now this build script just for window or ubantu, I have to make my own Dockerfile, But there many bug report.,,2021-11-13 00:14:30 +0000 UTC
3565,CLOSED,Is it possible to release the additional gpu memory occupied for inference on a batch?,,2021-11-17 05:18:35 +0000 UTC
3564,CLOSED,How to send batch with different sizes in inference client request,,2021-11-17 00:21:50 +0000 UTC
3563,CLOSED,triton server is down after inferencing with one request,bug,2022-03-25 16:47:22 +0000 UTC
3561,CLOSED,Failed to build triton server on an aarch64 device with error "error: ‘size_t’ does not name a type",,2021-11-11 14:11:48 +0000 UTC
3560,CLOSED,python backend model instances as threads instead of separate processes?,,2021-11-30 22:37:11 +0000 UTC
3559,CLOSED,Signal 11 received and server down when inferring an Onnx model,,2021-11-11 18:25:58 +0000 UTC
3554,CLOSED,RPC Client report Unimplemented desc,,2021-11-10 16:38:21 +0000 UTC
3553,CLOSED,How to measure performance (throughput, latency) when running two different models run concurrently?,,2021-11-10 17:01:50 +0000 UTC
3552,CLOSED,Unable to compile identity_backend,,2021-11-11 14:45:54 +0000 UTC
3551,CLOSED,Documentation for jetson should be updated for client installation,,2021-11-10 16:33:44 +0000 UTC
3548,CLOSED,Not able to load the BertForSequenceClassification model from huggingface,pytorch ngc,2022-07-27 13:08:26 +0000 UTC
3547,CLOSED,[Python Backend] Send PbTensor to cpu for calling as_numpy() or add a function as_cupy(),enhancement, investigating,2023-01-24 22:42:06 +0000 UTC
3546,CLOSED,CondaPackError: Cannot pack an environment with editable packages,,2021-11-15 10:49:19 +0000 UTC
3544,CLOSED,triton-client no matches found from pip install ?,,2022-07-13 10:52:39 +0000 UTC
3543,CLOSED,How to transfer GPU memory data to CPU memory in C++ custom backend,,2021-11-08 11:09:56 +0000 UTC
3542,CLOSED,how to set CMAKE_EXPORT_COMPILE_COMMANDS,,2021-11-09 01:00:44 +0000 UTC
3541,CLOSED,Need help authoring Model configuration for Pytorch MNIST,,2021-12-06 13:51:42 +0000 UTC
3537,CLOSED,output tensor of [1, 1000, 1, 1] 'fc6_1' output layer in densenet onnx,,2021-11-05 08:19:14 +0000 UTC
3534,CLOSED,String Outputs- Setting Shape and Output Config,,2021-11-30 22:33:52 +0000 UTC
3532,CLOSED,How can I check nvidia driver version built in images?,,2021-11-04 04:11:30 +0000 UTC
3529,CLOSED,TensorRT Backend Installation Error,question,2021-11-30 22:33:28 +0000 UTC
3528,CLOSED,Tensorflow Object Detection Prediction,bug, question,2021-11-30 22:39:20 +0000 UTC
3527,CLOSED,max_batch_size in config.pbtxt refer to model batch size or request batch size ?,question,2023-02-16 18:14:02 +0000 UTC
3525,CLOSED,Prometheus not working with Triton,bug,2022-05-23 16:38:43 +0000 UTC
3524,CLOSED,struct.error: unpack_from requires a buffer of at least 274435 bytes,bug,2021-11-29 19:53:44 +0000 UTC
3523,CLOSED,Custom backend (C++ or python) Whats the difference ?,question,2021-11-03 03:44:46 +0000 UTC
3522,CLOSED,How to debug a custom backend,question,2021-11-05 01:59:35 +0000 UTC
3519,CLOSED,Load buit-in OpenVINO failed,bug,2021-11-22 23:38:31 +0000 UTC
3518,CLOSED,`cudaCheck` fails when provided `_CUDA_COMPAT_REALLIB`,question,2021-11-02 09:01:50 +0000 UTC
3515,CLOSED,Standard Output in Python Backend,,2021-10-29 20:21:03 +0000 UTC
3512,CLOSED,perf_analyzer report indices element out of data bounds, idx=1936311911 must be within the inclusive range [-100000,99999],investigating,2022-03-01 01:52:59 +0000 UTC
3511,CLOSED,"Failed to allocate CUDA memory with byte size" WARNNING,question,2022-07-19 11:35:48 +0000 UTC
3509,CLOSED,unable to create shared memory region,,2021-11-30 22:29:18 +0000 UTC
3507,CLOSED,How to request batch inputs to server, I want recall 200 creative ideas, but the input in config.pbtxt only for one,,2021-11-10 07:25:08 +0000 UTC
3506,CLOSED,MIG deployment of triton gives error on GKE,,2021-10-31 09:06:21 +0000 UTC
3505,CLOSED,unexpected inference output 'OUTPUT__0' for model 'fil',,2021-10-27 22:11:44 +0000 UTC
3504,CLOSED,Error while deploying nvidia triton inference on AWS EKS,,2021-11-12 22:44:20 +0000 UTC
3503,CLOSED,к21.04 broken link in release notes,question,2021-10-28 19:35:03 +0000 UTC
3502,CLOSED,Does triton support TTS audio streaming synthesis？,,2021-11-12 22:40:38 +0000 UTC
3499,CLOSED,Save files in server,,2021-11-09 18:02:16 +0000 UTC
3498,CLOSED,Unable to build CPU only image,bug,2021-11-30 22:28:58 +0000 UTC
3496,CLOSED,Dynamic Batching in client script,,2021-11-12 22:41:15 +0000 UTC
3495,CLOSED,Creating custom python backend environment,bug,2021-12-17 14:42:31 +0000 UTC
3494,CLOSED,Unable to use shm using simple_http_shm_client,,2021-10-25 18:25:45 +0000 UTC
3493,CLOSED,pytorch backend has lowergpu utilization,question,2021-11-12 22:42:02 +0000 UTC
3492,CLOSED,custom backend handling of unexpectly closed sequence under sequence batcher oldest Strategy,enhancement, question,2023-07-06 18:15:30 +0000 UTC
3491,CLOSED,Witch backend should I use for gettting best performance,,2021-10-23 18:19:39 +0000 UTC
3490,CLOSED,Is it possible to redirect a gRPC request from the Triton client?,question,2021-11-30 22:27:33 +0000 UTC
3489,CLOSED,Clear documentation about all parameters in config.pbtxt meaning?,,2021-12-15 01:23:48 +0000 UTC
3488,CLOSED,core dumps error,investigating,2021-11-30 22:27:18 +0000 UTC
3485,CLOSED,Version policy `specific` doesn’t work as expected. It doesn’t respect the model_version parameter during inference.,bug,2022-01-21 18:13:54 +0000 UTC
3484,OPEN,Label_filename content request,enhancement,2023-02-07 23:12:42 +0000 UTC
3483,CLOSED,Can the ensembled model implement branching logic?,,2021-12-01 16:28:25 +0000 UTC
3482,OPEN,Is python backend going to support asyncio?,enhancement,2023-02-03 01:27:42 +0000 UTC
3480,OPEN,unable to load custom python environment with python backend,bug,2021-12-07 16:01:37 +0000 UTC
3479,CLOSED,Deploy Triton using Kubernetes,,2021-11-12 22:43:16 +0000 UTC
3478,CLOSED,looser throw specifier for ‘virtual const char* google::protobuf::FatalException::what() const’,,2021-11-17 00:43:01 +0000 UTC
3475,CLOSED,Intermittent issue with loading a python backend models (21.08),bug,2021-11-30 15:27:40 +0000 UTC
3474,CLOSED,Device Auto Reallocaton not working as expected,bug, investigating,2021-11-03 07:11:23 +0000 UTC
3473,CLOSED,ONNX with TensorRT Optimization Model Warmup not work,,2022-03-25 04:27:34 +0000 UTC
3471,CLOSED,Triton Explicit Model control mode cloud serving advice,enhancement,2022-09-22 20:52:26 +0000 UTC
3470,CLOSED,Using String outputs for Custom C++ Backend Models,,2021-10-27 17:48:16 +0000 UTC
3469,CLOSED,jimeter test, tps is not stable,,2021-11-01 16:20:23 +0000 UTC
3468,CLOSED,Unable to deploy Resnet.,,2021-11-12 22:54:35 +0000 UTC
3467,CLOSED,label_filename usage,,2021-10-22 22:05:25 +0000 UTC
3465,CLOSED,Unable to grpcurl to inference.GRPCInferenceService/ServerLive,,2021-11-12 22:55:06 +0000 UTC
3464,CLOSED,I saved a tensorflow model as the saved_model format of the serialized input of tf.Example, how to deploy the triton service and infer,bug,2023-05-19 22:37:38 +0000 UTC
3460,CLOSED,GPU metrics are not matching with nvidia-smi output,investigating,2021-11-30 22:52:33 +0000 UTC
3459,CLOSED,Condition checking in ensemble,,2021-12-22 17:49:52 +0000 UTC
3456,CLOSED,Not able to provide <perf-analyzer-flags> 'shape' for perf_analyzer in config.yaml, results in termination,,2021-10-12 17:34:15 +0000 UTC
3455,CLOSED,Can we provide pure cmake builds without mixing python scripts and cmake files and download steps?,,2021-11-01 16:19:18 +0000 UTC
3452,OPEN,Same model but different results between triton and native tensorrt engine,bug,2023-06-23 19:36:33 +0000 UTC
3451,CLOSED,Core dump caused by overwhelming requests,,2021-11-01 16:18:50 +0000 UTC
3450,CLOSED,Triton backend API version does not support this backend,,2021-10-12 18:08:51 +0000 UTC
3448,CLOSED,Python backend model not loading in recent release,,2021-12-22 17:30:21 +0000 UTC
3446,CLOSED,No throughput gain on increasing number of GPUs,,2021-10-20 12:29:44 +0000 UTC
3445,CLOSED,Unable to find lib triton_python.so,,2021-10-12 03:23:54 +0000 UTC
3444,CLOSED,Possibilities for shared memory and CUDA shared memory,,2021-10-14 21:48:18 +0000 UTC
3443,CLOSED,Change Documentation of Instance Groups to prevent Misunderstandings,,2021-11-30 22:42:50 +0000 UTC
3442,CLOSED,Add installation instructions for rhel,,2021-10-18 22:02:47 +0000 UTC
3441,CLOSED,will ensemble model pass the GPU pointer to next model?,question,2022-07-27 00:46:40 +0000 UTC
3429,CLOSED,Docker fails to register cuda shared memory,bug,2023-05-16 18:27:04 +0000 UTC
3428,CLOSED,Triton Server Shared Memory - Free Unused Memory from Multiple Processes in C++,,2021-10-06 03:21:31 +0000 UTC
3426,CLOSED,How to use a YOLO detection + DeepSORT tracking models in Triton?,,2023-03-08 11:21:01 +0000 UTC
3425,CLOSED,unable to build custom image: unexpected keyword argument,,2021-10-01 18:45:33 +0000 UTC
3420,CLOSED,how to profile GPU input/output tensors on python backend,,2021-09-29 19:21:59 +0000 UTC
3419,CLOSED,Ability to have optional inputs,enhancement,2021-12-14 00:49:59 +0000 UTC
3418,CLOSED,Onnxruntime execute failure,bug,2022-06-07 16:59:38 +0000 UTC
3417,CLOSED,TensorRT Backend Repo build failed.,,2021-09-30 05:29:21 +0000 UTC
3416,CLOSED,[Questions] Why is the queue time high?,,2021-09-29 22:22:51 +0000 UTC
3414,CLOSED,python-backend r21.09-py3 manifest unknown,,2021-09-28 19:21:18 +0000 UTC
3413,CLOSED,Improve ensemble concurrent performance,,2021-10-18 22:27:41 +0000 UTC
3410,CLOSED,Clear documentation on GRPC health endpoint,,2021-11-30 22:48:33 +0000 UTC
3409,OPEN,Packages for triton server and its components.,enhancement,2021-09-27 21:43:37 +0000 UTC
3407,CLOSED,Problem with sending float tensors to the server,,2022-12-07 08:56:55 +0000 UTC
3406,CLOSED,Out of order on processing sequence,,2021-10-07 01:19:07 +0000 UTC
3405,CLOSED,how to change a torchscript model to stateful model to use the sequence_batching config,,2021-10-26 19:04:13 +0000 UTC
3401,CLOSED,error: unable to run model: failed to parse the request JSON buffer: The document is empty. at 0,investigating,2021-12-22 00:43:52 +0000 UTC
3400,CLOSED,Why client expriment v2 api in r20.03 can not request 20.03 server,investigating,2021-09-27 03:50:31 +0000 UTC
3399,CLOSED,Misleading gRPC status codes returned for some error cases,enhancement, investigating,2022-07-13 21:42:23 +0000 UTC
3395,CLOSED,Dynamic Batching for Variable Shaped Inputs,,2021-10-07 19:15:08 +0000 UTC
3394,CLOSED,Asynchronous Operation for Request/Response for Python Backed,enhancement,2022-07-18 23:23:54 +0000 UTC
3393,CLOSED,TFLite Backend,,2022-11-07 22:01:28 +0000 UTC
3391,CLOSED,`perf_analyzer` error while loading shared libraries: `libcudart.so.11.0`,,2021-10-01 18:29:17 +0000 UTC
3390,CLOSED,multithread failed,,2021-09-24 19:43:26 +0000 UTC
3389,CLOSED,multithread failed,,2021-09-24 19:43:35 +0000 UTC
3386,CLOSED,Python backend on CPU is slower when serving a pytorch model,bug, investigating,2022-01-27 01:45:38 +0000 UTC
3385,CLOSED,TorchScript gelu signature difference,,2021-09-23 07:58:09 +0000 UTC
3384,CLOSED,Can't load model by API,,2021-11-30 22:50:03 +0000 UTC
3383,CLOSED,AWS S3 bucket model repository failure,,2021-11-01 12:51:16 +0000 UTC
3382,CLOSED,Inconsistency in CUDA compatibility check,,2022-01-21 23:16:20 +0000 UTC
3380,CLOSED,Release triton server image 21.09+,,2021-09-21 06:29:10 +0000 UTC
3379,CLOSED,GPU memory consumption increases after model is loaded,,2021-11-30 22:55:10 +0000 UTC
3378,CLOSED,Concurrent Model Execution In Same Client Script,,2021-10-26 13:22:37 +0000 UTC
3377,CLOSED,Multiple models inference in just 1 client script,,2021-10-07 18:56:27 +0000 UTC
3376,CLOSED,why a model might take longer per inference request after a different model is run?,investigating,2023-07-10 22:44:23 +0000 UTC
3374,CLOSED,Why expriment v2 api in r20.03 can't request 20.03 server,,2021-09-24 02:58:42 +0000 UTC
3373,OPEN,Ensemble of models are executed over different devices,enhancement,2021-09-17 15:54:37 +0000 UTC
3372,CLOSED,tritonclient.utils.shared_memory not available on Jetson tritonclient[all]=2.8.0,enhancement,2021-09-19 15:17:38 +0000 UTC
3371,CLOSED,Cannot load model using S3 / MinIO,,2021-09-15 10:44:12 +0000 UTC
3370,CLOSED,Does tensorflow-backend support multi-stream on a single GPU?,,2021-09-16 22:20:59 +0000 UTC
3369,CLOSED,2 or more models running in parallel,,2021-09-17 05:11:14 +0000 UTC
3364,CLOSED,Enable querying for desired output memory type,enhancement,2021-12-02 01:11:28 +0000 UTC
3362,CLOSED,Crashing when sending 1000 requests on 10 concurrent connections,,2021-09-15 13:11:53 +0000 UTC
3361,CLOSED,Loading CPU/GPU dyncamically,,2021-09-14 17:13:18 +0000 UTC
3360,CLOSED,Allow http server to bind to specific address/interface,enhancement, good first issue,2022-04-04 17:48:23 +0000 UTC
3358,CLOSED,Triton on AWS ec2 instance: cuda_memory_manager.cc:115] CUDA memory pool disabled,,2021-09-11 07:24:04 +0000 UTC
3353,CLOSED,Connection reset by peer,,2021-10-28 00:05:11 +0000 UTC
3350,CLOSED,about config.pbtxt,,2021-09-10 16:54:40 +0000 UTC
3349,CLOSED,Model Load Lifecycle,,2021-09-30 18:56:55 +0000 UTC
3348,CLOSED,PyTorch execute failure: isTensor(); Expected Tensor but got GenericList,,2021-10-28 01:44:20 +0000 UTC
3347,CLOSED,Is it possible to use tensorflow with GPU on custom python backend?,,2021-09-14 17:07:37 +0000 UTC
3344,CLOSED,An option to automatically re-assign device for CPU only ops in tensorflow SavedModel,,2021-09-17 09:13:48 +0000 UTC
3343,CLOSED,Pass different input size in a batch,,2021-10-18 22:45:08 +0000 UTC
3340,CLOSED,Tensorflow backend support for parallelism,,2021-09-09 19:18:46 +0000 UTC
3337,CLOSED,inference failed: in ensemble unexpected deadlock,,2021-09-09 08:40:50 +0000 UTC
3336,CLOSED,How to provide dynamic input size in ensemble model,,2021-09-08 18:58:39 +0000 UTC
3335,CLOSED,How can I improve the inference speed?,,2021-09-09 04:15:28 +0000 UTC
3334,CLOSED,Triton server is slower than pytorch model,,2021-11-15 23:01:55 +0000 UTC
3333,CLOSED,Defining multiple key-value pairs in ensemble model config.pbtxt,,2021-09-23 17:18:04 +0000 UTC
3332,CLOSED,AttributeError: module 'triton_python_backend_utils' has no attribute 'InferenceRequest',,2021-09-08 18:37:04 +0000 UTC
3328,CLOSED,How to pass different input sizes to different models in an ensemble?,,2021-09-08 07:21:09 +0000 UTC
3327,CLOSED,is the dynamic batcher setting sucess?,,2021-09-08 17:03:50 +0000 UTC
3326,CLOSED,Scaling multiple GPUs,,2021-11-30 22:55:20 +0000 UTC
3325,CLOSED,TRITON server returning wrong inferences,,2021-09-16 22:21:18 +0000 UTC
3322,CLOSED,make: *** No rule to make target 'triton-python-backend-stub'.,,2021-09-09 18:15:05 +0000 UTC
3321,CLOSED,triton_server slower than tensorflow serving and python API,question,2021-12-22 00:33:08 +0000 UTC
3320,CLOSED,Memory leak issue on using load/unload API to dynamic loading TensorRT model,bug,2021-12-16 06:25:25 +0000 UTC
3318,CLOSED,Shape problem when using SavedModel,,2021-09-07 16:44:16 +0000 UTC
3316,CLOSED,Allow selected models in the model repository not to be loaded,,2021-09-03 15:58:07 +0000 UTC
3312,CLOSED,Why the Yolov5s speed is very slow neither using dynamic batch size or multiple instances?,,2021-11-17 07:58:58 +0000 UTC
3308,CLOSED,Facing issue while using Yolov5s with Trition inference Server,,2022-11-22 03:18:08 +0000 UTC
3307,CLOSED,How to rename input name of torchscript model?,,2021-09-09 18:57:42 +0000 UTC
3306,CLOSED,Dynamically switch floating point precision for TensorRT engine,enhancement, question,2021-11-23 00:01:56 +0000 UTC
3303,CLOSED,http client for a model with multiple outputs,question,2023-07-05 12:47:44 +0000 UTC
3302,CLOSED,The current model becomes slow after running another model,,2021-09-18 06:10:18 +0000 UTC
3301,CLOSED,max_batch_size not in op error reported for tensorflow tensorrt model,investigating,2021-09-27 22:45:15 +0000 UTC
3300,CLOSED,Can NMT models can be deployed with triton inference server ?,question,2021-09-01 18:10:02 +0000 UTC
3299,CLOSED,Periodic dead of server while using python backend,bug,2023-02-21 17:51:26 +0000 UTC
3292,CLOSED,Using Triton as a custom service in GitLab CI,question,2021-09-01 22:35:28 +0000 UTC
3290,CLOSED,Half Precision TF inference error (DT_HALF and Einsum),investigating,2021-10-08 18:19:21 +0000 UTC
3288,CLOSED,Build error withou docker,,2021-08-30 16:51:45 +0000 UTC
3287,CLOSED,Triton21.05 has problem when load model from Minio,investigating,2022-02-24 16:38:43 +0000 UTC
3286,OPEN,Batched Prediction for Python backend,enhancement,2022-02-25 14:21:04 +0000 UTC
3284,CLOSED,Question regarding string request/response with HTTP frontend and python backend,question,2021-09-03 22:56:10 +0000 UTC
3283,CLOSED,Build ONNX Runtime Backend image failed,,2021-09-17 20:37:05 +0000 UTC
3282,CLOSED,Triton repository agent checksum - readme broken link,,2021-08-31 17:18:21 +0000 UTC
3281,CLOSED,【20210826】 multi model-repository is invalid,,2022-11-22 03:17:54 +0000 UTC
3275,CLOSED,Running nvidia inference server without an Nvidia GPU,,2021-08-25 13:37:53 +0000 UTC
3274,CLOSED,Performance on triton with python backend,performance,2021-08-30 14:38:15 +0000 UTC
3271,CLOSED,Does TIS supports other devices such as google's TPU?,,2021-09-03 21:50:59 +0000 UTC
3265,CLOSED,[PyTorch Backend] Triton server consumes too much GPU memory,,2021-09-03 22:57:05 +0000 UTC
3264,CLOSED,Sagemaker and KFServing port clash,,2021-08-23 13:57:39 +0000 UTC
3263,CLOSED,How to Add Custom Models with different framework in Triton inference server and have to serve the model and utilize the api,,2021-09-06 12:18:33 +0000 UTC
3262,CLOSED,Build modified triton from source in Jetson,,2021-09-28 12:15:32 +0000 UTC
3261,CLOSED,Image Segmentation Output,,2021-09-03 21:51:20 +0000 UTC
3260,CLOSED,Client Script Error,,2021-09-20 14:47:08 +0000 UTC
3259,CLOSED,python backend pb_utils.InferenceResponse can adding message argument for the responses output ?,,2021-08-23 14:17:09 +0000 UTC
3258,CLOSED,the result of ensemble model is wrong and the result of single model is right.,,2021-08-22 01:20:14 +0000 UTC
3256,CLOSED,Missing package configuration: prometheus-cpp,question,2021-08-23 15:26:43 +0000 UTC
3255,CLOSED,Kubernetes unable to kill container,,2021-08-26 08:03:36 +0000 UTC
3254,CLOSED,Tensorflow Segmentation Model Deployment On Triton Inference Server,,2021-09-03 21:52:23 +0000 UTC
3253,CLOSED,Build python backend error using docker,,2021-08-24 03:19:54 +0000 UTC
3252,CLOSED,I use my yolov5l tensorrt model to deploy on triton, it will instantly fill up the GPU memory,,2021-08-20 08:47:28 +0000 UTC
3251,CLOSED,Avg request latency to Avg HTTP time up 5000 usec,,2021-09-30 02:41:03 +0000 UTC
3248,CLOSED,Inconsistent implementation results,question,2021-09-03 21:51:49 +0000 UTC
3246,CLOSED,Triton Ensemble model: Unable to get multiple output,,2021-08-19 16:43:56 +0000 UTC
3245,CLOSED,Accumulate inference time with an ensemble model is way slower than the slowest individual,,2021-09-03 22:58:11 +0000 UTC
3244,CLOSED,Dockerfile in building triton,,2021-11-18 18:40:11 +0000 UTC
3239,CLOSED,Unable to set input with 0 dimension shape for non-batching model,,2021-08-17 17:04:38 +0000 UTC
3237,CLOSED,FileNotFoundError: [Errno 2] No such file or directory: '/tmp/folder31uWBi/1/model.py',bug,2021-08-18 23:04:45 +0000 UTC
3234,CLOSED,tritonserver:<version>py3-min Dockerfile,,2021-08-18 20:37:50 +0000 UTC
3228,CLOSED,Release Python client on PyPI,enhancement,2021-12-02 00:58:48 +0000 UTC
3227,CLOSED,Load triton_python_backend_stub from S3 model repository,bug,2021-09-07 02:09:52 +0000 UTC
3225,CLOSED,Can we fine tune a model with triton server?,,2021-08-12 15:42:43 +0000 UTC
3224,CLOSED,Install Triton Server From Source Code On Jetson Nano,,2021-08-11 17:27:00 +0000 UTC
3223,CLOSED,CPU triton server,,2021-08-30 21:02:00 +0000 UTC
3222,CLOSED,Triton with python backend: not Using Python execution env *.tar.gz file,,2021-08-13 02:44:23 +0000 UTC
3219,CLOSED,sending an output with wrong datatype (e.g. integers > 256 as datatype uint8) should fail,bug,2021-09-08 19:33:37 +0000 UTC
3218,CLOSED,Is dynamic batch trt model file required when using dynamic_batching,,2021-08-30 21:00:12 +0000 UTC
3217,CLOSED,[StatusCode.INVALID_ARGUMENT] unable to find data for input tensor 'input_tensor',,2021-08-10 07:23:01 +0000 UTC
3215,CLOSED,Triton Ensemble Model - seems to be executing each model multiple times.,question,2021-08-23 14:53:40 +0000 UTC
3213,CLOSED,Error in Build of Triton server using --no-container-build,,2021-08-30 20:59:57 +0000 UTC
3212,OPEN,Can I load costum layer or plugin from Minio or S3?,enhancement,2022-02-08 09:39:12 +0000 UTC
3211,CLOSED,Concurrent requests for the same model lead to inconsistent output results,,2022-06-30 03:59:47 +0000 UTC
3210,CLOSED,error: creating server: INTERNAL - Could not get MetaData for bucket with name 192.168.x.xxx:9000,,2021-08-09 05:24:48 +0000 UTC
3207,CLOSED,Unregistering system shared memory region before async requests complete causes server crash,question,2021-08-07 09:20:03 +0000 UTC
3206,CLOSED,How to send binary data in `perf_analyzer`?,question,2021-08-30 21:00:37 +0000 UTC
3204,CLOSED,Unable to run TensortRT execution accelerator with an ONNX model.,,2021-08-05 20:03:40 +0000 UTC
3203,CLOSED,How to build the PyTorch Backend on jetson?,question,2021-11-06 12:12:36 +0000 UTC
3198,CLOSED,Clarification of error message: Poll failed for model directory 'x': output 'output.0' for ensemble 'x' is not written,question,2021-08-04 21:23:27 +0000 UTC
3197,CLOSED,Include triton backend license files in tritonserver container image.,enhancement, investigating,2021-08-05 21:40:40 +0000 UTC
3196,CLOSED,segment fault when running tritonserver which is built from source,investigating,2021-08-05 03:30:13 +0000 UTC
3195,CLOSED,how can I control the cuda memory for models?,question,2021-08-11 21:25:17 +0000 UTC
3194,CLOSED,openvino_backend has much lower performance than tensorFlow_backend,performance, investigating,2022-01-07 18:13:56 +0000 UTC
3193,CLOSED,how can I checkout to r20.12?,,2021-08-04 06:16:35 +0000 UTC
3191,CLOSED,Certain FP16 traced pytorch models with batchnorm no longer work with r21.07,bug, investigating,2021-08-24 06:30:50 +0000 UTC
3189,CLOSED,python backend complains about modulenotfound,,2021-08-12 03:49:28 +0000 UTC
3188,CLOSED,Triton on KFServing errors on every request - "Infer failed: input 'features' already exists in request",,2021-09-03 22:58:32 +0000 UTC
3187,CLOSED,Triton on KFServing errors on every request - " Infer failed: input 'features' already exists in request",,2021-08-03 14:31:23 +0000 UTC
3183,CLOSED,Python backend server stuck after initializing,,2021-08-10 16:40:45 +0000 UTC
3179,CLOSED,No performance gain in models from Tensorflow model zoo,,2021-09-17 20:35:39 +0000 UTC
3178,CLOSED,SageMaker HTTPService automatically starts in r21.07/r21.06,,2021-08-04 18:42:19 +0000 UTC
3177,CLOSED,[INTERNAL] Attempting to access response which is not yet allocated,,2021-08-09 22:18:54 +0000 UTC
3176,CLOSED,[Docs] Clarify when backends need to release requests,,2021-08-03 00:19:34 +0000 UTC
3175,CLOSED,CUDA illegal memory error when calling bool() in torch script,,2021-08-02 15:46:14 +0000 UTC
3174,CLOSED,Trition Inference server docker container get exited during 2nd time inference on CPU,investigating,2021-12-22 00:22:39 +0000 UTC
3173,OPEN,`Magic tag does not match` even when `model.plan` is generated inside the same container of Triton,bug, investigating,2021-08-04 08:19:44 +0000 UTC
3172,OPEN,Parallel model initialization for python backend,enhancement,2021-08-02 17:23:02 +0000 UTC
3171,OPEN,Model Interpretability,enhancement,2021-08-02 17:23:19 +0000 UTC
3170,CLOSED,Python custom backend failed to start with error "no version information available (required by /bin/bash)",,2021-08-03 13:30:35 +0000 UTC
3169,CLOSED,Build and run Triton direclty in my machine instead of using docker,question,2021-08-30 21:02:58 +0000 UTC
3163,CLOSED,Missing tritonserver2.12.0-jetpack4.6.tgz,,2021-07-28 12:20:25 +0000 UTC
3162,CLOSED,/opt/tensorrtserver/nvidia_entrypoint.sh: line 92: exec: tritonserver: not found,,2021-07-28 02:41:17 +0000 UTC
3161,CLOSED,/opt/tensorrtserver/nvidia_entrypoint.sh: line 92: exec: tritonserver: not found,,2021-07-28 18:30:46 +0000 UTC
3160,CLOSED,Streaming connections interrupted for extremely long time series,bug,2021-10-23 19:20:19 +0000 UTC
3157,CLOSED,Framework support matrix has wrong version of onnxruntime for 21.06,,2021-07-28 18:31:58 +0000 UTC
3156,CLOSED,Triton python backend build failed (main branch),bug,2021-08-12 20:24:05 +0000 UTC
3155,CLOSED,Trition local build (branch r21.06) with docker failed,,2021-07-27 17:45:57 +0000 UTC
3154,CLOSED,py3-min Dockerfile,,2021-08-02 17:48:32 +0000 UTC
3152,CLOSED,got stucked when import torch,,2021-07-27 14:35:42 +0000 UTC
3151,CLOSED,Can Triton support MIG?,,2021-08-03 00:57:41 +0000 UTC
3150,CLOSED,Doesn't co-work with MPS?,bug,2022-04-12 17:05:48 +0000 UTC
3148,CLOSED,Can we build with Caffe2 without MKL?,,2021-07-23 18:36:18 +0000 UTC
3147,CLOSED,"failed to connect to all addresses" occurs by chance,,2021-09-01 09:28:58 +0000 UTC
3146,CLOSED,cmake bug in subproject core,,2021-07-26 18:50:43 +0000 UTC
3142,CLOSED,openvino error in loading network (with custom op),bug, duplicate,2021-08-14 02:42:37 +0000 UTC
3141,CLOSED,CMake problems for client library in v2.11.0,enhancement,2022-03-25 16:46:51 +0000 UTC
3138,CLOSED,Secure https endpoint,enhancement,2022-01-31 21:44:43 +0000 UTC
3135,CLOSED,Triton requiring config.pbtxt when loading models from s3 (MinIO)?,bug, investigating,2023-01-31 19:29:55 +0000 UTC
3134,CLOSED,dlSym cannot locate method 'CreateExtension',bug,2021-08-04 22:15:40 +0000 UTC
3131,OPEN,Support for Kafka endpoints,enhancement,2021-07-19 19:00:06 +0000 UTC
3130,CLOSED,Support for HTTP endpoint on windows,enhancement,2021-09-20 22:54:06 +0000 UTC
3129,CLOSED,openvino custom op,question,2021-07-21 02:09:35 +0000 UTC
3126,CLOSED,Triton spins on startup running in EGX,bug,2021-08-04 18:55:14 +0000 UTC
3122,CLOSED,[PyTorch Backend] Missing libtorch_python.so and libshm.so for custom ops,,2021-07-15 17:58:44 +0000 UTC
3121,CLOSED,GRPC: unable to provide 'output_3' in GPU, will use CPU,,2021-07-15 18:41:10 +0000 UTC
3120,CLOSED,Can Triton server be installed on Drive PX2?,,2021-07-15 18:46:44 +0000 UTC
3119,CLOSED,Triton server crashes silently with invalid input data,bug,2021-07-29 18:10:58 +0000 UTC
3117,CLOSED,failed to load torch script model, unexpected no host policy,,2021-07-15 04:06:15 +0000 UTC
3116,CLOSED,Wrong preferred_batch_size selected,,2021-07-21 08:26:01 +0000 UTC
3115,CLOSED,How to read image file from HTTP requests for python backend?,question,2021-07-14 15:35:14 +0000 UTC
3114,CLOSED,rapidjson.JSONDecodeError: Parse error at offset 0: Invalid value.,question,2021-07-16 07:42:51 +0000 UTC
3113,OPEN,perf_analyzer latency vs throughput mismatch: reduce the overhead of perf_analyzer when using synchronous infer API,bug,2021-07-29 19:18:33 +0000 UTC
3112,CLOSED,ONNX runtime failed to inferenced and shows onnx runtime error 2?,,2021-07-14 02:50:46 +0000 UTC
3111,CLOSED,Using Python backend for jetson platform,,2021-07-14 03:39:28 +0000 UTC
3110,CLOSED,Not able to connect to Dell EMC ECS,,2021-07-21 09:18:45 +0000 UTC
3107,CLOSED,Question about max_batch_size, dynamic_batching in python_backend,,2021-07-13 03:44:08 +0000 UTC
3106,CLOSED,How to set the cpu affinity of the model instance?,question,2021-08-02 22:02:58 +0000 UTC
3103,CLOSED,Multiple copies of perf_analyzer cannot be run in parallel with sequence batching due to sequence id collisions,bug, enhancement,2021-11-11 19:51:03 +0000 UTC
3100,CLOSED,No ModelWarmup examples,investigating,2021-07-13 17:02:59 +0000 UTC
3099,CLOSED,Support for Sequence IDs as strings,enhancement,2021-12-22 00:08:16 +0000 UTC
3098,CLOSED,Failed to initialize server,,2021-08-27 03:03:10 +0000 UTC
3097,CLOSED,Usage of packaged conda environments stored in s3 with python backend,enhancement,2021-09-07 02:08:55 +0000 UTC
3096,CLOSED,Multiple instances of same model-additional memory for weights?,,2021-07-13 05:54:53 +0000 UTC
3094,OPEN,python backend with custom packages reports error "Internal: Failed to initialize stub, stub process exited unexpectedly",bug,2021-07-13 14:12:43 +0000 UTC
3093,CLOSED,Failed to load tensorflow model: Op type not registered NormalizeUTF8,,2021-07-08 05:11:33 +0000 UTC
3090,CLOSED,When sending multiple pictures at the same time, the prediction result of the same picture will be different every time,,2021-08-03 18:47:27 +0000 UTC
3086,CLOSED,is_server_live and is_server_ready results in connection reset when tritonserver crash,,2021-08-02 22:29:41 +0000 UTC
3085,CLOSED,custom build tensorflow backend,,2021-07-06 21:14:23 +0000 UTC
3083,CLOSED,Versioning of other repositories,enhancement,2021-07-07 21:30:03 +0000 UTC
3081,CLOSED,2.11.0 tensorflow backend support TF_ENABLE_ONEDNN_OPTS=1,question, investigating,2022-02-16 00:47:37 +0000 UTC
3080,CLOSED,Question about tensorflow backend (savedmodel),question,2021-08-03 18:49:20 +0000 UTC
3076,CLOSED,Triton into Gitlab CI, Need exposed port 9000,,2021-07-07 21:50:06 +0000 UTC
3074,CLOSED,Python backend segfault with detectron2,bug,2021-09-22 14:05:31 +0000 UTC
3073,CLOSED,segfault at ... error 6 in libc-2.27.so,,2021-08-03 18:23:59 +0000 UTC
3070,CLOSED,backend build fails,,2021-07-16 16:53:35 +0000 UTC
3069,CLOSED,How to add resnet50 model for qa tests,question,2021-07-07 21:55:17 +0000 UTC
3066,CLOSED,Increasing number of instances of the model does not increase performance,question,2021-09-01 16:38:58 +0000 UTC
3061,CLOSED,Client library / triton server / config.pbtxt data_type naming convention missmatch,,2021-07-07 19:56:03 +0000 UTC
3060,CLOSED,Change some reported metrics from Prometheus' `Counter` to `Histogram`,enhancement,2023-03-07 00:25:01 +0000 UTC
3058,CLOSED,pytorch_backend build fails with `cannot find -ltorch`,,2021-07-13 17:04:06 +0000 UTC
3056,CLOSED,file not found: archive/constants.pkl error while loading model from TorchScript,,2021-06-29 01:19:11 +0000 UTC
3045,CLOSED,Concurrent requests to multiple models cause NaN values in output,bug, investigating,2021-10-07 23:18:20 +0000 UTC
3044,CLOSED,use var_length bert plan, tritonserver core dumped,,2021-07-13 17:04:47 +0000 UTC
3043,CLOSED,.proto files deleted in src/core/ directory,,2021-06-24 17:07:37 +0000 UTC
3040,CLOSED,Ensemble within ensemble,,2021-07-07 22:55:55 +0000 UTC
3038,OPEN,How to find out the number of unprocessed requests for an inference?,enhancement,2023-01-27 22:29:15 +0000 UTC
3037,CLOSED,No performance improvement when optimizing models,,2021-07-13 17:05:41 +0000 UTC
3036,CLOSED,Python Backend does not load due to shared memory issue,,2021-06-29 15:24:21 +0000 UTC
3034,CLOSED,CMake Error: The source directory "/tmp/citritonbuild/tritonserver/build/build" does not exist.,,2021-06-23 07:36:52 +0000 UTC
3033,CLOSED,cpu memory increase constantly when serving model with triton-inference-server,bug, investigating,2022-04-01 00:44:51 +0000 UTC
3032,CLOSED,Error details: model expected the shape of dimension 0 to be between 1 and 1 but received 5,,2021-08-02 22:53:33 +0000 UTC
3031,CLOSED,Constant URL for RESTAPI,,2021-06-25 01:19:56 +0000 UTC
3030,CLOSED,Error message while installing triton for jetpack,,2021-06-28 22:13:35 +0000 UTC
3029,CLOSED,Huge inference speed difference when loading a model from S3,bug, investigating,2022-02-24 16:37:01 +0000 UTC
3027,CLOSED,docker load invalid diffid,investigating,2021-07-13 17:07:00 +0000 UTC
3026,CLOSED,unexpected platform type tensorflow_savedmodel for rul,bug,2021-07-15 18:14:03 +0000 UTC
3025,CLOSED,from tritonclient.grpc import model_config_pb2,,2021-06-25 02:35:18 +0000 UTC
3023,CLOSED,Example to Test TensorFlow2 Backend in TritonServer Built Without Docker,,2021-06-24 18:20:58 +0000 UTC
3022,CLOSED,need a doc for the optimization of cuda graph,,2021-07-13 17:09:05 +0000 UTC
3021,CLOSED,When 21.06 Will be available to launch triton in unprivileged (non-root user) ?,,2021-06-30 08:03:57 +0000 UTC
3018,CLOSED,How to debug an ensemble model?,,2021-06-16 09:43:24 +0000 UTC
3014,CLOSED,tritonclient.utils.InferenceServerException: [StatusCode.INTERNAL] in ensemble \'ensemble_dali_face_detect\', request specifies invalid shape for input \'input\' for facedetect_trt_0_gpu0. Error details: model expected the shape of dimension 0 to be between 1 and 1 but received 2,,2021-07-13 17:09:40 +0000 UTC
3013,CLOSED,faild build requires deletion of `/tmp/citritonbuild/<backend>` git repo folders,enhancement, good first issue,2023-07-10 22:43:10 +0000 UTC
3012,CLOSED,warmup does not seem to work in libtorch backend,,2022-09-02 16:17:13 +0000 UTC
3011,CLOSED,Trouble understanding REST outputs from Python TritonClient,,2021-06-16 20:59:35 +0000 UTC
3010,CLOSED,Error cannot find -ltorch and ltorchvision in building PyTorch Backend along with Triton Server in Oracle Linux 7.9,enhancement,2021-07-13 17:11:17 +0000 UTC
3009,CLOSED,Error In Loading Zipped Model in Python Backend,,2021-06-21 21:53:13 +0000 UTC
3008,CLOSED,Deployment Strategies,enhancement,2021-07-13 17:13:18 +0000 UTC
3007,CLOSED,A suggestion: allowing the different batch size of inputs for "ensemble mode".,,2021-06-14 02:53:15 +0000 UTC
3004,CLOSED,Validation tool for concurrent model execution and dynamic batching,,2021-06-21 21:54:56 +0000 UTC
3001,CLOSED,Model Load request to a kubernetes cluster reaches only one pod (when replicationCount > 1),,2021-06-11 16:52:32 +0000 UTC
2999,CLOSED,How to minimize the docker image for deployment?,duplicate, question,2021-07-19 02:16:13 +0000 UTC
2998,CLOSED,Auto-Generated Model Configuration for tensorflow_savedmodel issue,enhancement, investigating,2021-07-13 17:13:56 +0000 UTC
2994,CLOSED,grpc::UNAVAILABLE, error_message_ = "Connect Failed" when max_batch_size is greater than 0,,2021-07-13 17:16:25 +0000 UTC
2993,CLOSED,Do we need to explicitly load/unload sub-models in an ensemble model.,question,2021-06-10 14:25:27 +0000 UTC
2989,CLOSED,How to load python backend model without restart server,question,2021-06-09 13:32:55 +0000 UTC
2988,CLOSED,Is there a way that the server do not exit when some model load failed,,2021-06-09 08:34:06 +0000 UTC
2987,CLOSED,how to set cuda version when build?,question,2021-06-09 10:42:08 +0000 UTC
2985,CLOSED,Triton (+ model.pt) silently exits,bug, investigating,2021-06-29 16:55:24 +0000 UTC
2979,CLOSED,Triton Server Onnx - CUDA failure 700,,2021-06-09 18:40:55 +0000 UTC
2978,CLOSED,Use real image data with perf_analyzer - Triton Inference Server,,2021-06-08 15:23:00 +0000 UTC
2977,CLOSED,yolov5 to onnx model image client throws 'expecting model output to be a vector' error,question,2021-06-08 17:39:57 +0000 UTC
2976,CLOSED,Failed to,invalid,2021-06-08 14:51:55 +0000 UTC
2975,CLOSED,Can't set ‘backend’ to run my tensorRT model,,2021-06-10 11:27:29 +0000 UTC
2974,CLOSED,QA：XGBoost/RAPIDS support?,question,2021-07-13 17:16:44 +0000 UTC
2971,CLOSED,Python backend returning wrong results converting from NHWC to NCHW,bug, investigating,2021-06-16 17:41:42 +0000 UTC
2970,CLOSED,input segment_ids[0] expected type int32 != int64,,2021-06-08 17:55:25 +0000 UTC
2969,CLOSED,tritonserver running in pytorch backend cannot do batch but change it to python backend it is ok,,2021-06-08 10:48:08 +0000 UTC
2968,CLOSED,rapidjson.JSONDecodeError Error in Running Python Model,bug, investigating,2021-06-10 14:08:39 +0000 UTC
2967,CLOSED,build with --no-container-build flag fails while building onnxruntime inside a container,duplicate,2021-06-08 12:42:39 +0000 UTC
2966,CLOSED,Failed to determine modification time of model on Azure Storage when starting up triton server after update to release 21.04,bug, investigating,2021-09-01 01:23:50 +0000 UTC
2960,CLOSED,Failed to load python model inside Self-built Triton Server in Centos 7,,2021-06-07 05:10:06 +0000 UTC
2959,CLOSED,when a large amount of data needs to be transferred between ensemble models, it is very time consuming,,2021-06-05 07:20:27 +0000 UTC
2958,CLOSED,python backend is much slower than the same code run in python environment,performance, investigating,2021-11-25 00:18:51 +0000 UTC
2953,CLOSED,Service Shutdown while multiple python backend to be loaded.,,2021-06-03 14:46:19 +0000 UTC
2952,CLOSED,The question of interrupting streams,question,2021-07-13 17:19:10 +0000 UTC
2951,CLOSED,Cannot find -ltensorflow_triton when building tensorflow_backend,,2021-06-04 16:00:01 +0000 UTC
2949,CLOSED,AWS Helm chart error,,2021-06-03 17:51:32 +0000 UTC
2948,CLOSED,triton client memory leak in python sdk,,2021-06-08 03:51:24 +0000 UTC
2944,CLOSED,triton C API How to build example,,2022-05-31 13:48:40 +0000 UTC
2943,CLOSED,Triton server hangs while trying to deploy models,,2021-06-11 22:11:09 +0000 UTC
2941,OPEN,HTTP/REST client supporting for Java/Scala,enhancement,2021-06-01 18:47:23 +0000 UTC
2940,CLOSED,tritonserver2.10.0-jetpack4.5.tgz - deepstream 5.1- jetson xavier NX,,2021-06-02 17:24:30 +0000 UTC
2939,CLOSED,Model loading is slow， and when it comes to loading multiple models, it gets stuck!,,2021-07-13 17:29:51 +0000 UTC
2938,CLOSED,Client build on Windows fails: cl : command line error D8021: invalid numeric argument '/Wno-implicit-fallthrough',,2021-06-06 13:25:07 +0000 UTC
2937,CLOSED,GRPC server always return `RawOutputContents` even if the `InferInputTensor::contents` is specified.,question,2021-06-07 11:43:38 +0000 UTC
2932,CLOSED,Triton (docker 21.05-py3) fails to load a model repository with two python backends,,2021-05-28 17:44:02 +0000 UTC
2931,CLOSED,server can not load mybackend shared library,,2021-05-28 07:17:41 +0000 UTC
2929,CLOSED,how can get tensorflow version in triton tensorflow backend,,2021-05-27 16:34:10 +0000 UTC
2928,CLOSED,How to serve a PyTorch model in Triton python_backend on multiple GPUs?,,2021-05-31 13:59:26 +0000 UTC
2927,CLOSED,Extending Triton to support more open source libraries,question,2021-06-02 16:29:37 +0000 UTC
2926,CLOSED,Get label with REST http client,,2021-06-08 15:58:30 +0000 UTC
2925,CLOSED,Cuda version for Jetpack 2.10.0,,2021-05-31 10:50:25 +0000 UTC
2922,CLOSED,ensemble_image_client client example giving error,,2021-05-26 16:58:38 +0000 UTC
2921,CLOSED,Onnx runtime on Jetson Xavier,,2021-05-26 16:07:58 +0000 UTC
2920,CLOSED,Triton server docker images for jetson xavier,,2021-05-26 16:08:13 +0000 UTC
2919,CLOSED,Use share memory sometime have a error in defferent process,,2021-05-27 11:42:20 +0000 UTC
2918,CLOSED,How to test performance benchmark with input type raw_image ?,,2021-05-27 16:22:49 +0000 UTC
2917,CLOSED,ensemble model load failed,,2021-05-27 07:20:28 +0000 UTC
2911,CLOSED,error: failed to get model metadata: Request for unknown model: 'inception_graphdef' is not found,,2021-06-01 19:18:44 +0000 UTC
2910,CLOSED,Docker minimum Triton example seems incomplete,investigating,2021-08-03 01:32:29 +0000 UTC
2907,CLOSED,My model is working fine when I use gpu:0 but it is giving error when I use gpu:1.,,2022-02-23 12:04:47 +0000 UTC
2906,CLOSED,unable to create TensorRT context,,2022-07-18 08:13:08 +0000 UTC
2902,CLOSED,GRPC Execute Failed, message: failed to connect to all addresses,,2021-05-21 20:42:13 +0000 UTC
2901,CLOSED,unexpected platform type caffe2_netdef for Triton 2.7.0 ?,,2021-05-21 20:47:47 +0000 UTC
2900,CLOSED,Can't generating linux_stamp.whl successfully on Intel i5,question,2021-05-24 03:46:06 +0000 UTC
2899,CLOSED,Running image_client.py causes core dump on Jetson NX,,2021-05-25 07:20:12 +0000 UTC
2895,CLOSED,Unable to compile Triton/client on CentOS 7.9,,2021-11-11 07:20:09 +0000 UTC
2893,CLOSED,Triton json cause assert Abort or Segmentfault,bug,2021-07-13 17:20:13 +0000 UTC
2892,CLOSED,Can not load densenet model,,2021-05-21 01:41:33 +0000 UTC
2888,CLOSED,What onnx version support this Add_1003 operator?,,2021-05-19 20:27:20 +0000 UTC
2887,CLOSED,[Question] [Model storage] Is there a way to download a model from cloud repo and store it to disk?,question,2021-06-08 17:39:03 +0000 UTC
2886,CLOSED,Backend version mismatch when building from sources,,2021-05-19 11:41:21 +0000 UTC
2885,CLOSED,ensemble model, inference input data-type is 'BYTES', model expects 'FP32',,2021-05-21 03:12:38 +0000 UTC
2883,CLOSED,Waiting for in-flight requests to complete,,2021-05-19 07:34:16 +0000 UTC
2882,CLOSED,tritonserver: error while loading shared libraries: libnvidia-ml.so.1 on Jetson NX,,2021-05-21 19:54:51 +0000 UTC
2877,CLOSED,Triton does not run without gpu (cpu-only),bug, question,2023-04-03 20:50:43 +0000 UTC
2876,CLOSED,CUDA copy error in python backend on Jetson,,2021-05-22 11:17:10 +0000 UTC
2875,CLOSED,Cmake error,,2021-05-18 23:47:49 +0000 UTC
2874,CLOSED,Python backend is not using python3 available in PATH,,2021-05-18 23:48:25 +0000 UTC
2873,CLOSED,triton load yensorrt error,,2021-06-08 15:40:28 +0000 UTC
2872,CLOSED,Questions related to shared memory,question,2021-06-08 17:21:40 +0000 UTC
2871,CLOSED,Triton does not refresh aws credentials when using IAM roles,enhancement, investigating,2022-05-25 17:37:22 +0000 UTC
2870,CLOSED,Reshape did not work for python backend,question, investigating,2021-05-21 15:36:13 +0000 UTC
2868,CLOSED,Yolov5 - preprocess in infer config file,,2021-05-18 00:17:15 +0000 UTC
2865,CLOSED,How to stop ensemble pipeline,,2021-05-17 23:57:42 +0000 UTC
2860,CLOSED,Troubleshooting execute function of Python Backend,,2021-06-02 16:25:00 +0000 UTC
2858,CLOSED,failed to load all models,,2021-06-08 15:27:05 +0000 UTC
2857,CLOSED,how to make dynamic_axes espcn(super resolution model) onnx config.pbtxt file?,,2021-05-14 14:05:20 +0000 UTC
2856,CLOSED,smaller size docker image with specific functions,,2021-05-14 15:21:27 +0000 UTC
2855,CLOSED,python backend streaming pybind11::error_already_set for perf_analyzer with concurrency > 1,bug, investigating,2021-08-30 14:56:44 +0000 UTC
2853,CLOSED,Triton does not load the latest available model in explicit mode.,bug, investigating,2021-06-08 10:49:15 +0000 UTC
2852,CLOSED,How to accelerate the Triton when loading TensorRT plan.,question,2021-07-13 17:30:54 +0000 UTC
2851,CLOSED,How to reduce the time consumption of tensor transfer among models in a ensemble model?,,2021-05-12 19:27:27 +0000 UTC
2850,CLOSED,TRITONBACKEND_InputProperties,,2021-05-14 16:04:08 +0000 UTC
2849,CLOSED,Python client support for variable size dimensions,,2021-05-14 21:41:27 +0000 UTC
2848,CLOSED,the preprocess VGG mean-substraction in image_client.cc,bug,2021-08-11 19:41:45 +0000 UTC
2846,CLOSED,InferenceServerException: PyTorch execute failure: Expected Tensor but got Tuple [ BART Summarization ],bug, investigating,2022-10-20 07:23:25 +0000 UTC
2845,CLOSED,identity_backend of input_buffer_count,,2021-05-11 13:13:35 +0000 UTC
2844,CLOSED,Update Release notes for clients - libopencv_imgcodecs.so.4.2: cannot open shared object file: No such file or directory,,2021-05-11 13:36:05 +0000 UTC
2843,CLOSED,so many server's handler caused server not work,bug, investigating,2021-06-08 00:01:55 +0000 UTC
2842,OPEN,Azure Kubernetes Service Deployment Sample,enhancement,2021-05-13 14:02:37 +0000 UTC
2838,CLOSED,Running Triton Without GPU,,2021-05-11 23:49:39 +0000 UTC
2836,CLOSED,infrence of torch script model much slower with triton than python environment,bug, performance, pytorch ngc,2022-04-11 13:44:14 +0000 UTC
2835,CLOSED,Unable to connect to S3 protocol of Dell EMC ECS EX300,bug,2021-05-19 14:44:20 +0000 UTC
2834,CLOSED,Windows build errors with grpc: missing pthread dependency, protobuf version inconsistency,,2021-05-12 03:36:29 +0000 UTC
2833,CLOSED,Windows docker build error, Could not create SSL/TLS secure channel.,,2021-06-07 14:11:49 +0000 UTC
2831,CLOSED,How to inference with tuple input in pytorch_backend?,,2021-05-17 02:47:54 +0000 UTC
2830,CLOSED,Triton Model Repository Format,enhancement, investigating,2021-05-10 23:17:26 +0000 UTC
2828,CLOSED,how to change the package name in java client,question, investigating,2021-05-10 23:08:35 +0000 UTC
2827,CLOSED,Question: How to call another model in a model.(like emsemble, speech recognition application),question,2021-05-17 22:07:46 +0000 UTC
2821,CLOSED,Triton Client build for windows,,2021-05-07 08:18:54 +0000 UTC
2818,CLOSED,Triton crashes while running TensorFlow model with reshape commands for outputs in config.pbtxt,,2021-05-19 21:47:07 +0000 UTC
2817,CLOSED,[BUG] cuDF context is initialized to gpu0 multiple times with Tritons python backend on a multi-gpu machine,,2021-05-05 23:05:56 +0000 UTC
2815,CLOSED,Dynamic Batching does not seem to work.,,2021-07-13 17:32:29 +0000 UTC
2811,CLOSED,Verifying dynamic batching is working,,2021-05-06 18:35:34 +0000 UTC
2810,CLOSED,Out of memory error on second inference,,2021-05-06 03:03:13 +0000 UTC
2807,CLOSED,Question: Performance differences between v1 and v2,,2021-05-05 13:54:49 +0000 UTC
2803,OPEN,Windows support for C API sample,enhancement,2021-05-06 16:27:35 +0000 UTC
2802,CLOSED,List dependent packages for Windows build in build.py,,2021-05-07 21:02:52 +0000 UTC
2801,CLOSED,Exclude non-needed GRPC/HTTP dependencies,,2021-05-07 21:02:52 +0000 UTC
2800,CLOSED,Exclude disabled cloud dependencies from CMAKE,,2021-05-11 02:35:04 +0000 UTC
2798,CLOSED,Incorrect NVTX annotations emitted by the inference server,,2021-05-04 00:52:08 +0000 UTC
2794,CLOSED,memory leak in s3 filesystem,,2021-05-03 16:08:32 +0000 UTC
2793,CLOSED,can not load onnx model,,2021-05-19 00:01:59 +0000 UTC
2792,CLOSED,How to deploy yolov5 model,,2021-07-30 04:14:30 +0000 UTC
2791,OPEN,Kubernetes Operator for install and lifecycle management,enhancement,2021-10-19 10:51:38 +0000 UTC
2790,CLOSED,When use command line to launch triton, it raise: attempt to access JSON non-number as double,,2021-05-06 06:24:57 +0000 UTC
2786,CLOSED,How to add ConfigProto when running a model,,2021-04-29 12:12:02 +0000 UTC
2785,CLOSED,Building triton server from min with Pytorch backend,,2021-05-05 09:12:32 +0000 UTC
2784,CLOSED,No models being mounted when running container,,2021-04-29 17:33:54 +0000 UTC
2783,CLOSED,Unusual HTTP send/recv latency with perf_analyzer,,2021-06-07 14:04:00 +0000 UTC
2781,CLOSED,Cannot load ensemble model,,2022-11-01 17:29:13 +0000 UTC
2780,CLOSED,Questions about Model Loading on GPU memory,,2021-05-18 00:39:22 +0000 UTC
2779,CLOSED,[Jetson] triton client health calls via grpc on jetson very slow/hangs,,2021-06-07 15:57:55 +0000 UTC
2777,CLOSED,How to use Triton server “ensemble model” with 1:N input/output to create patches from large image?,,2022-02-08 09:38:23 +0000 UTC
2773,CLOSED,Custom Operations for TensorFlow 2 error,,2021-12-21 23:51:59 +0000 UTC
2771,CLOSED,hello, everyone , here is a my demo of using ensemble model.,,2021-04-27 08:40:29 +0000 UTC
2770,CLOSED,tensorflow saved_model performance,,2021-05-05 18:10:04 +0000 UTC
2769,CLOSED,big inference latency difference between k8s (aws) and local laptop,,2021-05-04 23:53:03 +0000 UTC
2768,CLOSED,tritonclient.utils.InferenceServerException: request specifies invalid shape for input 'input' for face_det_tensorrt_0_3_gpu0. Error details: model expected the shape of dimension 0 to be between 1 and 1 but received 2,,2021-06-15 08:13:45 +0000 UTC
2767,CLOSED,Documentation for Windows alpha release,,2021-06-11 13:34:25 +0000 UTC
2766,CLOSED,503 Service Unavailable,,2021-07-13 17:33:13 +0000 UTC
2765,CLOSED,When I user triton server, How should I use it，batch concatenate and more GPU,,2021-07-13 17:33:27 +0000 UTC
2763,CLOSED,Unable to build r21.04 version,,2021-04-23 21:09:43 +0000 UTC
2760,CLOSED,Ensemble model throughput lower than member models,,2022-07-21 01:54:04 +0000 UTC
2759,CLOSED,Common access to DataType-manipulating functions,enhancement,2022-05-20 18:57:02 +0000 UTC
2758,CLOSED,containerd: OCI runtime create failed "stat /run/containerd/io.containerd.runtime.v1.linux/k8s.io/../nvidia: no such file or directory",,2021-04-23 19:09:31 +0000 UTC
2756,CLOSED,how to get open-source packages manually?,,2021-04-23 21:14:13 +0000 UTC
2755,CLOSED,metrics about Count and Latency,,2021-12-15 09:38:44 +0000 UTC
2752,CLOSED,Launching Triton worked first time now it doesn't (nothing changed),,2021-05-19 21:35:21 +0000 UTC
2750,CLOSED,Cpu memory keeps increasing while inferencing,,2021-06-21 11:49:38 +0000 UTC
2748,CLOSED,No error log for failed request.,,2021-04-21 14:07:20 +0000 UTC
2747,CLOSED,use python backend to add Huggingface Transformers example?,enhancement,2021-05-15 13:12:44 +0000 UTC
2744,CLOSED,[Question] Flask integration with Python HTTP Client,,2021-04-26 15:13:21 +0000 UTC
2743,CLOSED,error: creating server: Internal - failed to load all models,,2021-04-21 01:04:35 +0000 UTC
2740,CLOSED,Is there a way to shut down GPU/CPU memory fall back policy?,,2021-04-23 21:40:35 +0000 UTC
2739,CLOSED,grpc java client,,2021-05-06 02:11:45 +0000 UTC
2738,CLOSED,In the latest project of triton service ,How to create the imagepreprocess.so ? Thanks,,2021-04-19 16:49:43 +0000 UTC
2737,CLOSED,Questions about gPRC threads in Triton Inference Server,,2021-04-23 21:52:04 +0000 UTC
2735,CLOSED,Add trtorch backend,enhancement,2021-11-30 23:01:47 +0000 UTC
2732,CLOSED,Starting triton got stucked,,2021-04-19 07:49:01 +0000 UTC
2731,CLOSED,Triton did not update the model after users added a new model into model_repository | There is nothing on localhost:8000/api/status,,2021-04-20 00:36:38 +0000 UTC
2727,CLOSED,Question about the relationship between multiple instances and GPU usage,,2021-04-19 06:11:29 +0000 UTC
2726,CLOSED,nvbufsurftransform:cuInit failed : 3,,2021-04-14 11:44:34 +0000 UTC
2725,CLOSED,is_server_live results in connection reset,,2022-10-21 23:24:47 +0000 UTC
2720,CLOSED,Question about repeat backend,,2021-04-12 16:03:39 +0000 UTC
2719,CLOSED,new C++ client file,,2021-04-12 12:17:43 +0000 UTC
2716,CLOSED,c++ client compile fail to #include "triton/common/triton_json.h",,2021-08-04 11:01:28 +0000 UTC
2714,CLOSED,Latest nvidia-pyindex release breaks pip install of client libraries,,2021-04-09 18:37:31 +0000 UTC
2711,CLOSED,Question about custom backends under the 21.x API,,2021-04-23 22:07:44 +0000 UTC
2706,CLOSED,Golang GRPC Request caused Triton server with a fatal error,,2021-09-30 19:03:17 +0000 UTC
2702,CLOSED,set_data_from_numpy hangs with image data,,2021-04-14 03:24:00 +0000 UTC
2701,CLOSED,Getting error in a multi gpu machine,,2021-05-13 02:28:30 +0000 UTC
2699,CLOSED,c# .net5,,2021-04-23 22:10:32 +0000 UTC
2698,CLOSED,[Question] Infer from multiple models simultaneously on a single GPU without lower fps,,2022-01-19 13:20:10 +0000 UTC
2691,CLOSED,Rebuilding c++ example image_client within docker sdk image fails,,2021-04-02 17:24:12 +0000 UTC
2690,CLOSED,Can't find shared_memory module in tritonclient library,,2021-04-08 16:40:56 +0000 UTC
2689,CLOSED,Call other backends within python backend,,2021-04-07 03:56:19 +0000 UTC
2688,CLOSED,Trinton Errror during the initialization of the modell,,2021-04-01 16:27:58 +0000 UTC
2687,CLOSED,About Ensemble Models,,2021-04-08 17:28:45 +0000 UTC
2686,OPEN,perf_analyzer --async miscommunicates with server and runs out of memory,bug,2021-09-09 18:29:04 +0000 UTC
2685,CLOSED,How can I get percentile latency metrics (say p99) for online inferencing?,,2021-04-23 22:20:43 +0000 UTC
2681,CLOSED,How can I run identity_backend on trition,,2021-05-18 19:48:49 +0000 UTC
2678,CLOSED,Tesla T4 cards unusually hot while Triton is idling,,2022-06-28 09:34:38 +0000 UTC
2675,OPEN,Programmatic interface for Nvidia Framework Containers Support matrix,enhancement,2021-08-11 19:42:48 +0000 UTC
2674,CLOSED,QuickStart example without the --gpus flag fails for tritonserver:v20.12-py3 and later.,,2021-03-29 20:45:33 +0000 UTC
2673,CLOSED,using cuda shared memory with libtorch,,2021-03-31 17:19:29 +0000 UTC
2672,CLOSED,fail to load pytorch model,,2021-04-20 13:11:39 +0000 UTC
2671,CLOSED,The examples of the customised python backend are not working with the inference server. Cannot run interpreter host. Errno = 2,,2021-03-28 14:30:42 +0000 UTC
2668,CLOSED,Python gRPC and http clients throw warning when using Python 3.8,,2021-04-02 15:44:19 +0000 UTC
2667,CLOSED,Sometimes receive Socket Closed UNAVAILABLE from triton grpc server.,,2021-03-29 20:32:51 +0000 UTC
2661,CLOSED,Option (flag) to disable `optimized_execution` in pytorch backend,,2021-06-09 07:59:17 +0000 UTC
2659,CLOSED,How triton determine when to remove an idle instance?,,2021-03-31 16:22:52 +0000 UTC
2658,CLOSED,What do you support for Multi Tenancy?,,2021-03-31 16:27:47 +0000 UTC
2657,OPEN,Add support for STSAssumeRoleWebIdentityCredentialsProvider for S3 repositories,enhancement,2021-05-19 21:45:36 +0000 UTC
2653,CLOSED,config.pbtxt for openvino model,,2021-03-23 04:22:43 +0000 UTC
2652,CLOSED,Unable to execute model simultaneously on a multi gpu instance.,,2021-03-23 22:32:24 +0000 UTC
2650,CLOSED,Simplify example python client code.,,2021-04-06 16:41:32 +0000 UTC
2649,CLOSED,Error on loading onnx model,,2021-03-31 16:30:06 +0000 UTC
2644,CLOSED,Wrong error for loading ONNX models,,2021-03-31 16:30:31 +0000 UTC
2643,CLOSED,Tritonserver crashes with segmentation fault,bug,2021-04-08 16:34:12 +0000 UTC
2641,CLOSED,Unable to run pytorch model, CUDA copy error,,2022-05-03 10:25:35 +0000 UTC
2640,CLOSED,Failed to get device count,,2021-03-18 17:40:27 +0000 UTC
2637,CLOSED,Not able to run custom PyTorch model using Triton Inference Server & Seldon Core,,2021-03-31 16:35:57 +0000 UTC
2636,CLOSED,Label mismatch: value and index in reverse order,,2021-03-31 16:36:28 +0000 UTC
2635,CLOSED,Can't set max batch size when using strict-model-config = false #1466,,2021-03-25 15:57:49 +0000 UTC
2634,CLOSED,Question of starting serving inside docker,,2021-03-17 15:16:04 +0000 UTC
2633,CLOSED,Error when load model with http api,,2021-08-16 02:59:35 +0000 UTC
2629,CLOSED,DGX A100,,2021-03-18 10:50:38 +0000 UTC
2627,OPEN,TF-TRT Model can't be loaded,bug,2021-12-28 14:45:38 +0000 UTC
2626,CLOSED,PyTorch backend sometimes allocates input tensors on wrong GPU on multi-GPU systems,,2021-03-31 16:37:39 +0000 UTC
2625,OPEN,Triton server crashes with a model converted using tf-trt,bug,2023-07-09 11:01:39 +0000 UTC
2624,CLOSED,service not responding: Too many open files,,2023-06-03 05:39:26 +0000 UTC
2618,CLOSED,./rtSafe/safeContext.cpp (133) - Cudnn Error in configure: 7 (CUDNN_STATUS_MAPPING_ERROR),,2021-04-23 22:27:49 +0000 UTC
2617,CLOSED,[Ask for help & discussion] Understanding shared memory.,,2021-03-22 20:31:48 +0000 UTC
2616,CLOSED,Thread safety question about python grpcclient and server,,2021-03-31 16:38:14 +0000 UTC
2613,CLOSED,ONNX TensorRT optimization parameters,,2021-03-11 16:39:11 +0000 UTC
2612,CLOSED,Shared memory allocation,,2021-03-14 05:21:51 +0000 UTC
2607,CLOSED,reload ensemble model may cause server crash,bug,2022-05-31 17:54:18 +0000 UTC
2606,CLOSED,Unable to autofill for 'yolov4_nvidia', either all model tensor configuration should specify their dims or none,,2022-06-08 04:45:25 +0000 UTC
2604,CLOSED,Building Pytorch backend,,2021-03-10 17:11:12 +0000 UTC
2603,CLOSED,triton client C++,,2022-10-24 08:54:08 +0000 UTC
2600,CLOSED,Docker image does not run under arbitrary non-root user,,2022-01-10 03:16:13 +0000 UTC
2599,CLOSED,[Question] how to improve gpu-utilization with multi models,,2021-04-23 22:35:14 +0000 UTC
2598,CLOSED,AttributeError: 'NoneType' object has no attribute 'cancelled',,2021-03-10 03:41:01 +0000 UTC
2596,CLOSED,Error at Triton init on GKE while loading Pytorch custom ops,,2021-03-11 08:40:43 +0000 UTC
2594,CLOSED,Running torchscript exported model in Triton throws InferenceServerException,,2021-03-05 02:02:02 +0000 UTC
2593,CLOSED,In model configuration support defining list of tensors as input,,2021-10-25 04:09:45 +0000 UTC
2592,CLOSED,Error response from daemon: received unexpected HTTP status: 502 Bad Gateway,,2021-04-23 22:35:57 +0000 UTC
2591,CLOSED,TensorRT model uses Perf_Client to test the performance and finds that compute input takes too long and infer takes too short.,,2021-03-16 02:09:50 +0000 UTC
2584,CLOSED,What is the possible reason of "instance group 124M2_1 of model 124M2 specifies invalid or unsupported gpu id 1",,2021-03-11 09:19:18 +0000 UTC
2583,CLOSED,How to fork a request to multiple model instances?,,2021-03-05 02:02:17 +0000 UTC
2582,CLOSED,How to get outputs.content instead of raw_output_contents?,,2021-04-23 22:36:07 +0000 UTC
2578,CLOSED,model reload should be performed in backgroud but it does not,,2021-03-26 15:31:52 +0000 UTC
2576,CLOSED,No rule to make target "client",,2021-03-04 10:28:34 +0000 UTC
2575,CLOSED,CMake error in building from source (no container),,2021-03-02 19:03:18 +0000 UTC
2572,CLOSED,Deploy model trained with TLT,,2021-08-03 19:15:11 +0000 UTC
2571,CLOSED,InferenceServerException: input 'max_seqlen' batch size does not match other inputs for 'varsbert,,2021-06-25 05:47:14 +0000 UTC
2570,CLOSED,Model saved with tensorflow 2.4's tf.compat.v1.saved_model.simple_save inference ran errors.,,2021-12-21 23:37:01 +0000 UTC
2568,CLOSED,Can Tensorflow Backend be compiled a no-GPU version ?,,2021-04-23 22:38:23 +0000 UTC
2564,CLOSED,Triton not starting HTTP server on GKE,,2021-04-23 22:38:40 +0000 UTC
2563,CLOSED,How to find correct rest api params for POST /v2/models/:model/infer inference request?,,2021-03-01 17:40:21 +0000 UTC
2562,CLOSED,ERROR: No supported GPU(s) detected to run this container,,2021-02-26 06:30:42 +0000 UTC
2559,CLOSED,Error - "Internal - failed to load all models",,2022-10-10 05:43:25 +0000 UTC
2558,CLOSED,Build the TensorFlow Backend With Custom TensorFlow Failed.,,2021-02-26 18:44:33 +0000 UTC
2555,CLOSED,Start tensorrtserver on boot,,2021-02-24 17:14:33 +0000 UTC
2554,CLOSED,yolov3.onnx problem,,2021-02-25 02:45:47 +0000 UTC
2553,CLOSED,Problem with commiting changes to the prebuilt docker container,,2021-03-07 12:15:51 +0000 UTC
2552,CLOSED,QA：Framework installation is needed or not after the construction of the backend?,,2021-03-04 10:43:31 +0000 UTC
2549,CLOSED,Configuration documentation typo,,2021-02-24 20:08:35 +0000 UTC
2545,CLOSED,Trailing zeros in bytes truncated in binary data using python http client,,2021-03-16 15:28:00 +0000 UTC
2543,CLOSED,client latency is too high when model returns a large tensor,,2021-04-23 22:38:50 +0000 UTC
2542,CLOSED,build docker with build.py bug,,2021-05-19 21:36:58 +0000 UTC
2540,CLOSED,MyelinGraphError,,2021-03-04 16:10:18 +0000 UTC
2539,CLOSED,Decoding binary data,,2021-02-22 17:11:55 +0000 UTC
2537,CLOSED,a mistake in the description of custom docker build,,2021-02-22 18:41:08 +0000 UTC
2536,CLOSED,Is there any difference between tensorflow model and TensorRT engine?,,2023-02-27 06:10:35 +0000 UTC
2535,CLOSED,Memory allocation when using multiple platforms/backends,,2021-02-24 18:58:46 +0000 UTC
2532,CLOSED,Cannot import torch when using Python Backend,,2021-07-26 12:14:14 +0000 UTC
2531,CLOSED,Tensorflow 1/2 backend source code,,2021-02-22 18:26:19 +0000 UTC
2530,CLOSED,how to show model repository from the client?,,2021-02-18 13:40:28 +0000 UTC
2526,CLOSED,[LibTorch] Expected Tensor but got None with inception v3,,2022-04-28 10:44:33 +0000 UTC
2525,CLOSED,Pip install nvidia-pyindex not working,,2021-04-11 03:35:28 +0000 UTC
2521,CLOSED,Support List of Tensors in LibTorch backend,,2021-02-16 17:09:18 +0000 UTC
2520,CLOSED,Call HTTP/RestAPI with byte data in payload,,2021-02-18 17:39:24 +0000 UTC
2519,CLOSED,backend directory in jetson nano deploy,,2021-02-27 03:05:05 +0000 UTC
2518,CLOSED,Backend cmake build error,,2021-02-19 08:06:09 +0000 UTC
2517,CLOSED,Triton GRPC client creation has memory leak,,2021-02-23 02:28:45 +0000 UTC
2514,CLOSED,Decoding grpc output,,2021-08-11 19:43:24 +0000 UTC
2513,CLOSED,ArmNN Backend,,2021-11-12 22:58:46 +0000 UTC
2512,CLOSED,Reading raw content produces wrong values,,2021-02-12 17:56:19 +0000 UTC
2502,CLOSED,Wrong formatted config file from Auto-Generated Model Configuration,,2021-02-16 18:27:50 +0000 UTC
2501,CLOSED,Is it possible to compile a version for Centos7?,,2021-02-25 06:55:37 +0000 UTC
2500,CLOSED,Python backend doesn't support boolean outputs,,2021-02-11 16:54:45 +0000 UTC
2496,CLOSED,Unable to set dims for output with only batch dimension,,2021-09-30 19:02:17 +0000 UTC
2495,CLOSED,Failed to parse error (Quick Start),,2021-02-09 01:05:54 +0000 UTC
2493,CLOSED,Error in using S3-Compatible Storage [Oracle Cloud Infrastructure (OCI) Object Storage],,2021-07-13 09:13:42 +0000 UTC
2491,CLOSED,Is the pytorch backend correctly disabling gradient calculation?,,2021-02-08 19:08:29 +0000 UTC
2490,CLOSED,How to specify the memory type of an output and collect the output to a response?,,2021-02-10 19:05:18 +0000 UTC
2488,CLOSED,Using TensorRT acceleration with model running on both CPU and GPU,,2021-02-12 20:02:08 +0000 UTC
2486,OPEN,get query_params with python backend,enhancement,2022-11-22 19:37:18 +0000 UTC
2482,CLOSED,Simplify the source building process (i.e. doc and dependencies),,2021-02-09 08:14:59 +0000 UTC
2478,CLOSED,Trouble sending image data to Triton Server,,2021-11-08 08:41:04 +0000 UTC
2477,CLOSED,Nvidia T4 - perf_client low performance,,2021-02-08 22:03:14 +0000 UTC
2476,CLOSED,UnicodeEncodeError when deploying Vietnamese n-gram language model on the Triton Inference Server.,,2021-02-03 15:24:08 +0000 UTC
2475,CLOSED,Segmentation fault (core dumped) on ensemble model from Triton (GPU) to Python Backend (CPU),,2021-03-16 15:28:40 +0000 UTC
2472,CLOSED,tritonclient.utils.cuda_shared_memory.CudaSharedMemoryException: unable to set device successfully,,2021-02-01 05:57:05 +0000 UTC
2467,CLOSED,Getting the server inference time using the client library,,2021-02-02 18:17:28 +0000 UTC
2466,CLOSED,wrong word in README.md document,,2021-01-28 22:36:37 +0000 UTC
2462,CLOSED,perf client using json file to load real image data for ensemble DALI+model,,2022-12-14 21:52:32 +0000 UTC
2461,CLOSED,Triton server failed to load Tensorflow SavedModel,bug,2022-03-14 20:31:20 +0000 UTC
2460,CLOSED,20.11-py3 vs 20.11-py3-min?,,2021-01-26 17:54:31 +0000 UTC
2458,CLOSED,Build failing in python build script,,2021-01-26 16:38:03 +0000 UTC
2457,CLOSED,python_backend should not set name_ property,,2021-01-27 23:29:11 +0000 UTC
2456,CLOSED,memory leak before doing inference (windows http client),,2021-01-26 03:28:10 +0000 UTC
2453,CLOSED,Spend long time in python backend module Tensor function,,2021-02-07 03:48:02 +0000 UTC
2446,CLOSED,CPU memory usage constantly increases while doing inference (windows http client),,2021-01-26 02:47:45 +0000 UTC
2443,CLOSED,Build TF-Text by default with TensorFlow2 backend,,2021-10-18 22:00:17 +0000 UTC
2442,CLOSED,Streaming inference for heavily overlapping data,,2021-07-21 19:02:05 +0000 UTC
2439,CLOSED,Python backend fails with PyTorch > 1.6.0,,2022-06-17 08:27:43 +0000 UTC
2435,CLOSED,python_backend cmake error,,2021-02-03 01:02:33 +0000 UTC
2434,CLOSED,segment fault within http_server.cc about rapidjson,,2021-01-26 00:11:28 +0000 UTC
2433,CLOSED,Streaming generation,,2022-05-30 03:35:56 +0000 UTC
2432,CLOSED,python_backend cmake error,,2021-01-28 23:54:18 +0000 UTC
2431,CLOSED,Performance Analyzer sends input data with wrong order for Hugectr Model,,2021-01-22 08:07:38 +0000 UTC
2430,CLOSED,Which model should be fed to `simple` server?,,2021-01-19 02:23:30 +0000 UTC
2429,CLOSED,[question] Total avg queue time = 0 usec for ensemble model,,2021-01-26 17:33:48 +0000 UTC
2428,CLOSED,Unable to get "compute start" and "compute end" with ensemble model for trace command,,2021-03-31 16:38:09 +0000 UTC
2427,CLOSED,python_backend cmake,,2021-01-17 20:20:24 +0000 UTC
2421,CLOSED,Declaring Triton plugins, with kfserving,,2021-01-19 17:59:49 +0000 UTC
2419,CLOSED,tritonclient.utils.InferenceServerException / StatusCode.UNAVAILABLE] / Request for unknown model / model is not found,,2021-01-15 11:14:35 +0000 UTC
2418,CLOSED,How to use custom backend e.g. identity_backend,,2021-01-15 17:47:09 +0000 UTC
2417,CLOSED,Internal: failed to connect to all addresses,,2021-01-20 15:13:53 +0000 UTC
2416,CLOSED,build with docker failed,,2021-01-20 01:42:19 +0000 UTC
2415,CLOSED,Encounter error when running official python backend,,2021-01-14 15:29:30 +0000 UTC
2414,CLOSED,Linker error when torch installed in Python backend,,2021-01-14 17:54:59 +0000 UTC
2411,CLOSED,"unexpected shape for input" Error for a Model with Dynamic Input,,2021-01-13 17:16:56 +0000 UTC
2410,CLOSED,Loading Custom TRT Plugins while Serving the TRT Models on Jetson Nano,,2021-01-13 17:18:59 +0000 UTC
2409,CLOSED,Question: Changing GRPC compression options,,2021-01-13 17:22:17 +0000 UTC
2408,CLOSED,A make error here that complains that 'not finding grpc_service_pb2.py',,2021-01-13 07:16:45 +0000 UTC
2407,CLOSED,A make error here that complains that 'not finding grpc_service_pb2.py',,2021-01-13 03:23:46 +0000 UTC
2404,CLOSED,Missing typeinfo in shared libraries,,2021-01-12 21:38:35 +0000 UTC
2403,CLOSED,Can't Load Models Using Distributed MinIO,enhancement,2021-05-18 20:27:42 +0000 UTC
2402,CLOSED,Startup error while loading shared libraries,,2021-01-26 16:45:52 +0000 UTC
2401,CLOSED,ONNX Backend Support for Jetpack,,2021-01-26 00:22:53 +0000 UTC
2400,CLOSED,Triton Server Support for Jetson Nano,,2021-01-12 17:27:12 +0000 UTC
2399,CLOSED,How to optimize config: instance_group vs. dynamic_batching,,2021-01-12 02:45:56 +0000 UTC
2398,CLOSED,The maximum batch size of pytorch model hosted by triton, is much smaller than torch jit model.,,2021-01-26 03:19:20 +0000 UTC
2397,CLOSED,Remove unnecessary null pointer checks,,2021-01-26 00:26:22 +0000 UTC
2395,CLOSED,triton server core dumped when client infer (model: bert-tensorrt engine),,2021-01-26 00:28:01 +0000 UTC
2393,CLOSED,randomly same output with last batch,,2021-01-14 01:49:17 +0000 UTC
2392,CLOSED,PIL module not found,,2021-01-08 17:26:21 +0000 UTC
2389,CLOSED,Optimal/Suggested way of handling image requests for optimizing throughput,,2021-01-08 07:50:01 +0000 UTC
2388,CLOSED,Cannot run Pytorch and Tensorflow models consecutively on a single GPU,,2021-01-11 17:02:37 +0000 UTC
2387,CLOSED,Onnx batchsize greater than 1,,2021-08-30 16:53:23 +0000 UTC
2386,CLOSED,Not able to use GKE default driver with Triton Inference Server,,2021-01-07 00:22:27 +0000 UTC
2384,CLOSED,Performance improvement for numpy decoding in Python client,performance,2021-05-19 21:39:19 +0000 UTC
2381,CLOSED,Support for SwiftStack S3 API,enhancement,2021-05-18 20:27:53 +0000 UTC
2379,CLOSED,Intel MKL FATAL ERROR in 20.11 and 20.12,,2021-01-26 00:36:48 +0000 UTC
2377,CLOSED,Getting issue while load FP16 retinaface model,,2022-05-28 18:09:55 +0000 UTC
2376,CLOSED,Question: Kubernetes Deployment with stateful models,,2021-01-26 00:40:26 +0000 UTC
2375,CLOSED,NVTX error occurs when TRITON_ENABLE_CAFFE2 and TRITON_ENABLE_PYTORCH is OFF,,2021-01-26 00:41:20 +0000 UTC
2374,CLOSED,Triton Inference server 20.12 start error in Tesla P4.,,2021-02-03 01:03:35 +0000 UTC
2373,CLOSED,Yolov5s torchscript model shows pytorch backend bugs?,,2021-10-28 01:38:46 +0000 UTC
2372,CLOSED,TritonClient install in windows 10?,,2021-01-04 18:28:33 +0000 UTC
2371,CLOSED,bert optimized onnx model infer error when batch_size > 1,,2020-12-31 05:47:13 +0000 UTC
2370,CLOSED,When I set the concurrency increase, the service returns a data exception,,2021-01-04 17:25:08 +0000 UTC
2369,CLOSED,python backend not support TRITONSERVER_MEMORY_GPU,enhancement,2021-09-01 15:15:59 +0000 UTC
2368,CLOSED,Check failed: size >= 0 (-1655719932 vs. 0),,2021-01-05 16:55:19 +0000 UTC
2367,CLOSED,Triton Inference Server does not use GPU for Jetson Nano.,,2021-01-04 18:43:50 +0000 UTC
2366,CLOSED,pip install tritonclinet on win10,,2021-01-05 02:19:02 +0000 UTC
2365,CLOSED,fail to use perf_client,,2021-01-04 17:48:09 +0000 UTC
2364,CLOSED,HTTP client failed: Send failed since rewinding of the data stream failed,,2021-01-26 00:46:41 +0000 UTC
2363,CLOSED,FP32 inference - ctypes?,,2021-01-02 08:57:21 +0000 UTC
2362,CLOSED,how to build the C++ client libraries on windows,,2021-01-04 17:54:36 +0000 UTC
2361,CLOSED,L4T images for Triton,,2021-01-26 00:47:01 +0000 UTC
2360,CLOSED,Not able to compile client libraries using docker,,2021-01-06 18:54:04 +0000 UTC
2359,CLOSED,Call to GRPC Inference API fails using generated java code,,2021-01-26 00:49:31 +0000 UTC
2357,CLOSED,TensorRT on Jetpack Triton Build,,2020-12-17 17:18:03 +0000 UTC
2356,CLOSED,Tensorflow Saved Model Format tensorflow-gpu==2.0.0 vs. tensorflow==2.3.0,,2021-01-26 00:50:49 +0000 UTC
2355,CLOSED,Questions on building triton server,,2020-12-17 17:14:27 +0000 UTC
2354,CLOSED,docker pull failed from NGC,,2021-01-26 00:51:07 +0000 UTC
2353,CLOSED,CPU memory usage details in metrics,,2021-07-07 21:59:39 +0000 UTC
2347,CLOSED,grpc_simple_client.go "undefined: inference.GRPCInferenceServiceClient",,2020-12-16 16:47:30 +0000 UTC
2345,CLOSED,Is it possible to run Triton without starting a server?,,2023-05-24 11:30:32 +0000 UTC
2342,CLOSED,perf_analyzer failed,,2020-12-16 03:46:00 +0000 UTC
2339,CLOSED,[Libtorch] Triton server produces inconsistent results when hosting multiple models in one GPU,,2022-01-11 01:42:00 +0000 UTC
2337,CLOSED,[Kaldi] 8Khz Model " unexpected size for input tensor.",,2021-01-27 00:02:23 +0000 UTC
2334,OPEN,Option to not provide all inputs specified in config.pbtxt,enhancement,2021-05-10 22:33:45 +0000 UTC
2333,CLOSED,Support scalar input to Triton,,2023-07-02 22:15:22 +0000 UTC
2332,CLOSED,Add ability to choose which graph and signature_def to load on model load,,2021-04-21 17:56:57 +0000 UTC
2331,CLOSED,Supporting multiple signature_def's at runtime,,2021-05-10 22:36:54 +0000 UTC
2330,CLOSED,Pull 20.11-py3-sdk, manifest unknown,,2020-12-10 16:54:28 +0000 UTC
2329,CLOSED,[Typo] in the developer website of trition introduction,,2021-02-03 18:08:49 +0000 UTC
2324,CLOSED,Triton for JetPack does NOT support ONNX backend,enhancement,2021-05-10 22:38:24 +0000 UTC
2323,CLOSED,Unable to start server on CPU-only device,,2022-05-13 17:25:32 +0000 UTC
2322,CLOSED,SegmentationFault error related PinnedMemoryManager,,2021-02-05 21:17:24 +0000 UTC
2321,CLOSED,Implement a custom client using curl and jsoncpp,,2020-12-11 15:44:49 +0000 UTC
2317,CLOSED,Loading TorchScript model fails for Triton in DeepStream,,2021-01-26 00:53:18 +0000 UTC
2315,CLOSED,How to protect trtis (20.02-py3) python code,,2022-11-16 02:08:58 +0000 UTC
2312,CLOSED,HTTPService is starting instead of started,,2020-12-04 16:51:08 +0000 UTC
2311,CLOSED,Failed to finalize CUDA memory manager: CNMEM_STATUS_INVALID_ARGUMENT,,2020-12-03 06:42:22 +0000 UTC
2308,CLOSED,'torch.dtype' object has no attribute 'type',,2020-12-08 18:00:49 +0000 UTC
2307,CLOSED,Serialization Error in readExternam:0 (Type mismatch),,2020-12-17 17:17:44 +0000 UTC
2306,CLOSED,Why is there such a big performance difference between using http and grpc?,,2021-01-26 00:54:51 +0000 UTC
2303,CLOSED,multiple model instances running simultaneously maybe cause gpu memory exhaust. How to avoid it?,,2020-12-10 18:47:37 +0000 UTC
2301,CLOSED,How to load more than one custom plugin, LD_PRELOAD,,2021-04-06 02:32:42 +0000 UTC
2300,CLOSED,Memory leak,,2021-01-05 08:06:06 +0000 UTC
2299,CLOSED,RTX 3000 series?,,2021-01-26 00:55:09 +0000 UTC
2297,CLOSED,Build from sources r2.5 does not work correctly,,2020-12-05 15:48:57 +0000 UTC
2296,CLOSED,error with shared memory with client in triton client sdk: failed to register input shared memory region,,2022-07-06 11:18:31 +0000 UTC
2294,CLOSED,TorchScript concurrent inference net execution time is substantially higher than single inference execution time,,2021-01-26 00:56:06 +0000 UTC
2293,CLOSED,TensorRT version mismatch for NGC containers 20.11 and 20.10,,2021-05-19 09:07:24 +0000 UTC
2291,CLOSED,Fix S3 authentication for IAM roles by including Session token in env vars,,2022-12-16 19:09:53 +0000 UTC
2289,CLOSED,Error Handling in ` model.py `,,2020-12-03 21:44:54 +0000 UTC
2281,CLOSED,Does onnx backend utilize TensorRT while interfering?,,2020-11-22 16:37:54 +0000 UTC
2278,CLOSED,E1120 12:54:31.030207 49 model_repository_manager.cc:1007] failed to load 'yolov3-spp' version 1: Invalid argument: model 'yolov3-spp_0_gpu0', tensor '000_net': the model expects 4 dimensions (shape [1,3,608,608]) but the model configuration specifies 3 dimensions (shape [3,608,608]),,2021-01-26 00:56:54 +0000 UTC
2277,CLOSED,anyone build triton by cmake or buil.py success in ubuntu?,,2021-01-26 00:58:54 +0000 UTC
2276,CLOSED,TFLite support with Google Edge TPU acceleration,enhancement,2022-11-09 15:51:31 +0000 UTC
2275,CLOSED,Different result: ensemble_model vs. ensemble_client,,2020-12-07 17:43:16 +0000 UTC
2274,CLOSED,Client side memory leak in python sdk using shared memory,,2020-11-23 18:19:05 +0000 UTC
2269,CLOSED,[Crash] Triton Server 20.10 crashing with 'double free or corruption (out)' with TensorFlow XLA-GPU enabled,,2021-01-26 01:00:10 +0000 UTC
2268,CLOSED,Why can't the renamed "triton_python_backend_utils.py" and other modules be imported in models directory?,,2021-02-19 14:43:18 +0000 UTC
2267,CLOSED,Adding Custom layer to Triton,,2020-11-20 05:20:56 +0000 UTC
2265,CLOSED,onnx model inference cpu usage,,2021-01-26 01:00:57 +0000 UTC
2260,CLOSED,Issue while Serving the model using Triton Server,,2020-11-18 19:05:10 +0000 UTC
2258,CLOSED,Is there any guide to deploy QuartzNet through triton inference server?,,2022-12-22 13:58:12 +0000 UTC
2254,CLOSED,Request for unknown model: 'resnet50_netdef' is not found,,2020-11-17 05:59:04 +0000 UTC
2253,CLOSED,GCS permissions to load models from bucket?,,2020-11-17 05:46:35 +0000 UTC
2250,CLOSED,How to build libimagepreprocess.so?,,2020-11-16 17:16:33 +0000 UTC
2245,CLOSED,How to downgrade the cuda version in the "nvcr.io/nvidia/tritonserver:20.10-py3",,2020-11-11 23:14:37 +0000 UTC
2243,CLOSED,Protobuf version conflicts r19.10,,2020-11-12 13:36:05 +0000 UTC
2240,CLOSED,Jetson support of pytorch and PTH-TRT,enhancement,2023-07-10 22:58:28 +0000 UTC
2239,CLOSED,cmake build server error r20.10 or master,,2020-11-20 05:57:59 +0000 UTC
2238,CLOSED,cmake build server error r20.10,,2022-04-13 03:33:16 +0000 UTC
2237,CLOSED,Memory leaking with many TensorFlow models and warmup,,2020-11-11 23:31:25 +0000 UTC
2236,CLOSED,Can we put image download to server side?,,2021-02-04 17:57:09 +0000 UTC
2234,CLOSED,Use TRTIS optimized model instead of running optimization again on load,,2020-11-09 17:18:29 +0000 UTC
2231,CLOSED,Model with dynamic shapes and TensorRT optimization outputs nonsense,,2021-12-17 10:05:00 +0000 UTC
2229,CLOSED,sequence_batch_scheduler.cc:399 The previous sequence did not end before this sequence start,bug,2020-11-11 23:42:19 +0000 UTC
2228,CLOSED,Config.pbtx for Efficientdet-D0,,2020-11-09 20:37:44 +0000 UTC
2227,CLOSED,Segmentation Fault when launching the server with custom built TensorRT plugins,,2021-01-04 02:30:45 +0000 UTC
2226,CLOSED,Mismatch between config.pbtxt and reported model config json,,2020-11-12 21:25:51 +0000 UTC
2225,CLOSED,Error occurs when i run the triton server with docker(Quick start),,2020-11-05 17:22:25 +0000 UTC
2224,CLOSED,The README file in this repo has a bad link - [404:NotFound],,2020-11-04 19:01:57 +0000 UTC
2223,CLOSED,Failed to load Tensorflow models,,2020-11-05 12:54:57 +0000 UTC
2222,CLOSED,Support for AWS Inferentia?,,2022-05-11 09:11:08 +0000 UTC
2221,CLOSED,CUDA Shared Memory for pytorch cuda tensor?,,2021-04-08 01:02:55 +0000 UTC
2210,CLOSED,Array object is not Json Serializable on triton_client.infer,,2020-11-11 23:49:39 +0000 UTC
2209,CLOSED,Debian package, when?,,2020-11-11 23:50:16 +0000 UTC
2205,CLOSED,Triton Inference Server on ppc64le,,2021-07-07 22:01:12 +0000 UTC
2195,CLOSED,C++ Clients V2 Api - Inference results output and batch Id,,2020-10-30 19:50:17 +0000 UTC
2194,CLOSED,Python API references disappeared from documentation !,,2020-10-29 15:52:45 +0000 UTC
2193,CLOSED,provided PTX was compiled with an unsupported toolchain?,,2020-11-03 22:15:38 +0000 UTC
2192,CLOSED,Can't get max-batch-size to work,,2020-11-11 23:51:36 +0000 UTC
2187,CLOSED,Could you provide a tritonclient in Java?,,2020-11-11 23:51:49 +0000 UTC
2186,CLOSED,Error when build TRTIS Docker Image,,2020-11-11 23:52:19 +0000 UTC
2179,CLOSED,failed to load 'densenet_onnx' version 1: Invalid argument: unknown platform 'onnxruntime_onnx',,2023-01-20 09:17:47 +0000 UTC
2169,CLOSED,Fail to build identity_backend,,2020-10-26 12:38:28 +0000 UTC
2168,CLOSED,HTTP end point doesn't support models with decoupled transaction policy,,2020-10-26 17:29:39 +0000 UTC
2167,CLOSED,Bypass arguments in Ensemble Models' input and output,,2020-10-27 16:57:16 +0000 UTC
2166,CLOSED,IndexError: list index out of range,,2020-10-26 19:56:38 +0000 UTC
2165,CLOSED,"Invalid argument - repository path is not a valid directory" when running local S3 on port 80,,2021-01-26 01:05:44 +0000 UTC
2163,OPEN,Pull repository from different s3 accounts (multiple credentials),enhancement,2022-08-30 22:25:50 +0000 UTC
2160,CLOSED,CMake problems for client library in v2.3.0,,2021-07-07 22:21:57 +0000 UTC
2159,CLOSED,Support different network protocols,,2021-09-10 19:18:04 +0000 UTC
2158,CLOSED,Ragged batching support for ML backends,,2021-07-07 22:24:35 +0000 UTC
2157,CLOSED,Python backend cannot support KIND_GPU in model config,,2023-05-18 01:54:44 +0000 UTC
2156,CLOSED,Can multiple instances can use the same GPU shared memory?,,2022-05-30 02:50:27 +0000 UTC
2155,CLOSED,Do multiple instances of the same model share parameters?,,2020-10-22 15:50:26 +0000 UTC
2154,CLOSED,What is the difference between triton-inference-server/server/src/backends/backend and triton-inference-server/backend?,,2020-10-22 17:05:14 +0000 UTC
2153,CLOSED,Where is the definition of TRITONBACKEND_ModelSetState,,2020-10-22 16:01:35 +0000 UTC
2148,CLOSED,Wrong value of byte_size returned by TRITONBACKEND_InputProperties,,2020-10-27 23:27:21 +0000 UTC
2147,CLOSED,S3 Storage with POLL mode reloads models constantly,,2021-01-28 22:39:32 +0000 UTC
2144,CLOSED,Non-numeric subdirectories are not ignored for version convertion,,2020-10-27 02:08:34 +0000 UTC
2143,CLOSED,Windows support,,2020-10-20 15:47:48 +0000 UTC
2140,CLOSED,Failed to connect all addresses,,2020-10-20 23:58:03 +0000 UTC
2138,CLOSED,Failing to invoke triton methods via GRPC,,2020-10-20 07:13:06 +0000 UTC
2137,CLOSED,The overhead cost so much time,,2020-10-29 02:32:30 +0000 UTC
2136,CLOSED,Different batch-size requests causes error, even the loading model supports batching,,2021-08-30 02:53:14 +0000 UTC
2135,CLOSED,Wrong results when using an onnx model with tensorrt gpu_execution_accelerator and dynamic axes,,2021-05-19 21:44:16 +0000 UTC
2133,CLOSED,Deploying Bert TensorRT model with Triton,,2020-10-27 19:30:22 +0000 UTC
2130,CLOSED,perf_client fails with "Received message larger than max",,2022-10-14 20:36:38 +0000 UTC
2127,CLOSED,Triton server multiple initialization errors, under kubernetes,,2020-10-21 22:18:59 +0000 UTC
2126,CLOSED,How to run Triton Inference Server docker container on a Jetson Nano?,,2020-10-15 15:22:21 +0000 UTC
2123,CLOSED,Multi-inputs with dynamic axes in ONNX Graph not corrected reported on loading,,2020-10-14 21:03:55 +0000 UTC
2122,CLOSED,Prometheus output differs from nvidia-smi,,2020-11-13 22:31:41 +0000 UTC
2121,CLOSED,TRITON INFERENCE WITH ENSEMBLE MODEL,,2020-10-20 17:00:36 +0000 UTC
2120,CLOSED,Indiscriminate use of ExternalProject_Add,,2020-10-20 17:00:52 +0000 UTC
2118,CLOSED,Can request the Triton Inference Server by using the 'request package' in python rather than 'client library'?,,2021-05-12 09:09:33 +0000 UTC
2112,CLOSED,Missing methods in Kotlin/Java code generated using gRPC,,2021-01-26 01:07:21 +0000 UTC
2111,CLOSED,When batch_size is 1, return Stream removed.,,2020-11-11 23:55:02 +0000 UTC
2110,CLOSED,20.10 build error ( linking error ),,2021-05-19 17:23:00 +0000 UTC
2109,CLOSED,Memory leak in 20.09?,,2020-10-20 22:14:57 +0000 UTC
2107,CLOSED,Cannot deploy on AWS EKS gpu nodes (p3, g4, etc.),,2020-10-12 19:12:10 +0000 UTC
2104,CLOSED,Triton 20.08 hangs on inference when Custom Backend (Legacy, either v1 or v2) has no output tensors specified,,2020-10-20 16:06:39 +0000 UTC
2103,CLOSED,How to debug failed inference request,,2023-02-22 14:01:49 +0000 UTC
2101,CLOSED,Release 2.3.0- Missing custom backend sdk,,2020-10-08 21:34:55 +0000 UTC
2100,CLOSED,fail to reload a model with the same name and different configurations,,2020-10-16 17:48:50 +0000 UTC
2098,CLOSED,Jetson build for 20.09,,2020-10-08 07:40:20 +0000 UTC
2097,CLOSED,Tensorflow models don't seem to batch properly,,2021-07-30 18:55:18 +0000 UTC
2095,CLOSED,Triton unable to access GPU on Jetson Nano,,2020-10-07 18:55:42 +0000 UTC
2092,CLOSED,V2 API Migration,,2021-04-12 17:10:18 +0000 UTC
2091,CLOSED,Failing to comsume Triton's Prometheus in Grafana,,2021-11-03 17:04:14 +0000 UTC
2090,CLOSED,Unclear torch model failure message,,2022-05-25 10:02:34 +0000 UTC
2085,OPEN,MXNet support,enhancement,2020-10-09 18:21:27 +0000 UTC
2082,CLOSED,Issues with CentOS client build,,2021-10-26 13:09:07 +0000 UTC
2076,CLOSED,Model auto loading unuseful,,2022-02-25 19:39:42 +0000 UTC
2068,CLOSED,Failed to build identity_backend, square_backend and repeat_backend,,2020-10-26 16:57:39 +0000 UTC
2067,CLOSED,the results are in descending order, is it right?,,2020-10-09 01:23:01 +0000 UTC
2060,CLOSED,how to create .so file for a model,,2020-09-28 16:03:34 +0000 UTC
2059,CLOSED,Send a raw image file from a client to a Triton server,,2020-10-20 17:07:08 +0000 UTC
2053,CLOSED,Different GPUs cause huge memory consumption differences,,2020-09-22 15:20:21 +0000 UTC
2052,CLOSED,the model expects 0 dimensions (shape []) but 0 dimensionsid not allowed,,2020-09-22 15:21:59 +0000 UTC
2049,CLOSED,How to use custom-backend with Tritonbackend.h and run it on GPU,,2020-10-06 13:34:44 +0000 UTC
2048,CLOSED,Expected Tuple but got GenericDict,,2021-06-29 14:32:36 +0000 UTC
2046,CLOSED,Same code working for one postprocess backend but not for another.,,2020-09-23 17:59:28 +0000 UTC
2040,CLOSED,How to install tritonclient?,,2020-09-22 15:24:32 +0000 UTC
2036,CLOSED,Separate weights from plan file for TensorRT backend,,2020-09-25 05:06:57 +0000 UTC
2028,CLOSED,Use-after-free in ensemble scheduler when using legacy custom model,,2020-10-02 22:00:21 +0000 UTC
2027,CLOSED,Error when using Ceph S3 storage as a model repository,enhancement,2021-05-18 20:28:10 +0000 UTC
2025,CLOSED,Deploy Detectron2 Mask R-CNN inside Triton,,2022-12-16 11:41:44 +0000 UTC
2024,CLOSED,How to deploy Detectron2 model using pytorch?,,2022-03-01 02:30:31 +0000 UTC
2023,CLOSED,unload model not release mem,,2020-09-22 15:38:27 +0000 UTC
2021,CLOSED,Support input/output compression,,2021-04-30 17:22:14 +0000 UTC
2020,OPEN,Ability to disable or redirect cout/cerr,enhancement,2021-09-14 17:28:41 +0000 UTC
2019,CLOSED,Stricter model versioning,,2021-11-12 22:57:54 +0000 UTC
2018,CLOSED,Control number of threads used by CPU server,,2022-11-08 19:55:51 +0000 UTC
2014,CLOSED,Ensemble model stuck perf test,,2020-09-17 07:35:41 +0000 UTC
2008,CLOSED,can't parse path with --model-repository=s3://host:port/demobucket/path,,2020-09-16 02:34:16 +0000 UTC
2004,CLOSED,Triton Inference Server on Azure,,2020-10-05 18:35:58 +0000 UTC
2003,CLOSED,oom happens in GTX1080Ti(11GB) but not in RTX2080Ti(11GB),,2020-09-10 18:34:13 +0000 UTC
2002,CLOSED,TF-TRT model's TRTEngineOP loaded on the first GPU only,,2021-11-12 22:57:31 +0000 UTC
1996,CLOSED,Datatype difference in model config and HTTP request body,,2021-01-26 01:08:09 +0000 UTC
1995,CLOSED,Can't get the shape right,,2020-09-11 06:14:37 +0000 UTC
1994,CLOSED,Loading libimagepreprocess.so get undefined symbol: _ZNK6google8protobuf7Message25InitializationErrorStringEv,,2020-09-10 08:24:43 +0000 UTC
1993,CLOSED,[enforce fail at operator.cc:76] blob != nullptr. op Cast: Encountered a non-existing input blob: data,,2020-11-16 18:27:16 +0000 UTC
1990,CLOSED,Whats the equivalent of /api/status?format=json in v2?,,2020-09-09 09:21:59 +0000 UTC
1989,CLOSED,Unreasonable handler overhead using python client 20.08,,2020-09-09 16:33:59 +0000 UTC
1988,CLOSED,Invalid argument: unsupported datatype 'TYPE_BYTES' on 20.08,,2020-09-14 16:01:59 +0000 UTC
1982,CLOSED,the metric with ensemble_scheduling.,,2020-09-04 16:13:30 +0000 UTC
1972,CLOSED,is there pre-build docker images for jetson?,,2020-09-08 12:45:06 +0000 UTC
1971,CLOSED,GRPC python client get_model_config,,2020-09-09 18:16:08 +0000 UTC
1968,CLOSED,trion on kfserving performance,,2020-09-02 20:53:12 +0000 UTC
1966,CLOSED,How to use dynamic_batching in ensemble for a pipeline,,2020-09-02 15:45:09 +0000 UTC
1965,CLOSED,How do I use AsyncRun?,,2020-09-02 22:05:59 +0000 UTC
1964,CLOSED,simplest way to use tensorflow 1.12.0 in triton server 20.07,,2020-09-04 17:18:19 +0000 UTC
1963,CLOSED,REST API is too slow when using python request API,,2020-09-05 11:23:39 +0000 UTC
1962,CLOSED,REST API is too slow when using python request API,,2020-09-02 03:00:47 +0000 UTC
1961,CLOSED,docker install error,,2020-09-01 15:48:12 +0000 UTC
1960,CLOSED,Missing models to run client examples,,2020-09-10 15:53:52 +0000 UTC
1959,CLOSED,Custom Backend load custom model files,,2020-09-01 15:56:40 +0000 UTC
1958,CLOSED,Cannot set Tensorflow backend option "allow-soft-placement" with docker,,2020-09-02 21:41:56 +0000 UTC
1950,CLOSED,Please add max_queue_delay_microseconds and possibly preferred_batch_size to StrategyDirect,,2020-10-22 21:46:38 +0000 UTC
1948,CLOSED,:8000/v2/health/live and :8000/v2/health/ready returns 400,,2021-04-10 11:32:00 +0000 UTC
1947,CLOSED,Tritonbackend.h or Custom.h, unclear ensemble model,,2020-09-01 16:01:44 +0000 UTC
1940,CLOSED,OOM error API,,2020-10-09 18:25:30 +0000 UTC
1939,CLOSED,client request error,,2020-08-31 15:51:38 +0000 UTC
1935,CLOSED,Failed to load 'resnet50_netdef',,2020-09-01 06:05:14 +0000 UTC
1931,CLOSED,k8s triton cluster error: creating server: Internal - failed to stat file,,2020-08-31 17:47:10 +0000 UTC
1927,CLOSED,How to safely restart/pullout a triton server in prod env?,,2020-08-25 16:43:27 +0000 UTC
1925,CLOSED,jetson can not use gpu now?,,2020-08-28 05:39:33 +0000 UTC
1919,CLOSED,Perf_Client TF Warm-Up Period,,2020-08-24 15:45:52 +0000 UTC
1915,CLOSED,cudashm.get_contents_as_numpy always using gpu 0,,2020-09-11 20:12:56 +0000 UTC
1908,CLOSED,can we use inference server for the codes which doesnt have any model ?,,2020-08-31 17:54:44 +0000 UTC
1907,CLOSED,document empty error while running image client example,,2020-08-31 21:25:25 +0000 UTC
1906,CLOSED,PyTorch 1.6 support?,,2021-04-19 08:49:26 +0000 UTC
1905,CLOSED,[libprotobuf FATAL /workspace/build/grpc-repo/src/grpc/third_party/protobuf/src/google/protobuf/repeated_field.h:1193] CHECK failed: (index) < (current_size_): terminate called after throwing an instance of 'google::protobuf::FatalException' what(): CHECK failed: (index) < (current_size_): Aborted (core dumped),,2020-08-21 10:54:29 +0000 UTC
1904,CLOSED,CPU memory usage constantly increases while doing inference,,2021-04-20 11:39:38 +0000 UTC
1902,CLOSED,How is the concurrent model execution support feature performs? Is there any benchmark data?,,2020-08-13 18:06:42 +0000 UTC
1900,CLOSED,No error or warnings when underlying models of ensemble model request doesn't succeed,,2020-08-17 17:35:01 +0000 UTC
1899,CLOSED,Perf Client Failed while inferencing request on the loaded bert model !!,,2020-08-31 19:01:54 +0000 UTC
1894,CLOSED,tensorrtserver.api.InferenceServerException: [ 0] expecting 1 invocations of SetRaw for input 'INPUT__0', one per batch entry,,2020-08-13 16:39:46 +0000 UTC
1893,CLOSED,Example how to work with shared memory in multi-threaded application?,,2020-12-06 13:58:30 +0000 UTC
1889,CLOSED,Config.pbtxt Issue,,2020-08-11 00:36:15 +0000 UTC
1888,CLOSED,Docker build fails with ln: target 'libonnxruntime.so' is not a directory,,2020-08-11 01:51:00 +0000 UTC
1878,CLOSED,Question on perf_client (both client and server version at 20.06),,2020-08-07 02:32:41 +0000 UTC
1876,CLOSED,Question on dynamic batching and preferred batch size,,2020-08-06 03:18:01 +0000 UTC
1872,CLOSED,Typo in the v2.0.0 release notes - Jetson Jetpack Support,,2020-08-05 18:44:35 +0000 UTC
1871,CLOSED,String field 'nvidia.inferenceserver.ModelInferResponse.InferOutputTensor.ParametersEntry.key' contains invalid UTF-8 data when parsing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.,,2020-08-12 14:30:03 +0000 UTC
1870,CLOSED,Torchvision ops not compiled with GPU support,,2021-08-04 02:08:38 +0000 UTC
1869,CLOSED,Trition V2 incorrectly computing number of elements in the batch,,2020-08-07 02:27:55 +0000 UTC
1861,CLOSED,20.07 pre-built release not working?,,2020-08-03 16:08:03 +0000 UTC
1856,CLOSED,Are the Triton clients thread safe?,,2020-08-02 16:27:01 +0000 UTC
1854,CLOSED,TLT Examples,,2020-09-12 15:25:30 +0000 UTC
1853,CLOSED,Slow ONNX inference,,2020-08-05 16:46:33 +0000 UTC
1845,CLOSED,Inference with 20.06 using curl,,2020-07-29 15:56:11 +0000 UTC
1844,CLOSED,Understanding more about triton,,2020-08-04 15:48:18 +0000 UTC
1839,CLOSED,The useage of GPU memory in TRT environment is pretty more than Python environment for a torch scripte model.,,2020-07-30 10:15:49 +0000 UTC
1838,CLOSED,GPU support for triton container?,,2020-07-28 15:42:56 +0000 UTC
1835,CLOSED,Installation of Triton Server with helm chart,,2021-02-08 16:35:25 +0000 UTC
1834,CLOSED,Perf client output,,2020-07-31 19:19:16 +0000 UTC
1832,CLOSED,Error in request.raw_input_contents.extend([input_bytes]),,2020-07-27 17:44:55 +0000 UTC
1829,CLOSED,Docker container won't start due to entrypoint.sh error,,2020-07-24 22:36:39 +0000 UTC
1827,CLOSED,new v2 api embeds nvidia.inferenceserver as package name in .proto,,2020-07-30 16:01:57 +0000 UTC
1822,CLOSED,How to send binary tensor data to ensemble model by HTTP request?,,2020-07-24 15:37:31 +0000 UTC
1821,CLOSED,gRPC communication extremely slow,,2020-08-03 19:01:40 +0000 UTC
1820,CLOSED,Suggestion: Use distributed flags for commandline passing,,2021-05-10 22:29:57 +0000 UTC
1814,CLOSED,Jetson Triton v1 to v2 Tensorflow,,2020-07-23 18:11:55 +0000 UTC
1811,CLOSED,[question] Does it support distributed serving?,,2021-04-22 07:48:37 +0000 UTC
1810,CLOSED,warm up issue,,2020-07-20 23:17:54 +0000 UTC
1809,CLOSED,OpenVINO unsupported operation,,2020-07-20 23:21:58 +0000 UTC
1808,CLOSED,E0720 03:43:07.419723 1 main.cc:1099] error: creating server: INTERNAL - failed to load all models,,2020-08-05 09:37:21 +0000 UTC
1807,CLOSED,cmake build error,,2020-07-18 17:07:57 +0000 UTC
1806,CLOSED,model_version not set in inference response unless it is set in request,,2020-07-20 18:50:30 +0000 UTC
1804,CLOSED,TensorFlow 2 support,,2020-09-01 23:52:10 +0000 UTC
1803,CLOSED,Python API async_infer http callback,,2021-12-06 17:30:34 +0000 UTC
1800,CLOSED,Custom backend and generic computer vision algorithms,,2020-07-15 17:22:27 +0000 UTC
1798,CLOSED,how to stream multisource camera in real-time with trtis,,2020-07-22 03:01:54 +0000 UTC
1796,CLOSED,Invalid argument: warmup setting expects n bytes,,2022-04-26 08:33:20 +0000 UTC
1795,CLOSED,Unable to load model config with zero/random data warm up,,2020-07-15 18:26:54 +0000 UTC
1794,CLOSED,fp16 issue in 20.03,,2020-08-15 07:19:32 +0000 UTC
1793,CLOSED,Auto switching between GPU and CPU for production environment?,,2020-07-14 02:46:06 +0000 UTC
1790,CLOSED,Clip to predictions in ensembles,,2020-07-16 16:56:08 +0000 UTC
1787,CLOSED,CUDA out of memory during inference, not during model loading,,2020-07-14 02:41:11 +0000 UTC
1786,CLOSED,Choose TensorRT version to use for Triton,,2020-07-14 02:36:33 +0000 UTC
1778,CLOSED,Mask RCNN TensorRT in Triton,,2020-07-09 17:10:46 +0000 UTC
1777,OPEN,About Model Encrypted,enhancement,2022-03-15 00:52:19 +0000 UTC
1776,CLOSED,triton_client.infer(...) error "Received message larger than max (33816626 vs. 4194304)",,2020-07-16 22:42:24 +0000 UTC
1766,CLOSED,libevhtp patch step fails on sed: extra characters at the end of g command,,2020-07-11 06:18:03 +0000 UTC
1765,CLOSED,Failed to allocate CUDA memory with byte size 78643200 on GPU 0: CNMEM_STATUS_OUT_OF_MEMORY, falling back to pinned system memory,,2020-07-08 21:26:43 +0000 UTC
1764,CLOSED,Wrong output for my triton segmentation model,,2020-07-09 04:03:48 +0000 UTC
1762,CLOSED,Load onnx model error in trtis 19.09 container?,,2020-07-08 15:49:22 +0000 UTC
1752,CLOSED,TRT run the torchscripte model failed!,,2020-08-04 15:49:06 +0000 UTC
1749,CLOSED,tensorrtserver.api.InferenceServerException: [ 0] status request did not return status,,2020-08-04 15:49:43 +0000 UTC
1746,CLOSED,CPU memory slowly increases when reusing an InferContext object for many times,,2021-04-20 11:39:11 +0000 UTC
1745,CLOSED,tritonclientutils.utils.InferenceServerException: [StatusCode.UNIMPLEMENTED],,2020-11-28 00:33:47 +0000 UTC
1741,CLOSED,Changing batch sizes when using cuda shared memory,,2020-07-08 07:38:04 +0000 UTC
1733,CLOSED,Loading of models from GCS is prohibitively slow,,2020-08-04 15:51:06 +0000 UTC
1724,CLOSED,Triton container not loading the latest version of model when S3 location is updated,,2020-07-08 19:30:09 +0000 UTC
1723,CLOSED,Why does the client scale the image nparray with particular models?,,2020-07-02 20:47:06 +0000 UTC
1722,CLOSED,Can't download image 20.03.1-py3 on Kind k8s 1.15.11,,2020-08-04 15:51:55 +0000 UTC
1718,CLOSED,S3 custom model load is not working,,2020-07-24 16:12:07 +0000 UTC
1714,CLOSED,Docker build error,,2020-06-25 23:48:14 +0000 UTC
1708,CLOSED,Memory leak in get_async_run_results (when async call and error),,2020-07-01 23:46:37 +0000 UTC
1707,CLOSED,ONNX->TensorRT model fails supporting multiple batch sizes,,2020-07-21 07:55:14 +0000 UTC
1706,CLOSED,Broader input formatting supported,,2020-08-04 15:55:39 +0000 UTC
1705,CLOSED,WSL2 + CUDA,,2021-12-29 18:36:40 +0000 UTC
1693,CLOSED,async is too slow on http,,2020-08-04 15:57:23 +0000 UTC
1688,CLOSED,difference inferencing result between http v1 and grpc v2,,2020-06-22 04:01:53 +0000 UTC
1683,CLOSED,Client build fails when excluding GPU support,,2020-06-25 16:00:27 +0000 UTC
1682,CLOSED,A shape problem,,2020-06-22 17:19:32 +0000 UTC
1665,CLOSED,GRPC v2 low bandwidth,,2020-06-29 16:00:43 +0000 UTC
1663,CLOSED,only one host thread launch CUDA kernel in 20.03,,2021-01-26 01:09:03 +0000 UTC
1662,CLOSED,Fine grained Division,,2020-06-15 15:15:51 +0000 UTC
1661,CLOSED,instance_group and --concurrency-range,,2020-06-22 17:19:55 +0000 UTC
1660,CLOSED,Variable-length, row-based, TF Example and TF ExampleListWithContext support,,2021-09-08 20:23:34 +0000 UTC
1653,CLOSED,Cuda shared memory support for Custom Backends,,2020-06-11 18:03:30 +0000 UTC
1649,CLOSED,REST API V2 call is shutting down server,,2023-01-11 08:31:23 +0000 UTC
1648,CLOSED,Error when updating context stat,,2020-06-22 17:20:15 +0000 UTC
1647,CLOSED,using LOG_VERBOSE(L) in custom backend not work,,2020-06-10 15:58:56 +0000 UTC
1644,CLOSED,Distribute model across multiple GPUs,,2020-08-14 15:45:38 +0000 UTC
1637,CLOSED,Preformance seems to be poor in benchmark.,,2020-06-19 15:49:45 +0000 UTC
1636,CLOSED,Memory leak when running simple_cuda_shm_client?,,2020-06-29 18:05:34 +0000 UTC
1633,CLOSED,How to solve "no next action, trigger OnComplete()" plz?,,2020-06-09 00:39:42 +0000 UTC
1632,CLOSED,/opt/tritonserver/nvidia_entrypoint.sh: line 93: exec: --: invalid option,,2020-06-08 15:50:48 +0000 UTC
1625,CLOSED,Transfer Learning Toolkit,,2021-09-08 20:20:36 +0000 UTC
1624,CLOSED,Tritonhttpclient should support https,enhancement,2020-06-19 03:33:46 +0000 UTC
1621,CLOSED,Is triton inference server same with TRTIS?,,2020-06-05 16:23:56 +0000 UTC
1609,CLOSED,which gpu will instance model exist if not set gpus:[0],,2020-06-05 16:29:58 +0000 UTC
1598,CLOSED,Release dates 20.05 and 20.06,,2020-06-09 22:08:06 +0000 UTC
1593,CLOSED,Problems using Pyrthon API to access the triton server,,2020-06-02 15:56:53 +0000 UTC
1589,CLOSED,Troubleshooting "CUDA Driver unavailable",,2020-06-02 16:37:24 +0000 UTC
1588,CLOSED,docker run triton-inference-server error when I replace the models with my own.,,2020-06-02 16:38:13 +0000 UTC
1578,CLOSED,Differences between the result of Triton Inference Server and mere GPU,,2021-03-01 05:49:21 +0000 UTC
1577,CLOSED,Unable to build tritonserver_client through Dockerfile.client,,2020-06-25 17:33:45 +0000 UTC
1567,CLOSED,instance_group issue,,2020-05-28 20:14:01 +0000 UTC
1566,CLOSED,Compatibility of Instance Groups Setting,,2020-05-28 20:46:14 +0000 UTC
1557,CLOSED,Strange GPU usage on Jetson Nano,,2020-05-29 10:13:39 +0000 UTC
1556,CLOSED,tensorflow 1.13.1 model and triton server 20.03,,2020-06-02 17:37:03 +0000 UTC
1555,CLOSED,max_workspace_size_bytes can't set in config.pbtxt,,2020-05-28 20:07:54 +0000 UTC
1554,CLOSED,Docker Image Not Found,,2020-05-27 16:15:55 +0000 UTC
1546,CLOSED,Successfully loaded torchscript model failed with "CUDA error: CUBLAS_STATUS_NOT_INITIALIZED" when called for inference,,2020-06-22 19:25:53 +0000 UTC
1545,CLOSED,tensorrtserver.api.InferenceServerException: [inference:0 49] unexpected shape for output 'features', model configuration shape is [1,128], inference shape is [1,128],,2020-05-27 07:35:09 +0000 UTC
1544,CLOSED,perfclient shows weirdly low throughput compared to client application,,2020-06-12 03:22:08 +0000 UTC
1543,CLOSED,GRPC client failed / HTTP client failed,,2020-06-11 19:00:36 +0000 UTC
1534,CLOSED,regex incorrect defined for s3 path.,,2020-05-28 18:28:10 +0000 UTC
1529,CLOSED,TritonRT server on EKS not being able to read from AWS S3,,2020-05-22 20:44:04 +0000 UTC
1528,CLOSED,How to support models with both CPU and GPU?,,2020-06-25 17:32:52 +0000 UTC
1524,OPEN,SSD_MobileNetv1_COCO label_filename incorrect classification,enhancement,2022-08-10 06:40:43 +0000 UTC
1522,CLOSED,XLA Warmup for Multiple Instances and Batch Sizes,,2021-09-08 20:16:14 +0000 UTC
1521,CLOSED,GRPC Configuration Documentation, Defaults, and Examples,,2020-05-26 21:08:43 +0000 UTC
1517,CLOSED,Run on Jetson - continuation,,2020-05-21 08:33:27 +0000 UTC
1516,CLOSED,parameters in output tensor,,2020-05-20 19:50:48 +0000 UTC
1515,CLOSED,Provide steps for compiling PyTorch custom-ops library against the nightlies,,2020-05-26 21:56:40 +0000 UTC
1514,CLOSED,Document steps for how to compile Pytorch custom-ops library for running with Triton,,2020-05-21 23:13:33 +0000 UTC
1510,CLOSED,Docker Error for nvcr.io/nvidia/deepstream:5.0-dp-20.04-triton,,2020-05-19 21:50:34 +0000 UTC
1507,CLOSED,General observation of Triton Server OOMs.,,2021-09-30 18:58:31 +0000 UTC
1505,CLOSED,Compilation error using C++ client library on Win10 when doing inference with batch_size > 1,,2020-05-19 18:53:14 +0000 UTC
1502,CLOSED,Multiple Inputs Model Failing: Failed to update context stat: [ 0] INTERNAL - Timer not set correctly.,,2020-05-22 23:29:50 +0000 UTC
1501,CLOSED,How to use label_filename,,2021-06-27 13:23:00 +0000 UTC
1500,CLOSED,Running inference with Trits is much slower than running model Pytorch directly,,2020-06-02 18:15:20 +0000 UTC
1499,CLOSED,Facing OOM while running TF models,,2020-05-21 18:58:59 +0000 UTC
1495,CLOSED,HTTP Response Occasionally Cut Off,,2020-06-25 17:23:21 +0000 UTC
1485,CLOSED,Allow Triton to report all health metrics at startup,,2020-05-14 18:09:47 +0000 UTC
1483,CLOSED,where is the TRTSERVER_Server definition?,,2020-05-14 07:48:57 +0000 UTC
1477,CLOSED,Onnx GPU inference on GKE not possible,,2020-05-15 23:51:44 +0000 UTC
1476,CLOSED,[Question]support ONNX opset 11,,2020-05-13 14:06:55 +0000 UTC
1472,CLOSED,Enable AMP without any TensorRT optimization for Tensorflow in the inference server.,,2020-07-22 15:54:02 +0000 UTC
1471,CLOSED,predict failed with tensorrtserver-1.12.0,,2020-05-14 01:45:51 +0000 UTC
1468,CLOSED,Run on Jetson,,2022-09-26 23:09:14 +0000 UTC
1458,CLOSED,AttributeError: function 'SharedMemoryControlContextCudaRegister' not found (Windows 10),,2020-05-14 15:09:36 +0000 UTC
1454,CLOSED,saved model: Expected image, got empty file,,2020-05-12 10:01:24 +0000 UTC
1453,CLOSED,failed to build triton-client,,2020-05-15 06:34:45 +0000 UTC
1441,CLOSED,libpytorch backend occupies more gpu memory than pytorch,,2020-05-08 18:07:50 +0000 UTC
1440,CLOSED,How to size how many models can be served simultaneously.,,2020-05-13 16:06:24 +0000 UTC
1439,CLOSED,Sending some metrics from triton container,,2020-05-08 15:48:25 +0000 UTC
1429,CLOSED,Improve RapidJSON support in build,enhancement,2021-11-29 21:02:36 +0000 UTC
1428,CLOSED,Don't require curl CMake config,,2020-08-11 16:13:51 +0000 UTC
1427,CLOSED,Thread safety in channel map,,2020-07-11 00:17:43 +0000 UTC
1426,CLOSED,Support https connections,,2020-07-16 15:11:18 +0000 UTC
1425,CLOSED,Support client-side deadlines/timeouts for AsyncRun callback,,2020-07-11 00:17:43 +0000 UTC
1423,CLOSED,Windows 10 Triton Client Build Does Not Generate a .whl file,,2020-05-11 18:54:22 +0000 UTC
1415,CLOSED,fasterrcnn_resnet50_fpn TorchScript model cannot be loaded,,2021-06-29 16:55:47 +0000 UTC
1391,CLOSED,Reduce Docker Image Size,,2020-06-15 15:25:03 +0000 UTC
1390,CLOSED,how to send input request to the loaded bert_pt model,,2020-05-18 20:34:22 +0000 UTC
1385,CLOSED,Allow customization of cuda sync behavior,enhancement,2020-06-17 18:18:55 +0000 UTC
1378,CLOSED,how to make TensorRT Optimization?,,2020-05-08 16:34:11 +0000 UTC
1377,CLOSED,Server fails to load tensorflow_savedmodel when using s3 model repository,,2020-05-08 15:36:21 +0000 UTC
1372,CLOSED,perf_client batchsize -b issue,,2020-05-08 16:35:15 +0000 UTC
1371,OPEN,Is there any plan to support new pyTorch(==1.5) model archive format?,enhancement,2020-11-30 19:33:23 +0000 UTC
1370,CLOSED,How to free GPU memory in perf_client,,2020-05-08 16:36:09 +0000 UTC
1369,CLOSED,Error serving PyTorch image crowd counting model,,2020-05-02 17:12:14 +0000 UTC
1368,CLOSED,How to use dynamic variable size on multiple dimension?,,2020-05-07 07:26:41 +0000 UTC
1367,CLOSED,config.pbtxt should not contain any directory name,enhancement,2020-08-26 22:03:04 +0000 UTC
1366,CLOSED,Version number 01 fails to load.,,2020-04-29 21:10:27 +0000 UTC
1361,CLOSED,fp16 c++ support example,,2020-04-28 03:26:15 +0000 UTC
1360,CLOSED,How to pass scalar value via input - MTCNN,,2020-04-30 17:20:24 +0000 UTC
1358,OPEN,Is there way to log IP addresses of http or https requests ?,enhancement,2020-10-08 06:03:33 +0000 UTC
1352,CLOSED,Allow the use of different SavedModel signature_def,,2021-05-19 21:43:12 +0000 UTC
1351,CLOSED,OOMs on enabling multiple TF MaskRCNN models with FP16 optimization,,2020-05-19 16:33:58 +0000 UTC
1350,CLOSED,is it possible to downgrade to cuda-10.1?,,2020-04-23 00:34:48 +0000 UTC
1349,CLOSED,batch size >1 cann't speed in inference,,2020-04-30 17:18:32 +0000 UTC
1340,CLOSED,Wrong CUDA buffer size used in addsub.cu AllocateCudaBuffers() when reading payloads,,2020-04-27 15:51:27 +0000 UTC
1338,CLOSED,unable to load model 'bert', tensor 'input_ids': the model expects 1 dimensions but the model configuration specified 2 dimensions,,2020-04-28 11:36:37 +0000 UTC
1337,CLOSED,Raw binary data order of multiple inputs batch request,,2020-05-08 16:36:42 +0000 UTC
1331,CLOSED,Passing S3 Credentials as Env Vars,enhancement,2020-05-01 22:30:06 +0000 UTC
1329,CLOSED,Is there a Rest API version of Triton Client ?,,2020-04-19 17:58:06 +0000 UTC
1328,CLOSED,Got a problem in autofilling model config,,2020-04-23 17:38:06 +0000 UTC
1327,CLOSED,Question: Metrics - inferences per minute,,2020-04-17 10:15:08 +0000 UTC
1326,CLOSED,the return value of the function named 'parse_model_http' in v2_image_client.py,,2020-04-21 19:06:09 +0000 UTC
1321,CLOSED,Core dump when all CUDA-capable devices are busy or unavailable,,2020-05-15 23:46:09 +0000 UTC
1313,CLOSED,fasterrcnn_resnet50_fpn TorchScript model cannot be loaded,,2020-04-21 19:06:23 +0000 UTC
1311,CLOSED,error: creating server: INTERNAL - failed to load all models,,2021-12-10 23:55:48 +0000 UTC
1305,CLOSED,How to send HTTP request to resnet50_netdef model using curl?,,2020-04-15 12:08:39 +0000 UTC
1301,CLOSED,Unable to use TensorRT Execution Accelerator for ONNX Model,,2020-04-13 15:38:05 +0000 UTC
1299,CLOSED,Trtsever crashes !!,,2020-04-16 16:44:55 +0000 UTC
1297,CLOSED,unable to load model 'face_graphdef', configuration expects 1 inputs, model provides at most 0,,2020-04-15 06:01:02 +0000 UTC
1295,CLOSED,TRITON cannot access MINIO,,2020-04-10 18:21:26 +0000 UTC
1291,CLOSED,Spammy dynamic batching logs on 20.03,,2020-04-15 00:58:38 +0000 UTC
1285,CLOSED,Error when input define as identity,,2020-04-09 01:30:47 +0000 UTC
1284,CLOSED,unexpected shape for output when output is 4 dims,bug,2020-04-13 07:42:13 +0000 UTC
1276,CLOSED,Model loaded but no input,,2020-04-15 01:02:02 +0000 UTC
1274,CLOSED,Question: Will Stream endpoint increase GPU utilization?,,2020-04-06 09:57:03 +0000 UTC
1273,CLOSED,GRPC version update,,2020-04-06 16:21:36 +0000 UTC
1272,CLOSED,C# .NET,,2021-09-08 20:12:49 +0000 UTC
1271,CLOSED,Encountered a non-existing input blob,,2020-04-10 09:33:16 +0000 UTC
1270,CLOSED,Question: Performance difference with Tensorflow Serving,,2020-04-15 19:46:14 +0000 UTC
1269,CLOSED,Incorrect configuration on DockerFile,,2020-04-07 07:35:17 +0000 UTC
1264,CLOSED,How to to use model converted by torch2trt,,2020-04-07 23:37:47 +0000 UTC
1254,CLOSED,provide a way to get trtis's version info,,2020-05-28 20:36:21 +0000 UTC
1247,CLOSED,simple_perf_client inference size usage,,2020-03-31 15:44:37 +0000 UTC
1246,CLOSED,Does NOT support M40(maxwell) GPU any more?,,2020-04-01 01:09:13 +0000 UTC
1241,CLOSED,ONNX-exported torchvision FasterRCNN fails on inference request,,2020-04-16 18:28:30 +0000 UTC
1240,CLOSED,TF 2.1 SavedModel Format : unexpected input format FORMAT_NONE, expecting FORMAT_NHWC or FORMAT_NCHW,,2022-08-29 23:15:53 +0000 UTC
1239,CLOSED,How to transform caffe2 .pb model files to caffe2 netdef model files ?,,2020-04-09 01:58:35 +0000 UTC
1236,CLOSED,TorchScript model -Failed to update context stat: [ 0] INVALID_ARG - Timer not set correctly.,,2020-03-27 20:31:47 +0000 UTC
1231,CLOSED,Batched model works only with single instance, when sending parallel inference requests using non-cuda shared memory (works on 19.12),,2020-03-29 12:11:30 +0000 UTC
1222,CLOSED,Incorrect last_inference_timestamp_milliseconds values in api/status output in 1.10.0,,2020-03-31 21:33:15 +0000 UTC
1214,CLOSED,docker build error,,2020-03-23 15:28:52 +0000 UTC
1213,CLOSED,unexpected size for output,,2020-04-15 01:00:21 +0000 UTC
1209,CLOSED,perf_client does not support binary (image) data as TYPE_STRING?,,2020-04-03 17:32:24 +0000 UTC
1204,CLOSED,C++ Image client for object detection,,2020-03-20 22:54:40 +0000 UTC
1203,CLOSED,TRT inference server does not start in non-GPU mode,,2020-03-23 07:52:15 +0000 UTC
1201,OPEN,Pruning of requested outputs in ensemble models,enhancement,2020-04-16 16:01:29 +0000 UTC
1194,CLOSED,ensemble model question and model priority,,2020-12-10 18:48:43 +0000 UTC
1188,CLOSED,Access to shared memory & CUDA shared memory through HTTP REST API,,2020-03-16 16:31:05 +0000 UTC
1184,CLOSED,Accuracy difference between implementations of example image client,,2020-03-24 16:14:33 +0000 UTC
1178,CLOSED,Cannot build TRTIS clients from on Ubuntu 18.04 docker container.,,2020-05-05 14:30:55 +0000 UTC
1177,CLOSED,Can't build with glibc 2.30,,2020-03-12 15:12:11 +0000 UTC
1174,CLOSED,How to make dynamic requests?,,2020-03-09 16:28:02 +0000 UTC
1173,CLOSED,S3 file system should support remote minio server as well,,2020-03-12 04:56:02 +0000 UTC
1172,CLOSED,Any examples on stateful models?,,2020-12-29 11:19:12 +0000 UTC
1171,CLOSED,Error "NotImplementedError: memoryview: unsupported format <b",,2021-09-22 19:37:23 +0000 UTC
1158,CLOSED,not getting same performance as in perf_client,,2020-03-24 20:21:24 +0000 UTC
1157,CLOSED,20.02 model load err,,2020-03-08 18:01:58 +0000 UTC
1154,CLOSED,Error when running TRTIS-20.01 built from source code--"Intel MKL FATAL ERROR: Cannot load libmkl_intel_thread.so",,2020-03-04 21:14:20 +0000 UTC
1153,CLOSED,Multiple instance of the same model will load multiple copies of the model data in memory,,2020-03-06 15:23:31 +0000 UTC
1151,CLOSED,How to use perf_client on ensemble models and sequence models,enhancement,2020-04-03 17:38:05 +0000 UTC
1149,CLOSED,Is there any way to access the build script except pulling the image?,,2020-03-08 18:08:18 +0000 UTC
1148,CLOSED,branch r20.02 ubuntu 1604 build error,,2020-03-06 18:35:07 +0000 UTC
1147,CLOSED,HTTP API: Access-Control-Allow-Origin etc.,,2022-08-19 20:59:22 +0000 UTC
1144,CLOSED,How to send multiple input tensor by rest api,,2020-04-16 12:37:59 +0000 UTC
1140,CLOSED,Safeguard one model from failures in others,,2020-03-08 18:08:48 +0000 UTC
1138,CLOSED,perf_client unable to allocate memory on gpu,bug,2020-03-02 17:16:57 +0000 UTC
1129,CLOSED,Can V2 APIs client satisfied these requirement ?,,2020-05-09 03:20:36 +0000 UTC
1126,CLOSED,Ensemble model only delivers the first of its configured outputs,,2020-02-24 17:52:50 +0000 UTC
1125,CLOSED,"Error details: OK" error message from autofill.cc,,2020-02-21 13:11:48 +0000 UTC
1123,CLOSED,Problem when running sequence models,,2020-09-25 12:57:29 +0000 UTC
1119,CLOSED,Shared memory pre-allocation,,2020-02-21 22:23:30 +0000 UTC
1115,CLOSED,what is the motivation for v2 refactory?,,2020-02-19 08:43:32 +0000 UTC
1114,CLOSED,How to get the versioned model path when implementing a custom backend (Context),,2020-02-24 18:08:57 +0000 UTC
1113,CLOSED,clientsdk: gnutls "non-properly terminated" error in git clone interrupts build (v1.10.0),,2020-02-18 21:59:22 +0000 UTC
1112,CLOSED,Error while loading shared libraries: libtensorflow_framework.so.1,,2020-02-27 11:11:59 +0000 UTC
1106,CLOSED,Concurrency problem due to allocation of output buffers on device,bug,2020-03-18 18:05:38 +0000 UTC
1105,CLOSED,TRTIS failed to load 'trt model: unexpected configuration maximum batch size 64 for 'resnet50_trt_0_gpu0', model maximum is 1 as model does not contain an implicit batch dimension nor the explicit batch-dimension of 'gpu_0/data_0' is a wildcard,,2020-03-24 20:21:45 +0000 UTC
1102,CLOSED,Memory Leak on 20.02 ver,,2020-03-02 21:57:54 +0000 UTC
1100,CLOSED,golang client input of TYPE_STRING,,2020-02-18 03:18:06 +0000 UTC
1099,CLOSED,torch/script.h: No such file or directory,,2020-02-11 04:05:46 +0000 UTC
1098,CLOSED,problem about using pytorch_libtorch platform,,2020-02-11 08:35:14 +0000 UTC
1097,CLOSED,http,,2020-02-10 16:54:25 +0000 UTC
1096,CLOSED,custom backend with TYPE_STRING output?,,2020-02-10 17:07:29 +0000 UTC
1082,CLOSED,nv_gpu_memory_used_bytes metric does not decrease on model unload,,2020-02-04 16:24:21 +0000 UTC
1081,CLOSED,Build TRTIS by enabling trace, the inference time descreased 20%,,2020-02-03 02:54:32 +0000 UTC
1080,CLOSED,trtserver uses more than 20 CPUs,,2020-03-08 02:53:27 +0000 UTC
1063,CLOSED,Add support for unidirectional streaming inference,,2020-02-03 18:17:15 +0000 UTC
1062,CLOSED,Thread-safety of CustomGetNextInputFn and CustomGetOutputFn,,2020-02-03 18:18:11 +0000 UTC
1061,CLOSED,Multi-Process Server (MPS),,2022-08-22 22:22:27 +0000 UTC
1055,CLOSED,ensemble scheduler reshape not working as expected,,2020-01-24 18:53:34 +0000 UTC
1054,CLOSED,No postprocessing in case of aync call in image_client example,,2020-01-24 18:15:10 +0000 UTC
1053,CLOSED,What does userp mean?,,2020-01-24 03:51:26 +0000 UTC
1044,CLOSED,exceeds maximum batch size,,2020-01-22 22:39:05 +0000 UTC
1043,CLOSED,Where to find the model def for image_preprocess_nchw_3x224x224_inception?,,2020-11-12 02:14:22 +0000 UTC
1042,CLOSED,Is it possible to compile a version for MACOS?,,2021-02-16 18:31:35 +0000 UTC
1041,CLOSED,Reshape 's requested shape is incorrect,,2020-02-03 09:03:59 +0000 UTC
1040,CLOSED,ONNX multi dynamic_axes cause error,,2020-03-31 16:49:46 +0000 UTC
1038,CLOSED,What format is the status in?,,2020-01-19 22:55:29 +0000 UTC
1032,CLOSED,How to transfer request to sequence model on trtis by http?,,2020-01-19 01:51:58 +0000 UTC
1029,CLOSED,Tensorflow 2.0 models with TRTIS,,2020-03-20 19:46:40 +0000 UTC
1026,CLOSED,How can we share variables between distinct custom backends in the ensemble-model ?,,2020-01-19 13:54:06 +0000 UTC
1025,CLOSED,No error when asking for invalid TensorRT output tensor name,,2020-01-24 18:49:42 +0000 UTC
1024,CLOSED,Error in dlopen or dlsym: libthnvrtc.so,,2020-01-17 02:45:12 +0000 UTC
1023,CLOSED,local s3 storage,,2020-02-03 21:33:28 +0000 UTC
1021,CLOSED,How to optimize config file for MaskRCNN trt model on V100?,,2020-02-03 18:25:33 +0000 UTC
1020,CLOSED,Failed to deserialize trt model.,,2020-01-16 12:32:42 +0000 UTC
1019,CLOSED,deploying model using trtis is much slower than using frozen model directly,,2020-02-03 18:24:52 +0000 UTC
1018,CLOSED,same code. run ok on version 19.04 but cannot run in version 19.11 or 19.12,,2020-03-16 16:29:19 +0000 UTC
1015,CLOSED,maskrcnn-benchmark pytorch model error,,2020-01-16 21:18:55 +0000 UTC
1010,CLOSED,How to parse NV-InferResponse header fastly in HTTP response?,,2020-01-08 23:57:55 +0000 UTC
1009,CLOSED,TRTIS support TensorRT7,,2020-01-09 16:18:56 +0000 UTC
1001,CLOSED,CPU memory grows up while using CUDA shared memory,,2020-01-13 04:24:34 +0000 UTC
999,CLOSED,How to send integer string to inference server by http?,,2021-12-30 04:11:16 +0000 UTC
998,CLOSED,SavedModel load sees different input tensor shape than exists in the model,,2020-01-08 20:09:32 +0000 UTC
992,CLOSED,GPU memory didn't clean up as expected,,2020-01-15 16:47:58 +0000 UTC
991,CLOSED,Is tensorrt-inference-server support savedmodel model that include sparse tensor input?,,2020-01-03 22:25:11 +0000 UTC
990,CLOSED,Is InferContext in python/c++ api related to cuda context?,,2019-12-31 01:50:50 +0000 UTC
989,CLOSED,How to use predict interface for savedmodel that inputs have 'coo_sparse' what is SparseTensor?,,2019-12-31 01:42:21 +0000 UTC
988,CLOSED,Ensembling : Python custom operation,,2019-12-30 18:10:13 +0000 UTC
987,CLOSED,What is the reason not to use the tensorflow serving?,,2020-02-27 17:48:56 +0000 UTC
982,CLOSED,Questions Regarding Failures,,2020-01-03 15:07:09 +0000 UTC
981,CLOSED,The size of the input dimensions that correspand for each batch must be equal?,,2019-12-27 01:38:08 +0000 UTC
980,CLOSED,Requests support priority and timeout settings,enhancement,2020-03-03 19:00:55 +0000 UTC
979,CLOSED,/api/status call before initialization causes segfault?,,2020-02-10 17:17:20 +0000 UTC
976,CLOSED,Does the model support two variable size input dimensions(exclude batch dimension)?,,2019-12-20 02:06:28 +0000 UTC
975,CLOSED,Curl request error in tensorrt-inference-server (Infer failed: unexpected size for input 'im_info', expecting 12 bytes for model 'ssd_model2),,2020-01-02 07:31:43 +0000 UTC
971,CLOSED,I got a problem when I use trtis client,,2020-02-03 18:26:15 +0000 UTC
968,CLOSED,Allow batch size 0 in ensembles,,2022-06-01 23:24:18 +0000 UTC
967,CLOSED,Unexpected size for input,,2019-12-19 10:33:14 +0000 UTC
964,CLOSED,libopencv_imgcodecs.so.3.2 was not found when I tried Example Custom Backend,,2019-12-19 17:54:02 +0000 UTC
963,CLOSED,No Active Model,,2019-12-30 19:56:28 +0000 UTC
958,CLOSED,Error from tensorflow_savedmodel,,2019-12-17 12:50:15 +0000 UTC
944,CLOSED,What are the hardware requirements for trtis19_09，and tritis19_05?,,2019-12-10 11:08:58 +0000 UTC
939,CLOSED,tensorrtserver:19.09-py3+tensorflow1.14.0 encounter error,,2019-12-10 17:50:43 +0000 UTC
924,CLOSED,How to set batch size in onnx model,,2019-12-02 08:44:24 +0000 UTC
923,CLOSED,TensorFlow 2 support,,2020-03-08 18:05:56 +0000 UTC
922,CLOSED,Low GPU util without docker,,2020-01-06 16:33:46 +0000 UTC
915,CLOSED,Error with Sequence Batch,,2019-12-30 19:56:47 +0000 UTC
914,CLOSED,Server Queue,enhancement,2023-06-22 06:42:40 +0000 UTC
911,CLOSED,Cannot load onnx model,,2019-12-02 21:26:59 +0000 UTC
906,CLOSED,Dynamic batch scheduling in ensemble models,,2022-05-23 13:10:49 +0000 UTC
903,CLOSED,Request batch size greater than max_batch_size,,2019-11-21 20:35:01 +0000 UTC
900,CLOSED,pytorch bert model error,,2020-02-20 00:08:42 +0000 UTC
899,CLOSED,Docs provided to create local cmake build trt inference server with Tensorflow not working properly or don't have enough information to build the required builds,,2019-12-16 23:58:03 +0000 UTC
892,CLOSED,Allocations for TensorRT models with dynamic batch size are much too large,,2019-11-20 21:11:16 +0000 UTC
891,OPEN,Separate static / shared client lib dependencies,enhancement,2020-03-08 18:15:37 +0000 UTC
889,CLOSED,aws s3 model_repository error,,2020-11-24 19:53:06 +0000 UTC
880,CLOSED,dynamic batch size parameter not recognized by python api,,2019-12-30 19:58:08 +0000 UTC
877,CLOSED,network with output dims [ 1 ] , how to define config.pbtxt output field,,2019-11-18 23:58:14 +0000 UTC
874,CLOSED,Can't get hidden layer as output?,,2019-12-09 12:34:00 +0000 UTC
870,CLOSED,Parameters support for ensemble model during execution time,,2021-04-19 02:51:06 +0000 UTC
868,CLOSED,Do you have a web_UI to manage models ?,,2019-11-13 16:28:41 +0000 UTC
867,CLOSED,any plan for client sdk in other languages?,,2020-01-11 13:01:43 +0000 UTC
866,CLOSED,[Question] Best way to upscale video using TRTIS?,,2019-11-18 23:52:31 +0000 UTC
863,CLOSED,How to select the OpenVino execution provider for ONNX runtime backend in configuration file? Could you please provide an example?,,2019-11-13 03:25:23 +0000 UTC
862,CLOSED,How to select the OpenVino execution provider for ONNX runtime backend in configuration file? Could you please provide a example?,,2019-11-13 03:10:33 +0000 UTC
859,CLOSED,Warning: Explicit batch network detected and batch size specified, use enqueue without batch size instead.,,2021-07-19 06:35:19 +0000 UTC
857,CLOSED,python not found,,2019-11-12 16:15:34 +0000 UTC
855,CLOSED,Op type not registered 'BatchMatMulV2' in binary running,,2019-11-18 23:51:48 +0000 UTC
848,CLOSED,There are some bugs in ICaffeParser,,2019-11-11 21:39:53 +0000 UTC
843,CLOSED,HTTP allocation failed,,2019-11-10 15:50:42 +0000 UTC
842,CLOSED,Confusing error message on loading and checking SequenceControl information.,,2019-12-18 17:58:40 +0000 UTC
838,CLOSED,Tensorflow backend for Jetson,,2019-11-07 21:37:06 +0000 UTC
837,CLOSED,Improve error messages: backend not installed -> Segfault,bug,2019-11-15 20:24:42 +0000 UTC
819,CLOSED,Unable to run optimized bert from TensorRT python,,2021-03-21 23:32:49 +0000 UTC
808,CLOSED,RNN in ONNX model does not give correct output with batch_size > 1,,2019-11-05 14:56:49 +0000 UTC
796,CLOSED,unknown output name 'OUTPUT__0' for pytorch model and ctx.get_server_status() get protobuf error,,2019-11-07 01:20:51 +0000 UTC
794,CLOSED,Build Error with CMAKE,,2019-10-28 21:44:57 +0000 UTC
786,CLOSED,Unable to run optimized BERT: model shape expected by framework [-1,-1] doesn't match model configuration shape [-1,-1],,2019-10-28 23:56:20 +0000 UTC
776,CLOSED,Could I get request access log instead of starting server with --verbose-log=1?,enhancement,2023-07-11 21:14:55 +0000 UTC
775,CLOSED,How to config input has shape: <unknown> ?,,2019-10-24 16:28:29 +0000 UTC
772,CLOSED,Provide test harness for custom backends,,2019-10-25 15:22:38 +0000 UTC
768,CLOSED,Multiple instance limited with 50% utilization,,2019-10-31 18:48:44 +0000 UTC
767,CLOSED,About Custom Plugin,,2020-03-30 16:00:57 +0000 UTC
760,CLOSED,build error with cmake on version r19.09,,2019-10-25 23:12:33 +0000 UTC
754,CLOSED,Dynamic batching - don't discard built models,,2019-10-14 18:21:28 +0000 UTC
750,CLOSED,Can trt-server support TVM exported optimized libcode to run on datacenter?,,2019-10-11 16:52:38 +0000 UTC
749,CLOSED,savemodel error: creating server: INTERNAL - failed to open text file for read /models/pb_version18_batch_FP16_TRT_docker_32/config.pbtxt: No such file or directory,,2019-10-14 16:51:01 +0000 UTC
748,CLOSED,savemodel error: creating server: INTERNAL - failed to open text file for read /models/pb_version18_batch_FP16_TRT_docker_32/config.pbtxt: No such file or directory,,2019-10-10 15:38:13 +0000 UTC
746,CLOSED,[Documentation] Build tensorflow library from source lacks needed scripts,,2019-10-14 17:20:02 +0000 UTC
745,CLOSED,CUDNN_STATUS_MAPPING_ERROR,,2019-10-30 23:06:42 +0000 UTC
744,CLOSED,librequest.so undefined reference to `nvidia::inferenceserver::InferResponse::~InferResponse()' in C++ implementing,,2019-10-14 17:23:26 +0000 UTC
736,CLOSED,Integration with DALI-based custom backend for preprocessing?,,2020-09-30 17:51:50 +0000 UTC
735,CLOSED,trtis loads model but does not detect input/output nodes,,2019-10-14 16:46:36 +0000 UTC
727,CLOSED,failed to load Tensor shape expected by framework [] doesn't match model configuration shape [-1],,2020-04-28 16:28:00 +0000 UTC
714,CLOSED,Metrics on Jetson TX2 do not contain nv_gpu information,,2019-10-14 17:24:09 +0000 UTC
708,CLOSED,Feature request: model warmup,enhancement,2021-09-22 16:14:27 +0000 UTC
706,CLOSED,Loading tftrt optimized models,,2019-10-04 21:07:11 +0000 UTC
705,CLOSED,Update Protobuf dependencies,,2019-10-02 13:45:08 +0000 UTC
704,CLOSED,Can you help me how to freeze_graph right pb,,2019-09-30 15:59:09 +0000 UTC
703,CLOSED,Building clients fails on Raspberry Pi,,2019-10-14 17:25:55 +0000 UTC
698,CLOSED,Documentation missing for the C++ API with latest release,,2019-09-27 21:13:54 +0000 UTC
684,CLOSED,How to retrieve output tensor values (C++ Api),,2019-09-26 10:35:42 +0000 UTC
679,CLOSED,GDB cannot access memory,,2019-09-20 23:28:07 +0000 UTC
678,CLOSED,Docker Build Failing,,2019-09-24 23:01:42 +0000 UTC
675,CLOSED,Doesn't ModelControlAPI manage unloaded model's GPU memory?,,2020-04-29 21:18:05 +0000 UTC
669,CLOSED,HTTP keep-alive,,2019-09-23 07:27:27 +0000 UTC
666,CLOSED,GetRawAtCursor returns unexpected results for C++ gRPC client,,2019-10-05 00:29:16 +0000 UTC
663,CLOSED,The result is inf, inf, nan, nan,,2019-09-18 02:18:58 +0000 UTC
651,CLOSED,Basic examples with the gRPC and/or REST API Plzzzzzzzzzzzzzzzzz!,,2019-09-15 21:09:34 +0000 UTC
644,CLOSED,Skip "/api/status/model"?,,2019-09-11 22:47:04 +0000 UTC
643,CLOSED,Support specifying host header in example clients grpc?,,2022-06-16 00:10:52 +0000 UTC
641,CLOSED,How to preserve request / response order with the callback version of AsyncRun?,,2019-09-12 00:29:46 +0000 UTC
640,CLOSED,batch_size has no efffect on the infence time,,2019-09-26 21:40:05 +0000 UTC
635,CLOSED,cudaStreamCaptureModeGlobal not declared,,2021-11-16 02:30:43 +0000 UTC
634,CLOSED,[Pytorch] Multiple instance of the model on same GPU provide no speedup,,2020-03-08 18:17:34 +0000 UTC
633,CLOSED,Output data got from response body is in binary format when doing inference through http post.,,2019-09-26 21:41:12 +0000 UTC
617,CLOSED,incremental build error,,2019-09-09 19:36:30 +0000 UTC
612,CLOSED,taking to too much time for "HTTP client failed: Couldn't connect to server",,2021-01-15 22:01:58 +0000 UTC
610,CLOSED,Excessive memory usage for Tensorflow SavedModel,,2019-09-04 20:13:13 +0000 UTC
609,CLOSED,Publish client docker image in NGC,,2019-09-09 19:37:41 +0000 UTC
608,CLOSED,NGC Inference Server Container Website Broken,,2019-08-31 23:19:20 +0000 UTC
603,CLOSED,Changing batch size between requests with shared memory fails,,2019-09-12 22:19:30 +0000 UTC
597,CLOSED,Does TF-TRT used in TRTIS tensorflow runtime engine?,,2021-05-12 18:58:24 +0000 UTC
590,CLOSED,Ensemble configuration setup,,2019-09-09 19:39:09 +0000 UTC
584,CLOSED,error input size deply ssd on trtis,,2019-08-25 09:13:08 +0000 UTC
572,CLOSED,python3 in client docker complains about missing numpy,,2019-08-27 17:55:06 +0000 UTC
565,CLOSED,CustomGetNextInput_fn,,2019-08-20 19:55:55 +0000 UTC
564,CLOSED,model repository API?,,2019-08-27 00:53:35 +0000 UTC
563,CLOSED,TRTIS gRPU DRAM Out-Of-Memory,,2019-08-20 19:35:14 +0000 UTC
562,CLOSED,Feature request: add ByteSize() to Output class,enhancement,2019-09-19 00:33:32 +0000 UTC
561,CLOSED,facenet input shape problem,,2019-09-26 21:41:53 +0000 UTC
557,CLOSED,trtis cannot serve mask rcnn onnx model,,2019-08-27 18:33:27 +0000 UTC
556,CLOSED,Header path issues in librequest.so,bug,2019-08-29 15:43:47 +0000 UTC
550,CLOSED,trtserver: regionFormat.cpp:65: size_t nvinfer1::RegionFormatB::memorySize(int, const nvinfer1::Dims&) const: Assertion `batchSize > 0' failed.,,2022-06-30 16:22:28 +0000 UTC
547,CLOSED,upsample,,2019-08-27 00:57:36 +0000 UTC
546,CLOSED,image_cilent,,2019-08-27 00:57:17 +0000 UTC
545,CLOSED,Custom Operations with Docker,,2021-07-26 18:29:26 +0000 UTC
544,CLOSED,Shared Memory client fails for batch size != 1,,2019-08-16 00:20:18 +0000 UTC
543,CLOSED,Output shape with Pytorch model,,2019-09-12 03:41:59 +0000 UTC
539,CLOSED,UNIMPLEMENTED endpoints using java grpc,,2020-01-13 16:55:54 +0000 UTC
531,CLOSED,Cannot compile trtis-clients,,2019-08-12 21:07:12 +0000 UTC
521,CLOSED,TRT support for MaskRCNN,,2019-09-09 21:54:14 +0000 UTC
519,CLOSED,Java client from .proto files,,2020-01-11 13:04:09 +0000 UTC
517,CLOSED,Cannot run tensorrtserver with python3,,2019-08-12 21:12:22 +0000 UTC
514,CLOSED,Label map query,,2019-08-05 17:13:42 +0000 UTC
513,CLOSED,perf_client will silently ignore -f flag and not output a CSV in static concurrency mode,bug,2019-08-09 23:52:15 +0000 UTC
509,CLOSED,Allow manual batching for TensorRT plans with max_batch_size=0,enhancement,2019-08-07 22:27:11 +0000 UTC
501,CLOSED,docker installation in c++,,2019-07-29 22:07:31 +0000 UTC
498,CLOSED,client installation in c++,,2019-08-02 21:12:45 +0000 UTC
497,CLOSED,ensemble model MODEL_UNAVAILABLE,,2019-07-26 02:34:37 +0000 UTC
490,CLOSED,CMake build error r19.07,,2019-07-23 23:15:08 +0000 UTC
487,CLOSED,Computation performance difference between TRTIS and TRT with Yolov3 ONNX model,,2019-07-23 11:28:44 +0000 UTC
483,CLOSED,Specify which port to expose services at,,2019-07-19 21:31:00 +0000 UTC
481,CLOSED,Missing layout specification of serialized tensors in the inference API,,2019-07-23 00:52:39 +0000 UTC
477,CLOSED,TRT+CELERY unexpected: InferenceServerException('c_void_p(122485 792)',),,2021-06-08 16:40:44 +0000 UTC
471,CLOSED,How to migrate the preprocess and postprocess to the trt server?,,2019-07-24 16:18:12 +0000 UTC
470,CLOSED,how to accelerate docker pull?,,2019-07-22 07:05:01 +0000 UTC
469,CLOSED,Is there a matrix of backend version ?,,2019-07-17 16:34:09 +0000 UTC
459,CLOSED,Can't use GCS bucket name as model repository root,bug,2019-07-17 22:39:22 +0000 UTC
458,CLOSED,Explicitly configured max_batch_size is ignored by Autofill (ONNX, TensorFlow),,2019-07-16 20:56:44 +0000 UTC
457,CLOSED,Deploying TF-serving SavedModel format to TRT-5.1.5 without conversion,,2019-07-15 16:04:54 +0000 UTC
456,CLOSED,TensorRT server stucked when I run perf_client command,,2019-07-31 00:30:38 +0000 UTC
454,CLOSED,ModelRepositoryManager doesn't fail on zero-prefixed version directories,,2019-07-17 18:25:15 +0000 UTC
453,CLOSED,1.3.0 documentation doesn't mention removal of GCS support,,2019-07-16 19:23:03 +0000 UTC
452,CLOSED,run tensorrtserver_client on centos,,2019-11-11 15:27:21 +0000 UTC
448,CLOSED,Unable to load model repository on Google Cloud Storage with 1.3.0,,2019-07-13 17:58:44 +0000 UTC
447,CLOSED,Dynamic input and output shape definition for object detection,,2019-07-17 07:16:55 +0000 UTC
446,CLOSED,must map ensemble input INPUT for ensemble ensemble_model,,2019-07-11 16:27:05 +0000 UTC
444,CLOSED,perf_client with user-defined data and variable size,,2019-07-10 21:17:16 +0000 UTC
443,CLOSED,Ensembling and varying batch sizes,,2019-07-11 17:57:10 +0000 UTC
439,CLOSED,Arbitrary metadata with models,,2023-03-01 21:52:51 +0000 UTC
438,CLOSED,InferenceServerException: [ 0] invalid size 22110 bytes for input 'gpu_0/data', expects 4 bytes,,2019-07-09 17:00:59 +0000 UTC
435,CLOSED,Caffe2 model crashes on loading,bug,2019-07-15 16:08:55 +0000 UTC
432,CLOSED,Tensorflow shape detection fails with multiple versions.,bug,2019-07-09 23:07:32 +0000 UTC
418,CLOSED,How to deploy maskRCNN pytorch model?,,2021-05-05 17:00:13 +0000 UTC
409,CLOSED,Config proto formatting,,2019-06-27 17:35:26 +0000 UTC
386,CLOSED,Support specifying host header in example clients,enhancement,2019-06-27 19:04:40 +0000 UTC
385,CLOSED,curl example for simple-server,,2019-06-18 17:19:45 +0000 UTC
384,CLOSED,[feature request] Publish Python Client Library to PyPi,,2019-06-19 01:25:45 +0000 UTC
381,CLOSED,NMT output different from normal inference,,2019-07-27 23:05:50 +0000 UTC
378,CLOSED,Unable to run TensorRTIS on AKS,,2019-06-16 03:59:01 +0000 UTC
376,CLOSED,Support for SSD, NMT is missing from TensorRT Inference Server,,2021-08-30 08:57:05 +0000 UTC
373,CLOSED,What is the meaning of metrics: nv_inference_load_ratio?,,2019-06-13 22:09:08 +0000 UTC
366,CLOSED,Impact of "instance-group" option,,2019-06-14 22:58:37 +0000 UTC
363,CLOSED,TRTIS stops serving all models when uploading a new model,bug,2019-08-08 15:55:43 +0000 UTC
358,CLOSED,Support for XGBoost,enhancement,2021-04-30 02:59:24 +0000 UTC
356,CLOSED,On demand Polling,,2019-08-10 00:58:56 +0000 UTC
352,CLOSED,how do i calculate the overlap of two class?,,2019-06-11 17:39:12 +0000 UTC
351,CLOSED,tensorrtserver.api.InferenceServerException: [ 0] invalid size 4153344 bytes for input 'input/input_data', expects 2076672 bytes,,2019-06-17 23:43:51 +0000 UTC
343,CLOSED,Ensemble communication improvements: avoid tensor copies,enhancement,2019-12-30 18:20:16 +0000 UTC
331,CLOSED,ppc64le, TensorRT Inference Server Release 19.05,,2019-06-04 17:16:53 +0000 UTC
326,CLOSED,is_training/keep_prob tensor with 19.05,,2019-06-25 05:35:32 +0000 UTC
325,CLOSED,dims [-1] don't match configuration dims [-1],,2019-06-06 16:27:26 +0000 UTC
324,CLOSED,Support model repositories on s3 or azure blob,enhancement,2019-08-12 21:11:30 +0000 UTC
323,CLOSED,can the server docker built from source on jetson baord with aarch64 cpu ?,,2019-06-06 16:37:03 +0000 UTC
317,CLOSED,tensorrt version mismatch while serving plan files.,,2019-05-31 18:39:42 +0000 UTC
312,CLOSED,How to pass placeholders other than input - MTCNN,,2020-04-22 07:03:35 +0000 UTC
311,CLOSED,No PYVER in Dockerfile.client,,2019-06-19 17:48:17 +0000 UTC
310,CLOSED,Looser coupling against TensorRT versions,,2019-05-28 20:55:25 +0000 UTC
309,CLOSED,Directly crash after log "Adding visible gpu devices: 0",,2019-05-27 11:00:54 +0000 UTC
308,CLOSED,why the uff parser is so slow ?,,2019-05-28 16:00:27 +0000 UTC
307,CLOSED,memlock can not be setted in k8s,,2019-05-30 16:31:52 +0000 UTC
302,CLOSED,Time to get_async_run_results is very slow,,2019-05-30 16:44:33 +0000 UTC
300,CLOSED,About save the serialized model of tensor rt model...,,2019-05-22 16:06:04 +0000 UTC
298,CLOSED,caffe2_netdef InferenceServerException,bug,2019-06-04 19:31:56 +0000 UTC
297,CLOSED,grpc client for a model with multiple outputs and input dims is -1,,2019-06-02 11:05:15 +0000 UTC
296,CLOSED,grpc_image_client.py example with raw output reading?,,2019-05-17 17:04:32 +0000 UTC
295,CLOSED,nv_inference_load_ratio_bucket,,2019-09-09 21:48:43 +0000 UTC
294,CLOSED,Invalid argument: model input must specify 'dims',,2019-06-03 09:04:34 +0000 UTC
293,CLOSED,ONNX support,,2019-06-03 16:48:38 +0000 UTC
289,CLOSED,Execution of models in ensemble does not depend on requested outputs,enhancement,2019-09-09 18:18:27 +0000 UTC
287,CLOSED,AttributeError: module 'common' has no attribute 'allocate_buffers',,2019-05-11 11:49:08 +0000 UTC
285,CLOSED,Custom backends with errors are flagged as READY,bug,2019-06-27 17:32:42 +0000 UTC
282,CLOSED,YOLOv3 model configuration issue,,2020-02-01 15:37:57 +0000 UTC
281,CLOSED,Transform ONNX to TensorRT Fail,,2019-05-09 20:04:32 +0000 UTC
280,CLOSED,inference by CURL ?,,2019-05-10 02:40:18 +0000 UTC
279,CLOSED,InferContext.ResultFormat.CLASS for RNN,,2019-05-10 04:01:24 +0000 UTC
278,CLOSED,Windows Python Client Possibility,,2020-04-21 16:37:23 +0000 UTC
276,CLOSED,TRTIS in pod segfaulting,,2019-09-09 21:49:26 +0000 UTC
274,CLOSED,Wrong output order retrieval using ensemble model,,2019-05-14 16:27:55 +0000 UTC
273,CLOSED,Handling input with shape [seq_len, batch_size],,2019-05-08 06:28:13 +0000 UTC
272,CLOSED,TRTIS crashes in GCP after the first request without any error messages when more than 10 models are loaded.,,2019-05-07 17:05:08 +0000 UTC
270,CLOSED,Dynamic batch size for input with shape -1,,2021-11-03 22:49:23 +0000 UTC
269,CLOSED,Inference performance hit a limit with one trtserver instance via grpc,bug,2019-07-03 19:29:47 +0000 UTC
264,CLOSED,Config.pbtxt and max_batch_size setting for SavedModel,,2019-10-11 08:08:22 +0000 UTC
261,CLOSED,Model status API should report per-model-instance memory usage,enhancement,2023-07-08 00:16:15 +0000 UTC
260,CLOSED,Custom backend and label file,,2019-05-09 19:43:28 +0000 UTC
258,CLOSED,Jmeter failed to get response from trtis,,2019-05-07 07:01:37 +0000 UTC
257,CLOSED,Error ensemble_scheduling,,2019-04-29 16:13:12 +0000 UTC
252,CLOSED,about the ensemble,,2019-04-25 07:40:27 +0000 UTC
251,CLOSED,Error perf_client with dynamic batch Caffe2 model,,2019-04-29 15:47:11 +0000 UTC
246,CLOSED,TRTIS1.0.0: Timer not set correctly / No valid requests recorded within time interval / Must specify at least one target,,2019-04-24 16:34:02 +0000 UTC
239,CLOSED,Unexpected TensorRT5.1.2 Results vs TRTIS1.0.0 Results,,2019-06-06 16:33:28 +0000 UTC
238,CLOSED,Tool to convert an uff file to a plan file,,2019-04-22 15:46:22 +0000 UTC
237,CLOSED,Support dynamic Model Outputs,,2019-04-22 16:32:04 +0000 UTC
235,CLOSED,Incremental Builds failed,,2019-06-06 16:31:42 +0000 UTC
228,CLOSED,Does TRTIS support MXNet and Pytorch models?,,2019-04-15 21:10:41 +0000 UTC
223,CLOSED,ready_state: MODEL_UNAVAILABLE: TensorFlow SavedModel configuration,,2019-04-16 18:00:05 +0000 UTC
220,CLOSED,Client Examples for Several Applications,,2019-05-09 19:42:45 +0000 UTC
217,CLOSED,Object Detection Tensorflow example request,,2020-06-02 11:15:10 +0000 UTC
214,CLOSED,Specifying Optimization Policy while using TensorRT Inference Server,,2019-04-09 16:50:53 +0000 UTC
213,CLOSED,difference results between infer using .pb and infer using trtis,,2019-05-28 13:31:14 +0000 UTC
207,CLOSED,trtis uses too much ram,,2019-08-18 18:00:06 +0000 UTC
202,CLOSED,Run tensorrtserver failed!,,2019-04-04 17:07:34 +0000 UTC
200,CLOSED,i update TRT 5.0 to TRT 5.1 and it show a warning,,2019-04-09 16:57:14 +0000 UTC
194,CLOSED,Why TRTIS Accelerated limited?,,2019-04-09 16:56:02 +0000 UTC
192,CLOSED,TYPE_STRING: failed to set result for entire batch,,2019-04-04 15:30:51 +0000 UTC
187,CLOSED,Image compression,,2020-02-03 17:26:33 +0000 UTC
185,CLOSED,Can tensorrtservere-19.03-py3 support cuda-driver-396.26 and cuda9.0?,,2019-04-02 23:45:34 +0000 UTC
184,CLOSED,Caffe2 Backend: Support models with flexible input/output shapes in batch mode,,2019-04-16 17:51:17 +0000 UTC
175,CLOSED,Feature Request: Add Go Client,enhancement,2019-09-14 21:57:38 +0000 UTC
173,CLOSED,Account for xxx.yy.zz driver version numbers,,2019-03-22 23:52:46 +0000 UTC
171,CLOSED,CUDAStreams only supported for TensorRT backend?,,2019-03-21 16:08:02 +0000 UTC
170,CLOSED,Relationship between src/custom/ and src/servables/custom?,,2019-03-20 23:36:25 +0000 UTC
169,CLOSED,Building the Server Failed,,2020-11-23 13:31:16 +0000 UTC
168,CLOSED,How to use different max_batch_size configurations for version?,,2019-06-12 07:31:23 +0000 UTC
162,CLOSED,image_client demos don't support dynamic input/output shapes,,2019-03-20 15:42:38 +0000 UTC
156,CLOSED,perf_client not included in docker image,,2019-03-14 15:17:30 +0000 UTC
155,CLOSED,how can i get a TRTIS which support tensorrt 4.0?,,2019-03-14 11:22:49 +0000 UTC
147,CLOSED,Question: TRTIS Caffe2 does not support models with both CPU and GPU ops?,bug, enhancement,2020-06-25 17:24:57 +0000 UTC
145,CLOSED,TensorRT Plan blocking GPU Memory,,2020-04-29 21:14:17 +0000 UTC
142,CLOSED,Visualization tool that shows the TensorRT Inference Server Metrics,,2019-04-02 23:49:45 +0000 UTC
141,CLOSED,TRTIS and Kubeflow docker image,,2019-03-08 20:23:24 +0000 UTC
140,CLOSED,HTTP 200 response on invalid output names,bug,2019-04-26 17:20:31 +0000 UTC
139,CLOSED,can not run tensorrt-inference-servere-19.02 with cuda-driver-396.26,,2019-03-08 18:47:16 +0000 UTC
136,CLOSED,Performance Example Application: [ 0] INTERNAL - No valid requests recorded within time interval. Please use a larger time window.,,2019-03-08 15:47:16 +0000 UTC
135,CLOSED,tensorrtserver_clients docker image: ERROR: No supported GPU(s) detected to run this container,,2019-03-07 20:11:10 +0000 UTC
134,CLOSED,Unable to collect inference metrics for nullptr servable,bug,2019-03-21 15:48:06 +0000 UTC
133,CLOSED,How to deploy serialized models?,,2019-03-15 19:35:27 +0000 UTC
131,CLOSED,Does TRTIS support model parallelism?,,2023-01-30 02:19:49 +0000 UTC
122,CLOSED,Potential GPU memory leak for TensorFlow models?,bug,2022-02-17 10:01:14 +0000 UTC
118,CLOSED,image client build error,,2019-03-01 16:53:15 +0000 UTC
110,CLOSED,Linking TRTIS as a library,enhancement,2019-10-21 16:37:12 +0000 UTC
104,CLOSED,unexpected shape for input 'input' for model,,2019-02-21 17:05:00 +0000 UTC
103,CLOSED,Error With Running the samples,,2019-02-20 22:43:03 +0000 UTC
101,CLOSED,Spammy log: failed to get energy consumption,,2019-02-20 19:04:44 +0000 UTC
99,CLOSED,kInvalidBinNum error,,2019-02-19 14:46:49 +0000 UTC
95,CLOSED,Warning about Dynamic Batching and thread count,enhancement,2019-02-20 00:00:39 +0000 UTC
94,CLOSED,load error when signature changed,,2019-02-20 00:17:31 +0000 UTC
91,CLOSED,image_client.py due to message type error,,2019-02-15 16:27:52 +0000 UTC
76,CLOSED,TRTIS does not load model,,2019-02-11 15:58:31 +0000 UTC
72,CLOSED,image_client libopencv_highgui.so error,,2019-02-06 16:46:58 +0000 UTC
71,CLOSED,Issue with simple_string model in the examples,,2019-02-07 05:45:52 +0000 UTC
70,CLOSED,"authentication required" trying to build the server,,2019-02-03 01:25:10 +0000 UTC
65,CLOSED,Best practice for custom backend with additional resources,enhancement,2019-02-25 16:42:54 +0000 UTC
55,CLOSED,Multiple GPU scheduling,bug,2019-01-29 19:55:31 +0000 UTC
53,CLOSED,resnet50_netdef does not run on CPU,,2019-02-02 18:39:53 +0000 UTC
50,CLOSED,simple_client.py: unexpected additional input data for model 'simple',,2019-01-23 11:07:20 +0000 UTC
49,CLOSED,NVIDIA Quadro M1000M GPU not supported by tensorrtserver:18.12-py3,,2019-01-23 16:26:39 +0000 UTC
48,CLOSED,the TRTIS could not load tf-trt frozen model,,2019-01-31 03:11:21 +0000 UTC
47,CLOSED,Encountered an error while loading tensorflow savedmodel,,2019-01-30 16:44:13 +0000 UTC
44,CLOSED,the TRTIS can not load the trt model,,2019-01-23 16:20:51 +0000 UTC
40,CLOSED,Cannot deserialize plugin RPROI_TRT,,2019-01-14 17:04:33 +0000 UTC
37,CLOSED,How the tensorflow savedmodel worked in TRTIS?,,2019-06-26 12:22:23 +0000 UTC
36,CLOSED,New labels file is not detected,bug,2019-01-24 00:50:02 +0000 UTC
33,CLOSED,How the Input tensor definition for tensorflow GraphDef model in TRTIS graph.pbtxt file?,,2019-01-17 17:54:28 +0000 UTC
32,CLOSED,Why TRTIS not support dims: [ -1 ] for input and output tensor?,duplicate,2019-01-04 22:37:12 +0000 UTC
29,CLOSED,INTERNAL - unable to enqueue for inference,,2019-01-03 16:31:32 +0000 UTC
28,CLOSED,Clients build failed when set --build-arg "PYVER=3.6",help wanted,2019-04-22 22:23:43 +0000 UTC
27,CLOSED,cp: cannot stat 'bazel-bin/src/custom/addsub/libaddsub.so': No such file or directory,bug,2018-12-26 06:18:48 +0000 UTC
25,CLOSED,Implement Pre-process "add-on" to reduce TRTIS communication bottleneck,,2019-05-09 19:54:17 +0000 UTC
19,CLOSED,New model deployment is not detected,,2018-12-20 06:25:21 +0000 UTC
18,CLOSED,Does TRTIS support large models that place on multiple devices？,,2019-01-17 17:54:55 +0000 UTC
16,CLOSED,TRTIS should support TensorRT models that require custom plugins,enhancement,2020-01-30 13:03:26 +0000 UTC
14,CLOSED,perf_client has limits on concurrency,bug,2019-01-29 19:55:30 +0000 UTC
13,CLOSED,Encountered out of memoryError when using perf_client test with various batch sizes,bug,2020-03-08 18:20:21 +0000 UTC
12,CLOSED,Problem with ssd,,2018-12-17 01:05:13 +0000 UTC
8,CLOSED,TRTIS should support variable-sized input and output tensor dimensions,enhancement,2019-04-22 15:45:18 +0000 UTC
7,CLOSED,got problem while serving with TensorRT plan,,2018-12-03 02:05:00 +0000 UTC
5,CLOSED,How to deploy models where the shape of output tensor is not known,,2018-12-25 09:30:09 +0000 UTC
4,CLOSED,how to infer by http restful API,,2018-11-29 08:31:32 +0000 UTC
3,CLOSED,image_client error,,2020-07-24 15:43:54 +0000 UTC
1,CLOSED,Submitting raw data via IPC,enhancement,2019-10-21 16:39:10 +0000 UTC
