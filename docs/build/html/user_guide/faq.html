
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>FAQ &#8212; NVIDIA Triton Inference Server</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="https://js.hcaptcha.com/1/api.js"></script>
    <script src="//assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script src="../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/faq.html" />
    <link rel="shortcut icon" href="../_static/nvidia-logo-vert-rgb-blk-for-screen.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Inference Protocols and APIs" href="../customization_guide/inference_protocols.html" />
    <link rel="prev" title="Version 1 to Version 2 Migration" href="v1_to_v2.html" /> 
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
      
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">NVIDIA Triton Inference Server</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started/quickstart.html">
   Quickstart
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  User Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="performance_tuning.html">
   Deploying your trained model using Triton
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="architecture.html">
   Triton Architecture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_repository.html">
   Model Repository
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/repository_agents.html">
   Repository Agent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_configuration.html">
   Model Configuration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optimization.html">
   Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ragged_batching.html">
   Ragged Batching
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rate_limiter.html">
   Rate Limiter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_analyzer.html">
   Model Analyzer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perf_analyzer.html">
   Performance Analyzer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_management.html">
   Model Management
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="custom_operations.html">
   Custom Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="decoupled_models.html">
   Decoupled Backends and Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="response_cache.html">
   Triton Response Cache
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="metrics.html">
   Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="trace.html">
   Triton Server Trace
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jetson.html">
   Triton Inference Server Support for Jetson and JetPack
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="v1_to_v2.html">
   Version 1 to Version 2 Migration
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   FAQ
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Protocol Guides
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/inference_protocols.html">
   Inference Protocols and APIs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_binary_data.html">
   Binary Tensor Data Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_classification.html">
   Classification Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_logging.html">
   Logging Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_model_configuration.html">
   Model Configuration Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_model_repository.html">
   Model Repository Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_schedule_policy.html">
   Schedule Policy Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_sequence.html">
   Sequence Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_shared_memory.html">
   Shared-Memory Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_statistics.html">
   Statistics Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_trace.html">
   Trace Extension
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Customization Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/build.html">
   Building Triton
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/compose.html">
   Customize Triton Container
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/test.html">
   Testing Triton
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Examples
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../examples/jetson/README.html">
   Using Triton Inference Server as a shared library for execution on Jetson
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../examples/jetson/concurrency_and_dynamic_batching/README.html">
   Concurrent inference and dynamic batching
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/triton-inference-server/server"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/triton-inference-server/server/issues/new?title=Issue%20on%20page%20%2Fuser_guide/faq.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-are-the-advantages-of-running-a-model-with-triton-inference-server-compared-to-running-directly-using-the-model-s-framework-api">
   What are the advantages of running a model with Triton Inference Server compared to running directly using the model’s framework API?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#can-triton-inference-server-run-on-systems-that-don-t-have-gpus">
   Can Triton Inference Server run on systems that don’t have GPUs?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#can-triton-inference-server-be-used-in-non-docker-environments">
   Can Triton Inference Server be used in non-Docker environments?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#do-you-provide-client-libraries-for-languages-other-than-c-and-python">
   Do you provide client libraries for languages other than C++ and Python?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-would-you-use-triton-inference-server-within-the-aws-environment">
   How would you use Triton Inference Server within the AWS environment?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-i-measure-the-performance-of-my-model-running-in-the-triton-inference-server">
   How do I measure the performance of my model running in the Triton Inference Server?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-can-i-fully-utilize-the-gpu-with-triton-inference-server">
   How can I fully utilize the GPU with Triton Inference Server?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#if-i-have-a-server-with-multiple-gpus-should-i-use-one-triton-inference-server-to-manage-all-gpus-or-should-i-use-multiple-inference-servers-one-for-each-gpu">
   If I have a server with multiple GPUs should I use one Triton Inference Server to manage all GPUs or should I use multiple inference servers, one for each GPU?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#if-the-server-segfaults-how-can-i-debug-it">
   If the server segfaults, how can I debug it?
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>FAQ</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-are-the-advantages-of-running-a-model-with-triton-inference-server-compared-to-running-directly-using-the-model-s-framework-api">
   What are the advantages of running a model with Triton Inference Server compared to running directly using the model’s framework API?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#can-triton-inference-server-run-on-systems-that-don-t-have-gpus">
   Can Triton Inference Server run on systems that don’t have GPUs?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#can-triton-inference-server-be-used-in-non-docker-environments">
   Can Triton Inference Server be used in non-Docker environments?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#do-you-provide-client-libraries-for-languages-other-than-c-and-python">
   Do you provide client libraries for languages other than C++ and Python?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-would-you-use-triton-inference-server-within-the-aws-environment">
   How would you use Triton Inference Server within the AWS environment?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-i-measure-the-performance-of-my-model-running-in-the-triton-inference-server">
   How do I measure the performance of my model running in the Triton Inference Server?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-can-i-fully-utilize-the-gpu-with-triton-inference-server">
   How can I fully utilize the GPU with Triton Inference Server?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#if-i-have-a-server-with-multiple-gpus-should-i-use-one-triton-inference-server-to-manage-all-gpus-or-should-i-use-multiple-inference-servers-one-for-each-gpu">
   If I have a server with multiple GPUs should I use one Triton Inference Server to manage all GPUs or should I use multiple inference servers, one for each GPU?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#if-the-server-segfaults-how-can-i-debug-it">
   If the server segfaults, how can I debug it?
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                 <!--
# Copyright 2019-2021, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->
<div class="tex2jax_ignore mathjax_ignore section" id="faq">
<h1>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline">#</a></h1>
<div class="section" id="what-are-the-advantages-of-running-a-model-with-triton-inference-server-compared-to-running-directly-using-the-model-s-framework-api">
<h2>What are the advantages of running a model with Triton Inference Server compared to running directly using the model’s framework API?<a class="headerlink" href="#what-are-the-advantages-of-running-a-model-with-triton-inference-server-compared-to-running-directly-using-the-model-s-framework-api" title="Permalink to this headline">#</a></h2>
<p>When using Triton Inference Server the inference result will be the
same as when using the model’s framework directly. However, with
Triton you get benefits like <a class="reference internal" href="architecture.html#concurrent-model-execution"><span class="std std-doc">concurrent model
execution</span></a> (the ability to
run multiple models at the same time on the same GPU) and <span class="xref myst">dynamic
batching</span> to get better
throughput. You can also <a class="reference internal" href="model_management.html"><span class="doc std std-doc">replace or upgrade models while Triton and
client application are running</span></a>. Another benefit
is that Triton can be deployed as a Docker container, anywhere – on
premises and on public clouds. Triton Inference Server also <a class="reference external" href="https://github.com/triton-inference-server/backend">supports
multiple
frameworks</a> such
as TensorRT, TensorFlow, PyTorch, and ONNX on both GPUs and CPUs
leading to a streamlined deployment.</p>
</div>
<div class="section" id="can-triton-inference-server-run-on-systems-that-don-t-have-gpus">
<h2>Can Triton Inference Server run on systems that don’t have GPUs?<a class="headerlink" href="#can-triton-inference-server-run-on-systems-that-don-t-have-gpus" title="Permalink to this headline">#</a></h2>
<p>Yes, the QuickStart guide describes how to <a class="reference internal" href="../getting_started/quickstart.html#run-on-cpu-only-system"><span class="std std-doc">run Triton on a CPU-Only
System</span></a>.</p>
</div>
<div class="section" id="can-triton-inference-server-be-used-in-non-docker-environments">
<h2>Can Triton Inference Server be used in non-Docker environments?<a class="headerlink" href="#can-triton-inference-server-be-used-in-non-docker-environments" title="Permalink to this headline">#</a></h2>
<p>Yes. Triton Inference Server can also be <a class="reference internal" href="../customization_guide/build.html#a-name-ubuntu-without-docker-a-building-without-docker"><span class="std std-doc">built from
source</span></a> on your “bare metal”
system.</p>
</div>
<div class="section" id="do-you-provide-client-libraries-for-languages-other-than-c-and-python">
<h2>Do you provide client libraries for languages other than C++ and Python?<a class="headerlink" href="#do-you-provide-client-libraries-for-languages-other-than-c-and-python" title="Permalink to this headline">#</a></h2>
<p>We provide C++ and Python client libraries to make it easy for users
to write client applications that communicate with Triton. We chose
those languages because they were likely to be popular and performant
in the ML inference space, but in the future we can possibly add other
languages if there is a need.</p>
<p>We provide the GRPC API as a way to generate your own client library
for a large number of languages. By following the official GRPC
documentation and using
<a class="reference external" href="https://github.com/triton-inference-server/common/blob/main/protobuf/grpc_service.proto">grpc_service.proto</a>
you can generate language bindings for all the languages supported by
GRPC. We provide three examples of this for
<a class="reference external" href="https://github.com/triton-inference-server/client/blob/main/src/grpc_generated/go">Go</a>,
<a class="reference external" href="https://github.com/triton-inference-server/client/blob/main/src/python/examples/grpc_client.py">Python</a> and
<a class="reference external" href="https://github.com/triton-inference-server/client/blob/main/src/grpc_generated/java">Java</a>.</p>
<p>In general the client libraries (and client examples) are meant to be
just that, examples. We feel the client libraries are well written and
well tested, but they are not meant to serve every possible use
case. In some cases you may want to develop your own customized
library to suit your specific needs.</p>
</div>
<div class="section" id="how-would-you-use-triton-inference-server-within-the-aws-environment">
<h2>How would you use Triton Inference Server within the AWS environment?<a class="headerlink" href="#how-would-you-use-triton-inference-server-within-the-aws-environment" title="Permalink to this headline">#</a></h2>
<p>In an AWS environment, the Triton Inference Server docker container
can run on <span class="xref myst">CPU-only instances or GPU compute
instances</span>. Triton can run directly on the
compute instance or inside Elastic Kubernetes Service (EKS). In
addition, other AWS services such as Elastic Load Balancer (ELB) can
be used for load balancing traffic among multiple Triton
instances. Elastic Block Store (EBS) or S3 can be used for storing
deep-learning models loaded by the inference server.</p>
</div>
<div class="section" id="how-do-i-measure-the-performance-of-my-model-running-in-the-triton-inference-server">
<h2>How do I measure the performance of my model running in the Triton Inference Server?<a class="headerlink" href="#how-do-i-measure-the-performance-of-my-model-running-in-the-triton-inference-server" title="Permalink to this headline">#</a></h2>
<p>The Triton Inference Server exposes performance information in two
ways: by <a class="reference internal" href="metrics.html"><span class="doc std std-doc">Prometheus metrics</span></a> and by the statistics
available through the <a class="reference internal" href="../customization_guide/inference_protocols.html"><span class="doc std std-doc">HTTP/REST, GRPC, and C
APIs</span></a>.</p>
<p>A client application, <a class="reference internal" href="perf_analyzer.html"><span class="doc std std-doc">perf_analyzer</span></a>, allows you to
measure the performance of an individual model using a synthetic
load. The perf_analyzer application is designed to show you the
tradeoff of latency vs. throughput.</p>
</div>
<div class="section" id="how-can-i-fully-utilize-the-gpu-with-triton-inference-server">
<h2>How can I fully utilize the GPU with Triton Inference Server?<a class="headerlink" href="#how-can-i-fully-utilize-the-gpu-with-triton-inference-server" title="Permalink to this headline">#</a></h2>
<p>Triton Inference Server has several features designed to increase
GPU utilization:</p>
<ul class="simple">
<li><p>Triton can <a class="reference internal" href="architecture.html#concurrent-model-execution"><span class="std std-doc">simultaneously perform inference for multiple
models</span></a> (using either
the same or different frameworks) using the same GPU.</p></li>
<li><p>Triton can increase inference throughput by using <a class="reference internal" href="architecture.html#concurrent-model-execution"><span class="std std-doc">multiple
instances of the same
model</span></a> to handle multiple
simultaneous inferences requests to that model. Triton chooses
reasonable defaults but <a class="reference internal" href="model_configuration.html#instance-groups"><span class="std std-doc">you can also control the exact level of
concurrency</span></a> on a
model-by-model basis.</p></li>
<li><p>Triton can <span class="xref myst">batch together multiple inference requests into a single
inference execution</span>. Typically,
batching inference requests leads to much higher thoughput with only
a relatively small increase in latency.</p></li>
</ul>
<p>As a general rule, batching is the most beneficial way to increase GPU
utilization. So you should always try enabling the <span class="xref myst">dynamic
batcher</span> with your models. Using
multiple instances of a model can also provide some benefit but is
typically most useful for models that have small compute
requirements. Most models will benefit from using two instances but
more than that is often not useful.</p>
</div>
<div class="section" id="if-i-have-a-server-with-multiple-gpus-should-i-use-one-triton-inference-server-to-manage-all-gpus-or-should-i-use-multiple-inference-servers-one-for-each-gpu">
<h2>If I have a server with multiple GPUs should I use one Triton Inference Server to manage all GPUs or should I use multiple inference servers, one for each GPU?<a class="headerlink" href="#if-i-have-a-server-with-multiple-gpus-should-i-use-one-triton-inference-server-to-manage-all-gpus-or-should-i-use-multiple-inference-servers-one-for-each-gpu" title="Permalink to this headline">#</a></h2>
<p>Triton Inference Server will take advantage of all GPUs that it has
access to on the server. You can limit the GPUs available to Triton by
using the CUDA_VISIBLE_DEVICES environment variable (or with Docker
you can also use NVIDIA_VISIBLE_DEVICES or –gpus flag when launching
the container). When using multiple GPUs, Triton will distribute
inference request across the GPUs to keep them all equally
utilized. You can also <a class="reference internal" href="model_configuration.html#instance-groups"><span class="std std-doc">control more explicitly which models are
running on which GPUs</span></a>.</p>
<p>In some deployment and orchestration environments (for example,
Kubernetes) it may be more desirable to partition a single multi-GPU
server into multiple <em>nodes</em>, each with one GPU. In this case the
orchestration environment will run a different Triton for each GPU and
an load balancer will be used to divide inference requests across the
available Triton instances.</p>
</div>
<div class="section" id="if-the-server-segfaults-how-can-i-debug-it">
<h2>If the server segfaults, how can I debug it?<a class="headerlink" href="#if-the-server-segfaults-how-can-i-debug-it" title="Permalink to this headline">#</a></h2>
<p>The NGC build is a Release build and does not contain Debug symbols.
The build.py as well defaults to a Release build. Refer to the instructions
in <a class="reference internal" href="../customization_guide/build.html#building-with-debug-symbols"><span class="std std-doc">build.md</span></a> to create a Debug build
of Triton. This will help find the cause of the segmentation fault when
looking at the gdb trace for the segfault.</p>
<p>When opening a GitHub issue for the segfault with Triton, please include
the backtrace to better help us resolve the problem.</p>
</div>
</div>

<div class="section">
   
</div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="v1_to_v2.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Version 1 to Version 2 Migration</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../customization_guide/inference_protocols.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Inference Protocols and APIs</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By NVIDIA<br/>
  
      &copy; Copyright 2022 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.<br/>
    Last updated on Oct 05, 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


<script type="text/javascript">_satellite.pageBottom();</script>
  </body>
</html>