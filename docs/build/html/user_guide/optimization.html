
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Optimization &#8212; NVIDIA Triton Inference Server</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="https://js.hcaptcha.com/1/api.js"></script>
    <script src="//assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script src="../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/optimization.html" />
    <link rel="shortcut icon" href="../_static/nvidia-logo-vert-rgb-blk-for-screen.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Ragged Batching" href="ragged_batching.html" />
    <link rel="prev" title="Model Configuration" href="model_configuration.html" /> 
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
      
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/nvidia-logo-horiz-rgb-blk-for-screen.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">NVIDIA Triton Inference Server</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started/quickstart.html">
   Quickstart
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  User Guide
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="performance_tuning.html">
   Deploying your trained model using Triton
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="architecture.html">
   Triton Architecture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_repository.html">
   Model Repository
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/repository_agents.html">
   Repository Agent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_configuration.html">
   Model Configuration
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ragged_batching.html">
   Ragged Batching
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rate_limiter.html">
   Rate Limiter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_analyzer.html">
   Model Analyzer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="perf_analyzer.html">
   Performance Analyzer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_management.html">
   Model Management
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="custom_operations.html">
   Custom Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="decoupled_models.html">
   Decoupled Backends and Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="response_cache.html">
   Triton Response Cache
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="metrics.html">
   Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="trace.html">
   Triton Server Trace
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="jetson.html">
   Triton Inference Server Support for Jetson and JetPack
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="v1_to_v2.html">
   Version 1 to Version 2 Migration
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="faq.html">
   FAQ
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Protocol Guides
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/inference_protocols.html">
   Inference Protocols and APIs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_binary_data.html">
   Binary Tensor Data Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_classification.html">
   Classification Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_logging.html">
   Logging Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_model_configuration.html">
   Model Configuration Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_model_repository.html">
   Model Repository Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_schedule_policy.html">
   Schedule Policy Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_sequence.html">
   Sequence Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_shared_memory.html">
   Shared-Memory Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_statistics.html">
   Statistics Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../protocol/extension_trace.html">
   Trace Extension
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Customization Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/build.html">
   Building Triton
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/compose.html">
   Customize Triton Container
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../customization_guide/test.html">
   Testing Triton
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Examples
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../examples/jetson/README.html">
   Using Triton Inference Server as a shared library for execution on Jetson
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../examples/jetson/concurrency_and_dynamic_batching/README.html">
   Concurrent inference and dynamic batching
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/triton-inference-server/server"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/triton-inference-server/server/issues/new?title=Issue%20on%20page%20%2Fuser_guide/optimization.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-settings">
   Optimization Settings
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dynamic-batcher">
     Dynamic Batcher
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-instances">
     Model Instances
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#framework-specific-optimization">
   Framework-Specific Optimization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnx-with-tensorrt-optimization-ort-trt">
     ONNX with TensorRT Optimization (ORT-TRT)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnx-with-openvino-optimization">
     ONNX with OpenVINO Optimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensorflow-with-tensorrt-optimization-tf-trt">
     TensorFlow with TensorRT Optimization (TF-TRT)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensorflow-automatic-fp16-optimization">
     TensorFlow Automatic FP16 Optimization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#numa-optimization">
   NUMA Optimization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#host-policy">
     Host Policy
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Optimization</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-settings">
   Optimization Settings
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dynamic-batcher">
     Dynamic Batcher
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-instances">
     Model Instances
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#framework-specific-optimization">
   Framework-Specific Optimization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnx-with-tensorrt-optimization-ort-trt">
     ONNX with TensorRT Optimization (ORT-TRT)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#onnx-with-openvino-optimization">
     ONNX with OpenVINO Optimization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensorflow-with-tensorrt-optimization-tf-trt">
     TensorFlow with TensorRT Optimization (TF-TRT)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tensorflow-automatic-fp16-optimization">
     TensorFlow Automatic FP16 Optimization
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#numa-optimization">
   NUMA Optimization
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#host-policy">
     Host Policy
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                 <!--
# Copyright (c) 2019-2021, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->
<div class="tex2jax_ignore mathjax_ignore section" id="optimization">
<h1>Optimization<a class="headerlink" href="#optimization" title="Permalink to this headline">#</a></h1>
<p>The Triton Inference Server has many features that you can use to
decrease latency and increase throughput for your model. This section
discusses these features and demonstrates how you can use them to
improve the performance of your model. As a prerequisite you should
follow the <a class="reference internal" href="../getting_started/quickstart.html"><span class="doc std std-doc">QuickStart</span></a> to get Triton and client
examples running with the example model repository.</p>
<p>This section focuses on understanding latency and throughput tradeoffs
for a single model. The <a class="reference internal" href="model_analyzer.html"><span class="doc std std-doc">Model Analyzer</span></a> section
describes a tool that helps you understand the GPU memory utilization
of your models so you can decide how to best run multiple models on a
single GPU.</p>
<p>Unless you already have a client application suitable for measuring
the performance of your model on Triton, you should familiarize
yourself with <a class="reference internal" href="perf_analyzer.html"><span class="doc std std-doc">Performance Analyzer</span></a>. The
Performance Analyzer is an essential tool for optimizing your model’s
performance.</p>
<p>As a running example demonstrating the optimization features and
options, we will use a TensorFlow Inception model that you can obtain
by following the <a class="reference internal" href="../getting_started/quickstart.html"><span class="doc std std-doc">QuickStart</span></a>. As a baseline we use
perf_analyzer to determine the performance of the model using a <a class="reference download internal" download="" href="../_downloads/4a2b3af7977cfcda41e2eb9b53c1f797/config.pbtxt"><span class="xref download myst">basic
model configuration that does not enable any performance
features</span></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4
...
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 62.6 infer/sec, latency 21371 usec
Concurrency: 2, throughput: 73.2 infer/sec, latency 34381 usec
Concurrency: 3, throughput: 73.2 infer/sec, latency 50298 usec
Concurrency: 4, throughput: 73.4 infer/sec, latency 65569 usec
</pre></div>
</div>
<p>The results show that our non-optimized model configuration gives a
throughput of about 73 inferences per second. Note how there is a
significant throughput increase going from one concurrent request to
two concurrent requests and then throughput levels off. With one
concurrent request Triton is idle during the time when the response is
returned to the client and the next request is received at the
server. Throughput increases with a concurrency of two because Triton
overlaps the processing of one request with the communication of the
other. Because we are running perf_analyzer on the same system as
Triton, two requests are enough to completely hide the communication
latency.</p>
<div class="section" id="optimization-settings">
<h2>Optimization Settings<a class="headerlink" href="#optimization-settings" title="Permalink to this headline">#</a></h2>
<p>For most models, the Triton feature that provides the largest
performance improvement is <span class="xref myst">dynamic
batching</span>. If your model does not
support batching then you can skip ahead to <a class="reference internal" href="#model-instances"><span class="std std-doc">Model
Instances</span></a>.</p>
<div class="section" id="dynamic-batcher">
<h3>Dynamic Batcher<a class="headerlink" href="#dynamic-batcher" title="Permalink to this headline">#</a></h3>
<p>The dynamic batcher combines individual inference requests into a
larger batch that will often execute much more efficiently than
executing the individual requests independently. To enable the dynamic
batcher stop Triton, add the following line to the end of the <a class="reference download internal" download="" href="../_downloads/4a2b3af7977cfcda41e2eb9b53c1f797/config.pbtxt"><span class="xref download myst">model
configuration file for
inception_graphdef</span></a>,
and then restart Triton.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_batching</span> <span class="p">{</span> <span class="p">}</span>
</pre></div>
</div>
<p>The dynamic batcher allows Triton to handle a higher number of
concurrent requests because those requests are combined for
inference. To see this run perf_analyzer with request concurrency from
1 to 8.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:8
...
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 66.8 infer/sec, latency 19785 usec
Concurrency: 2, throughput: 80.8 infer/sec, latency 30732 usec
Concurrency: 3, throughput: 118 infer/sec, latency 32968 usec
Concurrency: 4, throughput: 165.2 infer/sec, latency 32974 usec
Concurrency: 5, throughput: 194.4 infer/sec, latency 33035 usec
Concurrency: 6, throughput: 217.6 infer/sec, latency 34258 usec
Concurrency: 7, throughput: 249.8 infer/sec, latency 34522 usec
Concurrency: 8, throughput: 272 infer/sec, latency 35988 usec
</pre></div>
</div>
<p>With eight concurrent requests the dynamic batcher allows Triton to
provide 272 inferences per second without increasing latency
compared to not using the dynamic batcher.</p>
<p>Instead of having perf_analyzer collect data for a range of request
concurrency values we can instead use a couple of simple rules that
typically applies when perf_analyzer is running on the same system as
Triton. The first rule is that for minimum latency set the request
concurrency to 1 and disable the dynamic batcher and use only 1 <a class="reference internal" href="#model-instances"><span class="std std-doc">model
instance</span></a>. The second rule is that for maximum
throughput set the request concurrency to be
<code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">&lt;maximum</span> <span class="pre">batch</span> <span class="pre">size&gt;</span> <span class="pre">*</span> <span class="pre">&lt;model</span> <span class="pre">instance</span> <span class="pre">count&gt;</span></code>. We will discuss model
instances <a class="reference internal" href="#model-instances"><span class="std std-doc">below</span></a>, for now we are working with one model
instance. So for maximum-batch-size 4 we want to run perf_analyzer
with request concurrency of <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">4</span> <span class="pre">*</span> <span class="pre">1</span> <span class="pre">=</span> <span class="pre">8</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 8
...
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 8, throughput: 267.8 infer/sec, latency 35590 usec
</pre></div>
</div>
</div>
<div class="section" id="model-instances">
<h3>Model Instances<a class="headerlink" href="#model-instances" title="Permalink to this headline">#</a></h3>
<p>Triton allows you to specify how many copies of each model you want to
make available for inferencing. By default you get one copy of each
model, but you can specify any number of instances in the model
configuration by using <a class="reference internal" href="model_configuration.html#instance-groups"><span class="std std-doc">instance
groups</span></a>. Typically, having two
instances of a model will improve performance because it allows
overlap of memory transfer operations (for example, CPU to/from GPU)
with inference compute. Multiple instances also improve GPU
utilization by allowing more inference work to be executed
simultaneously on the GPU. Smaller models may benefit from more than
two instances; you can use perf_analyzer to experiment.</p>
<p>To specify two instances of the inception_graphdef model: stop Triton,
remove any dynamic batching settings you may have previously added to
the model configuration (we discuss combining dynamic batcher and
multiple model instances below), add the following lines to the end of
the <a class="reference download internal" download="" href="../_downloads/4a2b3af7977cfcda41e2eb9b53c1f797/config.pbtxt"><span class="xref download myst">model configuration
file</span></a>, and
then restart Triton.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">instance_group</span> <span class="p">[</span> <span class="p">{</span> <span class="n">count</span><span class="p">:</span> <span class="mi">2</span> <span class="p">}]</span>
</pre></div>
</div>
<p>Now run perf_analyzer using the same options as for the baseline.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4
...
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 70.6 infer/sec, latency 19547 usec
Concurrency: 2, throughput: 106.6 infer/sec, latency 23532 usec
Concurrency: 3, throughput: 110.2 infer/sec, latency 36649 usec
Concurrency: 4, throughput: 108.6 infer/sec, latency 43588 usec
</pre></div>
</div>
<p>In this case having two instances of the model increases throughput
from about 73 inference per second to about 110 inferences per second
compared with one instance.</p>
<p>It is possible to enable both the dynamic batcher and multiple model
instances, for example, change the model configuration file to include
the following.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_batching</span> <span class="p">{</span> <span class="p">}</span>
<span class="n">instance_group</span> <span class="p">[</span> <span class="p">{</span> <span class="n">count</span><span class="p">:</span> <span class="mi">2</span> <span class="p">}]</span>
</pre></div>
</div>
<p>When we run perf_analyzer with the same options used for just the
dynamic batcher above.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 16
...
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 16, throughput: 289.6 infer/sec, latency 59817 usec
</pre></div>
</div>
<p>We see that two instances does not improve throughput much while
increasing latency, compared with just using the dynamic batcher and
one instance. This occurs because for this model the dynamic batcher
alone is capable of fully utilizing the GPU and so adding additional
model instances does not provide any performance advantage. In general
the benefit of the dynamic batcher and multiple instances is model
specific, so you should experiment with perf_analyzer to determine the
settings that best satisfy your throughput and latency requirements.</p>
</div>
</div>
<div class="section" id="framework-specific-optimization">
<h2>Framework-Specific Optimization<a class="headerlink" href="#framework-specific-optimization" title="Permalink to this headline">#</a></h2>
<p>Triton has several optimization settings that apply to only a subset
of the supported model frameworks. These optimization settings are
controlled by the model configuration <a class="reference internal" href="model_configuration.html#optimization-policy"><span class="std std-doc">optimization
policy</span></a>.</p>
<div class="section" id="onnx-with-tensorrt-optimization-ort-trt">
<h3>ONNX with TensorRT Optimization (ORT-TRT)<a class="headerlink" href="#onnx-with-tensorrt-optimization-ort-trt" title="Permalink to this headline">#</a></h3>
<p>One especially powerful optimization is to use TensorRT in
conjunction with an ONNX model. As an example of TensorRT optimization
applied to an ONNX model, we will use an ONNX DenseNet model that you
can obtain by following <a class="reference internal" href="../getting_started/quickstart.html"><span class="doc std std-doc">QuickStart</span></a>. As a baseline we
use perf_analyzer to determine the performance of the model using a
<a class="reference download internal" download="" href="../_downloads/3da32df3e3fd77534e90cb4ee7051ec9/config.pbtxt"><span class="xref download myst">basic model configuration that does not enable any performance
features</span></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ perf_analyzer -m densenet_onnx --percentile=95 --concurrency-range 1:4
...
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, 113.2 infer/sec, latency 8939 usec
Concurrency: 2, 138.2 infer/sec, latency 14548 usec
Concurrency: 3, 137.2 infer/sec, latency 21947 usec
Concurrency: 4, 136.8 infer/sec, latency 29661 usec
</pre></div>
</div>
<p>To enable TensorRT optimization for the model: stop Triton, add the
following lines to the end of the model configuration file, and then
restart Triton.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimization</span> <span class="p">{</span> <span class="n">execution_accelerators</span> <span class="p">{</span>
  <span class="n">gpu_execution_accelerator</span> <span class="p">:</span> <span class="p">[</span> <span class="p">{</span>
    <span class="n">name</span> <span class="p">:</span> <span class="s2">&quot;tensorrt&quot;</span>
    <span class="n">parameters</span> <span class="p">{</span> <span class="n">key</span><span class="p">:</span> <span class="s2">&quot;precision_mode&quot;</span> <span class="n">value</span><span class="p">:</span> <span class="s2">&quot;FP16&quot;</span> <span class="p">}</span>
    <span class="n">parameters</span> <span class="p">{</span> <span class="n">key</span><span class="p">:</span> <span class="s2">&quot;max_workspace_size_bytes&quot;</span> <span class="n">value</span><span class="p">:</span> <span class="s2">&quot;1073741824&quot;</span> <span class="p">}</span>
    <span class="p">}]</span>
<span class="p">}}</span>
</pre></div>
</div>
<p>As Triton starts you should check the console output and wait until
Triton prints the “Staring endpoints” message. ONNX model loading can
be significantly slower when TensorRT optimization is enabled. In
production you can use <a class="reference internal" href="model_configuration.html#model-warmup"><span class="std std-doc">model warmup</span></a>
to avoid this model startup/optimization slowdown. Now
run perf_analyzer using the same options as for the baseline.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ perf_analyzer -m densenet_onnx --percentile=95 --concurrency-range 1:4
...
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, 190.6 infer/sec, latency 5384 usec
Concurrency: 2, 273.8 infer/sec, latency 7347 usec
Concurrency: 3, 272.2 infer/sec, latency 11046 usec
Concurrency: 4, 266.8 infer/sec, latency 15089 usec
</pre></div>
</div>
<p>The TensorRT optimization provided 2x throughput improvement while
cutting latency in half. The benefit provided by TensorRT will vary
based on the model, but in general it can provide significant
performance improvement.</p>
</div>
<div class="section" id="onnx-with-openvino-optimization">
<h3>ONNX with OpenVINO Optimization<a class="headerlink" href="#onnx-with-openvino-optimization" title="Permalink to this headline">#</a></h3>
<p>ONNX models running on the CPU can also be accelerated by using
<a class="reference external" href="https://docs.openvinotoolkit.org/latest/index.html">OpenVINO</a>. To
enable OpenVINO optimization for an ONNX model, add the following
lines to the end of the model’s configuration file.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimization</span> <span class="p">{</span> <span class="n">execution_accelerators</span> <span class="p">{</span>
  <span class="n">cpu_execution_accelerator</span> <span class="p">:</span> <span class="p">[</span> <span class="p">{</span>
    <span class="n">name</span> <span class="p">:</span> <span class="s2">&quot;openvino&quot;</span>
  <span class="p">}]</span>
<span class="p">}}</span>
</pre></div>
</div>
</div>
<div class="section" id="tensorflow-with-tensorrt-optimization-tf-trt">
<h3>TensorFlow with TensorRT Optimization (TF-TRT)<a class="headerlink" href="#tensorflow-with-tensorrt-optimization-tf-trt" title="Permalink to this headline">#</a></h3>
<p>TensorRT optimization applied to a TensorFlow model works similarly to
TensorRT and ONNX described above. To enable TensorRT optimization you
must set the model configuration appropriately. For TensorRT
optimization of TensorFlow models there are several options that you
can enable, including selection of the compute precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimization</span> <span class="p">{</span> <span class="n">execution_accelerators</span> <span class="p">{</span>
  <span class="n">gpu_execution_accelerator</span> <span class="p">:</span> <span class="p">[</span> <span class="p">{</span>
    <span class="n">name</span> <span class="p">:</span> <span class="s2">&quot;tensorrt&quot;</span>
    <span class="n">parameters</span> <span class="p">{</span> <span class="n">key</span><span class="p">:</span> <span class="s2">&quot;precision_mode&quot;</span> <span class="n">value</span><span class="p">:</span> <span class="s2">&quot;FP16&quot;</span> <span class="p">}}]</span>
<span class="p">}}</span>
</pre></div>
</div>
<p>The options are described in detail in the
<a class="reference external" href="https://github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto">ModelOptimizationPolicy</a>
section of the model configuration protobuf.</p>
<p>As an example of TensorRT optimization applied to a TensorFlow model,
we will use a TensorFlow Inception model that you can obtain by
following the <a class="reference internal" href="../getting_started/quickstart.html"><span class="doc std std-doc">QuickStart</span></a>. As a baseline we use
perf_analyzer to determine the performance of the model using a <a class="reference download internal" download="" href="../_downloads/4a2b3af7977cfcda41e2eb9b53c1f797/config.pbtxt"><span class="xref download myst">basic
model configuration that does not enable any performance
features</span></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4
...
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 62.6 infer/sec, latency 21371 usec
Concurrency: 2, throughput: 73.2 infer/sec, latency 34381 usec
Concurrency: 3, throughput: 73.2 infer/sec, latency 50298 usec
Concurrency: 4, throughput: 73.4 infer/sec, latency 65569 usec
</pre></div>
</div>
<p>To enable TensorRT optimization for the model: stop Triton, add the
lines from above to the end of the model configuration file, and then
restart Triton. As Triton starts you should check the console output
and wait until the server prints the “Staring endpoints” message. Now
run perf_analyzer using the same options as for the baseline. Note
that the first run of perf_analyzer might timeout because the TensorRT
optimization is performed when the inference request is received and
may take significant time. In production you can use <a class="reference internal" href="model_configuration.html#model-warmup"><span class="std std-doc">model
warmup</span></a> to avoid this model
startup/optimization slowdown. For now, if this happens just run
perf_analyzer again.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ perf_analyzer -m inception_graphdef --percentile=95 --concurrency-range 1:4
...
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 140 infer/sec, latency 8987 usec
Concurrency: 2, throughput: 195.6 infer/sec, latency 12583 usec
Concurrency: 3, throughput: 189 infer/sec, latency 19020 usec
Concurrency: 4, throughput: 191.6 infer/sec, latency 24622 usec
</pre></div>
</div>
<p>The TensorRT optimization provided 2.5x throughput improvement while
cutting latency by more than half. The benefit provided by TensorRT
will vary based on the model, but in general it can provide
significant performance improvement.</p>
</div>
<div class="section" id="tensorflow-automatic-fp16-optimization">
<h3>TensorFlow Automatic FP16 Optimization<a class="headerlink" href="#tensorflow-automatic-fp16-optimization" title="Permalink to this headline">#</a></h3>
<p>TensorFlow has an option to provide FP16 optimization that can be
enabled in the model configuration. As with the TensorRT optimization
described above, you can enable this optimization by using the
gpu_execution_accelerator property.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimization</span> <span class="p">{</span> <span class="n">execution_accelerators</span> <span class="p">{</span>
  <span class="n">gpu_execution_accelerator</span> <span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span> <span class="n">name</span> <span class="p">:</span> <span class="s2">&quot;auto_mixed_precision&quot;</span> <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}}</span>
</pre></div>
</div>
<p>The options are described in detail in the
<a class="reference external" href="https://github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto">ModelOptimizationPolicy</a>
section of the model configuration protobuf.</p>
<p>You can follow the steps described above for TensorRT to see how this
automatic FP16 optimization benefits a model by using perf_analyzer
to evaluate the model’s performance with and without the optimization.</p>
</div>
</div>
<div class="section" id="numa-optimization">
<h2>NUMA Optimization<a class="headerlink" href="#numa-optimization" title="Permalink to this headline">#</a></h2>
<p>Many modern CPUs are composed of multiple cores, memories and interconnects that
expose different performance characteristics depending on how threads and
data are allocated.
Triton allows you to set host policies that describe this
<a class="reference external" href="https://www.kernel.org/doc/html/latest/mm/numa.html">NUMA</a> configuration for
your system and then assign model instances to different host policies
to exploit these NUMA properties.</p>
<div class="section" id="host-policy">
<h3>Host Policy<a class="headerlink" href="#host-policy" title="Permalink to this headline">#</a></h3>
<p>Triton allows you to specify host policy that associates with a policy name on
startup. A host policy will be applied to a model instance if the instance is
specified with the same policy name by using host policy field in <a class="reference internal" href="model_configuration.html#instance-groups"><span class="std std-doc">instance
groups</span></a>. Note that if not specified,
the host policy field will be set to default name based on the instance
property.</p>
<p>To specify a host policy, you can specify the following in command line option:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">host</span><span class="o">-</span><span class="n">policy</span><span class="o">=&lt;</span><span class="n">policy_name</span><span class="o">&gt;</span><span class="p">,</span><span class="o">&lt;</span><span class="n">setting</span><span class="o">&gt;=&lt;</span><span class="n">value</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Currently, the supported settings are the following:</p>
<ul class="simple">
<li><p><em>numa-node</em>: The NUMA node id that the host policy will be bound to, the
host policy restricts memory allocation to the node specified.</p></li>
<li><p><em>cpu-cores</em>: The CPU cores to be run on, the instance with this host policy
set will be running on one of those CPU cores.</p></li>
</ul>
<p>Assuming that the system is configured to bind GPU 0 with NUMA node 0 which has
CPU cores from 0 to 15, the following shows setting the numa-node and cpu-cores
policies for “gpu_0”:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ tritonserver --host-policy=gpu_0,numa-node=0 --host-policy=gpu_0,cpu-cores=0-15 ...
</pre></div>
</div>
</div>
</div>
</div>

<div class="section">
   
</div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="model_configuration.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Model Configuration</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="ragged_batching.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Ragged Batching</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By NVIDIA<br/>
  
      &copy; Copyright 2022 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.<br/>
    Last updated on Oct 05, 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


<script type="text/javascript">_satellite.pageBottom();</script>
  </body>
</html>