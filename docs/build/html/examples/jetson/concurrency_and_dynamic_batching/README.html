
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Concurrent inference and dynamic batching &#8212; NVIDIA Triton Inference Server</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="https://js.hcaptcha.com/1/api.js"></script>
    <script src="//assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script src="../../../_static/design-tabs.js"></script>
    <link rel="canonical" href="https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/examples/jetson/concurrency_and_dynamic_batching/README.html" />
    <link rel="shortcut icon" href="../../../_static/nvidia-logo-vert-rgb-blk-for-screen.png"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="prev" title="Using Triton Inference Server as a shared library for execution on Jetson" href="../README.html" /> 
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
      
<style type="text/css">
  ul.ablog-archive {
    list-style: none;
    overflow: auto;
    margin-left: 0px;
  }
  ul.ablog-archive li {
    float: left;
    margin-right: 5px;
    font-size: 80%;
  }
  ul.postlist a {
    font-style: italic;
  }
  ul.postlist-style-disc {
    list-style-type: disc;
  }
  ul.postlist-style-none {
    list-style-type: none;
  }
  ul.postlist-style-circle {
    list-style-type: circle;
  }
</style>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/nvidia-logo-horiz-rgb-blk-for-screen.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">NVIDIA Triton Inference Server</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../getting_started/quickstart.html">
   Quickstart
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  User Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/performance_tuning.html">
   Deploying your trained model using Triton
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/architecture.html">
   Triton Architecture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/model_repository.html">
   Model Repository
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../customization_guide/repository_agents.html">
   Repository Agent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/model_configuration.html">
   Model Configuration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/optimization.html">
   Optimization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/ragged_batching.html">
   Ragged Batching
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/rate_limiter.html">
   Rate Limiter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/model_analyzer.html">
   Model Analyzer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/perf_analyzer.html">
   Performance Analyzer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/model_management.html">
   Model Management
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/custom_operations.html">
   Custom Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/decoupled_models.html">
   Decoupled Backends and Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/response_cache.html">
   Triton Response Cache
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/metrics.html">
   Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/trace.html">
   Triton Server Trace
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/jetson.html">
   Triton Inference Server Support for Jetson and JetPack
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/v1_to_v2.html">
   Version 1 to Version 2 Migration
  </a>
 </li>
</ul>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../user_guide/faq.html">
   FAQ
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Protocol Guides
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../customization_guide/inference_protocols.html">
   Inference Protocols and APIs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_binary_data.html">
   Binary Tensor Data Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_classification.html">
   Classification Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_logging.html">
   Logging Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_model_configuration.html">
   Model Configuration Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_model_repository.html">
   Model Repository Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_schedule_policy.html">
   Schedule Policy Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_sequence.html">
   Sequence Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_shared_memory.html">
   Shared-Memory Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_statistics.html">
   Statistics Extension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../protocol/extension_trace.html">
   Trace Extension
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Customization Guide
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../customization_guide/build.html">
   Building Triton
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../customization_guide/compose.html">
   Customize Triton Container
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../customization_guide/test.html">
   Testing Triton
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Examples
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../README.html">
   Using Triton Inference Server as a shared library for execution on Jetson
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Concurrent inference and dynamic batching
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/triton-inference-server/server"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/triton-inference-server/server/issues/new?title=Issue%20on%20page%20%2Fexamples/jetson/concurrency_and_dynamic_batching/README.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acquiring-the-model">
   Acquiring the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#converting-the-model-to-tensorrt">
   Converting the model to TensorRT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-the-app">
   Building the app
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#demonstration-case-1-concurrent-model-execution">
   Demonstration  case 1: Concurrent model execution
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#running-the-sample">
     Running the sample
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expected-output">
     Expected output
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#demonstration-case-2-dynamic-batching">
   Demonstration case 2: Dynamic batching
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Running the sample
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Expected output
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Concurrent inference and dynamic batching</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acquiring-the-model">
   Acquiring the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#converting-the-model-to-tensorrt">
   Converting the model to TensorRT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-the-app">
   Building the app
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#demonstration-case-1-concurrent-model-execution">
   Demonstration  case 1: Concurrent model execution
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#running-the-sample">
     Running the sample
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expected-output">
     Expected output
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#demonstration-case-2-dynamic-batching">
   Demonstration case 2: Dynamic batching
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Running the sample
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Expected output
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                 <!--
# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->
<div class="tex2jax_ignore mathjax_ignore section" id="concurrent-inference-and-dynamic-batching">
<h1>Concurrent inference and dynamic batching<a class="headerlink" href="#concurrent-inference-and-dynamic-batching" title="Permalink to this headline">#</a></h1>
<p>The purpose of this sample is to demonstrate the important features of Triton Inference Server such as concurrent model execution and dynamic batching.</p>
<p>We will be using a purpose built deployable people detection model, which we download from <a class="reference external" href="https://ngc.nvidia.com/">Nvidia GPU Cloud (NGC)</a>.</p>
<div class="section" id="acquiring-the-model">
<h2>Acquiring the model<a class="headerlink" href="#acquiring-the-model" title="Permalink to this headline">#</a></h2>
<p>Download the pruned <a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:tlt_peoplenet">PeopleNet</a> model from the NGC. This model is available as a ready-to-use model, and you can download it from NGC using either <code class="docutils literal notranslate"><span class="pre">wget</span></code> method:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/tao/peoplenet/versions/pruned_v2.1/zip -O pruned_v2.1.zip
</pre></div>
</div>
<p>or via CLI command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ngc registry model download-version <span class="s2">&quot;nvidia/tao/peoplenet:pruned_v2.1&quot;</span>
</pre></div>
</div>
<p>For latter you need to setup the <a class="reference external" href="https://ngc.nvidia.com/setup">NGC CLI</a>.</p>
<p>Having downloaded the model from the NGC, unzip the archive <code class="docutils literal notranslate"><span class="pre">peoplenet_pruned_v2.1.zip</span></code> into <code class="docutils literal notranslate"><span class="pre">concurrency_and_dynamic_batching/tao/models/peoplenet</span></code>.</p>
<p>If you have the zip archive in the <code class="docutils literal notranslate"><span class="pre">concurrency_and_dynamic_batching</span></code> directory, the following will automatically place the model to the correct location:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>unzip pruned_v2.1.zip -d <span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/tao/models/peoplenet
</pre></div>
</div>
<p>Verify that you can see the model file <code class="docutils literal notranslate"><span class="pre">resnet34_peoplenet_pruned.etlt</span></code> under</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>concurrency_and_dynamic_batching
└── tao
       └── models
           └── peoplenet
               ├── labels.txt
               └── resnet34_peoplenet_pruned.etlt
</pre></div>
</div>
</div>
<div class="section" id="converting-the-model-to-tensorrt">
<h2>Converting the model to TensorRT<a class="headerlink" href="#converting-the-model-to-tensorrt" title="Permalink to this headline">#</a></h2>
<p>After you have acquired the model file in <code class="docutils literal notranslate"><span class="pre">.etlt</span></code> format, you will need to convert the model to <a class="reference external" href="https://developer.nvidia.com/tensorrt">TensorRT</a> format. NVIDIA TensorRT is an SDK for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. The latest versions of JetPack include TensorRT.</p>
<p>In order to convert an <code class="docutils literal notranslate"><span class="pre">.etlt</span></code> model to TensorRT format, you need to use the <code class="docutils literal notranslate"><span class="pre">tao-converter</span></code> tool.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">tao-converter</span></code> tool is available as a compiled release file for different platforms. The download links corresponding to your deployment system are provided among the <a class="reference external" href="https://developer.nvidia.com/tlt-get-started">TLT Getting Started resources</a>.</p>
<p>After you have downloaded <code class="docutils literal notranslate"><span class="pre">tao-converter</span></code>, you might need to execute</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>chmod <span class="m">777</span> tao-converter 
</pre></div>
</div>
<p>in the directory with the tool.</p>
<p>We provide a conversion script <code class="docutils literal notranslate"><span class="pre">tao/convert_peoplenet.sh</span></code> which expects the model to be present at the location.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>tao
└──  models
   └── peoplenet
</pre></div>
</div>
<p>To execute it, you can place the <code class="docutils literal notranslate"><span class="pre">tao-converter</span></code> executable to the <code class="docutils literal notranslate"><span class="pre">tao</span></code> directory of the project and in the same directory run</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash convert_peoplenet.sh
</pre></div>
</div>
<p>After you execute it, verify that a <code class="docutils literal notranslate"><span class="pre">model.plan</span></code> file was placed to to the directories <code class="docutils literal notranslate"><span class="pre">/trtis_model_repo_sample_1/peoplenet/1</span></code> and <code class="docutils literal notranslate"><span class="pre">/trtis_model_repo_sample_2/peoplenet/1</span></code>. Note that we have two slightly different repositories for the same model to demonstrate different features of Triton.</p>
<p>Also note that this step has to be performed on the target hardware: if you are planning to execute this application on Jetson, the conversion has to be performed on Jetson.</p>
<p>To learn more about <code class="docutils literal notranslate"><span class="pre">tao-converter</span></code>parameters, run:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>./tao-converter -h
</pre></div>
</div>
</div>
<div class="section" id="building-the-app">
<h2>Building the app<a class="headerlink" href="#building-the-app" title="Permalink to this headline">#</a></h2>
<p>To compile the sample, pull the following repositories:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/triton-inference-server/server">https://github.com/triton-inference-server/server</a></p></li>
<li><p><a class="reference external" href="https://github.com/triton-inference-server/core">https://github.com/triton-inference-server/core</a></p></li>
</ul>
<p>Make sure you copied the contents of the release you downloaded to <code class="docutils literal notranslate"><span class="pre">$HOME</span></code></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo cp -rf tritonserver2.x.y-jetpack4.6 <span class="nv">$HOME</span>/tritonserver
</pre></div>
</div>
<p>Open the terminal in <code class="docutils literal notranslate"><span class="pre">concurrency_and_dynamic_batching</span></code> and build the app executing</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>make
</pre></div>
</div>
<p>An example Makefile is provided for Jetson.</p>
</div>
<div class="section" id="demonstration-case-1-concurrent-model-execution">
<h2>Demonstration  case 1: Concurrent model execution<a class="headerlink" href="#demonstration-case-1-concurrent-model-execution" title="Permalink to this headline">#</a></h2>
<p>With Triton Inference Server, multiple models (or multiple instances of the same model) can run simultaneously on the same GPU or on multiple GPUs. In this example, we are demonstrating how to run multiple instances of the same model on a single Jetson GPU.</p>
<div class="section" id="running-the-sample">
<h3>Running the sample<a class="headerlink" href="#running-the-sample" title="Permalink to this headline">#</a></h3>
<p>To execute from the terminal, run from the <code class="docutils literal notranslate"><span class="pre">concurrency_and_dynamic_batching</span></code> directory:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$HOME</span>/tritonserver/lib ./people_detection -m system -v -r <span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/trtis_model_repo_sample_1 -t <span class="m">6</span> -s <span class="nb">false</span> -p <span class="nv">$HOME</span>/tritonserver
</pre></div>
</div>
<p>The parameter <code class="docutils literal notranslate"><span class="pre">-t</span></code> controlls the number of concurrent inference calls we want to execute. We will be executing the same model on the same sample image with the purpose of demonstrating how setting different concurency options affects the performance.</p>
<p>You can enable saving detected bounding boxes in the project directory in form of overlays over the original image for each execution thread. You can turn the visualization on by setting the parameter <code class="docutils literal notranslate"><span class="pre">-s</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code> upon execution (<code class="docutils literal notranslate"><span class="pre">-s</span></code> is set to <code class="docutils literal notranslate"><span class="pre">false</span></code> by default).</p>
</div>
<div class="section" id="expected-output">
<h3>Expected output<a class="headerlink" href="#expected-output" title="Permalink to this headline">#</a></h3>
<p>Upon execution, in the terminal log you will see <em>Model ‘peoplenet’ Stats</em> in json format reflecting the inference performance. We also output <em>TOTAL INFERENCE TIME</em> which simply reflects the elapsed time requred to run the application including data loading, pre-processing and post-processing.</p>
<p>A typical output in the log for <em>Model ‘peoplenet’ Stats</em> looks as follows:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>{
   &quot;model_stats&quot;:[
      {
         &quot;name&quot;:&quot;peoplenet&quot;,
         &quot;version&quot;:&quot;1&quot;,
         &quot;last_inference&quot;:1626448309997,
         &quot;inference_count&quot;:6,
         &quot;execution_count&quot;:6,
         &quot;inference_stats&quot;:{
            &quot;success&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:574589968
            },
            &quot;fail&quot;:{
               &quot;count&quot;:0,
               &quot;ns&quot;:0
            },
            &quot;queue&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:234669630
            },
            &quot;compute_input&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:194884512
            },
            &quot;compute_infer&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:97322636
            },
            &quot;compute_output&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:47700806
            }
         },
         &quot;batch_stats&quot;:[
            {
               &quot;batch_size&quot;:1,
               &quot;compute_input&quot;:{
                  &quot;count&quot;:6,
                  &quot;ns&quot;:194884512
               },
               &quot;compute_infer&quot;:{
                  &quot;count&quot;:6,
                  &quot;ns&quot;:97322636
               },
               &quot;compute_output&quot;:{
                  &quot;count&quot;:6,
                  &quot;ns&quot;:47700806
               }
            }
         ]
      }
   ]
}

TOTAL INFERENCE TIME: 174ms
</pre></div>
</div>
<p>To learn about different statistics check out the <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_statistics.md#statistics-extension">documentation</a>.</p>
<p>To see how setting different values for concurrency affects total execution time and its componets reflected in the model stats, you need to modify a single parameter in the model config file.</p>
<p>To enable concurrent model execution support for a model, corresponding model config file <code class="docutils literal notranslate"><span class="pre">trtis_model_repo_sample_1/peoplenet/config.pbtxt</span></code> includes the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">instance_group</span> <span class="p">[</span>
  <span class="p">{</span>
    <span class="n">count</span><span class="p">:</span> <span class="mi">3</span>
    <span class="n">kind</span><span class="p">:</span> <span class="n">KIND_GPU</span>
  <span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
<p>You can change the count of allowed inferences for the same model instance and observe how it affects performance in <em>Model ‘peoplenet’ Stats</em> and <em>TOTAL INFERENCE TIME</em>. Note that on Jetson we dont recommend setting values too high: for instance, on a device like a Jetson Xavier AGX we don’t recommend setting the number larger than 6. The values in the range 1-3 are optimal.</p>
<p>While trying out different values, note how it affects total inference time as well as some inference statistics (like queue and compute times)</p>
</div>
</div>
<div class="section" id="demonstration-case-2-dynamic-batching">
<h2>Demonstration case 2: Dynamic batching<a class="headerlink" href="#demonstration-case-2-dynamic-batching" title="Permalink to this headline">#</a></h2>
<p>For models that support batching, Triton implements multiple scheduling and batching algorithms that combine individual inference requests together to improve inference throughput. In this example, we want to demonstrate how enbling automatic dynamic batching affects inference performance.</p>
<div class="section" id="id1">
<h3>Running the sample<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>To observe the effect of dynamic batching, from the <code class="docutils literal notranslate"><span class="pre">concurrency_and_dynamic_batching</span></code> directory execute:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$HOME</span>/tritonserver/lib ./people_detection -m system -v -r <span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>/trtis_model_repo_sample_2 -t <span class="m">6</span> -s <span class="nb">false</span> -p <span class="nv">$HOME</span>/tritonserver
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h3>Expected output<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<p>Take a look at <em>Model ‘peoplenet’ Stats</em> and <em>TOTAL INFERENCE TIME</em> to see the effect of dynamic batching. A possible outcome should look like that:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span>{
   &quot;model_stats&quot;:[
      {
         &quot;name&quot;:&quot;peoplenet&quot;,
         &quot;version&quot;:&quot;1&quot;,
         &quot;last_inference&quot;:1626447787832,
         &quot;inference_count&quot;:6,
         &quot;execution_count&quot;:2,
         &quot;inference_stats&quot;:{
            &quot;success&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:558981051
            },
            &quot;fail&quot;:{
               &quot;count&quot;:0,
               &quot;ns&quot;:0
            },
            &quot;queue&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:49271380
            },
            &quot;compute_input&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:170634044
            },
            &quot;compute_infer&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:338079193
            },
            &quot;compute_output&quot;:{
               &quot;count&quot;:6,
               &quot;ns&quot;:950544
            }
         },
         &quot;batch_stats&quot;:[
            {
               &quot;batch_size&quot;:1,
               &quot;compute_input&quot;:{
                  &quot;count&quot;:1,
                  &quot;ns&quot;:15955684
               },
               &quot;compute_infer&quot;:{
                  &quot;count&quot;:1,
                  &quot;ns&quot;:29917093
               },
               &quot;compute_output&quot;:{
                  &quot;count&quot;:1,
                  &quot;ns&quot;:152264
               }
            },
            {
               &quot;batch_size&quot;:5,
               &quot;compute_input&quot;:{
                  &quot;count&quot;:1,
                  &quot;ns&quot;:30935672
               },
               &quot;compute_infer&quot;:{
                  &quot;count&quot;:1,
                  &quot;ns&quot;:61632420
               },
               &quot;compute_output&quot;:{
                  &quot;count&quot;:1,
                  &quot;ns&quot;:159656
               }
            }
         ]
      }
   ]
}

TOTAL INFERENCE TIME: 162ms
</pre></div>
</div>
<p>Notice that this time the model was executed only twice (as indicated by <code class="docutils literal notranslate"><span class="pre">execution_count</span></code>). Also, unlike in the previous example, the <code class="docutils literal notranslate"><span class="pre">batch_stats</span></code> part of the statitstics looks different: we see that our model was executed one time with <code class="docutils literal notranslate"><span class="pre">batch</span> <span class="pre">=</span> <span class="pre">1</span></code> and the second time with <code class="docutils literal notranslate"><span class="pre">batch</span> <span class="pre">=</span> <span class="pre">5</span></code>. It helped to decrease the total inference time.</p>
<p>In order to enable dynamic batching, the following is present in the model config <code class="docutils literal notranslate"><span class="pre">trtis_model_repo_sample_2/peoplenet/config.pbtxt</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic_batching</span> <span class="p">{</span>
<span class="p">}</span>
</pre></div>
</div>
<p>To try further options of dynamic batcher see the <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#dynamic-batcher">documentation</a>.</p>
<p>You can also try enabling both concurrent model execution and dynamic batching.</p>
</div>
</div>
</div>

<div class="section">
   
</div>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../README.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Using Triton Inference Server as a shared library for execution on Jetson</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By NVIDIA<br/>
  
      &copy; Copyright 2022 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.<br/>
    Last updated on Oct 05, 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


<script type="text/javascript">_satellite.pageBottom();</script>
  </body>
</html>