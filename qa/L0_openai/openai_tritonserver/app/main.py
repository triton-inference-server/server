# generated by fastapi-codegen:
#   filename:  openai_trimmed.yml
#   timestamp: 2024-05-05T21:52:36+00:00

from __future__ import annotations

import argparse
import os
import time
import uuid
from contextlib import asynccontextmanager
from typing import Optional, Union

import numpy
import tritonserver
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import Response, StreamingResponse

# TODO: transformer utils needed?
from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast

from .openai_protocol_types import (
    ChatCompletionChoice,
    ChatCompletionFinishReason,
    ChatCompletionResponseMessage,
    ChatCompletionStreamingResponseChoice,
    ChatCompletionStreamResponseDelta,
    Choice,
    CreateChatCompletionRequest,
    CreateChatCompletionResponse,
    CreateChatCompletionStreamResponse,
    CreateCompletionRequest,
    CreateCompletionResponse,
    FinishReason,
    ListModelsResponse,
    Model,
    ObjectType,
)
from .transformers_utils.tokenizer import get_tokenizer

# TODO: Remove
SUPPORTED_BACKENDS: set = {"vllm", "tensorrtllm"}
KNOWN_MODELS = {"gpt2": "hf:gpt2"}

# TODO: What is this for?
OWNED_BY = "ACME"
TIMEOUT_KEEP_ALIVE = 5  # seconds

server: tritonserver.Server
model: tritonserver.Model
model_source_name: str
model_create_time: int
backend: str
tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
create_inference_request = None


# TODO: Re-organize helpers
def load_model(server):
    model = None
    backends = []
    tokenizer = None
    model_source_name = None
    for model_name, version in server.models().keys():
        if version != -1:
            continue
        current_model = server.load(model_name)
        backends.append(current_model.config()["backend"])
        if model_name in KNOWN_MODELS.keys():
            model = current_model
            model_source_name = KNOWN_MODELS[model_name].replace("hf:", "")
            tokenizer = get_tokenizer(model_source_name)
    if model and tokenizer:
        for backend in backends:
            if backend in SUPPORTED_BACKENDS:
                return model, int(time.time()), backend, tokenizer, model_source_name
    return None, None, None, None, None


def init_tritonserver():
    # TODO: How to pass arguments to server here?
    model_repository = os.environ.get(
        "TRITON_MODEL_REPOSITORY", "/opt/tritonserver/models"
    )

    print("Starting Triton Server Core...")
    server = tritonserver.Server(
        model_repository=model_repository,
        log_verbose=1,
        log_info=True,
        log_warn=True,
        log_error=True,
        model_control_mode=tritonserver.ModelControlMode.EXPLICIT,
    ).start(wait_until_ready=True)

    # TODO: Cleanup
    # print("Loading Model...\n\n")

    # model, model_create_time, backend, tokenizer, model_source_name = load_model(server)

    # if not (model and backend and tokenizer and model_create_time):
    #    raise Exception("Unknown Model")

    # print(f"\n\nModel: {model.name} Loaded with Backend: {backend}\n\n")

    # if backend == "vllm":
    #    create_inference_request = create_vllm_inference_request
    # elif backend == "tensorrtllm":
    #    create_inference_request = create_trtllm_inference_request

    return server


@asynccontextmanager
async def lifespan(app: FastAPI):
    print("Starting FastAPI app lifespan...")
    # Start the tritonserver on FastAPI app startup
    try:
        print("Starting Triton Inference Server...")
        app.server = init_tritonserver()
    except Exception as e:
        print(f"Failed to start Triton Inference Server: {e}")
        app.server = None

    yield

    # Cleanup the tritonserver on FastAPI app shutdown
    print("Shutting down FastAPI app lifespan...")
    if app.server:
        print("Shutting down Triton Inference Server...")
        app.server.stop()


app = FastAPI(
    title="OpenAI API",
    description="The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.",
    version="2.0.0",
    termsOfService="https://openai.com/policies/terms-of-use",
    contact={"name": "OpenAI Support", "url": "https://help.openai.com/"},
    license={
        "name": "MIT",
        "url": "https://github.com/openai/openai-openapi/blob/master/LICENSE",
    },
    servers=[{"url": "https://api.openai.com/v1"}],
    lifespan=lifespan,
)


# TODO: use router?
@app.get("/health")
def health() -> Response:
    if not app.server:
        raise HTTPException(
            status_code=400,
            detail="Triton Inference Server failed to start successfully.",
        )

    if not app.server.live():
        raise HTTPException(
            status_code=400, detail="Triton Inference Server is not live."
        )

    return Response(status_code=200)


def get_output(response):
    if "text_output" in response.outputs:
        try:
            return response.outputs["text_output"].to_string_array()[0]
        except:
            return str(response.outputs["text_output"].to_bytes_array()[0])
    return None


def streaming_chat_completion_response(request_id, created, model, role, responses):
    # first chunk

    choice = ChatCompletionStreamingResponseChoice(
        index=0,
        delta=ChatCompletionStreamResponseDelta(
            role=role, content=None, function_call=None
        ),
        logprobs=None,
        finish_reason=None,
    )
    chunk = CreateChatCompletionStreamResponse(
        id=request_id,
        choices=[choice],
        created=created,
        model=model,
        system_fingerprint=None,
        object=ObjectType.chat_completion_chunk,
    )
    yield f"data: {chunk.json(exclude_unset=True)}\n\n"

    for response in responses:
        text = get_output(response)

        choice = ChatCompletionStreamingResponseChoice(
            index=0,
            delta=ChatCompletionStreamResponseDelta(
                role=None, content=text, function_call=None
            ),
            logprobs=None,
            finish_reason=ChatCompletionFinishReason.stop if response.final else None,
        )

        chunk = CreateChatCompletionStreamResponse(
            id=request_id,
            choices=[choice],
            created=created,
            model=model,
            system_fingerprint=None,
            object=ObjectType.chat_completion_chunk,
        )

        yield f"data: {chunk.json(exclude_unset=True)}\n\n"

    yield "data: [DONE]\n\n"


def create_vllm_inference_request(
    model, prompt, request: CreateChatCompletionRequest | CreateCompletionRequest
):
    inputs = {}
    sampling_parameters = request.copy(
        exclude={"model", "stream", "messages", "prompt", "echo"},
    ).model_dump(exclude_none=True)
    inputs["text_input"] = [prompt]
    inputs["stream"] = [request.stream]
    exclude_input_in_output = True
    echo = getattr(request, "echo", None)
    if echo:
        exclude_input_in_output = not echo
    inputs["exclude_input_in_output"] = [exclude_input_in_output]
    return model.create_request(inputs=inputs, parameters=sampling_parameters)


def create_trtllm_inference_request(
    model, prompt, request: CreateChatCompletionRequest | CreateCompletionRequest
):
    inputs = {}
    if model.name == "llama-3-8b-instruct":
        inputs["stop_words"] = [["<|eot_id|>", "<|end_of_text|>"]]
    inputs["text_input"] = [[prompt]]
    inputs["stream"] = [[request.stream]]
    if request.max_tokens:
        inputs["max_tokens"] = numpy.int32([[request.max_tokens]])
    if request.stop:
        if isinstance(request.stop, str):
            request.stop = [request.stop]
        inputs["stop_words"] = [request.stop]
    if request.top_p:
        inputs["top_p"] = numpy.float32([[request.top_p]])
    if request.frequency_penalty:
        inputs["frequency_penalty"] = numpy.float32([[request.frequency_penalty]])
    if request.presence_penalty:
        inputs["presence_penalty":] = numpy.int32([[request.presence_penalty]])
    if request.seed:
        inputs["random_seed"] = numpy.uint64([[request.seed]])
    if request.temperature:
        inputs["temperature"] = numpy.float32([[request.temperature]])

    return model.create_request(inputs=inputs)


@app.post(
    "/v1/chat/completions", response_model=CreateChatCompletionResponse, tags=["Chat"]
)
def create_chat_completion(
    request: CreateChatCompletionRequest,
) -> CreateChatCompletionResponse | StreamingResponse:
    """
    Creates a model response for the given chat conversation.
    """

    if not model or not tokenizer or not create_inference_request:
        raise Exception("Unknown Model")

    add_generation_prompt_default = True
    default_role = "assistant"

    if request.model != model.name and request.model != model_source_name:
        raise HTTPException(status_code=404, detail=f"Unknown model: {request.model}")

    if request.n and request.n > 1:
        raise HTTPException(status_code=400, detail=f"Only single choice is supported")

    conversation = [
        {"role": str(message.role), "content": str(message.content)}
        for message in request.messages
    ]

    prompt = tokenizer.apply_chat_template(
        conversation=conversation,
        tokenize=False,
        add_generation_prompt=add_generation_prompt_default,
    )

    request_id = f"cmpl-{uuid.uuid1()}"
    created = int(time.time())

    responses = model.infer(create_inference_request(model, prompt, request))

    if request.stream:
        return StreamingResponse(
            streaming_chat_completion_response(
                request_id, created, request.model, conversation[-1]["role"], responses
            )
        )

    response = list(responses)[0]

    text = get_output(response)

    return CreateChatCompletionResponse(
        id=request_id,
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatCompletionResponseMessage(
                    content=text, role=default_role, function_call=None
                ),
                logprobs=None,
                finish_reason=ChatCompletionFinishReason.stop,
            )
        ],
        created=created,
        model=request.model,
        system_fingerprint=None,
        object=ObjectType.chat_completion,
    )


def streaming_completion_response(request_id, created, model, responses):
    for response in responses:
        text = get_output(response)

        choice = Choice(
            finish_reason=FinishReason.stop if response.final else None,
            index=0,
            logprobs=None,
            text=text,
        )
        response = CreateCompletionResponse(
            id=request_id,
            choices=[choice],
            system_fingerprint=None,
            object=ObjectType.text_completion,
            created=created,
            model=model,
        )

        yield f"data: {response.json(exclude_unset=True)}\n\n"
    yield "data: [DONE]\n\n"


@app.post(
    "/v1/completions", response_model=CreateCompletionResponse, tags=["Completions"]
)
def create_completion(
    request: CreateCompletionRequest, raw_request: Request
) -> CreateCompletionResponse | StreamingResponse:
    """
    Creates a completion for the provided prompt and parameters.
    """

    if not model or not tokenizer or not create_inference_request:
        raise Exception("Unknown Model")

    if request.suffix is not None:
        raise HTTPException(status_code=400, detail="suffix is not currently supported")

    if request.model != model.name and request.model != model_source_name:
        raise HTTPException(status_code=404, detail=f"Unknown model: {request.model}")

    if request.prompt is None:
        request.prompt = "<|endoftext|>"

    # Currently only support single string as input
    if not isinstance(request.prompt, str):
        raise HTTPException(
            status_code=400, detail="only single string input is supported"
        )

    if request.logit_bias is not None or request.logprobs is not None:
        raise HTTPException(
            status_code=400, detail="logit bias and log probs not supported"
        )

    request_id = f"cmpl-{uuid.uuid1()}"
    created = int(time.time())

    responses = model.infer(create_inference_request(model, request.prompt, request))
    if request.stream:
        return StreamingResponse(
            streaming_completion_response(request_id, created, model.name, responses)
        )
    response = list(responses)[0]
    text = get_output(response)

    choice = Choice(
        finish_reason=FinishReason.stop if response.final else None,
        index=0,
        logprobs=None,
        text=text,
    )
    return CreateCompletionResponse(
        id=request_id,
        choices=[choice],
        system_fingerprint=None,
        object=ObjectType.text_completion,
        created=created,
        model=model.name,
    )


@app.get("/metrics")
def metrics() -> str:
    return server.metrics()


@app.get("/v1/models", response_model=ListModelsResponse, tags=["Models"])
def list_models() -> ListModelsResponse:
    """
    Lists the currently available models, and provides basic information about each one such as the owner and availability.
    """

    model_list = [
        Model(
            id=model.name,
            created=model_create_time,
            object=ObjectType.model,
            owned_by=OWNED_BY,
        ),
        Model(
            id=model_source_name,
            created=model_create_time,
            object=ObjectType.model,
            owned_by=OWNED_BY,
        ),
    ]

    return ListModelsResponse(object=ObjectType.list, data=model_list)


@app.get("/v1/models/{model_name}", response_model=Model, tags=["Models"])
def retrieve_model(model_name: str) -> Model:
    """
    Retrieves a model instance, providing basic information about the model such as the owner and permissioning.
    """

    if model_name == model.name:
        return Model(
            id=model.name,
            created=model_create_time,
            object=ObjectType.model,
            owned_by=OWNED_BY,
        )

    if model_name == model_source_name:
        return Model(
            id=model_source_name,
            created=model_create_time,
            object=ObjectType.model,
            owned_by=OWNED_BY,
        )

    raise HTTPException(status_code=404, detail=f"Unknown model: {model_name}")


def parse_args():
    parser = argparse.ArgumentParser(
        description="Triton OpenAI Compatible RESTful API server."
    )
    parser.add_argument("--host", type=str, default=None, help="host name")
    parser.add_argument("--port", type=int, default=8000, help="port number")
    parser.add_argument(
        "--uvicorn-log-level",
        type=str,
        default="info",
        choices=["debug", "info", "warning", "error", "critical", "trace"],
        help="log level for uvicorn",
    )
    parser.add_argument(
        "--response-role", type=str, default="assistant", help="The role name to return"
    )

    parser.add_argument(
        "--tritonserver-log-level",
        type=int,
        default=0,
        help="The tritonserver log level",
    )

    parser.add_argument(
        "--model-repository",
        type=str,
        default="/workspace/llm-models",
        help="model repository",
    )
    return parser.parse_args()


if __name__ == "__main__":
    # TODO: Cleanup
    args = parse_args()

    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level=args.uvicorn_log_level,
        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,
    )
